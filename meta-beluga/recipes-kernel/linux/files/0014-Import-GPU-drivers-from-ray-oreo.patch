From a0925b32f5605dbda451fcb9bdb08fe52c6cc94f Mon Sep 17 00:00:00 2001
From: MagneFire <dgriet@gmail.com>
Date: Sat, 26 Mar 2022 23:42:30 +0100
Subject: [PATCH] Import GPU drivers from ray oreo.

---
 drivers/gpu/msm/Kconfig                |   30 +-
 drivers/gpu/msm/Makefile               |   15 +-
 drivers/gpu/msm/a3xx_reg.h             |  316 +-
 drivers/gpu/msm/a4xx_reg.h             |   83 +-
 drivers/gpu/msm/a5xx_reg.h             |   39 +-
 drivers/gpu/msm/a6xx_reg.h             | 1074 -------
 drivers/gpu/msm/adreno-gpulist.h       |  173 +-
 drivers/gpu/msm/adreno.c               | 1727 +++--------
 drivers/gpu/msm/adreno.h               |  849 +----
 drivers/gpu/msm/adreno_a3xx.c          |  221 +-
 drivers/gpu/msm/adreno_a3xx.h          |   30 +-
 drivers/gpu/msm/adreno_a3xx_snapshot.c |    3 +-
 drivers/gpu/msm/adreno_a4xx.c          |  920 ++++--
 drivers/gpu/msm/adreno_a4xx.h          |   44 +-
 drivers/gpu/msm/adreno_a4xx_preempt.c  |  573 ----
 drivers/gpu/msm/adreno_a4xx_snapshot.c |   19 +-
 drivers/gpu/msm/adreno_a5xx.c          | 1730 +++++++----
 drivers/gpu/msm/adreno_a5xx.h          |  112 +-
 drivers/gpu/msm/adreno_a5xx_preempt.c  |  630 ----
 drivers/gpu/msm/adreno_a5xx_snapshot.c |  598 ++--
 drivers/gpu/msm/adreno_a6xx.c          | 3952 ------------------------
 drivers/gpu/msm/adreno_a6xx.h          |  136 -
 drivers/gpu/msm/adreno_a6xx_preempt.c  |  823 -----
 drivers/gpu/msm/adreno_a6xx_snapshot.c | 1955 ------------
 drivers/gpu/msm/adreno_compat.c        |   81 +-
 drivers/gpu/msm/adreno_compat.h        |    8 +-
 drivers/gpu/msm/adreno_coresight.c     |  567 ++--
 drivers/gpu/msm/adreno_cp_parser.c     |   59 +-
 drivers/gpu/msm/adreno_cp_parser.h     |    5 +-
 drivers/gpu/msm/adreno_debugfs.c       |   99 +-
 drivers/gpu/msm/adreno_dispatch.c      | 2163 ++++++-------
 drivers/gpu/msm/adreno_dispatch.h      |   84 +-
 drivers/gpu/msm/adreno_drawctxt.c      |  202 +-
 drivers/gpu/msm/adreno_drawctxt.h      |   45 +-
 drivers/gpu/msm/adreno_ioctl.c         |   94 +-
 drivers/gpu/msm/adreno_iommu.c         |  204 +-
 drivers/gpu/msm/adreno_iommu.h         |    6 +-
 drivers/gpu/msm/adreno_llc.h           |   98 -
 drivers/gpu/msm/adreno_perfcounter.c   |  252 +-
 drivers/gpu/msm/adreno_perfcounter.h   |    9 +-
 drivers/gpu/msm/adreno_pm4types.h      |   91 +-
 drivers/gpu/msm/adreno_profile.c       |   20 +-
 drivers/gpu/msm/adreno_ringbuffer.c    | 1005 +++---
 drivers/gpu/msm/adreno_ringbuffer.h    |   69 +-
 drivers/gpu/msm/adreno_snapshot.c      |  425 ++-
 drivers/gpu/msm/adreno_sysfs.c         |  195 +-
 drivers/gpu/msm/adreno_trace.h         |  368 ++-
 drivers/gpu/msm/kgsl.c                 | 1818 +++--------
 drivers/gpu/msm/kgsl.h                 |  159 +-
 drivers/gpu/msm/kgsl_cffdump.c         |  740 +++++
 drivers/gpu/msm/kgsl_cffdump.h         |  183 ++
 drivers/gpu/msm/kgsl_cmdbatch.c        |  985 ++++++
 drivers/gpu/msm/kgsl_cmdbatch.h        |  172 ++
 drivers/gpu/msm/kgsl_compat.c          |   32 +-
 drivers/gpu/msm/kgsl_compat.h          |   24 +-
 drivers/gpu/msm/kgsl_debugfs.c         |  218 +-
 drivers/gpu/msm/kgsl_debugfs.h         |    6 +-
 drivers/gpu/msm/kgsl_device.h          |  323 +-
 drivers/gpu/msm/kgsl_drawobj.c         | 1154 -------
 drivers/gpu/msm/kgsl_drawobj.h         |  236 --
 drivers/gpu/msm/kgsl_events.c          |   47 +-
 drivers/gpu/msm/kgsl_gmu.c             | 1695 ----------
 drivers/gpu/msm/kgsl_gmu.h             |  270 --
 drivers/gpu/msm/kgsl_hfi.c             |  650 ----
 drivers/gpu/msm/kgsl_hfi.h             |  364 ---
 drivers/gpu/msm/kgsl_ioctl.c           |   31 +-
 drivers/gpu/msm/kgsl_iommu.c           | 1064 ++-----
 drivers/gpu/msm/kgsl_iommu.h           |   42 +-
 drivers/gpu/msm/kgsl_log.h             |   47 +-
 drivers/gpu/msm/kgsl_mmu.c             |  185 +-
 drivers/gpu/msm/kgsl_mmu.h             |   68 +-
 drivers/gpu/msm/kgsl_pool.c            |  579 ----
 drivers/gpu/msm/kgsl_pool.h            |   45 -
 drivers/gpu/msm/kgsl_pwrctrl.c         | 1505 +++------
 drivers/gpu/msm/kgsl_pwrctrl.h         |   63 +-
 drivers/gpu/msm/kgsl_pwrscale.c        |  361 +--
 drivers/gpu/msm/kgsl_pwrscale.h        |   23 +-
 drivers/gpu/msm/kgsl_sharedmem.c       |  422 ++-
 drivers/gpu/msm/kgsl_sharedmem.h       |  102 +-
 drivers/gpu/msm/kgsl_snapshot.c        |  401 +--
 drivers/gpu/msm/kgsl_snapshot.h        |   40 +-
 drivers/gpu/msm/kgsl_sync.c            |  700 ++---
 drivers/gpu/msm/kgsl_sync.h            |  147 +-
 drivers/gpu/msm/kgsl_trace.h           |  251 +-
 drivers/staging/android/Kconfig        |   27 +
 drivers/staging/android/Makefile       |    3 +
 drivers/staging/android/oneshot_sync.c |  434 +++
 drivers/staging/android/sw_sync.c      |  266 ++
 drivers/staging/android/sync.c         | 1028 ++++++
 drivers/staging/android/trace/sync.h   |   82 +
 include/linux/oneshot_sync.h           |   58 +
 include/uapi/linux/oneshot_sync.h      |   49 +
 92 files changed, 11831 insertions(+), 27469 deletions(-)
 delete mode 100644 drivers/gpu/msm/a6xx_reg.h
 delete mode 100644 drivers/gpu/msm/adreno_a4xx_preempt.c
 delete mode 100644 drivers/gpu/msm/adreno_a5xx_preempt.c
 delete mode 100644 drivers/gpu/msm/adreno_a6xx.c
 delete mode 100644 drivers/gpu/msm/adreno_a6xx.h
 delete mode 100644 drivers/gpu/msm/adreno_a6xx_preempt.c
 delete mode 100644 drivers/gpu/msm/adreno_a6xx_snapshot.c
 delete mode 100644 drivers/gpu/msm/adreno_llc.h
 mode change 100755 => 100644 drivers/gpu/msm/kgsl.c
 create mode 100644 drivers/gpu/msm/kgsl_cffdump.c
 create mode 100644 drivers/gpu/msm/kgsl_cffdump.h
 create mode 100644 drivers/gpu/msm/kgsl_cmdbatch.c
 create mode 100644 drivers/gpu/msm/kgsl_cmdbatch.h
 delete mode 100644 drivers/gpu/msm/kgsl_drawobj.c
 delete mode 100644 drivers/gpu/msm/kgsl_drawobj.h
 delete mode 100644 drivers/gpu/msm/kgsl_gmu.c
 delete mode 100644 drivers/gpu/msm/kgsl_gmu.h
 delete mode 100644 drivers/gpu/msm/kgsl_hfi.c
 delete mode 100644 drivers/gpu/msm/kgsl_hfi.h
 delete mode 100644 drivers/gpu/msm/kgsl_pool.c
 delete mode 100644 drivers/gpu/msm/kgsl_pool.h
 create mode 100644 drivers/staging/android/oneshot_sync.c
 create mode 100644 drivers/staging/android/sw_sync.c
 create mode 100644 drivers/staging/android/sync.c
 create mode 100644 drivers/staging/android/trace/sync.h
 create mode 100644 include/linux/oneshot_sync.h
 create mode 100644 include/uapi/linux/oneshot_sync.h

diff --git a/drivers/gpu/msm/Kconfig b/drivers/gpu/msm/Kconfig
index b65ed83f093a..cdd196750365 100644
--- a/drivers/gpu/msm/Kconfig
+++ b/drivers/gpu/msm/Kconfig
@@ -1,5 +1,5 @@
 config QCOM_KGSL
-	tristate "Qualcomm Technologies, Inc. 3D Graphics driver"
+	tristate "MSM 3D Graphics driver"
 	default n
 	depends on ARCH_QCOM
 	select GENERIC_ALLOCATOR
@@ -9,11 +9,31 @@ config QCOM_KGSL
 	select DEVFREQ_GOV_PERFORMANCE
 	select DEVFREQ_GOV_QCOM_ADRENO_TZ
 	select DEVFREQ_GOV_QCOM_GPUBW_MON
+	select ONESHOT_SYNC if SYNC
 	---help---
-	  3D graphics driver for the Adreno family of GPUs from QTI.
-	  Required to use hardware accelerated OpenGL, compute and Vulkan
-	  on QTI targets. This includes power management, memory management,
-	  and scheduling for the Adreno GPUs.
+	  3D graphics driver. Required to use hardware accelerated
+	  OpenGL ES 2.0 and 1.1.
+
+config MSM_KGSL_CFF_DUMP
+	bool "Enable KGSL Common File Format (CFF) Dump Feature [Use with caution]"
+	default n
+	depends on QCOM_KGSL
+	select RELAY
+	---help---
+	  This is an analysis and diagnostic feature only, and should only be
+	  turned on during KGSL GPU diagnostics and will slow down the KGSL
+	  performance sigificantly, hence *do not use in production builds*.
+	  When enabled, CFF Dump is on at boot. It can be turned off at runtime
+	  via 'echo 0 > /d/kgsl/cff_dump'.  The log can be captured via
+	  /d/kgsl-cff/cpu[0|1].
+
+config MSM_KGSL_CFF_DUMP_NO_CONTEXT_MEM_DUMP
+	bool "When selected will disable KGSL CFF Dump for context switches"
+	default n
+	depends on MSM_KGSL_CFF_DUMP
+	---help---
+	  Dumping all the memory for every context switch can produce quite
+	  huge log files, to reduce this, turn this feature on.
 
 config QCOM_ADRENO_DEFAULT_GOVERNOR
 	string "devfreq governor for the adreno core"
diff --git a/drivers/gpu/msm/Makefile b/drivers/gpu/msm/Makefile
index 0e48fcf040bb..3da1504e68b0 100644
--- a/drivers/gpu/msm/Makefile
+++ b/drivers/gpu/msm/Makefile
@@ -1,22 +1,20 @@
-ccflags-y := -Iinclude/linux -Iinclude/uapi/drm -Iinclude/drm -Idrivers/gpu/msm
+ccflags-y := -Iinclude/uapi/drm -Iinclude/drm -Idrivers/gpu/msm
 
 msm_kgsl_core-y = \
 	kgsl.o \
 	kgsl_trace.o \
-	kgsl_drawobj.o \
+	kgsl_cmdbatch.o \
 	kgsl_ioctl.o \
 	kgsl_sharedmem.o \
 	kgsl_pwrctrl.o \
 	kgsl_pwrscale.o \
 	kgsl_mmu.o \
 	kgsl_snapshot.o \
-	kgsl_events.o \
-	kgsl_pool.o \
-	kgsl_gmu.o \
-	kgsl_hfi.o
+	kgsl_events.o
 
 msm_kgsl_core-$(CONFIG_QCOM_KGSL_IOMMU) += kgsl_iommu.o
 msm_kgsl_core-$(CONFIG_DEBUG_FS) += kgsl_debugfs.o
+msm_kgsl_core-$(CONFIG_MSM_KGSL_CFF_DUMP) += kgsl_cffdump.o
 msm_kgsl_core-$(CONFIG_SYNC_FILE) += kgsl_sync.o
 msm_kgsl_core-$(CONFIG_COMPAT) += kgsl_compat.o
 
@@ -31,14 +29,9 @@ msm_adreno-y += \
 	adreno_a3xx.o \
 	adreno_a4xx.o \
 	adreno_a5xx.o \
-	adreno_a6xx.o \
 	adreno_a3xx_snapshot.o \
 	adreno_a4xx_snapshot.o \
 	adreno_a5xx_snapshot.o \
-	adreno_a6xx_snapshot.o \
-	adreno_a4xx_preempt.o \
-	adreno_a5xx_preempt.o \
-	adreno_a6xx_preempt.o \
 	adreno_sysfs.o \
 	adreno.o \
 	adreno_cp_parser.o \
diff --git a/drivers/gpu/msm/a3xx_reg.h b/drivers/gpu/msm/a3xx_reg.h
index fc17470d6aac..d8f94757ba28 100644
--- a/drivers/gpu/msm/a3xx_reg.h
+++ b/drivers/gpu/msm/a3xx_reg.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -44,20 +44,35 @@
 /* CP_EVENT_WRITE events */
 #define CACHE_FLUSH_TS 4
 
+/* CP_INTERRUPT masks */
+
+#define CP_INTERRUPT_IB2 0x20000000
+#define CP_INTERRUPT_IB1 0x40000000
+#define CP_INTERRUPT_RB  0x80000000
+
 /* Register definitions */
 
+#define A3XX_RBBM_HW_VERSION 0x000
+#define A3XX_RBBM_HW_RELEASE 0x001
+#define A3XX_RBBM_HW_CONFIGURATION 0x002
 #define A3XX_RBBM_CLOCK_CTL 0x010
 #define A3XX_RBBM_SP_HYST_CNT 0x012
 #define A3XX_RBBM_SW_RESET_CMD 0x018
 #define A3XX_RBBM_AHB_CTL0 0x020
 #define A3XX_RBBM_AHB_CTL1 0x021
 #define A3XX_RBBM_AHB_CMD 0x022
+#define A3XX_RBBM_AHB_ME_SPLIT_STATUS 0x25
+#define A3XX_RBBM_AHB_PFP_SPLIT_STATUS 0x26
 #define A3XX_RBBM_AHB_ERROR_STATUS 0x027
 #define A3XX_RBBM_GPR0_CTL 0x02E
 /* This the same register as on A2XX, just in a different place */
 #define A3XX_RBBM_STATUS 0x030
 #define A3XX_RBBM_WAIT_IDLE_CLOCKS_CTL 0x33
 #define A3XX_RBBM_INTERFACE_HANG_INT_CTL 0x50
+#define A3XX_RBBM_INTERFACE_HANG_MASK_CTL0 0x51
+#define A3XX_RBBM_INTERFACE_HANG_MASK_CTL1 0x54
+#define A3XX_RBBM_INTERFACE_HANG_MASK_CTL2 0x57
+#define A3XX_RBBM_INTERFACE_HANG_MASK_CTL3 0x5A
 #define A3XX_RBBM_INT_CLEAR_CMD 0x061
 #define A3XX_RBBM_INT_0_MASK 0x063
 #define A3XX_RBBM_INT_0_STATUS 0x064
@@ -173,8 +188,10 @@
 #define A3XX_RBBM_EXT_TRACE_CMD 0x122
 #define A3XX_CP_RB_BASE 0x01C0
 #define A3XX_CP_RB_CNTL 0x01C1
+#define A3XX_CP_RB_RPTR_ADDR 0x01C3
 #define A3XX_CP_RB_RPTR 0x01C4
 #define A3XX_CP_RB_WPTR 0x01C5
+#define A3XX_CP_RB_RPTR_WR 0x01C7
 /* Following two are same as on A2XX, just in a different place */
 #define A3XX_CP_PFP_UCODE_ADDR 0x1C9
 #define A3XX_CP_PFP_UCODE_DATA 0x1CA
@@ -186,6 +203,8 @@
 #define A3XX_CP_QUEUE_THRESHOLDS 0x01D5
 #define A3XX_CP_MEQ_ADDR 0x1DA
 #define A3XX_CP_MEQ_DATA 0x1DB
+#define A3XX_CP_SCRATCH_UMSK 0x01DC
+#define A3XX_CP_SCRATCH_ADDR 0x01DD
 #define A3XX_CP_STATE_DEBUG_INDEX 0x01EC
 #define A3XX_CP_STATE_DEBUG_DATA 0x01ED
 #define A3XX_CP_CNTL 0x01F4
@@ -206,28 +225,53 @@
 #define A3XX_CP_IB2_BUFSZ 0x045B
 
 #define A3XX_CP_HW_FAULT  0x45C
+#define A3XX_CP_AHB_FAULT 0x54D
 #define A3XX_CP_PROTECT_CTRL 0x45E
 #define A3XX_CP_PROTECT_STATUS 0x45F
 #define A3XX_CP_PROTECT_REG_0 0x460
+#define A3XX_CP_PROTECT_REG_1 0x461
+#define A3XX_CP_PROTECT_REG_2 0x462
+#define A3XX_CP_PROTECT_REG_3 0x463
+#define A3XX_CP_PROTECT_REG_4 0x464
+#define A3XX_CP_PROTECT_REG_5 0x465
+#define A3XX_CP_PROTECT_REG_6 0x466
+#define A3XX_CP_PROTECT_REG_7 0x467
+#define A3XX_CP_PROTECT_REG_8 0x468
+#define A3XX_CP_PROTECT_REG_9 0x469
+#define A3XX_CP_PROTECT_REG_A 0x46A
+#define A3XX_CP_PROTECT_REG_B 0x46B
+#define A3XX_CP_PROTECT_REG_C 0x46C
+#define A3XX_CP_PROTECT_REG_D 0x46D
+#define A3XX_CP_PROTECT_REG_E 0x46E
+#define A3XX_CP_PROTECT_REG_F 0x46F
 #define A3XX_CP_STAT 0x047F
 #define A3XX_CP_SCRATCH_REG0 0x578
 #define A3XX_CP_SCRATCH_REG6 0x57E
 #define A3XX_CP_SCRATCH_REG7 0x57F
+#define A3XX_VSC_BIN_SIZE 0xC01
 #define A3XX_VSC_SIZE_ADDRESS 0xC02
+#define A3XX_VSC_PIPE_CONFIG_0 0xC06
 #define A3XX_VSC_PIPE_DATA_ADDRESS_0 0xC07
 #define A3XX_VSC_PIPE_DATA_LENGTH_0 0xC08
+#define A3XX_VSC_PIPE_CONFIG_1 0xC09
 #define A3XX_VSC_PIPE_DATA_ADDRESS_1 0xC0A
 #define A3XX_VSC_PIPE_DATA_LENGTH_1 0xC0B
+#define A3XX_VSC_PIPE_CONFIG_2 0xC0C
 #define A3XX_VSC_PIPE_DATA_ADDRESS_2 0xC0D
 #define A3XX_VSC_PIPE_DATA_LENGTH_2 0xC0E
+#define A3XX_VSC_PIPE_CONFIG_3 0xC0F
 #define A3XX_VSC_PIPE_DATA_ADDRESS_3 0xC10
 #define A3XX_VSC_PIPE_DATA_LENGTH_3 0xC11
+#define A3XX_VSC_PIPE_CONFIG_4 0xC12
 #define A3XX_VSC_PIPE_DATA_ADDRESS_4 0xC13
 #define A3XX_VSC_PIPE_DATA_LENGTH_4 0xC14
+#define A3XX_VSC_PIPE_CONFIG_5 0xC15
 #define A3XX_VSC_PIPE_DATA_ADDRESS_5 0xC16
 #define A3XX_VSC_PIPE_DATA_LENGTH_5 0xC17
+#define A3XX_VSC_PIPE_CONFIG_6 0xC18
 #define A3XX_VSC_PIPE_DATA_ADDRESS_6 0xC19
 #define A3XX_VSC_PIPE_DATA_LENGTH_6 0xC1A
+#define A3XX_VSC_PIPE_CONFIG_7 0xC1B
 #define A3XX_VSC_PIPE_DATA_ADDRESS_7 0xC1C
 #define A3XX_VSC_PIPE_DATA_LENGTH_7 0xC1D
 #define A3XX_PC_PERFCOUNTER0_SELECT 0xC48
@@ -269,7 +313,7 @@
 #define A3XX_RB_PERFCOUNTER1_SELECT   0xCC7
 #define A3XX_RB_FRAME_BUFFER_DIMENSION 0xCE0
 #define A3XX_SQ_GPR_MANAGEMENT 0x0D00
-#define A3XX_SQ_INST_STORE_MANAGEMENT 0x0D02
+#define A3XX_SQ_INST_STORE_MANAGMENT 0x0D02
 #define A3XX_HLSQ_PERFCOUNTER0_SELECT 0xE00
 #define A3XX_HLSQ_PERFCOUNTER1_SELECT 0xE01
 #define A3XX_HLSQ_PERFCOUNTER2_SELECT 0xE02
@@ -373,6 +417,10 @@
 #define A3XX_RB_SAMPLE_COUNT_ADDR 0x2111
 #define A3XX_RB_Z_CLAMP_MIN 0x2114
 #define A3XX_RB_Z_CLAMP_MAX 0x2115
+#define A3XX_PC_VSTREAM_CONTROL 0x21E4
+#define A3XX_PC_VERTEX_REUSE_BLOCK_CNTL 0x21EA
+#define A3XX_PC_PRIM_VTX_CNTL 0x21EC
+#define A3XX_PC_RESTART_INDEX 0x21ED
 #define A3XX_HLSQ_CONTROL_0_REG 0x2200
 #define A3XX_HLSQ_CONTROL_1_REG 0x2201
 #define A3XX_HLSQ_CONTROL_2_REG 0x2202
@@ -395,6 +443,11 @@
 #define A3XX_HLSQ_CL_KERNEL_GROUP_Y_REG 0x2216
 #define A3XX_HLSQ_CL_KERNEL_GROUP_Z_REG 0x2217
 #define A3XX_HLSQ_CL_WG_OFFSET_REG 0x221A
+#define A3XX_VFD_CONTROL_0 0x2240
+#define A3XX_VFD_INDEX_MIN 0x2242
+#define A3XX_VFD_INDEX_MAX 0x2243
+#define A3XX_VFD_FETCH_INSTR_0_0 0x2246
+#define A3XX_VFD_FETCH_INSTR_0_4 0x224E
 #define A3XX_VFD_FETCH_INSTR_1_0 0x2247
 #define A3XX_VFD_FETCH_INSTR_1_1 0x2249
 #define A3XX_VFD_FETCH_INSTR_1_2 0x224B
@@ -411,6 +464,10 @@
 #define A3XX_VFD_FETCH_INSTR_1_D 0x2261
 #define A3XX_VFD_FETCH_INSTR_1_E 0x2263
 #define A3XX_VFD_FETCH_INSTR_1_F 0x2265
+#define A3XX_VFD_DECODE_INSTR_0 0x2266
+#define A3XX_VFD_VS_THREADING_THRESHOLD 0x227E
+#define A3XX_VPC_ATTR 0x2280
+#define A3XX_VPC_VARY_CYLWRAP_ENABLE_1 0x228B
 #define A3XX_SP_SP_CTRL_REG 0x22C0
 #define A3XX_SP_VS_CTRL_REG0 0x22C4
 #define A3XX_SP_VS_CTRL_REG1 0x22C5
@@ -453,7 +510,13 @@
 #define A3XX_SP_FS_IMAGE_OUTPUT_REG_3 0x22F7
 #define A3XX_SP_FS_LENGTH_REG 0x22FF
 #define A3XX_PA_SC_AA_CONFIG 0x2301
+#define A3XX_TPL1_TP_VS_TEX_OFFSET 0x2340
+#define A3XX_TPL1_TP_FS_TEX_OFFSET 0x2342
+#define A3XX_TPL1_TP_FS_BORDER_COLOR_BASE_ADDR 0x2343
 #define A3XX_VBIF_CLKON 0x3001
+#define A3XX_VBIF_FIXED_SORT_EN 0x300C
+#define A3XX_VBIF_FIXED_SORT_SEL0 0x300D
+#define A3XX_VBIF_FIXED_SORT_SEL1 0x300E
 #define A3XX_VBIF_ABIT_SORT 0x301C
 #define A3XX_VBIF_ABIT_SORT_CONF 0x301D
 #define A3XX_VBIF_GATE_OFF_WRREQ_EN 0x302A
@@ -466,8 +529,12 @@
 #define A3XX_VBIF_DDR_OUT_MAX_BURST 0x3036
 #define A3XX_VBIF_ARB_CTL 0x303C
 #define A3XX_VBIF_ROUND_ROBIN_QOS_ARB 0x3049
+#define A3XX_VBIF_OUT_AXI_AMEMTYPE_CONF0 0x3058
 #define A3XX_VBIF_OUT_AXI_AOOO_EN 0x305E
 #define A3XX_VBIF_OUT_AXI_AOOO 0x305F
+#define A3XX_VBIF_PERF_CNT_EN 0x3070
+#define A3XX_VBIF_PERF_CNT_CLR 0x3071
+#define A3XX_VBIF_PERF_CNT_SEL 0x3072
 #define A3XX_VBIF_PERF_CNT0_LO 0x3073
 #define A3XX_VBIF_PERF_CNT0_HI 0x3074
 #define A3XX_VBIF_PERF_CNT1_LO 0x3075
@@ -486,6 +553,14 @@
 #define A3XX_VBIF_XIN_HALT_CTRL1 0x3081
 
 /* VBIF register offsets for A306 */
+#define A3XX_VBIF2_PERF_CNT_EN0 0x30c0
+#define A3XX_VBIF2_PERF_CNT_EN1 0x30c1
+#define A3XX_VBIF2_PERF_CNT_EN2 0x30c2
+#define A3XX_VBIF2_PERF_CNT_EN3 0x30c3
+#define A3XX_VBIF2_PERF_CNT_CLR0 0x30c8
+#define A3XX_VBIF2_PERF_CNT_CLR1 0x30c9
+#define A3XX_VBIF2_PERF_CNT_CLR2 0x30ca
+#define A3XX_VBIF2_PERF_CNT_CLR3 0x30cb
 #define A3XX_VBIF2_PERF_CNT_SEL0 0x30d0
 #define A3XX_VBIF2_PERF_CNT_SEL1 0x30d1
 #define A3XX_VBIF2_PERF_CNT_SEL2 0x30d2
@@ -502,6 +577,9 @@
 #define A3XX_VBIF2_PERF_PWR_CNT_EN0 0x3100
 #define A3XX_VBIF2_PERF_PWR_CNT_EN1 0x3101
 #define A3XX_VBIF2_PERF_PWR_CNT_EN2 0x3102
+#define A3XX_VBIF2_PERF_PWR_CNT_CLR0 0x3108
+#define A3XX_VBIF2_PERF_PWR_CNT_CLR1 0x3109
+#define A3XX_VBIF2_PERF_PWR_CNT_CLR2 0x310A
 #define A3XX_VBIF2_PERF_PWR_CNT_LOW0 0x3110
 #define A3XX_VBIF2_PERF_PWR_CNT_LOW1 0x3111
 #define A3XX_VBIF2_PERF_PWR_CNT_LOW2 0x3112
@@ -512,7 +590,240 @@
 #define A3XX_VBIF_DDR_OUTPUT_RECOVERABLE_HALT_CTRL0 0x3800
 #define A3XX_VBIF_DDR_OUTPUT_RECOVERABLE_HALT_CTRL1 0x3801
 
+/* Various flags used by the context switch code */
+
+#define SP_MULTI 0
+#define SP_BUFFER_MODE 1
+#define SP_TWO_VTX_QUADS 0
+#define SP_PIXEL_BASED 0
+#define SP_R8G8B8A8_UNORM 8
+#define SP_FOUR_PIX_QUADS 1
+
+#define HLSQ_DIRECT 0
+#define HLSQ_BLOCK_ID_SP_VS 4
+#define HLSQ_SP_VS_INSTR 0
+#define HLSQ_SP_FS_INSTR 0
+#define HLSQ_BLOCK_ID_SP_FS 6
+#define HLSQ_TWO_PIX_QUADS 0
+#define HLSQ_TWO_VTX_QUADS 0
+#define HLSQ_BLOCK_ID_TP_TEX 2
+#define HLSQ_TP_TEX_SAMPLERS 0
+#define HLSQ_TP_TEX_MEMOBJ 1
+#define HLSQ_BLOCK_ID_TP_MIPMAP 3
+#define HLSQ_TP_MIPMAP_BASE 1
+#define HLSQ_FOUR_PIX_QUADS 1
+
+#define RB_FACTOR_ONE 1
+#define RB_BLEND_OP_ADD 0
+#define RB_FACTOR_ZERO 0
+#define RB_DITHER_DISABLE 0
+#define RB_DITHER_ALWAYS 1
+#define RB_FRAG_NEVER 0
+#define RB_ENDIAN_NONE 0
+#define RB_R8G8B8A8_UNORM 8
+#define RB_RESOLVE_PASS 2
+#define RB_CLEAR_MODE_RESOLVE 1
+#define RB_TILINGMODE_LINEAR 0
+#define RB_REF_NEVER 0
+#define RB_FRAG_LESS 1
+#define RB_REF_ALWAYS 7
+#define RB_STENCIL_KEEP 0
+#define RB_RENDERING_PASS 0
+#define RB_TILINGMODE_32X32 2
+
+#define PC_DRAW_TRIANGLES 2
+#define PC_DI_PT_RECTLIST 8
+#define PC_DI_SRC_SEL_AUTO_INDEX 2
+#define PC_DI_INDEX_SIZE_16_BIT 0
+#define PC_DI_IGNORE_VISIBILITY 0
+#define PC_DI_PT_TRILIST 4
+#define PC_DI_SRC_SEL_IMMEDIATE 1
+#define PC_DI_INDEX_SIZE_32_BIT 1
+
+#define UCHE_ENTIRE_CACHE 1
+#define UCHE_OP_INVALIDATE 1
+
+/*
+ * The following are bit field shifts within some of the registers defined
+ * above. These are used in the context switch code in conjunction with the
+ * _SET macro
+ */
+
+#define GRAS_CL_CLIP_CNTL_CLIP_DISABLE 16
+#define GRAS_CL_CLIP_CNTL_IJ_PERSP_CENTER 12
+#define GRAS_CL_CLIP_CNTL_PERSP_DIVISION_DISABLE 21
+#define GRAS_CL_CLIP_CNTL_VP_CLIP_CODE_IGNORE 19
+#define GRAS_CL_CLIP_CNTL_VP_XFORM_DISABLE 20
+#define GRAS_CL_CLIP_CNTL_ZFAR_CLIP_DISABLE 17
+#define GRAS_CL_VPORT_XSCALE_VPORT_XSCALE 0
+#define GRAS_CL_VPORT_YSCALE_VPORT_YSCALE 0
+#define GRAS_CL_VPORT_ZSCALE_VPORT_ZSCALE 0
+#define GRAS_SC_CONTROL_RASTER_MODE 12
+#define GRAS_SC_CONTROL_RENDER_MODE 4
+#define GRAS_SC_SCREEN_SCISSOR_BR_BR_X 0
+#define GRAS_SC_SCREEN_SCISSOR_BR_BR_Y 16
+#define GRAS_SC_WINDOW_SCISSOR_BR_BR_X 0
+#define GRAS_SC_WINDOW_SCISSOR_BR_BR_Y 16
+#define GRAS_SU_CTRLMODE_LINEHALFWIDTH 03
+#define HLSQ_CONSTFSPRESERVEDRANGEREG_ENDENTRY 16
+#define HLSQ_CONSTFSPRESERVEDRANGEREG_STARTENTRY 0
+#define HLSQ_CTRL0REG_CHUNKDISABLE 26
+#define HLSQ_CTRL0REG_CONSTSWITCHMODE 27
+#define HLSQ_CTRL0REG_FSSUPERTHREADENABLE 6
+#define HLSQ_CTRL0REG_FSTHREADSIZE 4
+#define HLSQ_CTRL0REG_LAZYUPDATEDISABLE 28
+#define HLSQ_CTRL0REG_RESERVED2 10
+#define HLSQ_CTRL0REG_SPCONSTFULLUPDATE 29
+#define HLSQ_CTRL0REG_SPSHADERRESTART 9
+#define HLSQ_CTRL0REG_TPFULLUPDATE 30
+#define HLSQ_CTRL1REG_RESERVED1 9
+#define HLSQ_CTRL1REG_VSSUPERTHREADENABLE 8
+#define HLSQ_CTRL1REG_VSTHREADSIZE 6
+#define HLSQ_CTRL2REG_PRIMALLOCTHRESHOLD 26
+#define HLSQ_FSCTRLREG_FSCONSTLENGTH 0
+#define HLSQ_FSCTRLREG_FSCONSTSTARTOFFSET 12
+#define HLSQ_FSCTRLREG_FSINSTRLENGTH 24
+#define HLSQ_VSCTRLREG_VSINSTRLENGTH 24
+#define PC_PRIM_VTX_CONTROL_POLYMODE_BACK_PTYPE 8
+#define PC_PRIM_VTX_CONTROL_POLYMODE_FRONT_PTYPE 5
+#define PC_PRIM_VTX_CONTROL_PROVOKING_VTX_LAST 25
+#define PC_PRIM_VTX_CONTROL_STRIDE_IN_VPC 0
+#define PC_DRAW_INITIATOR_PRIM_TYPE 0
+#define PC_DRAW_INITIATOR_SOURCE_SELECT 6
+#define PC_DRAW_INITIATOR_VISIBILITY_CULLING_MODE 9
+#define PC_DRAW_INITIATOR_INDEX_SIZE 0x0B
+#define PC_DRAW_INITIATOR_SMALL_INDEX 0x0D
+#define PC_DRAW_INITIATOR_PRE_DRAW_INITIATOR_ENABLE 0x0E
+#define RB_COPYCONTROL_COPY_GMEM_BASE 14
+#define RB_COPYCONTROL_RESOLVE_CLEAR_MODE 4
+#define RB_COPYDESTBASE_COPY_DEST_BASE 4
+#define RB_COPYDESTINFO_COPY_COMPONENT_ENABLE 14
+#define RB_COPYDESTINFO_COPY_DEST_ENDIAN 18
+#define RB_COPYDESTINFO_COPY_DEST_FORMAT 2
+#define RB_COPYDESTINFO_COPY_DEST_TILE 0
+#define RB_COPYDESTPITCH_COPY_DEST_PITCH 0
+#define RB_DEPTHCONTROL_Z_TEST_FUNC 4
+#define RB_MODECONTROL_RENDER_MODE 8
+#define RB_MODECONTROL_MARB_CACHE_SPLIT_MODE 15
+#define RB_MODECONTROL_PACKER_TIMER_ENABLE 16
+#define RB_MRTBLENDCONTROL_ALPHA_BLEND_OPCODE 21
+#define RB_MRTBLENDCONTROL_ALPHA_DEST_FACTOR 24
+#define RB_MRTBLENDCONTROL_ALPHA_SRC_FACTOR 16
+#define RB_MRTBLENDCONTROL_CLAMP_ENABLE 29
+#define RB_MRTBLENDCONTROL_RGB_BLEND_OPCODE 5
+#define RB_MRTBLENDCONTROL_RGB_DEST_FACTOR 8
+#define RB_MRTBLENDCONTROL_RGB_SRC_FACTOR 0
+#define RB_MRTBUFBASE_COLOR_BUF_BASE 4
+#define RB_MRTBUFINFO_COLOR_BUF_PITCH 17
+#define RB_MRTBUFINFO_COLOR_FORMAT 0
+#define RB_MRTBUFINFO_COLOR_TILE_MODE 6
+#define RB_MRTCONTROL_COMPONENT_ENABLE 24
+#define RB_MRTCONTROL_DITHER_MODE 12
+#define RB_MRTCONTROL_READ_DEST_ENABLE 3
+#define RB_MRTCONTROL_ROP_CODE 8
+#define RB_MSAACONTROL_MSAA_DISABLE 10
+#define RB_MSAACONTROL_SAMPLE_MASK 16
+#define RB_RENDERCONTROL_ALPHA_TEST_FUNC 24
+#define RB_RENDERCONTROL_BIN_WIDTH 4
+#define RB_RENDERCONTROL_DISABLE_COLOR_PIPE 12
+#define RB_STENCILCONTROL_STENCIL_FAIL 11
+#define RB_STENCILCONTROL_STENCIL_FAIL_BF 23
+#define RB_STENCILCONTROL_STENCIL_FUNC 8
+#define RB_STENCILCONTROL_STENCIL_FUNC_BF 20
+#define RB_STENCILCONTROL_STENCIL_ZFAIL 17
+#define RB_STENCILCONTROL_STENCIL_ZFAIL_BF 29
+#define RB_STENCILCONTROL_STENCIL_ZPASS 14
+#define RB_STENCILCONTROL_STENCIL_ZPASS_BF 26
+#define SP_FSCTRLREG0_FSFULLREGFOOTPRINT 10
+#define SP_FSCTRLREG0_FSHALFREGFOOTPRINT 4
+#define SP_FSCTRLREG0_FSICACHEINVALID 2
+#define SP_FSCTRLREG0_FSINOUTREGOVERLAP 18
+#define SP_FSCTRLREG0_FSINSTRBUFFERMODE 1
+#define SP_FSCTRLREG0_FSLENGTH 24
+#define SP_FSCTRLREG0_FSSUPERTHREADMODE 21
+#define SP_FSCTRLREG0_FSTHREADMODE 0
+#define SP_FSCTRLREG0_FSTHREADSIZE 20
+#define SP_FSCTRLREG0_PIXLODENABLE 22
+#define SP_FSCTRLREG1_FSCONSTLENGTH 0
+#define SP_FSCTRLREG1_FSINITIALOUTSTANDING 20
+#define SP_FSCTRLREG1_HALFPRECVAROFFSET 24
+#define SP_FSMRTREG_REGID 0
+#define SP_FSMRTREG_PRECISION 8
+#define SP_FSOUTREG_PAD0 2
+#define SP_IMAGEOUTPUTREG_MRTFORMAT 0
+#define SP_IMAGEOUTPUTREG_DEPTHOUTMODE 3
+#define SP_IMAGEOUTPUTREG_PAD0 6
+#define SP_OBJOFFSETREG_CONSTOBJECTSTARTOFFSET 16
+#define SP_OBJOFFSETREG_SHADEROBJOFFSETINIC 25
+#define SP_SHADERLENGTH_LEN 0
+#define SP_SPCTRLREG_CONSTMODE 18
+#define SP_SPCTRLREG_LOMODE 22
+#define SP_SPCTRLREG_SLEEPMODE 20
+#define SP_VSCTRLREG0_VSFULLREGFOOTPRINT 10
+#define SP_VSCTRLREG0_VSICACHEINVALID 2
+#define SP_VSCTRLREG0_VSINSTRBUFFERMODE 1
+#define SP_VSCTRLREG0_VSLENGTH 24
+#define SP_VSCTRLREG0_VSSUPERTHREADMODE 21
+#define SP_VSCTRLREG0_VSTHREADMODE 0
+#define SP_VSCTRLREG0_VSTHREADSIZE 20
+#define SP_VSCTRLREG1_VSINITIALOUTSTANDING 24
+#define SP_VSOUTREG_COMPMASK0 9
+#define SP_VSPARAMREG_POSREGID 0
+#define SP_VSPARAMREG_PSIZEREGID 8
+#define SP_VSPARAMREG_TOTALVSOUTVAR 20
+#define SP_VSVPCDSTREG_OUTLOC0 0
+#define TPL1_TPTEXOFFSETREG_BASETABLEPTR 16
+#define TPL1_TPTEXOFFSETREG_MEMOBJOFFSET 8
+#define TPL1_TPTEXOFFSETREG_SAMPLEROFFSET 0
+#define UCHE_INVALIDATE1REG_OPCODE 0x1C
+#define UCHE_INVALIDATE1REG_ALLORPORTION 0x1F
+#define VFD_BASEADDR_BASEADDR 0
+#define VFD_CTRLREG0_PACKETSIZE 18
+#define VFD_CTRLREG0_STRMDECINSTRCNT 22
+#define VFD_CTRLREG0_STRMFETCHINSTRCNT 27
+#define VFD_CTRLREG0_TOTALATTRTOVS 0
+#define VFD_CTRLREG1_MAXSTORAGE 0
+#define VFD_CTRLREG1_REGID4INST 24
+#define VFD_CTRLREG1_REGID4VTX 16
+#define VFD_DECODEINSTRUCTIONS_CONSTFILL 4
+#define VFD_DECODEINSTRUCTIONS_FORMAT 6
+#define VFD_DECODEINSTRUCTIONS_LASTCOMPVALID 29
+#define VFD_DECODEINSTRUCTIONS_REGID 12
+#define VFD_DECODEINSTRUCTIONS_SHIFTCNT 24
+#define VFD_DECODEINSTRUCTIONS_SWITCHNEXT 30
+#define VFD_DECODEINSTRUCTIONS_WRITEMASK 0
+#define VFD_FETCHINSTRUCTIONS_BUFSTRIDE 7
+#define VFD_FETCHINSTRUCTIONS_FETCHSIZE 0
+#define VFD_FETCHINSTRUCTIONS_INDEXDECODE 18
+#define VFD_FETCHINSTRUCTIONS_STEPRATE 24
+#define VFD_FETCHINSTRUCTIONS_SWITCHNEXT 17
+#define VFD_THREADINGTHRESHOLD_REGID_VTXCNT 8
+#define VFD_THREADINGTHRESHOLD_REGID_THRESHOLD 0
+#define VFD_THREADINGTHRESHOLD_RESERVED6 4
+#define VPC_VPCATTR_LMSIZE 28
+#define VPC_VPCATTR_THRHDASSIGN 12
+#define VPC_VPCATTR_TOTALATTR 0
+#define VPC_VPCPACK_NUMFPNONPOSVAR 8
+#define VPC_VPCPACK_NUMNONPOSVSVAR 16
+#define VPC_VPCVARPSREPLMODE_COMPONENT08 0
+#define VPC_VPCVARPSREPLMODE_COMPONENT09 2
+#define VPC_VPCVARPSREPLMODE_COMPONENT0A 4
+#define VPC_VPCVARPSREPLMODE_COMPONENT0B 6
+#define VPC_VPCVARPSREPLMODE_COMPONENT0C 8
+#define VPC_VPCVARPSREPLMODE_COMPONENT0D 10
+#define VPC_VPCVARPSREPLMODE_COMPONENT0E 12
+#define VPC_VPCVARPSREPLMODE_COMPONENT0F 14
+#define VPC_VPCVARPSREPLMODE_COMPONENT10 16
+#define VPC_VPCVARPSREPLMODE_COMPONENT11 18
+#define VPC_VPCVARPSREPLMODE_COMPONENT12 20
+#define VPC_VPCVARPSREPLMODE_COMPONENT13 22
+#define VPC_VPCVARPSREPLMODE_COMPONENT14 24
+#define VPC_VPCVARPSREPLMODE_COMPONENT15 26
+#define VPC_VPCVARPSREPLMODE_COMPONENT16 28
+#define VPC_VPCVARPSREPLMODE_COMPONENT17 30
+
 /* RBBM Debug bus block IDs */
+#define RBBM_BLOCK_ID_NONE             0x0
 #define RBBM_BLOCK_ID_CP               0x1
 #define RBBM_BLOCK_ID_RBBM             0x2
 #define RBBM_BLOCK_ID_VBIF             0x3
@@ -560,6 +871,7 @@
 
 /* VBIF countables */
 #define VBIF_AXI_TOTAL_BEATS 85
+#define VBIF_DDR_TOTAL_CYCLES 110
 
 /* VBIF Recoverable HALT bit value */
 #define VBIF_RECOVERABLE_HALT_CTRL 0x1
diff --git a/drivers/gpu/msm/a4xx_reg.h b/drivers/gpu/msm/a4xx_reg.h
index 8e658c1d54d2..78db8dd2da40 100644
--- a/drivers/gpu/msm/a4xx_reg.h
+++ b/drivers/gpu/msm/a4xx_reg.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -102,6 +102,7 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_RBBM_INTERFACE_HANG_INT_CTL	0x2f
 #define A4XX_RBBM_INT_CLEAR_CMD			0x36
 #define A4XX_RBBM_INT_0_MASK			0x37
+#define A4XX_RBBM_ALWAYSON_COUNTER_CNTL		0x3d
 #define A4XX_RBBM_RBBM_CTL			0x3e
 #define A4XX_RBBM_CLOCK_CTL2			0x42
 #define A4XX_RBBM_BLOCK_SW_RESET_CMD		0x45
@@ -112,8 +113,13 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_RBBM_CFG_DEBBUS_SEL_D		0x4d
 #define A4XX_RBBM_CFG_DEBBUS_SEL_PING_INDEX_SHIFT	0
 #define A4XX_RBBM_CFG_DEBBUS_SEL_PING_BLK_SEL_SHIFT	8
+#define A4XX_RBBM_CFG_DEBBUS_SEL_PONG_INDEX_SHIFT	16
+#define A4XX_RBBM_CFG_DEBBUS_SEL_PONG_BLK_SEL_SHIFT	24
 
 #define A4XX_RBBM_CFG_DEBBUS_CTLT		0x4e
+#define A4XX_RBBM_CFG_DEBBUS_CTLT_ENT_SHIFT		0
+#define A4XX_RBBM_CFG_DEBBUS_CTLT_GRANU_SHIFT		12
+#define A4XX_RBBM_CFG_DEBBUS_CTLT_SEGT_SHIFT		28
 
 #define A4XX_RBBM_CFG_DEBBUS_CTLM		0x4f
 #define A4XX_RBBM_CFG_DEBBUS_CTLT_ENABLE_SHIFT		24
@@ -132,7 +138,24 @@ enum a4xx_rb_perfctr_rb_sel {
 
 
 #define A4XX_RBBM_CFG_DEBBUS_BYTEL_0		0x5a
-#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1		0x5b
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL0_SHIFT	0
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL1_SHIFT	4
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL2_SHIFT	8
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL3_SHIFT	12
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL4_SHIFT	16
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL5_SHIFT	20
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL6_SHIFT	24
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_0_BYTEL7_SHIFT	28
+
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1			0x5b
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL8_SHIFT	0
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL9_SHIFT	4
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL10_SHIFT	8
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL11_SHIFT	12
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL12_SHIFT	16
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL13_SHIFT	20
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL14_SHIFT	24
+#define A4XX_RBBM_CFG_DEBBUS_BYTEL_1_BYTEL15_SHIFT	28
 
 #define A4XX_RBBM_CFG_DEBBUS_IVTE_0		0x5c
 #define A4XX_RBBM_CFG_DEBBUS_IVTE_1		0x5d
@@ -197,7 +220,6 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_RBBM_CFG_DEBBUS_CLRC		0x94
 #define A4XX_RBBM_CFG_DEBBUS_LOADIVT		0x95
 
-#define A4XX_RBBM_CLOCK_CTL_IP			0x97
 #define A4XX_RBBM_POWER_CNTL_IP			0x98
 #define A4XX_RBBM_SP_REGFILE_SLEEP_CNTL_0	0x99
 #define A4XX_RBBM_SP_REGFILE_SLEEP_CNTL_1	0x9a
@@ -403,7 +425,14 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_RBBM_AHB_PFP_SPLIT_STATUS		0x18d
 #define A4XX_RBBM_AHB_ERROR_STATUS		0x18f
 #define A4XX_RBBM_STATUS			0x191
+#define A4XX_RBBM_CFG_COUNTER0			0x1a2
+#define A4XX_RBBM_CFG_DEBBUS_TRACE_BUF0		0x1a9
+#define A4XX_RBBM_CFG_DEBBUS_TRACE_BUF1		0x1aa
+#define A4XX_RBBM_CFG_DEBBUS_TRACE_BUF2		0x1ab
+#define A4XX_RBBM_CFG_DEBBUS_TRACE_BUF3		0x1ac
 #define A4XX_RBBM_CFG_DEBBUS_TRACE_BUF4		0x1ad
+#define A4XX_RBBM_CFG_DEBBUS_MISR0		0x1ae
+#define A4XX_RBBM_CFG_DEBBUS_MISR1		0x1af
 #define A4XX_RBBM_POWER_STATUS			0x1b0
 #define A4XX_RBBM_PPD_V2_SP_PWR_WEIGHTS		0x1b2
 #define A4XX_RBBM_PPD_V2_SP_RB_EPOCH_TH		0x1b3
@@ -411,6 +440,8 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_RBBM_PPD_RAMP_V2_CONTROL		0x1b5
 #define A4XX_RBBM_WAIT_IDLE_CLOCKS_CTL2		0x1b8
 #define A4XX_RBBM_PPD_CTRL			0x1b9
+#define A4XX_RBBM_PPD_EPOCH_INTRA_TH_1		0x1ba
+#define A4XX_RBBM_PPD_EPOCH_INTRA_TH_2		0x1bb
 #define A4XX_RBBM_PPD_EPOCH_INTER_TH_HIGH_CLEAR_THR  0x1bc
 #define A4XX_RBBM_PPD_EPOCH_INTER_TH_LOW	0x1bd
 /* SECVID registers */
@@ -442,8 +473,13 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_CP_ME_RAM_WADDR		0x225
 #define A4XX_CP_ME_RAM_RADDR		0x226
 #define A4XX_CP_ME_RAM_DATA		0x227
+#define A4XX_CP_SCRATCH_UMASK		0x228
+#define A4XX_CP_SCRATCH_ADDR		0x229
 
 #define A4XX_CP_PREEMPT			0x22a
+/* PREEMPT register bit shifts */
+#define A4XX_CP_PREEMPT_STOP_SHIFT	0
+#define A4XX_CP_PREEMPT_RESUME_SHIFT	1
 
 #define A4XX_CP_PREEMPT_DISABLE		0x22b
 #define A4XX_CP_CNTL			0x22c
@@ -477,6 +513,7 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_CP_PERFCTR_CP_SEL_6	0x506
 #define A4XX_CP_PERFCTR_CP_SEL_7	0x507
 
+#define A4XX_CP_SCRATCH_REG0		0x578
 #define A4XX_CP_SCRATCH_REG6		0x57e
 #define A4XX_CP_SCRATCH_REG7		0x57f
 #define A4XX_CP_SCRATCH_REG8		0x580
@@ -503,6 +540,9 @@ enum a4xx_rb_perfctr_rb_sel {
 #define A4XX_SP_CS_CTRL_0		0x2300
 #define A4XX_SP_CS_OBJ_OFFSET		0x2301
 #define A4XX_SP_CS_OBJ_START		0x2302
+#define A4XX_SP_CS_PVT_MEM_PARAM	0x2303
+#define A4XX_SP_CS_PVT_MEM_ADDR		0x2304
+#define A4XX_SP_CS_PVT_MEM_SIZE		0x2305
 #define A4XX_SP_CS_LENGTH		0x2306
 #define A4XX_SP_MODE_CONTROL		0xec3
 #define A4XX_SP_PERFCTR_SP_SEL_0	0xec4
@@ -571,6 +611,8 @@ enum a4xx_sp_perfctr_sp_sel {
 #define A4XX_VSC_PERFCTR_VSC_SEL_1	0xc51
 
 /* VFD registers */
+#define A4XX_VFD_CONTROL_0		0x2200
+#define A4XX_VFD_FETCH_INSTR_0_0	0x220a
 #define A4XX_VFD_FETCH_INSTR_1_31	0x2287
 #define A4XX_VFD_PERFCTR_VFD_SEL_0	0xe43
 #define A4XX_VFD_PERFCTR_VFD_SEL_1	0xe44
@@ -657,6 +699,14 @@ enum a4xx_vfd_perfctr_vfd_sel {
 
 #define A4XX_VBIF_TEST_BUS_OUT		0x308c
 
+#define A4XX_VBIF_PERF_CNT_EN0		0x30c0
+#define A4XX_VBIF_PERF_CNT_EN1		0x30c1
+#define A4XX_VBIF_PERF_CNT_EN2		0x30c2
+#define A4XX_VBIF_PERF_CNT_EN3		0x30c3
+#define A4XX_VBIF_PERF_CNT_CLR0		0x30c8
+#define A4XX_VBIF_PERF_CNT_CLR1		0x30c9
+#define A4XX_VBIF_PERF_CNT_CLR2		0x30ca
+#define A4XX_VBIF_PERF_CNT_CLR3		0x30cb
 #define A4XX_VBIF_PERF_CNT_SEL0		0x30d0
 #define A4XX_VBIF_PERF_CNT_SEL1		0x30d1
 #define A4XX_VBIF_PERF_CNT_SEL2		0x30d2
@@ -674,6 +724,10 @@ enum a4xx_vfd_perfctr_vfd_sel {
 #define A4XX_VBIF_PERF_PWR_CNT_EN1	0x3101
 #define A4XX_VBIF_PERF_PWR_CNT_EN2	0x3102
 #define A4XX_VBIF_PERF_PWR_CNT_EN3	0x3103
+#define A4XX_VBIF_PERF_PWR_CNT_CLR0	0x3108
+#define A4XX_VBIF_PERF_PWR_CNT_CLR1	0x3109
+#define A4XX_VBIF_PERF_PWR_CNT_CLR2	0x310A
+#define A4XX_VBIF_PERF_PWR_CNT_CLR3	0x310B
 #define A4XX_VBIF_PERF_PWR_CNT_LOW0	0x3110
 #define A4XX_VBIF_PERF_PWR_CNT_LOW1	0x3111
 #define A4XX_VBIF_PERF_PWR_CNT_LOW2	0x3112
@@ -683,6 +737,12 @@ enum a4xx_vfd_perfctr_vfd_sel {
 #define A4XX_VBIF_PERF_PWR_CNT_HIGH2	0x311a
 #define A4XX_VBIF_PERF_PWR_CNT_HIGH3	0x311b
 
+/* Bit flags for RBBM_CTL */
+#define A4XX_RBBM_RBBM_CTL_RESET_PWR_CTR0	0x00000001
+#define A4XX_RBBM_RBBM_CTL_RESET_PWR_CTR1	0x00000002
+#define A4XX_RBBM_RBBM_CTL_ENABLE_PWR_CTR0	0x00000010
+#define A4XX_RBBM_RBBM_CTL_ENABLE_PWR_CTR1	0x00000020
+
 /* GRAS registers */
 #define A4XX_GRAS_PERFCTR_TSE_SEL_0	0xc88
 #define A4XX_GRAS_PERFCTR_TSE_SEL_1	0xc89
@@ -712,6 +772,7 @@ enum a4xx_pc_perfctr_pc_sel {
 
 /* HLSQ registers */
 #define A4XX_HLSQ_TIMEOUT_THRESHOLD     0xe00
+#define A4XX_HLSQ_STATE_RESTORE_TRIGGER	0xe01
 #define A4XX_HLSQ_MODE_CONTROL		0xe05
 #define A4XX_HLSQ_PERFCTR_HLSQ_SEL_0	0xe06
 #define A4XX_HLSQ_PERFCTR_HLSQ_SEL_1	0xe07
@@ -723,11 +784,24 @@ enum a4xx_pc_perfctr_pc_sel {
 #define A4XX_HLSQ_PERFCTR_HLSQ_SEL_7	0xe0d
 #define A4XX_HLSQ_SPTP_RDSEL		0xe30
 #define A4xx_HLSQ_CONTROL_0		0x23c0
+#define A4xx_HLSQ_CONTROL_1		0x23c1
+#define A4xx_HLSQ_CONTROL_2		0x23c2
+#define A4xx_HLSQ_CONTROL_3		0x23c3
+#define A4xx_HLSQ_CONTROL_4		0x23c4
 #define A4XX_HLSQ_CS_CONTROL		0x23ca
 #define A4XX_HLSQ_CL_NDRANGE_0		0x23cd
+#define A4XX_HLSQ_CL_NDRANGE_1		0x23ce
+#define A4XX_HLSQ_CL_NDRANGE_2		0x23cf
+#define A4XX_HLSQ_CL_NDRANGE_3		0x23d0
+#define A4XX_HLSQ_CL_NDRANGE_4		0x23d1
+#define A4XX_HLSQ_CL_NDRANGE_5		0x23d2
+#define A4XX_HLSQ_CL_NDRANGE_6		0x23d3
 #define A4XX_HLSQ_CL_CONTROL_0		0x23d4
+#define A4XX_HLSQ_CL_CONTROL_1		0x23d5
 #define A4XX_HLSQ_CL_KERNEL_CONST	0x23d6
 #define A4XX_HLSQ_CL_KERNEL_GROUP_X	0x23d7
+#define A4XX_HLSQ_CL_KERNEL_GROUP_Y	0x23d8
+#define A4XX_HLSQ_CL_KERNEL_GROUP_Z	0x23d9
 #define A4XX_HLSQ_CL_WG_OFFSET		0x23da
 #define A4XX_HLSQ_UPDATE_CONTROL	0x23db
 
@@ -789,6 +863,9 @@ enum a4xx_uche_perfctr_uche_sel {
 #define A4XX_TPL1_PERFCTR_TP_SEL_6	0xf0a
 #define A4XX_TPL1_PERFCTR_TP_SEL_7	0xf0b
 #define A4XX_TPL1_TP_TEX_TSIZE_1	0x23a0
+#define A4XX_TPL1_TP_CS_BORDER_COLOR_BASE_ADDR	0x23A4
+#define A4XX_TPL1_TP_CS_SAMPLER_BASE_ADDR	0x23A5
+#define A4XX_TPL1_TP_CS_TEXMEMOBJ_BASE_ADDR	0x23A6
 
 enum a4xx_tpl1_perfctr_tp_sel {
 	TP_OUTPUT_TEXELS_POINT = 0x2,
diff --git a/drivers/gpu/msm/a5xx_reg.h b/drivers/gpu/msm/a5xx_reg.h
index ef2861ca96dd..45aabeb3c4a2 100644
--- a/drivers/gpu/msm/a5xx_reg.h
+++ b/drivers/gpu/msm/a5xx_reg.h
@@ -60,8 +60,6 @@
 #define A5XX_CP_RB_BASE                  0x800
 #define A5XX_CP_RB_BASE_HI               0x801
 #define A5XX_CP_RB_CNTL                  0x802
-#define A5XX_CP_RB_RPTR_ADDR_LO          0x804
-#define A5XX_CP_RB_RPTR_ADDR_HI          0x805
 #define A5XX_CP_RB_RPTR                  0x806
 #define A5XX_CP_RB_WPTR                  0x807
 #define A5XX_CP_PFP_STAT_ADDR            0x808
@@ -133,6 +131,9 @@
 #define A5XX_CP_POWERCTR_CP_SEL_2        0xBBC
 #define A5XX_CP_POWERCTR_CP_SEL_3        0xBBD
 
+/* CP_EVENT_WRITE events */
+#define A5XX_CACHE_FLUSH_TS              0x4
+
 /* RBBM registers */
 #define A5XX_RBBM_CFG_DBGBUS_SEL_A               0x4
 #define A5XX_RBBM_CFG_DBGBUS_SEL_B               0x5
@@ -140,6 +141,8 @@
 #define A5XX_RBBM_CFG_DBGBUS_SEL_D               0x7
 #define A5XX_RBBM_CFG_DBGBUS_SEL_PING_INDEX_SHIFT    0x0
 #define A5XX_RBBM_CFG_DBGBUS_SEL_PING_BLK_SEL_SHIFT  0x8
+#define A5XX_RBBM_CFG_DBGBUS_SEL_PONG_INDEX_SHIFT    0x10
+#define A5XX_RBBM_CFG_DBGBUS_SEL_PONG_BLK_SEL_SHIFT  0x18
 
 #define A5XX_RBBM_CFG_DBGBUS_CNTLT               0x8
 #define A5XX_RBBM_CFG_DBGBUS_CNTLM               0x9
@@ -533,6 +536,10 @@
 #define A5XX_RBBM_SECVID_TSB_TRUSTED_BASE_HI     0xF801
 #define A5XX_RBBM_SECVID_TSB_TRUSTED_SIZE        0xF802
 #define A5XX_RBBM_SECVID_TSB_CNTL                0xF803
+#define A5XX_RBBM_SECVID_TSB_COMP_STATUS_LO      0xF804
+#define A5XX_RBBM_SECVID_TSB_COMP_STATUS_HI      0xF805
+#define A5XX_RBBM_SECVID_TSB_UCHE_STATUS_LO      0xF806
+#define A5XX_RBBM_SECVID_TSB_UCHE_STATUS_HI      0xF807
 #define A5XX_RBBM_SECVID_TSB_ADDR_MODE_CNTL      0xF810
 
 /* VSC registers */
@@ -561,7 +568,6 @@
 
 
 /* RB registers */
-#define A5XX_RB_DBG_ECO_CNT                 0xCC4
 #define A5XX_RB_ADDR_MODE_CNTL              0xCC5
 #define A5XX_RB_MODE_CNTL                   0xCC6
 #define A5XX_RB_PERFCTR_RB_SEL_0            0xCD0
@@ -608,7 +614,7 @@
 #define A5XX_PC_PERFCTR_PC_SEL_7            0xD17
 
 /* HLSQ registers */
-#define A5XX_HLSQ_DBG_ECO_CNTL		    0xE04
+#define A5XX_HLSQ_TIMEOUT_THRESHOLD         0xE00
 #define A5XX_HLSQ_ADDR_MODE_CNTL            0xE05
 #define A5XX_HLSQ_PERFCTR_HLSQ_SEL_0        0xE10
 #define A5XX_HLSQ_PERFCTR_HLSQ_SEL_1        0xE11
@@ -618,6 +624,7 @@
 #define A5XX_HLSQ_PERFCTR_HLSQ_SEL_5        0xE15
 #define A5XX_HLSQ_PERFCTR_HLSQ_SEL_6        0xE16
 #define A5XX_HLSQ_PERFCTR_HLSQ_SEL_7        0xE17
+#define A5XX_HLSQ_SPTP_RDSEL                0xF08
 #define A5XX_HLSQ_DBG_READ_SEL              0xBC00
 #define A5XX_HLSQ_DBG_AHB_READ_APERTURE     0xA000
 
@@ -633,7 +640,6 @@
 #define A5XX_VFD_PERFCTR_VFD_SEL_7          0xE57
 
 /* VPC registers */
-#define A5XX_VPC_DBG_ECO_CNTL		    0xE60
 #define A5XX_VPC_ADDR_MODE_CNTL             0xE61
 #define A5XX_VPC_PERFCTR_VPC_SEL_0          0xE64
 #define A5XX_VPC_PERFCTR_VPC_SEL_1          0xE65
@@ -642,7 +648,7 @@
 
 /* UCHE registers */
 #define A5XX_UCHE_ADDR_MODE_CNTL            0xE80
-#define A5XX_UCHE_MODE_CNTL                 0xE81
+#define A5XX_UCHE_SVM_CNTL                  0xE82
 #define A5XX_UCHE_WRITE_THRU_BASE_LO        0xE87
 #define A5XX_UCHE_WRITE_THRU_BASE_HI        0xE88
 #define A5XX_UCHE_TRAP_BASE_LO              0xE89
@@ -715,8 +721,12 @@
 #define A5XX_VBIF_CLKON_FORCE_ON_TESTBUS_MASK   0x1
 #define A5XX_VBIF_CLKON_FORCE_ON_TESTBUS_SHIFT  0x1
 
+#define A5XX_VBIF_ABIT_SORT                0x3028
+#define A5XX_VBIF_ABIT_SORT_CONF           0x3029
 #define A5XX_VBIF_ROUND_ROBIN_QOS_ARB      0x3049
 #define A5XX_VBIF_GATE_OFF_WRREQ_EN        0x302A
+#define A5XX_VBIF_IN_RD_LIM_CONF0          0x302C
+#define A5XX_VBIF_IN_RD_LIM_CONF1          0x302D
 
 #define A5XX_VBIF_XIN_HALT_CTRL0	   0x3080
 #define A5XX_VBIF_XIN_HALT_CTRL0_MASK	   0xF
@@ -734,7 +744,7 @@
 
 #define A5XX_VBIF_TEST_BUS2_CTRL0                   0x3087
 #define A5XX_VBIF_TEST_BUS2_CTRL1                   0x3088
-#define A5XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_MASK     0x1FF
+#define A5XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_MASK     0xF
 #define A5XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_SHIFT    0x0
 
 #define A5XX_VBIF_TEST_BUS_OUT             0x308c
@@ -791,9 +801,6 @@
 /* COUNTABLE FOR TSE PERFCOUNTER */
 #define A5XX_TSE_INPUT_PRIM_NUM            0x6
 
-/* COUNTABLE FOR RBBM PERFCOUNTER */
-#define A5XX_RBBM_ALWAYS_COUNT		0x0
-
 /* GPMU POWER COUNTERS */
 #define A5XX_SP_POWER_COUNTER_0_LO		0xA840
 #define A5XX_SP_POWER_COUNTER_0_HI		0xA841
@@ -864,14 +871,15 @@
 #define A5XX_GPMU_ALWAYS_ON_COUNTER_RESET	0xA87B
 #define A5XX_GPMU_POWER_COUNTER_SELECT_0	0xA87C
 #define A5XX_GPMU_POWER_COUNTER_SELECT_1	0xA87D
-#define A5XX_GPMU_GPMU_SP_CLOCK_CONTROL		0xA880
 
 #define A5XX_GPMU_CLOCK_THROTTLE_CTRL		0xA8A3
 #define A5XX_GPMU_THROTTLE_UNMASK_FORCE_CTRL	0xA8A8
 
 #define A5XX_GPMU_TEMP_SENSOR_ID		0xAC00
 #define A5XX_GPMU_TEMP_SENSOR_CONFIG		0xAC01
+#define A5XX_GPMU_TEMP_VAL			0xAC02
 #define A5XX_GPMU_DELTA_TEMP_THRESHOLD		0xAC03
+#define A5XX_GPMU_TEMP_THRESHOLD_INTR_STATUS	0xAC05
 #define A5XX_GPMU_TEMP_THRESHOLD_INTR_EN_MASK	0xAC06
 
 #define A5XX_GPMU_LEAKAGE_TEMP_COEFF_0_1	0xAC40
@@ -886,25 +894,18 @@
 #define A5XX_GPMU_GPMU_PWR_THRESHOLD		0xAC80
 #define A5XX_GPMU_GPMU_LLM_GLM_SLEEP_CTRL	0xACC4
 #define A5XX_GPMU_GPMU_LLM_GLM_SLEEP_STATUS	0xACC5
-#define A5XX_GPMU_GPMU_ISENSE_CTRL		0xACD0
 
 #define A5XX_GDPM_CONFIG1			0xB80C
+#define A5XX_GDPM_CONFIG2			0xB80D
 #define A5XX_GDPM_INT_EN			0xB80F
 #define A5XX_GDPM_INT_MASK			0xB811
 #define A5XX_GPMU_BEC_ENABLE			0xB9A0
 
-/* ISENSE registers */
-#define A5XX_GPU_CS_DECIMAL_ALIGN		0xC16A
-#define A5XX_GPU_CS_SENSOR_PARAM_CORE_1	0xC126
-#define A5XX_GPU_CS_SENSOR_PARAM_CORE_2	0xC127
-#define A5XX_GPU_CS_SW_OV_FUSE_EN		0xC168
 #define A5XX_GPU_CS_SENSOR_GENERAL_STATUS	0xC41A
 #define A5XX_GPU_CS_AMP_CALIBRATION_STATUS1_0	0xC41D
 #define A5XX_GPU_CS_AMP_CALIBRATION_STATUS1_2	0xC41F
 #define A5XX_GPU_CS_AMP_CALIBRATION_STATUS1_4	0xC421
 #define A5XX_GPU_CS_ENABLE_REG			0xC520
 #define A5XX_GPU_CS_AMP_CALIBRATION_CONTROL1	0xC557
-#define A5XX_GPU_CS_AMP_CALIBRATION_DONE	0xC565
-#define A5XX_GPU_CS_ENDPOINT_CALIBRATION_DONE   0xC556
 #endif /* _A5XX_REG_H */
 
diff --git a/drivers/gpu/msm/a6xx_reg.h b/drivers/gpu/msm/a6xx_reg.h
deleted file mode 100644
index ef0d7f10a58c..000000000000
--- a/drivers/gpu/msm/a6xx_reg.h
+++ /dev/null
@@ -1,1074 +0,0 @@
-/* Copyright (c) 2017-2018, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#ifndef _A6XX_REG_H
-#define _A6XX_REG_H
-
-/* A6XX interrupt bits */
-#define A6XX_INT_RBBM_GPU_IDLE          0
-#define A6XX_INT_CP_AHB_ERROR           1
-#define A6XX_INT_ATB_ASYNCFIFO_OVERFLOW 6
-#define A6XX_INT_RBBM_GPC_ERROR         7
-#define A6XX_INT_CP_SW                  8
-#define A6XX_INT_CP_HW_ERROR            9
-#define A6XX_INT_CP_CCU_FLUSH_DEPTH_TS  10
-#define A6XX_INT_CP_CCU_FLUSH_COLOR_TS  11
-#define A6XX_INT_CP_CCU_RESOLVE_TS      12
-#define A6XX_INT_CP_IB2                 13
-#define A6XX_INT_CP_IB1                 14
-#define A6XX_INT_CP_RB                  15
-#define A6XX_INT_CP_RB_DONE_TS          17
-#define A6XX_INT_CP_WT_DONE_TS          18
-#define A6XX_INT_CP_CACHE_FLUSH_TS      20
-#define A6XX_INT_RBBM_ATB_BUS_OVERFLOW  22
-#define A6XX_INT_RBBM_HANG_DETECT       23
-#define A6XX_INT_UCHE_OOB_ACCESS        24
-#define A6XX_INT_UCHE_TRAP_INTR         25
-#define A6XX_INT_DEBBUS_INTR_0          26
-#define A6XX_INT_DEBBUS_INTR_1          27
-#define A6XX_INT_ISDB_CPU_IRQ           30
-#define A6XX_INT_ISDB_UNDER_DEBUG       31
-
-/* CP Interrupt bits */
-#define A6XX_CP_OPCODE_ERROR                    0
-#define A6XX_CP_UCODE_ERROR                     1
-#define A6XX_CP_HW_FAULT_ERROR                  2
-#define A6XX_CP_REGISTER_PROTECTION_ERROR       4
-#define A6XX_CP_AHB_ERROR                       5
-#define A6XX_CP_VSD_PARITY_ERROR                6
-#define A6XX_CP_ILLEGAL_INSTR_ERROR             7
-
-/* CP registers */
-#define A6XX_CP_RB_BASE                  0x800
-#define A6XX_CP_RB_BASE_HI               0x801
-#define A6XX_CP_RB_CNTL                  0x802
-#define A6XX_CP_RB_RPTR_ADDR_LO          0x804
-#define A6XX_CP_RB_RPTR_ADDR_HI          0x805
-#define A6XX_CP_RB_RPTR                  0x806
-#define A6XX_CP_RB_WPTR                  0x807
-#define A6XX_CP_SQE_CNTL                 0x808
-#define A6XX_CP_HW_FAULT                 0x821
-#define A6XX_CP_INTERRUPT_STATUS         0x823
-#define A6XX_CP_PROTECT_STATUS           0X824
-#define A6XX_CP_SQE_INSTR_BASE_LO        0x830
-#define A6XX_CP_SQE_INSTR_BASE_HI        0x831
-#define A6XX_CP_MISC_CNTL                0x840
-#define A6XX_CP_ROQ_THRESHOLDS_1         0x8C1
-#define A6XX_CP_ROQ_THRESHOLDS_2         0x8C2
-#define A6XX_CP_MEM_POOL_SIZE            0x8C3
-#define A6XX_CP_CHICKEN_DBG              0x841
-#define A6XX_CP_ADDR_MODE_CNTL           0x842
-#define A6XX_CP_DBG_ECO_CNTL             0x843
-#define A6XX_CP_PROTECT_CNTL             0x84F
-#define A6XX_CP_PROTECT_REG              0x850
-#define A6XX_CP_CONTEXT_SWITCH_CNTL      0x8A0
-#define A6XX_CP_CONTEXT_SWITCH_SMMU_INFO_LO   0x8A1
-#define A6XX_CP_CONTEXT_SWITCH_SMMU_INFO_HI   0x8A2
-#define A6XX_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_LO   0x8A3
-#define A6XX_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_HI   0x8A4
-#define A6XX_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_LO   0x8A5
-#define A6XX_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_HI   0x8A6
-#define A6XX_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_LO   0x8A7
-#define A6XX_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_HI   0x8A8
-#define A6XX_CP_CONTEXT_SWITCH_LEVEL_STATUS 0x8AB
-#define A6XX_CP_PERFCTR_CP_SEL_0         0x8D0
-#define A6XX_CP_PERFCTR_CP_SEL_1         0x8D1
-#define A6XX_CP_PERFCTR_CP_SEL_2         0x8D2
-#define A6XX_CP_PERFCTR_CP_SEL_3         0x8D3
-#define A6XX_CP_PERFCTR_CP_SEL_4         0x8D4
-#define A6XX_CP_PERFCTR_CP_SEL_5         0x8D5
-#define A6XX_CP_PERFCTR_CP_SEL_6         0x8D6
-#define A6XX_CP_PERFCTR_CP_SEL_7         0x8D7
-#define A6XX_CP_PERFCTR_CP_SEL_8         0x8D8
-#define A6XX_CP_PERFCTR_CP_SEL_9         0x8D9
-#define A6XX_CP_PERFCTR_CP_SEL_10        0x8DA
-#define A6XX_CP_PERFCTR_CP_SEL_11        0x8DB
-#define A6XX_CP_PERFCTR_CP_SEL_12        0x8DC
-#define A6XX_CP_PERFCTR_CP_SEL_13        0x8DD
-#define A6XX_CP_CRASH_SCRIPT_BASE_LO     0x900
-#define A6XX_CP_CRASH_SCRIPT_BASE_HI     0x901
-#define A6XX_CP_CRASH_DUMP_CNTL          0x902
-#define A6XX_CP_CRASH_DUMP_STATUS        0x903
-#define A6XX_CP_SQE_STAT_ADDR            0x908
-#define A6XX_CP_SQE_STAT_DATA            0x909
-#define A6XX_CP_DRAW_STATE_ADDR          0x90A
-#define A6XX_CP_DRAW_STATE_DATA          0x90B
-#define A6XX_CP_ROQ_DBG_ADDR             0x90C
-#define A6XX_CP_ROQ_DBG_DATA             0x90D
-#define A6XX_CP_MEM_POOL_DBG_ADDR        0x90E
-#define A6XX_CP_MEM_POOL_DBG_DATA        0x90F
-#define A6XX_CP_SQE_UCODE_DBG_ADDR       0x910
-#define A6XX_CP_SQE_UCODE_DBG_DATA       0x911
-#define A6XX_CP_IB1_BASE                 0x928
-#define A6XX_CP_IB1_BASE_HI              0x929
-#define A6XX_CP_IB1_REM_SIZE             0x92A
-#define A6XX_CP_IB2_BASE                 0x92B
-#define A6XX_CP_IB2_BASE_HI              0x92C
-#define A6XX_CP_IB2_REM_SIZE             0x92D
-#define A6XX_CP_ALWAYS_ON_COUNTER_LO     0x980
-#define A6XX_CP_ALWAYS_ON_COUNTER_HI     0x981
-#define A6XX_CP_AHB_CNTL                 0x98D
-#define A6XX_CP_APERTURE_CNTL_HOST       0xA00
-#define A6XX_CP_APERTURE_CNTL_CD         0xA03
-#define A6XX_VSC_ADDR_MODE_CNTL          0xC01
-
-/* RBBM registers */
-#define A6XX_RBBM_INT_0_STATUS                   0x201
-#define A6XX_RBBM_STATUS                         0x210
-#define A6XX_RBBM_STATUS3                        0x213
-#define A6XX_RBBM_VBIF_GX_RESET_STATUS           0x215
-#define A6XX_RBBM_PERFCTR_CP_0_LO                0x400
-#define A6XX_RBBM_PERFCTR_CP_0_HI                0x401
-#define A6XX_RBBM_PERFCTR_CP_1_LO                0x402
-#define A6XX_RBBM_PERFCTR_CP_1_HI                0x403
-#define A6XX_RBBM_PERFCTR_CP_2_LO                0x404
-#define A6XX_RBBM_PERFCTR_CP_2_HI                0x405
-#define A6XX_RBBM_PERFCTR_CP_3_LO                0x406
-#define A6XX_RBBM_PERFCTR_CP_3_HI                0x407
-#define A6XX_RBBM_PERFCTR_CP_4_LO                0x408
-#define A6XX_RBBM_PERFCTR_CP_4_HI                0x409
-#define A6XX_RBBM_PERFCTR_CP_5_LO                0x40a
-#define A6XX_RBBM_PERFCTR_CP_5_HI                0x40b
-#define A6XX_RBBM_PERFCTR_CP_6_LO                0x40c
-#define A6XX_RBBM_PERFCTR_CP_6_HI                0x40d
-#define A6XX_RBBM_PERFCTR_CP_7_LO                0x40e
-#define A6XX_RBBM_PERFCTR_CP_7_HI                0x40f
-#define A6XX_RBBM_PERFCTR_CP_8_LO                0x410
-#define A6XX_RBBM_PERFCTR_CP_8_HI                0x411
-#define A6XX_RBBM_PERFCTR_CP_9_LO                0x412
-#define A6XX_RBBM_PERFCTR_CP_9_HI                0x413
-#define A6XX_RBBM_PERFCTR_CP_10_LO               0x414
-#define A6XX_RBBM_PERFCTR_CP_10_HI               0x415
-#define A6XX_RBBM_PERFCTR_CP_11_LO               0x416
-#define A6XX_RBBM_PERFCTR_CP_11_HI               0x417
-#define A6XX_RBBM_PERFCTR_CP_12_LO               0x418
-#define A6XX_RBBM_PERFCTR_CP_12_HI               0x419
-#define A6XX_RBBM_PERFCTR_CP_13_LO               0x41a
-#define A6XX_RBBM_PERFCTR_CP_13_HI               0x41b
-#define A6XX_RBBM_PERFCTR_RBBM_0_LO              0x41c
-#define A6XX_RBBM_PERFCTR_RBBM_0_HI              0x41d
-#define A6XX_RBBM_PERFCTR_RBBM_1_LO              0x41e
-#define A6XX_RBBM_PERFCTR_RBBM_1_HI              0x41f
-#define A6XX_RBBM_PERFCTR_RBBM_2_LO              0x420
-#define A6XX_RBBM_PERFCTR_RBBM_2_HI              0x421
-#define A6XX_RBBM_PERFCTR_RBBM_3_LO              0x422
-#define A6XX_RBBM_PERFCTR_RBBM_3_HI              0x423
-#define A6XX_RBBM_PERFCTR_PC_0_LO                0x424
-#define A6XX_RBBM_PERFCTR_PC_0_HI                0x425
-#define A6XX_RBBM_PERFCTR_PC_1_LO                0x426
-#define A6XX_RBBM_PERFCTR_PC_1_HI                0x427
-#define A6XX_RBBM_PERFCTR_PC_2_LO                0x428
-#define A6XX_RBBM_PERFCTR_PC_2_HI                0x429
-#define A6XX_RBBM_PERFCTR_PC_3_LO                0x42a
-#define A6XX_RBBM_PERFCTR_PC_3_HI                0x42b
-#define A6XX_RBBM_PERFCTR_PC_4_LO                0x42c
-#define A6XX_RBBM_PERFCTR_PC_4_HI                0x42d
-#define A6XX_RBBM_PERFCTR_PC_5_LO                0x42e
-#define A6XX_RBBM_PERFCTR_PC_5_HI                0x42f
-#define A6XX_RBBM_PERFCTR_PC_6_LO                0x430
-#define A6XX_RBBM_PERFCTR_PC_6_HI                0x431
-#define A6XX_RBBM_PERFCTR_PC_7_LO                0x432
-#define A6XX_RBBM_PERFCTR_PC_7_HI                0x433
-#define A6XX_RBBM_PERFCTR_VFD_0_LO               0x434
-#define A6XX_RBBM_PERFCTR_VFD_0_HI               0x435
-#define A6XX_RBBM_PERFCTR_VFD_1_LO               0x436
-#define A6XX_RBBM_PERFCTR_VFD_1_HI               0x437
-#define A6XX_RBBM_PERFCTR_VFD_2_LO               0x438
-#define A6XX_RBBM_PERFCTR_VFD_2_HI               0x439
-#define A6XX_RBBM_PERFCTR_VFD_3_LO               0x43a
-#define A6XX_RBBM_PERFCTR_VFD_3_HI               0x43b
-#define A6XX_RBBM_PERFCTR_VFD_4_LO               0x43c
-#define A6XX_RBBM_PERFCTR_VFD_4_HI               0x43d
-#define A6XX_RBBM_PERFCTR_VFD_5_LO               0x43e
-#define A6XX_RBBM_PERFCTR_VFD_5_HI               0x43f
-#define A6XX_RBBM_PERFCTR_VFD_6_LO               0x440
-#define A6XX_RBBM_PERFCTR_VFD_6_HI               0x441
-#define A6XX_RBBM_PERFCTR_VFD_7_LO               0x442
-#define A6XX_RBBM_PERFCTR_VFD_7_HI               0x443
-#define A6XX_RBBM_PERFCTR_HLSQ_0_LO              0x444
-#define A6XX_RBBM_PERFCTR_HLSQ_0_HI              0x445
-#define A6XX_RBBM_PERFCTR_HLSQ_1_LO              0x446
-#define A6XX_RBBM_PERFCTR_HLSQ_1_HI              0x447
-#define A6XX_RBBM_PERFCTR_HLSQ_2_LO              0x448
-#define A6XX_RBBM_PERFCTR_HLSQ_2_HI              0x449
-#define A6XX_RBBM_PERFCTR_HLSQ_3_LO              0x44a
-#define A6XX_RBBM_PERFCTR_HLSQ_3_HI              0x44b
-#define A6XX_RBBM_PERFCTR_HLSQ_4_LO              0x44c
-#define A6XX_RBBM_PERFCTR_HLSQ_4_HI              0x44d
-#define A6XX_RBBM_PERFCTR_HLSQ_5_LO              0x44e
-#define A6XX_RBBM_PERFCTR_HLSQ_5_HI              0x44f
-#define A6XX_RBBM_PERFCTR_VPC_0_LO               0x450
-#define A6XX_RBBM_PERFCTR_VPC_0_HI               0x451
-#define A6XX_RBBM_PERFCTR_VPC_1_LO               0x452
-#define A6XX_RBBM_PERFCTR_VPC_1_HI               0x453
-#define A6XX_RBBM_PERFCTR_VPC_2_LO               0x454
-#define A6XX_RBBM_PERFCTR_VPC_2_HI               0x455
-#define A6XX_RBBM_PERFCTR_VPC_3_LO               0x456
-#define A6XX_RBBM_PERFCTR_VPC_3_HI               0x457
-#define A6XX_RBBM_PERFCTR_VPC_4_LO               0x458
-#define A6XX_RBBM_PERFCTR_VPC_4_HI               0x459
-#define A6XX_RBBM_PERFCTR_VPC_5_LO               0x45a
-#define A6XX_RBBM_PERFCTR_VPC_5_HI               0x45b
-#define A6XX_RBBM_PERFCTR_CCU_0_LO               0x45c
-#define A6XX_RBBM_PERFCTR_CCU_0_HI               0x45d
-#define A6XX_RBBM_PERFCTR_CCU_1_LO               0x45e
-#define A6XX_RBBM_PERFCTR_CCU_1_HI               0x45f
-#define A6XX_RBBM_PERFCTR_CCU_2_LO               0x460
-#define A6XX_RBBM_PERFCTR_CCU_2_HI               0x461
-#define A6XX_RBBM_PERFCTR_CCU_3_LO               0x462
-#define A6XX_RBBM_PERFCTR_CCU_3_HI               0x463
-#define A6XX_RBBM_PERFCTR_CCU_4_LO               0x464
-#define A6XX_RBBM_PERFCTR_CCU_4_HI               0x465
-#define A6XX_RBBM_PERFCTR_TSE_0_LO               0x466
-#define A6XX_RBBM_PERFCTR_TSE_0_HI               0x467
-#define A6XX_RBBM_PERFCTR_TSE_1_LO               0x468
-#define A6XX_RBBM_PERFCTR_TSE_1_HI               0x469
-#define A6XX_RBBM_PERFCTR_TSE_2_LO               0x46a
-#define A6XX_RBBM_PERFCTR_CCU_4_HI               0x465
-#define A6XX_RBBM_PERFCTR_TSE_0_LO               0x466
-#define A6XX_RBBM_PERFCTR_TSE_0_HI               0x467
-#define A6XX_RBBM_PERFCTR_TSE_1_LO               0x468
-#define A6XX_RBBM_PERFCTR_TSE_1_HI               0x469
-#define A6XX_RBBM_PERFCTR_TSE_2_LO               0x46a
-#define A6XX_RBBM_PERFCTR_TSE_2_HI               0x46b
-#define A6XX_RBBM_PERFCTR_TSE_3_LO               0x46c
-#define A6XX_RBBM_PERFCTR_TSE_3_HI               0x46d
-#define A6XX_RBBM_PERFCTR_RAS_0_LO               0x46e
-#define A6XX_RBBM_PERFCTR_RAS_0_HI               0x46f
-#define A6XX_RBBM_PERFCTR_RAS_1_LO               0x470
-#define A6XX_RBBM_PERFCTR_RAS_1_HI               0x471
-#define A6XX_RBBM_PERFCTR_RAS_2_LO               0x472
-#define A6XX_RBBM_PERFCTR_RAS_2_HI               0x473
-#define A6XX_RBBM_PERFCTR_RAS_3_LO               0x474
-#define A6XX_RBBM_PERFCTR_RAS_3_HI               0x475
-#define A6XX_RBBM_PERFCTR_UCHE_0_LO              0x476
-#define A6XX_RBBM_PERFCTR_UCHE_0_HI              0x477
-#define A6XX_RBBM_PERFCTR_UCHE_1_LO              0x478
-#define A6XX_RBBM_PERFCTR_UCHE_1_HI              0x479
-#define A6XX_RBBM_PERFCTR_UCHE_2_LO              0x47a
-#define A6XX_RBBM_PERFCTR_UCHE_2_HI              0x47b
-#define A6XX_RBBM_PERFCTR_UCHE_3_LO              0x47c
-#define A6XX_RBBM_PERFCTR_UCHE_3_HI              0x47d
-#define A6XX_RBBM_PERFCTR_UCHE_4_LO              0x47e
-#define A6XX_RBBM_PERFCTR_UCHE_4_HI              0x47f
-#define A6XX_RBBM_PERFCTR_UCHE_5_LO              0x480
-#define A6XX_RBBM_PERFCTR_UCHE_5_HI              0x481
-#define A6XX_RBBM_PERFCTR_UCHE_6_LO              0x482
-#define A6XX_RBBM_PERFCTR_UCHE_6_HI              0x483
-#define A6XX_RBBM_PERFCTR_UCHE_7_LO              0x484
-#define A6XX_RBBM_PERFCTR_UCHE_7_HI              0x485
-#define A6XX_RBBM_PERFCTR_UCHE_8_LO              0x486
-#define A6XX_RBBM_PERFCTR_UCHE_8_HI              0x487
-#define A6XX_RBBM_PERFCTR_UCHE_9_LO              0x488
-#define A6XX_RBBM_PERFCTR_UCHE_9_HI              0x489
-#define A6XX_RBBM_PERFCTR_UCHE_10_LO             0x48a
-#define A6XX_RBBM_PERFCTR_UCHE_10_HI             0x48b
-#define A6XX_RBBM_PERFCTR_UCHE_11_LO             0x48c
-#define A6XX_RBBM_PERFCTR_UCHE_11_HI             0x48d
-#define A6XX_RBBM_PERFCTR_TP_0_LO                0x48e
-#define A6XX_RBBM_PERFCTR_TP_0_HI                0x48f
-#define A6XX_RBBM_PERFCTR_TP_1_LO                0x490
-#define A6XX_RBBM_PERFCTR_TP_1_HI                0x491
-#define A6XX_RBBM_PERFCTR_TP_2_LO                0x492
-#define A6XX_RBBM_PERFCTR_TP_2_HI                0x493
-#define A6XX_RBBM_PERFCTR_TP_3_LO                0x494
-#define A6XX_RBBM_PERFCTR_TP_3_HI                0x495
-#define A6XX_RBBM_PERFCTR_TP_4_LO                0x496
-#define A6XX_RBBM_PERFCTR_TP_4_HI                0x497
-#define A6XX_RBBM_PERFCTR_TP_5_LO                0x498
-#define A6XX_RBBM_PERFCTR_TP_5_HI                0x499
-#define A6XX_RBBM_PERFCTR_TP_6_LO                0x49a
-#define A6XX_RBBM_PERFCTR_TP_6_HI                0x49b
-#define A6XX_RBBM_PERFCTR_TP_7_LO                0x49c
-#define A6XX_RBBM_PERFCTR_TP_7_HI                0x49d
-#define A6XX_RBBM_PERFCTR_TP_8_LO                0x49e
-#define A6XX_RBBM_PERFCTR_TP_8_HI                0x49f
-#define A6XX_RBBM_PERFCTR_TP_9_LO                0x4a0
-#define A6XX_RBBM_PERFCTR_TP_9_HI                0x4a1
-#define A6XX_RBBM_PERFCTR_TP_10_LO               0x4a2
-#define A6XX_RBBM_PERFCTR_TP_10_HI               0x4a3
-#define A6XX_RBBM_PERFCTR_TP_11_LO               0x4a4
-#define A6XX_RBBM_PERFCTR_TP_11_HI               0x4a5
-#define A6XX_RBBM_PERFCTR_SP_0_LO                0x4a6
-#define A6XX_RBBM_PERFCTR_SP_0_HI                0x4a7
-#define A6XX_RBBM_PERFCTR_SP_1_LO                0x4a8
-#define A6XX_RBBM_PERFCTR_SP_1_HI                0x4a9
-#define A6XX_RBBM_PERFCTR_SP_2_LO                0x4aa
-#define A6XX_RBBM_PERFCTR_SP_2_HI                0x4ab
-#define A6XX_RBBM_PERFCTR_SP_3_LO                0x4ac
-#define A6XX_RBBM_PERFCTR_SP_3_HI                0x4ad
-#define A6XX_RBBM_PERFCTR_SP_4_LO                0x4ae
-#define A6XX_RBBM_PERFCTR_SP_4_HI                0x4af
-#define A6XX_RBBM_PERFCTR_SP_5_LO                0x4b0
-#define A6XX_RBBM_PERFCTR_SP_5_HI                0x4b1
-#define A6XX_RBBM_PERFCTR_SP_6_LO                0x4b2
-#define A6XX_RBBM_PERFCTR_SP_6_HI                0x4b3
-#define A6XX_RBBM_PERFCTR_SP_7_LO                0x4b4
-#define A6XX_RBBM_PERFCTR_SP_7_HI                0x4b5
-#define A6XX_RBBM_PERFCTR_SP_8_LO                0x4b6
-#define A6XX_RBBM_PERFCTR_SP_8_HI                0x4b7
-#define A6XX_RBBM_PERFCTR_SP_9_LO                0x4b8
-#define A6XX_RBBM_PERFCTR_SP_9_HI                0x4b9
-#define A6XX_RBBM_PERFCTR_SP_10_LO               0x4ba
-#define A6XX_RBBM_PERFCTR_SP_10_HI               0x4bb
-#define A6XX_RBBM_PERFCTR_SP_11_LO               0x4bc
-#define A6XX_RBBM_PERFCTR_SP_11_HI               0x4bd
-#define A6XX_RBBM_PERFCTR_SP_12_LO               0x4be
-#define A6XX_RBBM_PERFCTR_SP_12_HI               0x4bf
-#define A6XX_RBBM_PERFCTR_SP_13_LO               0x4c0
-#define A6XX_RBBM_PERFCTR_SP_13_HI               0x4c1
-#define A6XX_RBBM_PERFCTR_SP_14_LO               0x4c2
-#define A6XX_RBBM_PERFCTR_SP_14_HI               0x4c3
-#define A6XX_RBBM_PERFCTR_SP_15_LO               0x4c4
-#define A6XX_RBBM_PERFCTR_SP_15_HI               0x4c5
-#define A6XX_RBBM_PERFCTR_SP_16_LO               0x4c6
-#define A6XX_RBBM_PERFCTR_SP_16_HI               0x4c7
-#define A6XX_RBBM_PERFCTR_SP_17_LO               0x4c8
-#define A6XX_RBBM_PERFCTR_SP_17_HI               0x4c9
-#define A6XX_RBBM_PERFCTR_SP_18_LO               0x4ca
-#define A6XX_RBBM_PERFCTR_SP_18_HI               0x4cb
-#define A6XX_RBBM_PERFCTR_SP_19_LO               0x4cc
-#define A6XX_RBBM_PERFCTR_SP_19_HI               0x4cd
-#define A6XX_RBBM_PERFCTR_SP_20_LO               0x4ce
-#define A6XX_RBBM_PERFCTR_SP_20_HI               0x4cf
-#define A6XX_RBBM_PERFCTR_SP_21_LO               0x4d0
-#define A6XX_RBBM_PERFCTR_SP_21_HI               0x4d1
-#define A6XX_RBBM_PERFCTR_SP_22_LO               0x4d2
-#define A6XX_RBBM_PERFCTR_SP_22_HI               0x4d3
-#define A6XX_RBBM_PERFCTR_SP_23_LO               0x4d4
-#define A6XX_RBBM_PERFCTR_SP_23_HI               0x4d5
-#define A6XX_RBBM_PERFCTR_RB_0_LO                0x4d6
-#define A6XX_RBBM_PERFCTR_RB_0_HI                0x4d7
-#define A6XX_RBBM_PERFCTR_RB_1_LO                0x4d8
-#define A6XX_RBBM_PERFCTR_RB_1_HI                0x4d9
-#define A6XX_RBBM_PERFCTR_RB_2_LO                0x4da
-#define A6XX_RBBM_PERFCTR_RB_2_HI                0x4db
-#define A6XX_RBBM_PERFCTR_RB_3_LO                0x4dc
-#define A6XX_RBBM_PERFCTR_RB_3_HI                0x4dd
-#define A6XX_RBBM_PERFCTR_RB_4_LO                0x4de
-#define A6XX_RBBM_PERFCTR_RB_4_HI                0x4df
-#define A6XX_RBBM_PERFCTR_RB_5_LO                0x4e0
-#define A6XX_RBBM_PERFCTR_RB_5_HI                0x4e1
-#define A6XX_RBBM_PERFCTR_RB_6_LO                0x4e2
-#define A6XX_RBBM_PERFCTR_RB_6_HI                0x4e3
-#define A6XX_RBBM_PERFCTR_RB_7_LO                0x4e4
-#define A6XX_RBBM_PERFCTR_RB_7_HI                0x4e5
-#define A6XX_RBBM_PERFCTR_VSC_0_LO               0x4e6
-#define A6XX_RBBM_PERFCTR_VSC_0_HI               0x4e7
-#define A6XX_RBBM_PERFCTR_VSC_1_LO               0x4e8
-#define A6XX_RBBM_PERFCTR_VSC_1_HI               0x4e9
-#define A6XX_RBBM_PERFCTR_LRZ_0_LO               0x4ea
-#define A6XX_RBBM_PERFCTR_LRZ_0_HI               0x4eb
-#define A6XX_RBBM_PERFCTR_LRZ_1_LO               0x4ec
-#define A6XX_RBBM_PERFCTR_LRZ_1_HI               0x4ed
-#define A6XX_RBBM_PERFCTR_LRZ_2_LO               0x4ee
-#define A6XX_RBBM_PERFCTR_LRZ_2_HI               0x4ef
-#define A6XX_RBBM_PERFCTR_LRZ_3_LO               0x4f0
-#define A6XX_RBBM_PERFCTR_LRZ_3_HI               0x4f1
-#define A6XX_RBBM_PERFCTR_CMP_0_LO               0x4f2
-#define A6XX_RBBM_PERFCTR_CMP_0_HI               0x4f3
-#define A6XX_RBBM_PERFCTR_CMP_1_LO               0x4f4
-#define A6XX_RBBM_PERFCTR_CMP_1_HI               0x4f5
-#define A6XX_RBBM_PERFCTR_CMP_2_LO               0x4f6
-#define A6XX_RBBM_PERFCTR_CMP_2_HI               0x4f7
-#define A6XX_RBBM_PERFCTR_CMP_3_LO               0x4f8
-#define A6XX_RBBM_PERFCTR_CMP_3_HI               0x4f9
-#define A6XX_RBBM_PERFCTR_CNTL                   0x500
-#define A6XX_RBBM_PERFCTR_LOAD_CMD0              0x501
-#define A6XX_RBBM_PERFCTR_LOAD_CMD1              0x502
-#define A6XX_RBBM_PERFCTR_LOAD_CMD2              0x503
-#define A6XX_RBBM_PERFCTR_LOAD_CMD3              0x504
-#define A6XX_RBBM_PERFCTR_LOAD_VALUE_LO          0x505
-#define A6XX_RBBM_PERFCTR_LOAD_VALUE_HI          0x506
-#define A6XX_RBBM_PERFCTR_RBBM_SEL_0             0x507
-#define A6XX_RBBM_PERFCTR_RBBM_SEL_1             0x508
-#define A6XX_RBBM_PERFCTR_RBBM_SEL_2             0x509
-#define A6XX_RBBM_PERFCTR_RBBM_SEL_3             0x50A
-#define A6XX_RBBM_PERFCTR_GPU_BUSY_MASKED        0x50B
-
-#define A6XX_RBBM_ISDB_CNT                       0x533
-
-#define A6XX_RBBM_SECVID_TRUST_CNTL              0xF400
-#define A6XX_RBBM_SECVID_TSB_TRUSTED_BASE_LO     0xF800
-#define A6XX_RBBM_SECVID_TSB_TRUSTED_BASE_HI     0xF801
-#define A6XX_RBBM_SECVID_TSB_TRUSTED_SIZE        0xF802
-#define A6XX_RBBM_SECVID_TSB_CNTL                0xF803
-#define A6XX_RBBM_SECVID_TSB_ADDR_MODE_CNTL      0xF810
-
-#define A6XX_RBBM_VBIF_CLIENT_QOS_CNTL   0x00010
-#define A6XX_RBBM_GPR0_CNTL              0x00018
-#define A6XX_RBBM_INTERFACE_HANG_INT_CNTL 0x0001f
-#define A6XX_RBBM_INT_CLEAR_CMD          0x00037
-#define A6XX_RBBM_INT_0_MASK             0x00038
-#define A6XX_RBBM_SP_HYST_CNT            0x00042
-#define A6XX_RBBM_SW_RESET_CMD           0x00043
-#define A6XX_RBBM_RAC_THRESHOLD_CNT      0x00044
-#define A6XX_RBBM_BLOCK_SW_RESET_CMD     0x00045
-#define A6XX_RBBM_BLOCK_SW_RESET_CMD2    0x00046
-#define A6XX_RBBM_CLOCK_CNTL             0x000ae
-#define A6XX_RBBM_CLOCK_CNTL_SP0         0x000b0
-#define A6XX_RBBM_CLOCK_CNTL_SP1         0x000b1
-#define A6XX_RBBM_CLOCK_CNTL_SP2         0x000b2
-#define A6XX_RBBM_CLOCK_CNTL_SP3         0x000b3
-#define A6XX_RBBM_CLOCK_CNTL2_SP0        0x000b4
-#define A6XX_RBBM_CLOCK_CNTL2_SP1        0x000b5
-#define A6XX_RBBM_CLOCK_CNTL2_SP2        0x000b6
-#define A6XX_RBBM_CLOCK_CNTL2_SP3        0x000b7
-#define A6XX_RBBM_CLOCK_DELAY_SP0        0x000b8
-#define A6XX_RBBM_CLOCK_DELAY_SP1        0x000b9
-#define A6XX_RBBM_CLOCK_DELAY_SP2        0x000ba
-#define A6XX_RBBM_CLOCK_DELAY_SP3        0x000bb
-#define A6XX_RBBM_CLOCK_HYST_SP0         0x000bc
-#define A6XX_RBBM_CLOCK_HYST_SP1         0x000bd
-#define A6XX_RBBM_CLOCK_HYST_SP2         0x000be
-#define A6XX_RBBM_CLOCK_HYST_SP3         0x000bf
-#define A6XX_RBBM_CLOCK_CNTL_TP0         0x000c0
-#define A6XX_RBBM_CLOCK_CNTL_TP1         0x000c1
-#define A6XX_RBBM_CLOCK_CNTL_TP2         0x000c2
-#define A6XX_RBBM_CLOCK_CNTL_TP3         0x000c3
-#define A6XX_RBBM_CLOCK_CNTL2_TP0        0x000c4
-#define A6XX_RBBM_CLOCK_CNTL2_TP1        0x000c5
-#define A6XX_RBBM_CLOCK_CNTL2_TP2        0x000c6
-#define A6XX_RBBM_CLOCK_CNTL2_TP3        0x000c7
-#define A6XX_RBBM_CLOCK_CNTL3_TP0        0x000c8
-#define A6XX_RBBM_CLOCK_CNTL3_TP1        0x000c9
-#define A6XX_RBBM_CLOCK_CNTL3_TP2        0x000ca
-#define A6XX_RBBM_CLOCK_CNTL3_TP3        0x000cb
-#define A6XX_RBBM_CLOCK_CNTL4_TP0        0x000cc
-#define A6XX_RBBM_CLOCK_CNTL4_TP1        0x000cd
-#define A6XX_RBBM_CLOCK_CNTL4_TP2        0x000ce
-#define A6XX_RBBM_CLOCK_CNTL4_TP3        0x000cf
-#define A6XX_RBBM_CLOCK_DELAY_TP0        0x000d0
-#define A6XX_RBBM_CLOCK_DELAY_TP1        0x000d1
-#define A6XX_RBBM_CLOCK_DELAY_TP2        0x000d2
-#define A6XX_RBBM_CLOCK_DELAY_TP3        0x000d3
-#define A6XX_RBBM_CLOCK_DELAY2_TP0       0x000d4
-#define A6XX_RBBM_CLOCK_DELAY2_TP1       0x000d5
-#define A6XX_RBBM_CLOCK_DELAY2_TP2       0x000d6
-#define A6XX_RBBM_CLOCK_DELAY2_TP3       0x000d7
-#define A6XX_RBBM_CLOCK_DELAY3_TP0       0x000d8
-#define A6XX_RBBM_CLOCK_DELAY3_TP1       0x000d9
-#define A6XX_RBBM_CLOCK_DELAY3_TP2       0x000da
-#define A6XX_RBBM_CLOCK_DELAY3_TP3       0x000db
-#define A6XX_RBBM_CLOCK_DELAY4_TP0       0x000dc
-#define A6XX_RBBM_CLOCK_DELAY4_TP1       0x000dd
-#define A6XX_RBBM_CLOCK_DELAY4_TP2       0x000de
-#define A6XX_RBBM_CLOCK_DELAY4_TP3       0x000df
-#define A6XX_RBBM_CLOCK_HYST_TP0         0x000e0
-#define A6XX_RBBM_CLOCK_HYST_TP1         0x000e1
-#define A6XX_RBBM_CLOCK_HYST_TP2         0x000e2
-#define A6XX_RBBM_CLOCK_HYST_TP3         0x000e3
-#define A6XX_RBBM_CLOCK_HYST2_TP0        0x000e4
-#define A6XX_RBBM_CLOCK_HYST2_TP1        0x000e5
-#define A6XX_RBBM_CLOCK_HYST2_TP2        0x000e6
-#define A6XX_RBBM_CLOCK_HYST2_TP3        0x000e7
-#define A6XX_RBBM_CLOCK_HYST3_TP0        0x000e8
-#define A6XX_RBBM_CLOCK_HYST3_TP1        0x000e9
-#define A6XX_RBBM_CLOCK_HYST3_TP2        0x000ea
-#define A6XX_RBBM_CLOCK_HYST3_TP3        0x000eb
-#define A6XX_RBBM_CLOCK_HYST4_TP0        0x000ec
-#define A6XX_RBBM_CLOCK_HYST4_TP1        0x000ed
-#define A6XX_RBBM_CLOCK_HYST4_TP2        0x000ee
-#define A6XX_RBBM_CLOCK_HYST4_TP3        0x000ef
-#define A6XX_RBBM_CLOCK_CNTL_RB0         0x000f0
-#define A6XX_RBBM_CLOCK_CNTL_RB1         0x000f1
-#define A6XX_RBBM_CLOCK_CNTL_RB2         0x000f2
-#define A6XX_RBBM_CLOCK_CNTL_RB3         0x000f3
-#define A6XX_RBBM_CLOCK_CNTL2_RB0        0x000f4
-#define A6XX_RBBM_CLOCK_CNTL2_RB1        0x000f5
-#define A6XX_RBBM_CLOCK_CNTL2_RB2        0x000f6
-#define A6XX_RBBM_CLOCK_CNTL2_RB3        0x000f7
-#define A6XX_RBBM_CLOCK_CNTL_CCU0        0x000f8
-#define A6XX_RBBM_CLOCK_CNTL_CCU1        0x000f9
-#define A6XX_RBBM_CLOCK_CNTL_CCU2        0x000fa
-#define A6XX_RBBM_CLOCK_CNTL_CCU3        0x000fb
-#define A6XX_RBBM_CLOCK_HYST_RB_CCU0     0x00100
-#define A6XX_RBBM_CLOCK_HYST_RB_CCU1     0x00101
-#define A6XX_RBBM_CLOCK_HYST_RB_CCU2     0x00102
-#define A6XX_RBBM_CLOCK_HYST_RB_CCU3     0x00103
-#define A6XX_RBBM_CLOCK_CNTL_RAC         0x00104
-#define A6XX_RBBM_CLOCK_CNTL2_RAC        0x00105
-#define A6XX_RBBM_CLOCK_DELAY_RAC        0x00106
-#define A6XX_RBBM_CLOCK_HYST_RAC         0x00107
-#define A6XX_RBBM_CLOCK_CNTL_TSE_RAS_RBBM 0x00108
-#define A6XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM 0x00109
-#define A6XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM 0x0010a
-#define A6XX_RBBM_CLOCK_CNTL_UCHE        0x0010b
-#define A6XX_RBBM_CLOCK_CNTL2_UCHE       0x0010c
-#define A6XX_RBBM_CLOCK_CNTL3_UCHE       0x0010d
-#define A6XX_RBBM_CLOCK_CNTL4_UCHE       0x0010e
-#define A6XX_RBBM_CLOCK_DELAY_UCHE       0x0010f
-#define A6XX_RBBM_CLOCK_HYST_UCHE        0x00110
-#define A6XX_RBBM_CLOCK_MODE_VFD         0x00111
-#define A6XX_RBBM_CLOCK_DELAY_VFD        0x00112
-#define A6XX_RBBM_CLOCK_HYST_VFD         0x00113
-#define A6XX_RBBM_CLOCK_MODE_GPC         0x00114
-#define A6XX_RBBM_CLOCK_DELAY_GPC        0x00115
-#define A6XX_RBBM_CLOCK_HYST_GPC         0x00116
-#define A6XX_RBBM_CLOCK_DELAY_HLSQ_2	 0x00117
-#define A6XX_RBBM_CLOCK_CNTL_GMU_GX      0x00118
-#define A6XX_RBBM_CLOCK_DELAY_GMU_GX     0x00119
-#define A6XX_RBBM_CLOCK_HYST_GMU_GX      0x0011a
-#define A6XX_RBBM_CLOCK_MODE_HLSQ	 0x0011b
-#define A6XX_RBBM_CLOCK_DELAY_HLSQ       0x0011c
-
-/* DBGC_CFG registers */
-#define A6XX_DBGC_CFG_DBGBUS_SEL_A                  0x600
-#define A6XX_DBGC_CFG_DBGBUS_SEL_B                  0x601
-#define A6XX_DBGC_CFG_DBGBUS_SEL_C                  0x602
-#define A6XX_DBGC_CFG_DBGBUS_SEL_D                  0x603
-#define A6XX_DBGC_CFG_DBGBUS_SEL_PING_INDEX_SHIFT   0x0
-#define A6XX_DBGC_CFG_DBGBUS_SEL_PING_BLK_SEL_SHIFT 0x8
-#define A6XX_DBGC_CFG_DBGBUS_CNTLT                  0x604
-#define A6XX_DBGC_CFG_DBGBUS_CNTLT_TRACEEN_SHIFT    0x0
-#define A6XX_DBGC_CFG_DBGBUS_CNTLT_GRANU_SHIFT      0xC
-#define A6XX_DBGC_CFG_DBGBUS_CNTLT_SEGT_SHIFT       0x1C
-#define A6XX_DBGC_CFG_DBGBUS_CNTLM                  0x605
-#define A6XX_DBGC_CFG_DBGBUS_CTLTM_ENABLE_SHIFT     0x18
-#define A6XX_DBGC_CFG_DBGBUS_OPL                    0x606
-#define A6XX_DBGC_CFG_DBGBUS_OPE                    0x607
-#define A6XX_DBGC_CFG_DBGBUS_IVTL_0                 0x608
-#define A6XX_DBGC_CFG_DBGBUS_IVTL_1                 0x609
-#define A6XX_DBGC_CFG_DBGBUS_IVTL_2                 0x60a
-#define A6XX_DBGC_CFG_DBGBUS_IVTL_3                 0x60b
-#define A6XX_DBGC_CFG_DBGBUS_MASKL_0                0x60c
-#define A6XX_DBGC_CFG_DBGBUS_MASKL_1                0x60d
-#define A6XX_DBGC_CFG_DBGBUS_MASKL_2                0x60e
-#define A6XX_DBGC_CFG_DBGBUS_MASKL_3                0x60f
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL_0                0x610
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL_1                0x611
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL0_SHIFT           0x0
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL1_SHIFT           0x4
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL2_SHIFT           0x8
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL3_SHIFT           0xC
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL4_SHIFT           0x10
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL5_SHIFT           0x14
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL6_SHIFT           0x18
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL7_SHIFT           0x1C
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL8_SHIFT           0x0
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL9_SHIFT           0x4
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL10_SHIFT          0x8
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL11_SHIFT          0xC
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL12_SHIFT          0x10
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL13_SHIFT          0x14
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL14_SHIFT          0x18
-#define A6XX_DBGC_CFG_DBGBUS_BYTEL15_SHIFT          0x1C
-#define A6XX_DBGC_CFG_DBGBUS_IVTE_0                 0x612
-#define A6XX_DBGC_CFG_DBGBUS_IVTE_1                 0x613
-#define A6XX_DBGC_CFG_DBGBUS_IVTE_2                 0x614
-#define A6XX_DBGC_CFG_DBGBUS_IVTE_3                 0x615
-#define A6XX_DBGC_CFG_DBGBUS_MASKE_0                0x616
-#define A6XX_DBGC_CFG_DBGBUS_MASKE_1                0x617
-#define A6XX_DBGC_CFG_DBGBUS_MASKE_2                0x618
-#define A6XX_DBGC_CFG_DBGBUS_MASKE_3                0x619
-#define A6XX_DBGC_CFG_DBGBUS_NIBBLEE                0x61a
-#define A6XX_DBGC_CFG_DBGBUS_PTRC0                  0x61b
-#define A6XX_DBGC_CFG_DBGBUS_PTRC1                  0x61c
-#define A6XX_DBGC_CFG_DBGBUS_LOADREG                0x61d
-#define A6XX_DBGC_CFG_DBGBUS_IDX                    0x61e
-#define A6XX_DBGC_CFG_DBGBUS_CLRC                   0x61f
-#define A6XX_DBGC_CFG_DBGBUS_LOADIVT                0x620
-#define A6XX_DBGC_VBIF_DBG_CNTL                     0x621
-#define A6XX_DBGC_DBG_LO_HI_GPIO                    0x622
-#define A6XX_DBGC_EXT_TRACE_BUS_CNTL                0x623
-#define A6XX_DBGC_READ_AHB_THROUGH_DBG              0x624
-#define A6XX_DBGC_CFG_DBGBUS_TRACE_BUF1             0x62f
-#define A6XX_DBGC_CFG_DBGBUS_TRACE_BUF2             0x630
-#define A6XX_DBGC_EVT_CFG                           0x640
-#define A6XX_DBGC_EVT_INTF_SEL_0                    0x641
-#define A6XX_DBGC_EVT_INTF_SEL_1                    0x642
-#define A6XX_DBGC_PERF_ATB_CFG                      0x643
-#define A6XX_DBGC_PERF_ATB_COUNTER_SEL_0            0x644
-#define A6XX_DBGC_PERF_ATB_COUNTER_SEL_1            0x645
-#define A6XX_DBGC_PERF_ATB_COUNTER_SEL_2            0x646
-#define A6XX_DBGC_PERF_ATB_COUNTER_SEL_3            0x647
-#define A6XX_DBGC_PERF_ATB_TRIG_INTF_SEL_0          0x648
-#define A6XX_DBGC_PERF_ATB_TRIG_INTF_SEL_1          0x649
-#define A6XX_DBGC_PERF_ATB_DRAIN_CMD                0x64a
-#define A6XX_DBGC_ECO_CNTL                          0x650
-#define A6XX_DBGC_AHB_DBG_CNTL                      0x651
-
-/* VSC registers */
-#define A6XX_VSC_PERFCTR_VSC_SEL_0          0xCD8
-#define A6XX_VSC_PERFCTR_VSC_SEL_1          0xCD9
-
-/* GRAS registers */
-#define A6XX_GRAS_ADDR_MODE_CNTL            0x8601
-#define A6XX_GRAS_PERFCTR_TSE_SEL_0         0x8610
-#define A6XX_GRAS_PERFCTR_TSE_SEL_1         0x8611
-#define A6XX_GRAS_PERFCTR_TSE_SEL_2         0x8612
-#define A6XX_GRAS_PERFCTR_TSE_SEL_3         0x8613
-#define A6XX_GRAS_PERFCTR_RAS_SEL_0         0x8614
-#define A6XX_GRAS_PERFCTR_RAS_SEL_1         0x8615
-#define A6XX_GRAS_PERFCTR_RAS_SEL_2         0x8616
-#define A6XX_GRAS_PERFCTR_RAS_SEL_3         0x8617
-#define A6XX_GRAS_PERFCTR_LRZ_SEL_0         0x8618
-#define A6XX_GRAS_PERFCTR_LRZ_SEL_1         0x8619
-#define A6XX_GRAS_PERFCTR_LRZ_SEL_2         0x861A
-#define A6XX_GRAS_PERFCTR_LRZ_SEL_3         0x861B
-
-/* RB registers */
-#define A6XX_RB_ADDR_MODE_CNTL              0x8E05
-#define A6XX_RB_NC_MODE_CNTL                0x8E08
-#define A6XX_RB_PERFCTR_RB_SEL_0            0x8E10
-#define A6XX_RB_PERFCTR_RB_SEL_1            0x8E11
-#define A6XX_RB_PERFCTR_RB_SEL_2            0x8E12
-#define A6XX_RB_PERFCTR_RB_SEL_3            0x8E13
-#define A6XX_RB_PERFCTR_RB_SEL_4            0x8E14
-#define A6XX_RB_PERFCTR_RB_SEL_5            0x8E15
-#define A6XX_RB_PERFCTR_RB_SEL_6            0x8E16
-#define A6XX_RB_PERFCTR_RB_SEL_7            0x8E17
-#define A6XX_RB_PERFCTR_CCU_SEL_0           0x8E18
-#define A6XX_RB_PERFCTR_CCU_SEL_1           0x8E19
-#define A6XX_RB_PERFCTR_CCU_SEL_2           0x8E1A
-#define A6XX_RB_PERFCTR_CCU_SEL_3           0x8E1B
-#define A6XX_RB_PERFCTR_CCU_SEL_4           0x8E1C
-#define A6XX_RB_PERFCTR_CMP_SEL_0           0x8E2C
-#define A6XX_RB_PERFCTR_CMP_SEL_1           0x8E2D
-#define A6XX_RB_PERFCTR_CMP_SEL_2           0x8E2E
-#define A6XX_RB_PERFCTR_CMP_SEL_3           0x8E2F
-#define A6XX_RB_RB_SUB_BLOCK_SEL_CNTL_HOST  0x8E3B
-#define A6XX_RB_RB_SUB_BLOCK_SEL_CNTL_CD    0x8E3D
-#define A6XX_RB_CONTEXT_SWITCH_GMEM_SAVE_RESTORE 0x8E50
-
-/* PC registers */
-#define A6XX_PC_DBG_ECO_CNTL                0x9E00
-#define A6XX_PC_ADDR_MODE_CNTL              0x9E01
-#define A6XX_PC_PERFCTR_PC_SEL_0            0x9E34
-#define A6XX_PC_PERFCTR_PC_SEL_1            0x9E35
-#define A6XX_PC_PERFCTR_PC_SEL_2            0x9E36
-#define A6XX_PC_PERFCTR_PC_SEL_3            0x9E37
-#define A6XX_PC_PERFCTR_PC_SEL_4            0x9E38
-#define A6XX_PC_PERFCTR_PC_SEL_5            0x9E39
-#define A6XX_PC_PERFCTR_PC_SEL_6            0x9E3A
-#define A6XX_PC_PERFCTR_PC_SEL_7            0x9E3B
-
-/* HLSQ registers */
-#define A6XX_HLSQ_ADDR_MODE_CNTL            0xBE05
-#define A6XX_HLSQ_PERFCTR_HLSQ_SEL_0        0xBE10
-#define A6XX_HLSQ_PERFCTR_HLSQ_SEL_1        0xBE11
-#define A6XX_HLSQ_PERFCTR_HLSQ_SEL_2        0xBE12
-#define A6XX_HLSQ_PERFCTR_HLSQ_SEL_3        0xBE13
-#define A6XX_HLSQ_PERFCTR_HLSQ_SEL_4        0xBE14
-#define A6XX_HLSQ_PERFCTR_HLSQ_SEL_5        0xBE15
-#define A6XX_HLSQ_DBG_AHB_READ_APERTURE     0xC800
-#define A6XX_HLSQ_DBG_READ_SEL              0xD000
-
-/* VFD registers */
-#define A6XX_VFD_ADDR_MODE_CNTL             0xA601
-#define A6XX_VFD_PERFCTR_VFD_SEL_0          0xA610
-#define A6XX_VFD_PERFCTR_VFD_SEL_1          0xA611
-#define A6XX_VFD_PERFCTR_VFD_SEL_2          0xA612
-#define A6XX_VFD_PERFCTR_VFD_SEL_3          0xA613
-#define A6XX_VFD_PERFCTR_VFD_SEL_4          0xA614
-#define A6XX_VFD_PERFCTR_VFD_SEL_5          0xA615
-#define A6XX_VFD_PERFCTR_VFD_SEL_6          0xA616
-#define A6XX_VFD_PERFCTR_VFD_SEL_7          0xA617
-
-/* VPC registers */
-#define A6XX_VPC_ADDR_MODE_CNTL             0x9601
-#define A6XX_VPC_PERFCTR_VPC_SEL_0          0x9604
-#define A6XX_VPC_PERFCTR_VPC_SEL_1          0x9605
-#define A6XX_VPC_PERFCTR_VPC_SEL_2          0x9606
-#define A6XX_VPC_PERFCTR_VPC_SEL_3          0x9607
-#define A6XX_VPC_PERFCTR_VPC_SEL_4          0x9608
-#define A6XX_VPC_PERFCTR_VPC_SEL_5          0x9609
-
-/* UCHE registers */
-#define A6XX_UCHE_ADDR_MODE_CNTL            0xE00
-#define A6XX_UCHE_MODE_CNTL                 0xE01
-#define A6XX_UCHE_WRITE_RANGE_MAX_LO        0xE05
-#define A6XX_UCHE_WRITE_RANGE_MAX_HI        0xE06
-#define A6XX_UCHE_WRITE_THRU_BASE_LO        0xE07
-#define A6XX_UCHE_WRITE_THRU_BASE_HI        0xE08
-#define A6XX_UCHE_TRAP_BASE_LO              0xE09
-#define A6XX_UCHE_TRAP_BASE_HI              0xE0A
-#define A6XX_UCHE_GMEM_RANGE_MIN_LO         0xE0B
-#define A6XX_UCHE_GMEM_RANGE_MIN_HI         0xE0C
-#define A6XX_UCHE_GMEM_RANGE_MAX_LO         0xE0D
-#define A6XX_UCHE_GMEM_RANGE_MAX_HI         0xE0E
-#define A6XX_UCHE_CACHE_WAYS                0xE17
-#define A6XX_UCHE_FILTER_CNTL               0xE18
-#define A6XX_UCHE_CLIENT_PF                 0xE19
-#define A6XX_UCHE_CLIENT_PF_CLIENT_ID_MASK  0x7
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_0        0xE1C
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_1        0xE1D
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_2        0xE1E
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_3        0xE1F
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_4        0xE20
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_5        0xE21
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_6        0xE22
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_7        0xE23
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_8        0xE24
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_9        0xE25
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_10       0xE26
-#define A6XX_UCHE_PERFCTR_UCHE_SEL_11       0xE27
-#define A6XX_UCHE_GBIF_GX_CONFIG            0xE3A
-
-/* SP registers */
-#define A6XX_SP_ADDR_MODE_CNTL              0xAE01
-#define A6XX_SP_NC_MODE_CNTL                0xAE02
-#define A6XX_SP_PERFCTR_SP_SEL_0            0xAE10
-#define A6XX_SP_PERFCTR_SP_SEL_1            0xAE11
-#define A6XX_SP_PERFCTR_SP_SEL_2            0xAE12
-#define A6XX_SP_PERFCTR_SP_SEL_3            0xAE13
-#define A6XX_SP_PERFCTR_SP_SEL_4            0xAE14
-#define A6XX_SP_PERFCTR_SP_SEL_5            0xAE15
-#define A6XX_SP_PERFCTR_SP_SEL_6            0xAE16
-#define A6XX_SP_PERFCTR_SP_SEL_7            0xAE17
-#define A6XX_SP_PERFCTR_SP_SEL_8            0xAE18
-#define A6XX_SP_PERFCTR_SP_SEL_9            0xAE19
-#define A6XX_SP_PERFCTR_SP_SEL_10           0xAE1A
-#define A6XX_SP_PERFCTR_SP_SEL_11           0xAE1B
-#define A6XX_SP_PERFCTR_SP_SEL_12           0xAE1C
-#define A6XX_SP_PERFCTR_SP_SEL_13           0xAE1D
-#define A6XX_SP_PERFCTR_SP_SEL_14           0xAE1E
-#define A6XX_SP_PERFCTR_SP_SEL_15           0xAE1F
-#define A6XX_SP_PERFCTR_SP_SEL_16           0xAE20
-#define A6XX_SP_PERFCTR_SP_SEL_17           0xAE21
-#define A6XX_SP_PERFCTR_SP_SEL_18           0xAE22
-#define A6XX_SP_PERFCTR_SP_SEL_19           0xAE23
-#define A6XX_SP_PERFCTR_SP_SEL_20           0xAE24
-#define A6XX_SP_PERFCTR_SP_SEL_21           0xAE25
-#define A6XX_SP_PERFCTR_SP_SEL_22           0xAE26
-#define A6XX_SP_PERFCTR_SP_SEL_23           0xAE27
-
-/* TP registers */
-#define A6XX_TPL1_ADDR_MODE_CNTL            0xB601
-#define A6XX_TPL1_NC_MODE_CNTL              0xB604
-#define A6XX_TPL1_PERFCTR_TP_SEL_0          0xB610
-#define A6XX_TPL1_PERFCTR_TP_SEL_1          0xB611
-#define A6XX_TPL1_PERFCTR_TP_SEL_2          0xB612
-#define A6XX_TPL1_PERFCTR_TP_SEL_3          0xB613
-#define A6XX_TPL1_PERFCTR_TP_SEL_4          0xB614
-#define A6XX_TPL1_PERFCTR_TP_SEL_5          0xB615
-#define A6XX_TPL1_PERFCTR_TP_SEL_6          0xB616
-#define A6XX_TPL1_PERFCTR_TP_SEL_7          0xB617
-#define A6XX_TPL1_PERFCTR_TP_SEL_8          0xB618
-#define A6XX_TPL1_PERFCTR_TP_SEL_9          0xB619
-#define A6XX_TPL1_PERFCTR_TP_SEL_10         0xB61A
-#define A6XX_TPL1_PERFCTR_TP_SEL_11         0xB61B
-
-/* VBIF registers */
-#define A6XX_VBIF_VERSION                       0x3000
-#define A6XX_VBIF_CLKON                         0x3001
-#define A6XX_VBIF_CLKON_FORCE_ON_TESTBUS_MASK   0x1
-#define A6XX_VBIF_CLKON_FORCE_ON_TESTBUS_SHIFT  0x1
-#define A6XX_VBIF_GATE_OFF_WRREQ_EN             0x302A
-#define A6XX_VBIF_XIN_HALT_CTRL0                0x3080
-#define A6XX_VBIF_XIN_HALT_CTRL0_MASK           0xF
-#define A6XX_VBIF_XIN_HALT_CTRL1                0x3081
-#define A6XX_VBIF_TEST_BUS_OUT_CTRL             0x3084
-#define A6XX_VBIF_TEST_BUS_OUT_CTRL_EN_MASK     0x1
-#define A6XX_VBIF_TEST_BUS_OUT_CTRL_EN_SHIFT    0x0
-#define A6XX_VBIF_TEST_BUS1_CTRL0               0x3085
-#define A6XX_VBIF_TEST_BUS1_CTRL1               0x3086
-#define A6XX_VBIF_TEST_BUS1_CTRL1_DATA_SEL_MASK 0xF
-#define A6XX_VBIF_TEST_BUS1_CTRL1_DATA_SEL_SHIFT 0x0
-#define A6XX_VBIF_TEST_BUS2_CTRL0               0x3087
-#define A6XX_VBIF_TEST_BUS2_CTRL1               0x3088
-#define A6XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_MASK 0x1FF
-#define A6XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_SHIFT 0x0
-#define A6XX_VBIF_TEST_BUS_OUT                  0x308C
-#define A6XX_VBIF_PERF_CNT_SEL0                 0x30d0
-#define A6XX_VBIF_PERF_CNT_SEL1                 0x30d1
-#define A6XX_VBIF_PERF_CNT_SEL2                 0x30d2
-#define A6XX_VBIF_PERF_CNT_SEL3                 0x30d3
-#define A6XX_VBIF_PERF_CNT_LOW0                 0x30d8
-#define A6XX_VBIF_PERF_CNT_LOW1                 0x30d9
-#define A6XX_VBIF_PERF_CNT_LOW2                 0x30da
-#define A6XX_VBIF_PERF_CNT_LOW3                 0x30db
-#define A6XX_VBIF_PERF_CNT_HIGH0                0x30e0
-#define A6XX_VBIF_PERF_CNT_HIGH1                0x30e1
-#define A6XX_VBIF_PERF_CNT_HIGH2                0x30e2
-#define A6XX_VBIF_PERF_CNT_HIGH3                0x30e3
-#define A6XX_VBIF_PERF_PWR_CNT_EN0              0x3100
-#define A6XX_VBIF_PERF_PWR_CNT_EN1              0x3101
-#define A6XX_VBIF_PERF_PWR_CNT_EN2              0x3102
-#define A6XX_VBIF_PERF_PWR_CNT_LOW0             0x3110
-#define A6XX_VBIF_PERF_PWR_CNT_LOW1             0x3111
-#define A6XX_VBIF_PERF_PWR_CNT_LOW2             0x3112
-#define A6XX_VBIF_PERF_PWR_CNT_HIGH0            0x3118
-#define A6XX_VBIF_PERF_PWR_CNT_HIGH1            0x3119
-#define A6XX_VBIF_PERF_PWR_CNT_HIGH2            0x311a
-
-/* GBIF countables */
-#define GBIF_AXI0_READ_DATA_TOTAL_BEATS    34
-#define GBIF_AXI1_READ_DATA_TOTAL_BEATS    35
-#define GBIF_AXI0_WRITE_DATA_TOTAL_BEATS   46
-#define GBIF_AXI1_WRITE_DATA_TOTAL_BEATS   47
-
-/* GBIF registers */
-#define A6XX_GBIF_HALT                    0x3c45
-#define A6XX_GBIF_HALT_ACK                0x3c46
-#define A6XX_GBIF_HALT_MASK               0x2
-
-#define A6XX_GBIF_PERF_PWR_CNT_EN         0x3cc0
-#define A6XX_GBIF_PERF_CNT_SEL            0x3cc2
-#define A6XX_GBIF_PERF_CNT_LOW0           0x3cc4
-#define A6XX_GBIF_PERF_CNT_LOW1           0x3cc5
-#define A6XX_GBIF_PERF_CNT_LOW2           0x3cc6
-#define A6XX_GBIF_PERF_CNT_LOW3           0x3cc7
-#define A6XX_GBIF_PERF_CNT_HIGH0          0x3cc8
-#define A6XX_GBIF_PERF_CNT_HIGH1          0x3cc9
-#define A6XX_GBIF_PERF_CNT_HIGH2          0x3cca
-#define A6XX_GBIF_PERF_CNT_HIGH3          0x3ccb
-#define A6XX_GBIF_PWR_CNT_LOW0            0x3ccc
-#define A6XX_GBIF_PWR_CNT_LOW1            0x3ccd
-#define A6XX_GBIF_PWR_CNT_LOW2            0x3cce
-#define A6XX_GBIF_PWR_CNT_HIGH0           0x3ccf
-#define A6XX_GBIF_PWR_CNT_HIGH1           0x3cd0
-#define A6XX_GBIF_PWR_CNT_HIGH2           0x3cd1
-
-
-/* CX_DBGC_CFG registers */
-#define A6XX_CX_DBGC_CFG_DBGBUS_SEL_A                   0x18400
-#define A6XX_CX_DBGC_CFG_DBGBUS_SEL_B                   0x18401
-#define A6XX_CX_DBGC_CFG_DBGBUS_SEL_C                   0x18402
-#define A6XX_CX_DBGC_CFG_DBGBUS_SEL_D                   0x18403
-#define A6XX_CX_DBGC_CFG_DBGBUS_SEL_PING_INDEX_SHIFT    0x0
-#define A6XX_CX_DBGC_CFG_DBGBUS_SEL_PING_BLK_SEL_SHIFT  0x8
-#define A6XX_CX_DBGC_CFG_DBGBUS_CNTLT                   0x18404
-#define A6XX_CX_DBGC_CFG_DBGBUS_CNTLT_TRACEEN_SHIFT     0x0
-#define A6XX_CX_DBGC_CFG_DBGBUS_CNTLT_GRANU_SHIFT       0xC
-#define A6XX_CX_DBGC_CFG_DBGBUS_CNTLT_SEGT_SHIFT        0x1C
-#define A6XX_CX_DBGC_CFG_DBGBUS_CNTLM                   0x18405
-#define A6XX_CX_DBGC_CFG_DBGBUS_CNTLM_ENABLE_SHIFT      0x18
-#define A6XX_CX_DBGC_CFG_DBGBUS_OPL                     0x18406
-#define A6XX_CX_DBGC_CFG_DBGBUS_OPE                     0x18407
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTL_0                  0x18408
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTL_1                  0x18409
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTL_2                  0x1840A
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTL_3                  0x1840B
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKL_0                 0x1840C
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKL_1                 0x1840D
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKL_2                 0x1840E
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKL_3                 0x1840F
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_0                 0x18410
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_1                 0x18411
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL0_SHIFT            0x0
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL1_SHIFT            0x4
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL2_SHIFT            0x8
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL3_SHIFT            0xC
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL4_SHIFT            0x10
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL5_SHIFT            0x14
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL6_SHIFT            0x18
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL7_SHIFT            0x1C
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL8_SHIFT            0x0
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL9_SHIFT            0x4
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL10_SHIFT           0x8
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL11_SHIFT           0xC
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL12_SHIFT           0x10
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL13_SHIFT           0x14
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL14_SHIFT           0x18
-#define A6XX_CX_DBGC_CFG_DBGBUS_BYTEL15_SHIFT           0x1C
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTE_0                  0x18412
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTE_1                  0x18413
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTE_2                  0x18414
-#define A6XX_CX_DBGC_CFG_DBGBUS_IVTE_3                  0x18415
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKE_0                 0x18416
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKE_1                 0x18417
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKE_2                 0x18418
-#define A6XX_CX_DBGC_CFG_DBGBUS_MASKE_3                 0x18419
-#define A6XX_CX_DBGC_CFG_DBGBUS_NIBBLEE                 0x1841A
-#define A6XX_CX_DBGC_CFG_DBGBUS_PTRC0                   0x1841B
-#define A6XX_CX_DBGC_CFG_DBGBUS_PTRC1                   0x1841C
-#define A6XX_CX_DBGC_CFG_DBGBUS_LOADREG                 0x1841D
-#define A6XX_CX_DBGC_CFG_DBGBUS_IDX                     0x1841E
-#define A6XX_CX_DBGC_CFG_DBGBUS_CLRC                    0x1841F
-#define A6XX_CX_DBGC_CFG_DBGBUS_LOADIVT                 0x18420
-#define A6XX_CX_DBGC_VBIF_DBG_CNTL                      0x18421
-#define A6XX_CX_DBGC_DBG_LO_HI_GPIO                     0x18422
-#define A6XX_CX_DBGC_EXT_TRACE_BUS_CNTL                 0x18423
-#define A6XX_CX_DBGC_READ_AHB_THROUGH_DBG               0x18424
-#define A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF1              0x1842F
-#define A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF2              0x18430
-#define A6XX_CX_DBGC_EVT_CFG                            0x18440
-#define A6XX_CX_DBGC_EVT_INTF_SEL_0                     0x18441
-#define A6XX_CX_DBGC_EVT_INTF_SEL_1                     0x18442
-#define A6XX_CX_DBGC_PERF_ATB_CFG                       0x18443
-#define A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_0             0x18444
-#define A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_1             0x18445
-#define A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_2             0x18446
-#define A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_3             0x18447
-#define A6XX_CX_DBGC_PERF_ATB_TRIG_INTF_SEL_0           0x18448
-#define A6XX_CX_DBGC_PERF_ATB_TRIG_INTF_SEL_1           0x18449
-#define A6XX_CX_DBGC_PERF_ATB_DRAIN_CMD                 0x1844A
-#define A6XX_CX_DBGC_ECO_CNTL                           0x18450
-#define A6XX_CX_DBGC_AHB_DBG_CNTL                       0x18451
-
-/* GMU control registers */
-#define A6XX_GPU_GMU_GX_SPTPRAC_CLOCK_CONTROL   0x1A880
-#define A6XX_GMU_GX_SPTPRAC_POWER_CONTROL	0x1A881
-#define A6XX_GMU_CM3_ITCM_START			0x1B400
-#define A6XX_GMU_CM3_DTCM_START			0x1C400
-#define A6XX_GMU_NMI_CONTROL_STATUS		0x1CBF0
-#define A6XX_GMU_BOOT_SLUMBER_OPTION		0x1CBF8
-#define A6XX_GMU_GX_VOTE_IDX			0x1CBF9
-#define A6XX_GMU_MX_VOTE_IDX			0x1CBFA
-#define A6XX_GMU_DCVS_ACK_OPTION		0x1CBFC
-#define A6XX_GMU_DCVS_PERF_SETTING		0x1CBFD
-#define A6XX_GMU_DCVS_BW_SETTING		0x1CBFE
-#define A6XX_GMU_DCVS_RETURN			0x1CBFF
-#define A6XX_GMU_SYS_BUS_CONFIG			0x1F40F
-#define A6XX_GMU_CM3_SYSRESET			0x1F800
-#define A6XX_GMU_CM3_BOOT_CONFIG		0x1F801
-#define A6XX_GMU_CX_GMU_WFI_CONFIG		0x1F802
-#define A6XX_GMU_CM3_FW_BUSY			0x1F81A
-#define A6XX_GMU_CM3_FW_INIT_RESULT		0x1F81C
-#define A6XX_GMU_CM3_CFG			0x1F82D
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_ENABLE	0x1F840
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_0	0x1F841
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_1	0x1F842
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_0_L	0x1F844
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_0_H	0x1F845
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_1_L	0x1F846
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_1_H	0x1F847
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_2_L	0x1F848
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_2_H	0x1F849
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_3_L	0x1F84A
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_3_H	0x1F84B
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_4_L	0x1F84C
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_4_H	0x1F84D
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_5_L	0x1F84E
-#define A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_5_H	0x1F84F
-#define A6XX_GMU_CX_GMU_ALWAYS_ON_COUNTER_L	0x1F888
-#define A6XX_GMU_CX_GMU_ALWAYS_ON_COUNTER_H	0x1F889
-#define A6XX_GMU_PWR_COL_INTER_FRAME_CTRL	0x1F8C0
-#define A6XX_GMU_PWR_COL_INTER_FRAME_HYST	0x1F8C1
-#define A6XX_GMU_PWR_COL_SPTPRAC_HYST		0x1F8C2
-#define A6XX_GMU_SPTPRAC_PWR_CLK_STATUS		0x1F8D0
-#define A6XX_GMU_GPU_NAP_CTRL			0x1F8E4
-#define A6XX_GMU_RPMH_CTRL			0x1F8E8
-#define A6XX_GMU_RPMH_HYST_CTRL			0x1F8E9
-#define A6XX_GPU_GMU_CX_GMU_RPMH_POWER_STATE    0x1F8EC
-#define A6XX_GMU_BOOT_KMD_LM_HANDSHAKE		0x1F9F0
-#define A6XX_GMU_LLM_GLM_SLEEP_CTRL		0x1F957
-#define A6XX_GMU_LLM_GLM_SLEEP_STATUS		0x1F958
-
-/* HFI registers*/
-#define A6XX_GMU_ALWAYS_ON_COUNTER_L		0x1F888
-#define A6XX_GMU_ALWAYS_ON_COUNTER_H		0x1F889
-#define A6XX_GMU_GMU_PWR_COL_KEEPALIVE		0x1F8C3
-#define A6XX_GMU_HFI_CTRL_STATUS		0x1F980
-#define A6XX_GMU_HFI_VERSION_INFO		0x1F981
-#define A6XX_GMU_HFI_SFR_ADDR			0x1F982
-#define A6XX_GMU_HFI_MMAP_ADDR			0x1F983
-#define A6XX_GMU_HFI_QTBL_INFO			0x1F984
-#define A6XX_GMU_HFI_QTBL_ADDR			0x1F985
-#define A6XX_GMU_HFI_CTRL_INIT			0x1F986
-#define A6XX_GMU_GMU2HOST_INTR_SET		0x1F990
-#define A6XX_GMU_GMU2HOST_INTR_CLR		0x1F991
-#define A6XX_GMU_GMU2HOST_INTR_INFO		0x1F992
-#define A6XX_GMU_GMU2HOST_INTR_MASK		0x1F993
-#define A6XX_GMU_HOST2GMU_INTR_SET		0x1F994
-#define A6XX_GMU_HOST2GMU_INTR_CLR		0x1F995
-#define A6XX_GMU_HOST2GMU_INTR_RAW_INFO		0x1F996
-#define A6XX_GMU_HOST2GMU_INTR_EN_0		0x1F997
-#define A6XX_GMU_HOST2GMU_INTR_EN_1		0x1F998
-#define A6XX_GMU_HOST2GMU_INTR_EN_2		0x1F999
-#define A6XX_GMU_HOST2GMU_INTR_EN_3		0x1F99A
-#define A6XX_GMU_HOST2GMU_INTR_INFO_0		0x1F99B
-#define A6XX_GMU_HOST2GMU_INTR_INFO_1		0x1F99C
-#define A6XX_GMU_HOST2GMU_INTR_INFO_2		0x1F99D
-#define A6XX_GMU_HOST2GMU_INTR_INFO_3		0x1F99E
-#define A6XX_GMU_GENERAL_1			0x1F9C6
-#define A6XX_GMU_GENERAL_7			0x1F9CC
-
-/* ISENSE registers */
-#define A6XX_GMU_ISENSE_CTRL			0x1F95D
-#define A6XX_GPU_GMU_CX_GMU_ISENSE_CTRL		0x1f95d
-#define A6XX_GPU_CS_ENABLE_REG			0x23120
-
-/* LM registers */
-#define A6XX_GPU_GMU_CX_GMU_PWR_THRESHOLD       0x1F94D
-
-
-#define A6XX_GMU_AO_INTERRUPT_EN		0x23B03
-#define A6XX_GMU_AO_HOST_INTERRUPT_CLR		0x23B04
-#define A6XX_GMU_AO_HOST_INTERRUPT_STATUS	0x23B05
-#define A6XX_GMU_AO_HOST_INTERRUPT_MASK		0x23B06
-#define A6XX_GPU_GMU_AO_GMU_CGC_MODE_CNTL       0x23B09
-#define A6XX_GPU_GMU_AO_GMU_CGC_DELAY_CNTL      0x23B0A
-#define A6XX_GPU_GMU_AO_GMU_CGC_HYST_CNTL       0x23B0B
-#define A6XX_GPU_GMU_AO_GPU_CX_BUSY_STATUS	0x23B0C
-#define A6XX_GPU_GMU_AO_GPU_CX_BUSY_STATUS2	0x23B0D
-#define A6XX_GPU_GMU_AO_GPU_CX_BUSY_MASK	0x23B0E
-#define A6XX_GMU_AO_AHB_FENCE_CTRL		0x23B10
-#define A6XX_GMU_AHB_FENCE_STATUS		0x23B13
-#define A6XX_GMU_RBBM_INT_UNMASKED_STATUS	0x23B15
-#define A6XX_GMU_AO_SPARE_CNTL			0x23B16
-
-/* GMU RSC control registers */
-#define A6XX_GPU_RSCC_RSC_STATUS0_DRV0		0x23404
-#define A6XX_GMU_RSCC_CONTROL_REQ		0x23B07
-#define A6XX_GMU_RSCC_CONTROL_ACK		0x23B08
-
-/* FENCE control registers */
-#define A6XX_GMU_AHB_FENCE_RANGE_0		0x23B11
-#define A6XX_GMU_AHB_FENCE_RANGE_1		0x23B12
-
-/* GPUCC registers */
-#define A6XX_GPU_CC_GX_GDSCR                   0x24403
-#define A6XX_GPU_CC_GX_DOMAIN_MISC		0x24542
-
-/* GPU RSC sequencer registers */
-#define	A6XX_RSCC_PDC_SEQ_START_ADDR			0x23408
-#define A6XX_RSCC_PDC_MATCH_VALUE_LO			0x23409
-#define A6XX_RSCC_PDC_MATCH_VALUE_HI			0x2340A
-#define A6XX_RSCC_PDC_SLAVE_ID_DRV0			0x2340B
-#define A6XX_RSCC_HIDDEN_TCS_CMD0_ADDR			0x2340D
-#define A6XX_RSCC_HIDDEN_TCS_CMD0_DATA			0x2340E
-#define A6XX_RSCC_TIMESTAMP_UNIT0_TIMESTAMP_L_DRV0	0x23482
-#define A6XX_RSCC_TIMESTAMP_UNIT0_TIMESTAMP_H_DRV0	0x23483
-#define A6XX_RSCC_TIMESTAMP_UNIT1_EN_DRV0		0x23489
-#define A6XX_RSCC_TIMESTAMP_UNIT1_OUTPUT_DRV0		0x2348C
-#define A6XX_RSCC_OVERRIDE_START_ADDR			0x23500
-#define A6XX_RSCC_SEQ_BUSY_DRV0				0x23501
-#define A6XX_RSCC_SEQ_MEM_0_DRV0			0x23580
-#define A6XX_RSCC_TCS0_DRV0_STATUS			0x23746
-#define A6XX_RSCC_TCS1_DRV0_STATUS                      0x238AE
-#define A6XX_RSCC_TCS2_DRV0_STATUS                      0x23A16
-#define A6XX_RSCC_TCS3_DRV0_STATUS                      0x23B7E
-
-/* GPU PDC sequencer registers in AOSS.RPMh domain */
-#define	PDC_GPU_ENABLE_PDC			0x21140
-#define PDC_GPU_SEQ_START_ADDR			0x21148
-#define PDC_GPU_TCS0_CONTROL			0x21540
-#define PDC_GPU_TCS0_CMD_ENABLE_BANK		0x21541
-#define PDC_GPU_TCS0_CMD_WAIT_FOR_CMPL_BANK	0x21542
-#define PDC_GPU_TCS0_CMD0_MSGID			0x21543
-#define PDC_GPU_TCS0_CMD0_ADDR			0x21544
-#define PDC_GPU_TCS0_CMD0_DATA			0x21545
-#define PDC_GPU_TCS1_CONTROL			0x21572
-#define PDC_GPU_TCS1_CMD_ENABLE_BANK		0x21573
-#define PDC_GPU_TCS1_CMD_WAIT_FOR_CMPL_BANK	0x21574
-#define PDC_GPU_TCS1_CMD0_MSGID			0x21575
-#define PDC_GPU_TCS1_CMD0_ADDR			0x21576
-#define PDC_GPU_TCS1_CMD0_DATA			0x21577
-#define PDC_GPU_TCS2_CONTROL			0x215A4
-#define PDC_GPU_TCS2_CMD_ENABLE_BANK		0x215A5
-#define PDC_GPU_TCS2_CMD_WAIT_FOR_CMPL_BANK	0x215A6
-#define PDC_GPU_TCS2_CMD0_MSGID			0x215A7
-#define PDC_GPU_TCS2_CMD0_ADDR			0x215A8
-#define PDC_GPU_TCS2_CMD0_DATA			0x215A9
-#define PDC_GPU_TCS3_CONTROL			0x215D6
-#define PDC_GPU_TCS3_CMD_ENABLE_BANK		0x215D7
-#define PDC_GPU_TCS3_CMD_WAIT_FOR_CMPL_BANK	0x215D8
-#define PDC_GPU_TCS3_CMD0_MSGID			0x215D9
-#define PDC_GPU_TCS3_CMD0_ADDR			0x215DA
-#define PDC_GPU_TCS3_CMD0_DATA			0x215DB
-#define PDC_GPU_SEQ_MEM_0			0xA0000
-
-#endif /* _A6XX_REG_H */
-
diff --git a/drivers/gpu/msm/adreno-gpulist.h b/drivers/gpu/msm/adreno-gpulist.h
index 0a5933432399..78832b8a9712 100644
--- a/drivers/gpu/msm/adreno-gpulist.h
+++ b/drivers/gpu/msm/adreno-gpulist.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -20,7 +20,6 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.major = 0,
 		.minor = 6,
 		.patchid = 0x00,
-		.features = ADRENO_SOFT_FAULT_DETECT,
 		.pm4fw_name = "a300_pm4.fw",
 		.pfpfw_name = "a300_pfp.fw",
 		.gpudev = &adreno_a3xx_gpudev,
@@ -33,7 +32,6 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.major = 0,
 		.minor = 6,
 		.patchid = 0x20,
-		.features = ADRENO_SOFT_FAULT_DETECT,
 		.pm4fw_name = "a300_pm4.fw",
 		.pfpfw_name = "a300_pfp.fw",
 		.gpudev = &adreno_a3xx_gpudev,
@@ -46,7 +44,6 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.major = 0,
 		.minor = 4,
 		.patchid = 0x00,
-		.features = ADRENO_SOFT_FAULT_DETECT,
 		.pm4fw_name = "a300_pm4.fw",
 		.pfpfw_name = "a300_pfp.fw",
 		.gpudev = &adreno_a3xx_gpudev,
@@ -59,7 +56,7 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.major = 0,
 		.minor = 5,
 		.patchid = ANY_ID,
-		.features = ADRENO_SOFT_FAULT_DETECT,
+		.features = 0,
 		.pm4fw_name = "a420_pm4.fw",
 		.pfpfw_name = "a420_pfp.fw",
 		.gpudev = &adreno_a4xx_gpudev,
@@ -73,7 +70,7 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.minor = 0,
 		.patchid = ANY_ID,
 		.features = ADRENO_USES_OCMEM | ADRENO_WARM_START |
-			ADRENO_USE_BOOTSTRAP | ADRENO_SOFT_FAULT_DETECT,
+					ADRENO_USE_BOOTSTRAP,
 		.pm4fw_name = "a420_pm4.fw",
 		.pfpfw_name = "a420_pfp.fw",
 		.gpudev = &adreno_a4xx_gpudev,
@@ -95,8 +92,7 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.patchid = ANY_ID,
 		.features = ADRENO_USES_OCMEM  | ADRENO_WARM_START |
 			ADRENO_USE_BOOTSTRAP | ADRENO_SPTP_PC | ADRENO_PPD |
-			ADRENO_CONTENT_PROTECTION | ADRENO_PREEMPTION |
-			ADRENO_SOFT_FAULT_DETECT,
+			ADRENO_CONTENT_PROTECTION | ADRENO_PREEMPTION,
 		.pm4fw_name = "a420_pm4.fw",
 		.pfpfw_name = "a420_pfp.fw",
 		.gpudev = &adreno_a4xx_gpudev,
@@ -120,8 +116,7 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.minor = 8,
 		.patchid = ANY_ID,
 		.features = ADRENO_USES_OCMEM  | ADRENO_WARM_START |
-			ADRENO_USE_BOOTSTRAP | ADRENO_SPTP_PC |
-			ADRENO_SOFT_FAULT_DETECT,
+			ADRENO_USE_BOOTSTRAP | ADRENO_SPTP_PC,
 		.pm4fw_name = "a420_pm4.fw",
 		.pfpfw_name = "a420_pfp.fw",
 		.gpudev = &adreno_a4xx_gpudev,
@@ -201,31 +196,15 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.max_power = 5448,
 		.regfw_name = "a530v3_seq.fw2",
 	},
-	{
-		.gpurev = ADRENO_REV_A504,
-		.core = 5,
-		.major = 0,
-		.minor = 4,
-		.patchid = ANY_ID,
-		.features = ADRENO_PREEMPTION | ADRENO_64BIT,
-		.pm4fw_name = "a530_pm4.fw",
-		.pfpfw_name = "a530_pfp.fw",
-		.gpudev = &adreno_a5xx_gpudev,
-		.gmem_size = (SZ_128K + SZ_8K),
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-	},
 	{
 		.gpurev = ADRENO_REV_A505,
 		.core = 5,
 		.major = 0,
 		.minor = 5,
 		.patchid = ANY_ID,
-		.features = ADRENO_PREEMPTION | ADRENO_64BIT |
-			ADRENO_CONTENT_PROTECTION | ADRENO_CPZ_RETENTION,
+		.features = ADRENO_PREEMPTION | ADRENO_64BIT,
 		.pm4fw_name = "a530_pm4.fw",
 		.pfpfw_name = "a530_pfp.fw",
-		.zap_name = "a506_zap",
 		.gpudev = &adreno_a5xx_gpudev,
 		.gmem_size = (SZ_128K + SZ_8K),
 		.num_protected_regs = 0x20,
@@ -260,28 +239,6 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.num_protected_regs = 0x20,
 		.busy_mask = 0xFFFFFFFE,
 	},
-	{
-		.gpurev = ADRENO_REV_A540,
-		.core = 5,
-		.major = 4,
-		.minor = 0,
-		.patchid = 0,
-		.features = ADRENO_PREEMPTION | ADRENO_64BIT |
-			ADRENO_CONTENT_PROTECTION |
-			ADRENO_GPMU | ADRENO_SPTP_PC,
-		.pm4fw_name = "a530_pm4.fw",
-		.pfpfw_name = "a530_pfp.fw",
-		.zap_name = "a540_zap",
-		.gpudev = &adreno_a5xx_gpudev,
-		.gmem_size = SZ_1M,
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-		.gpmufw_name = "a540_gpmu.fw2",
-		.gpmu_major = 3,
-		.gpmu_minor = 0,
-		.gpmu_tsens = 0x000C000D,
-		.max_power = 5448,
-	},
 	{
 		.gpurev = ADRENO_REV_A540,
 		.core = 5,
@@ -289,130 +246,18 @@ static const struct adreno_gpu_core adreno_gpulist[] = {
 		.minor = 0,
 		.patchid = ANY_ID,
 		.features = ADRENO_PREEMPTION | ADRENO_64BIT |
-			ADRENO_CONTENT_PROTECTION |
-			ADRENO_GPMU | ADRENO_SPTP_PC,
+			ADRENO_CONTENT_PROTECTION,
 		.pm4fw_name = "a530_pm4.fw",
 		.pfpfw_name = "a530_pfp.fw",
-		.zap_name = "a540_zap",
+		.zap_name = "a530_zap",
 		.gpudev = &adreno_a5xx_gpudev,
 		.gmem_size = SZ_1M,
 		.num_protected_regs = 0x20,
 		.busy_mask = 0xFFFFFFFE,
-		.gpmufw_name = "a540_gpmu.fw2",
+		.gpmufw_name = "a540v1_gpmu.fw2",
 		.gpmu_major = 3,
 		.gpmu_minor = 0,
 		.gpmu_tsens = 0x000C000D,
 		.max_power = 5448,
 	},
-	{
-		.gpurev = ADRENO_REV_A512,
-		.core = 5,
-		.major = 1,
-		.minor = 2,
-		.patchid = ANY_ID,
-		.features = ADRENO_PREEMPTION | ADRENO_64BIT |
-			ADRENO_CONTENT_PROTECTION | ADRENO_CPZ_RETENTION,
-		.pm4fw_name = "a530_pm4.fw",
-		.pfpfw_name = "a530_pfp.fw",
-		.zap_name = "a512_zap",
-		.gpudev = &adreno_a5xx_gpudev,
-		.gmem_size = (SZ_256K + SZ_16K),
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-	},
-	{
-		.gpurev = ADRENO_REV_A508,
-		.core = 5,
-		.major = 0,
-		.minor = 8,
-		.patchid = ANY_ID,
-		.features = ADRENO_PREEMPTION | ADRENO_64BIT |
-			ADRENO_CONTENT_PROTECTION | ADRENO_CPZ_RETENTION,
-		.pm4fw_name = "a530_pm4.fw",
-		.pfpfw_name = "a530_pfp.fw",
-		.zap_name = "a508_zap",
-		.gpudev = &adreno_a5xx_gpudev,
-		.gmem_size = (SZ_128K + SZ_8K),
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-	},
-	{
-		.gpurev = ADRENO_REV_A630,
-		.core = 6,
-		.major = 3,
-		.minor = 0,
-		.patchid = 0,
-		.features = ADRENO_64BIT | ADRENO_RPMH |
-			ADRENO_GPMU | ADRENO_CONTENT_PROTECTION | ADRENO_LM,
-		.sqefw_name = "a630_sqe.fw",
-		.zap_name = "a630_zap",
-		.gpudev = &adreno_a6xx_gpudev,
-		.gmem_size = SZ_1M,
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-		.gpmufw_name = "a630_gmu.bin",
-		.gpmu_major = 0x1,
-		.gpmu_minor = 0x003,
-		.gpmu_tsens = 0x000C000D,
-		.max_power = 5448,
-	},
-	{
-		.gpurev = ADRENO_REV_A630,
-		.core = 6,
-		.major = 3,
-		.minor = 0,
-		.patchid = ANY_ID,
-		.features = ADRENO_64BIT | ADRENO_RPMH | ADRENO_IFPC |
-			ADRENO_GPMU | ADRENO_CONTENT_PROTECTION |
-			ADRENO_IOCOHERENT | ADRENO_PREEMPTION,
-		.sqefw_name = "a630_sqe.fw",
-		.zap_name = "a630_zap",
-		.gpudev = &adreno_a6xx_gpudev,
-		.gmem_size = SZ_1M,
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-		.gpmufw_name = "a630_gmu.bin",
-		.gpmu_major = 0x1,
-		.gpmu_minor = 0x003,
-		.gpmu_tsens = 0x000C000D,
-		.max_power = 5448,
-	},
-	{
-		.gpurev = ADRENO_REV_A615,
-		.core = 6,
-		.major = 1,
-		.minor = 5,
-		.patchid = ANY_ID,
-		.features = ADRENO_64BIT | ADRENO_RPMH | ADRENO_PREEMPTION |
-			ADRENO_GPMU | ADRENO_CONTENT_PROTECTION | ADRENO_IFPC |
-			ADRENO_IOCOHERENT,
-		.sqefw_name = "a630_sqe.fw",
-		.zap_name = "a615_zap",
-		.gpudev = &adreno_a6xx_gpudev,
-		.gmem_size = SZ_512K,
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-		.gpmufw_name = "a630_gmu.bin",
-		.gpmu_major = 0x1,
-		.gpmu_minor = 0x003,
-	},
-	{
-		.gpurev = ADRENO_REV_A616,
-		.core = 6,
-		.major = 1,
-		.minor = 6,
-		.patchid = ANY_ID,
-		.features = ADRENO_64BIT | ADRENO_RPMH | ADRENO_PREEMPTION |
-			ADRENO_GPMU | ADRENO_CONTENT_PROTECTION | ADRENO_IFPC |
-			ADRENO_IOCOHERENT,
-		.sqefw_name = "a630_sqe.fw",
-		.zap_name = "a615_zap",
-		.gpudev = &adreno_a6xx_gpudev,
-		.gmem_size = SZ_512K,
-		.num_protected_regs = 0x20,
-		.busy_mask = 0xFFFFFFFE,
-		.gpmufw_name = "a630_gmu.bin",
-		.gpmu_major = 0x1,
-		.gpmu_minor = 0x003,
-	},
 };
diff --git a/drivers/gpu/msm/adreno.c b/drivers/gpu/msm/adreno.c
index 2585a27452bc..bafb6540a2b7 100644
--- a/drivers/gpu/msm/adreno.c
+++ b/drivers/gpu/msm/adreno.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -16,8 +16,8 @@
 #include <linux/of.h>
 #include <linux/of_device.h>
 #include <linux/delay.h>
+//#include <linux/of_coresight.h>
 #include <linux/input.h>
-#include <linux/io.h>
 #include <soc/qcom/scm.h>
 
 #include <linux/msm-bus-board.h>
@@ -25,10 +25,10 @@
 
 #include "kgsl.h"
 #include "kgsl_pwrscale.h"
+#include "kgsl_cffdump.h"
 #include "kgsl_sharedmem.h"
 #include "kgsl_iommu.h"
 #include "kgsl_trace.h"
-#include "adreno_llc.h"
 
 #include "adreno.h"
 #include "adreno_iommu.h"
@@ -37,12 +37,10 @@
 #include "adreno_trace.h"
 
 #include "a3xx_reg.h"
-#include "a6xx_reg.h"
 #include "adreno_snapshot.h"
 
 /* Include the master list of GPU cores that are supported */
 #include "adreno-gpulist.h"
-#include "adreno_dispatch.h"
 
 #undef MODULE_PARAM_PREFIX
 #define MODULE_PARAM_PREFIX "adreno."
@@ -51,13 +49,12 @@ static bool nopreempt;
 module_param(nopreempt, bool, 0444);
 MODULE_PARM_DESC(nopreempt, "Disable GPU preemption");
 
-static bool swfdetect;
-module_param(swfdetect, bool, 0444);
-MODULE_PARM_DESC(swfdetect, "Enable soft fault detection");
-
 #define DRIVER_VERSION_MAJOR   3
 #define DRIVER_VERSION_MINOR   1
 
+/* Number of times to try hard reset */
+#define NUM_TIMES_RESET_RETRY 5
+
 #define KGSL_LOG_LEVEL_DEFAULT 3
 
 static void adreno_input_work(struct work_struct *work);
@@ -79,9 +76,6 @@ static struct adreno_device device_3d0 = {
 		.pwrscale = KGSL_PWRSCALE_INIT(&adreno_tz_data),
 		.name = DEVICE_3D0_NAME,
 		.id = KGSL_DEVICE_3D0,
-		.gmu = {
-			.load_mode = TCM_BOOT,
-		},
 		.pwrctrl = {
 			.irq_name = "kgsl_3d0_irq",
 		},
@@ -94,31 +88,18 @@ static struct adreno_device device_3d0 = {
 		.mem_log = KGSL_LOG_LEVEL_DEFAULT,
 		.pwr_log = KGSL_LOG_LEVEL_DEFAULT,
 	},
-	.fw[0] = {
-		.fwvirt = NULL
-	},
-	.fw[1] = {
-		.fwvirt = NULL
-	},
 	.gmem_size = SZ_256K,
+	.pfp_fw = NULL,
+	.pm4_fw = NULL,
 	.ft_policy = KGSL_FT_DEFAULT_POLICY,
 	.ft_pf_policy = KGSL_FT_PAGEFAULT_DEFAULT_POLICY,
+	.fast_hang_detect = 1,
 	.long_ib_detect = 1,
 	.input_work = __WORK_INITIALIZER(device_3d0.input_work,
 		adreno_input_work),
 	.pwrctrl_flag = BIT(ADRENO_SPTP_PC_CTRL) | BIT(ADRENO_PPD_CTRL) |
-		BIT(ADRENO_LM_CTRL) | BIT(ADRENO_HWCG_CTRL) |
-		BIT(ADRENO_THROTTLING_CTRL),
+		BIT(ADRENO_LM_CTRL) | BIT(ADRENO_HWCG_CTRL),
 	.profile.enabled = false,
-	.active_list = LIST_HEAD_INIT(device_3d0.active_list),
-	.active_list_lock = __SPIN_LOCK_UNLOCKED(device_3d0.active_list_lock),
-	.gpu_llc_slice_enable = true,
-	.gpuhtw_llc_slice_enable = true,
-	.preempt = {
-		.preempt_level = 1,
-		.skipsaverestore = 1,
-		.usesgmem = 1,
-	},
 };
 
 /* Ptr to array for the current set of fault detect registers */
@@ -187,30 +168,6 @@ void adreno_writereg64(struct adreno_device *adreno_dev,
 			gpudev->reg_offsets->offsets[hi], upper_32_bits(val));
 }
 
-/**
- * adreno_get_rptr() - Get the current ringbuffer read pointer
- * @rb: Pointer the ringbuffer to query
- *
- * Get the latest rptr
- */
-unsigned int adreno_get_rptr(struct adreno_ringbuffer *rb)
-{
-	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
-	unsigned int rptr = 0;
-
-	if (adreno_is_a3xx(adreno_dev))
-		adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR,
-				&rptr);
-	else {
-		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-		kgsl_sharedmem_readl(&device->scratch, &rptr,
-				SCRATCH_RPTR_OFFSET(rb->id));
-	}
-
-	return rptr;
-}
-
 /**
  * adreno_of_read_property() - Adreno read property
  * @node: Device node
@@ -221,7 +178,6 @@ static inline int adreno_of_read_property(struct device_node *node,
 	const char *prop, unsigned int *ptr)
 {
 	int ret = of_property_read_u32(node, prop, ptr);
-
 	if (ret)
 		KGSL_CORE_ERR("Unable to read '%s'\n", prop);
 	return ret;
@@ -279,59 +235,6 @@ int adreno_efuse_read_u32(struct adreno_device *adreno_dev, unsigned int offset,
 	return 0;
 }
 
-void adreno_efuse_speed_bin_array(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	int ret, count, i = 0;
-	unsigned int val, vector_size = 3;
-	unsigned int *bin_vector;
-
-	/*
-	 * Here count is no of 32 bit elements in the
-	 * speed-bin-vector array. If there are two fuses
-	 * i.e If no of fuses are 2 then no of elements will be
-	 * 2 * 3 = 6(elements of 32 bit each).
-	 */
-	count = of_property_count_u32_elems(device->pdev->dev.of_node,
-				"qcom,gpu-speed-bin-vectors");
-
-	if ((count <= 0) || (count % vector_size))
-		return;
-
-	bin_vector = kmalloc_array(count, sizeof(unsigned int), GFP_KERNEL);
-	if (bin_vector == NULL) {
-		KGSL_DRV_ERR(device,
-				"Unable to allocate memory for speed-bin vector\n");
-		return;
-	}
-
-	if (of_property_read_u32_array(device->pdev->
-			dev.of_node, "qcom,gpu-speed-bin-vectors",
-			bin_vector, count)) {
-		KGSL_DRV_ERR(device,
-				"Speed-bin-vectors is invalid\n");
-		kfree(bin_vector);
-		return;
-	}
-
-	/*
-	 * Final value of adreno_dev->speed_bin is the value formed by
-	 * OR'ing the values read from all the fuses.
-	 */
-	while (i < count) {
-		ret = adreno_efuse_read_u32(adreno_dev, bin_vector[i], &val);
-
-		if (ret < 0)
-			break;
-
-		adreno_dev->speed_bin |= (val & bin_vector[i+1])
-				>> bin_vector[i+2];
-		i += vector_size;
-	}
-
-	kfree(bin_vector);
-}
-
 static int _get_counter(struct adreno_device *adreno_dev,
 		int group, int countable, unsigned int *lo,
 		unsigned int *hi)
@@ -571,6 +474,8 @@ static struct input_handler adreno_input_handler = {
 	.id_table = adreno_input_ids,
 };
 
+static int adreno_soft_reset(struct kgsl_device *device);
+
 /*
  * _soft_reset() - Soft reset GPU
  * @adreno_dev: Pointer to adreno device
@@ -581,7 +486,7 @@ static struct input_handler adreno_input_handler = {
  * all the HW logic, restores GPU registers to default state and
  * flushes out pending VBIF transactions.
  */
-static int _soft_reset(struct adreno_device *adreno_dev)
+static void _soft_reset(struct adreno_device *adreno_dev)
 {
 	struct adreno_gpudev *gpudev  = ADRENO_GPU_DEVICE(adreno_dev);
 	unsigned int reg;
@@ -610,17 +515,9 @@ static int _soft_reset(struct adreno_device *adreno_dev)
 
 	if (gpudev->regulator_enable)
 		gpudev->regulator_enable(adreno_dev);
-
-	return 0;
 }
 
-/**
- * adreno_irqctrl() - Enables/disables the RBBM interrupt mask
- * @adreno_dev: Pointer to an adreno_device
- * @state: 1 for masked or 0 for unmasked
- * Power: The caller of this function must make sure to use OOBs
- * so that we know that the GPU is powered on
- */
+
 void adreno_irqctrl(struct adreno_device *adreno_dev, int state)
 {
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
@@ -629,7 +526,7 @@ void adreno_irqctrl(struct adreno_device *adreno_dev, int state)
 	adreno_writereg(adreno_dev, ADRENO_REG_RBBM_INT_0_MASK, mask);
 }
 
-/*
+ /*
  * adreno_hang_int_callback() - Isr for fatal interrupts that hang GPU
  * @adreno_dev: Pointer to device
  * @bit: Interrupt bit
@@ -645,7 +542,7 @@ void adreno_hang_int_callback(struct adreno_device *adreno_dev, int bit)
 	adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
 }
 
-/*
+ /*
  * adreno_cp_callback() - CP interrupt handler
  * @adreno_dev: Adreno device pointer
  * @irq: irq number
@@ -656,6 +553,7 @@ void adreno_cp_callback(struct adreno_device *adreno_dev, int bit)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
+	kgsl_schedule_work(&device->event_work);
 	adreno_dispatcher_schedule(device);
 }
 
@@ -665,67 +563,11 @@ static irqreturn_t adreno_irq_handler(struct kgsl_device *device)
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	struct adreno_irq *irq_params = gpudev->irq;
 	irqreturn_t ret = IRQ_NONE;
-	unsigned int status = 0, fence = 0, fence_retries = 0, tmp, int_bit;
-	unsigned int shadow_status = 0;
+	unsigned int status = 0, tmp;
 	int i;
 
-	atomic_inc(&adreno_dev->pending_irq_refcnt);
-	/* Ensure this increment is done before the IRQ status is updated */
-	smp_mb__after_atomic();
-
-	/*
-	 * On A6xx, the GPU can power down once the INT_0_STATUS is read
-	 * below. But there still might be some register reads required
-	 * so force the GMU/GPU into KEEPALIVE mode until done with the ISR.
-	 */
-	if (gpudev->gpu_keepalive)
-		gpudev->gpu_keepalive(adreno_dev, true);
-
-	/*
-	 * If the AHB fence is not in ALLOW mode when we receive an RBBM
-	 * interrupt, something went wrong. This means that we cannot proceed
-	 * since the IRQ status and clear registers are not accessible.
-	 * This is usually harmless because the GMU will abort power collapse
-	 * and change the fence back to ALLOW. Poll so that this can happen.
-	 */
-	if (kgsl_gmu_isenabled(device)) {
-		adreno_readreg(adreno_dev,
-				ADRENO_REG_GMU_AO_AHB_FENCE_CTRL,
-				&fence);
-
-		while (fence != 0) {
-			/* Wait for small time before trying again */
-			udelay(1);
-			adreno_readreg(adreno_dev,
-					ADRENO_REG_GMU_AO_AHB_FENCE_CTRL,
-					&fence);
-
-			if (fence_retries == FENCE_RETRY_MAX && fence != 0) {
-				adreno_readreg(adreno_dev,
-					ADRENO_REG_GMU_RBBM_INT_UNMASKED_STATUS,
-					&shadow_status);
-
-				KGSL_DRV_CRIT_RATELIMIT(device,
-					"AHB fence stuck in ISR: Shadow INT status=%8.8X\n",
-					shadow_status & irq_params->mask);
-				goto done;
-			}
-			fence_retries++;
-		}
-	}
-
 	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_INT_0_STATUS, &status);
 
-	/*
-	 * Clear all the interrupt bits but ADRENO_INT_RBBM_AHB_ERROR. Because
-	 * even if we clear it here, it will stay high until it is cleared
-	 * in its respective handler. Otherwise, the interrupt handler will
-	 * fire again.
-	 */
-	int_bit = ADRENO_INT_BIT(adreno_dev, ADRENO_INT_RBBM_AHB_ERROR);
-	adreno_writereg(adreno_dev, ADRENO_REG_RBBM_INT_CLEAR_CMD,
-				status & ~int_bit);
-
 	/* Loop through all set interrupts and call respective handlers */
 	for (tmp = status; tmp != 0;) {
 		i = fls(tmp) - 1;
@@ -744,28 +586,9 @@ static irqreturn_t adreno_irq_handler(struct kgsl_device *device)
 
 	gpudev->irq_trace(adreno_dev, status);
 
-	/*
-	 * Clear ADRENO_INT_RBBM_AHB_ERROR bit after this interrupt has been
-	 * cleared in its respective handler
-	 */
-	if (status & int_bit)
+	if (status)
 		adreno_writereg(adreno_dev, ADRENO_REG_RBBM_INT_CLEAR_CMD,
-				int_bit);
-
-done:
-	/* Turn off the KEEPALIVE vote from earlier unless hard fault set */
-	if (gpudev->gpu_keepalive) {
-		/* If hard fault, then let snapshot turn off the keepalive */
-		if (!(adreno_gpu_fault(adreno_dev) & ADRENO_HARD_FAULT))
-			gpudev->gpu_keepalive(adreno_dev, false);
-	}
-
-	/* Make sure the regwrites are done before the decrement */
-	smp_mb__before_atomic();
-	atomic_dec(&adreno_dev->pending_irq_refcnt);
-	/* Ensure other CPUs see the decrement */
-	smp_mb__after_atomic();
-
+				status);
 	return ret;
 
 }
@@ -794,76 +617,6 @@ static inline const struct adreno_gpu_core *_get_gpu_core(unsigned int chipid)
 	return NULL;
 }
 
-static struct {
-	unsigned int quirk;
-	const char *prop;
-} adreno_quirks[] = {
-	 { ADRENO_QUIRK_TWO_PASS_USE_WFI, "qcom,gpu-quirk-two-pass-use-wfi" },
-	 { ADRENO_QUIRK_IOMMU_SYNC, "qcom,gpu-quirk-iommu-sync" },
-	 { ADRENO_QUIRK_CRITICAL_PACKETS, "qcom,gpu-quirk-critical-packets" },
-	 { ADRENO_QUIRK_FAULT_DETECT_MASK, "qcom,gpu-quirk-fault-detect-mask" },
-	 { ADRENO_QUIRK_DISABLE_RB_DP2CLOCKGATING,
-			"qcom,gpu-quirk-dp2clockgating-disable" },
-	 { ADRENO_QUIRK_DISABLE_LMLOADKILL,
-			"qcom,gpu-quirk-lmloadkill-disable" },
-	{ ADRENO_QUIRK_HFI_USE_REG, "qcom,gpu-quirk-hfi-use-reg" },
-	{ ADRENO_QUIRK_SECVID_SET_ONCE, "qcom,gpu-quirk-secvid-set-once" },
-	{ ADRENO_QUIRK_LIMIT_UCHE_GBIF_RW,
-			"qcom,gpu-quirk-limit-uche-gbif-rw" },
-	{ ADRENO_QUIRK_MMU_SECURE_CB_ALT, "qcom,gpu-quirk-mmu-secure-cb-alt" },
-};
-
-static struct device_node *
-adreno_get_soc_hw_revision_node(struct adreno_device *adreno_dev,
-	struct platform_device *pdev)
-{
-	struct device_node *node, *child;
-	unsigned int rev;
-
-	node = of_find_node_by_name(pdev->dev.of_node, "qcom,soc-hw-revisions");
-	if (node == NULL)
-		return NULL;
-
-	for_each_child_of_node(node, child) {
-		if (of_property_read_u32(child, "qcom,soc-hw-revision", &rev))
-			continue;
-
-		if (rev == adreno_dev->soc_hw_rev)
-			return child;
-	}
-
-	KGSL_DRV_WARN(KGSL_DEVICE(adreno_dev),
-		"No matching SOC HW revision found for efused HW rev=%u\n",
-		adreno_dev->soc_hw_rev);
-	return NULL;
-}
-
-static int adreno_update_soc_hw_revision_quirks(
-		struct adreno_device *adreno_dev, struct platform_device *pdev)
-{
-	struct device_node *node;
-	int i;
-
-	node = adreno_get_soc_hw_revision_node(adreno_dev, pdev);
-	if (node == NULL)
-		node = pdev->dev.of_node;
-
-	/* get chip id, fall back to parent if revision node does not have it */
-	if (of_property_read_u32(node, "qcom,chipid", &adreno_dev->chipid))
-		if (of_property_read_u32(pdev->dev.of_node,
-				"qcom,chipid", &adreno_dev->chipid))
-			KGSL_DRV_FATAL(KGSL_DEVICE(adreno_dev),
-			"No GPU chip ID was specified\n");
-
-	/* update quirk */
-	for (i = 0; i < ARRAY_SIZE(adreno_quirks); i++) {
-		if (of_property_read_bool(node, adreno_quirks[i].prop))
-			adreno_dev->quirks |= adreno_quirks[i].quirk;
-	}
-
-	return 0;
-}
-
 static void
 adreno_identify_gpu(struct adreno_device *adreno_dev)
 {
@@ -871,6 +624,11 @@ adreno_identify_gpu(struct adreno_device *adreno_dev)
 	struct adreno_gpudev *gpudev;
 	int i;
 
+	if (kgsl_property_read_u32(KGSL_DEVICE(adreno_dev), "qcom,chipid",
+		&adreno_dev->chipid))
+		KGSL_DRV_FATAL(KGSL_DEVICE(adreno_dev),
+			"No GPU chip ID was specified\n");
+
 	adreno_dev->gpucore = _get_gpu_core(adreno_dev->chipid);
 
 	if (adreno_dev->gpucore == NULL)
@@ -914,58 +672,6 @@ static const struct of_device_id adreno_match_table[] = {
 	{}
 };
 
-static void adreno_of_get_ca_target_pwrlevel(struct adreno_device *adreno_dev,
-		struct device_node *node)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int ca_target_pwrlevel = 1;
-
-	of_property_read_u32(node, "qcom,ca-target-pwrlevel",
-		&ca_target_pwrlevel);
-
-	if (ca_target_pwrlevel > device->pwrctrl.num_pwrlevels - 2)
-		ca_target_pwrlevel = 1;
-
-	device->pwrscale.ctxt_aware_target_pwrlevel = ca_target_pwrlevel;
-}
-
-static void adreno_of_get_ca_aware_properties(struct adreno_device *adreno_dev,
-		struct device_node *parent)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_pwrscale *pwrscale = &device->pwrscale;
-	struct device_node *node, *child;
-	unsigned int bin = 0;
-
-	pwrscale->ctxt_aware_enable =
-		of_property_read_bool(parent, "qcom,enable-ca-jump");
-
-	if (pwrscale->ctxt_aware_enable) {
-		if (of_property_read_u32(parent, "qcom,ca-busy-penalty",
-			&pwrscale->ctxt_aware_busy_penalty))
-			pwrscale->ctxt_aware_busy_penalty = 12000;
-
-		node = of_find_node_by_name(parent, "qcom,gpu-pwrlevel-bins");
-		if (node == NULL) {
-			adreno_of_get_ca_target_pwrlevel(adreno_dev, parent);
-			return;
-		}
-
-		for_each_child_of_node(node, child) {
-			if (of_property_read_u32(child, "qcom,speed-bin", &bin))
-				continue;
-
-			if (bin == adreno_dev->speed_bin) {
-				adreno_of_get_ca_target_pwrlevel(adreno_dev,
-					child);
-				return;
-			}
-		}
-
-		pwrscale->ctxt_aware_target_pwrlevel = 1;
-	}
-}
-
 static int adreno_of_parse_pwrlevels(struct adreno_device *adreno_dev,
 		struct device_node *node)
 {
@@ -1053,13 +759,13 @@ static int adreno_of_get_pwrlevels(struct adreno_device *adreno_dev,
 		struct device_node *parent)
 {
 	struct device_node *node, *child;
-	unsigned int bin = 0;
 
 	node = of_find_node_by_name(parent, "qcom,gpu-pwrlevel-bins");
 	if (node == NULL)
 		return adreno_of_get_legacy_pwrlevels(adreno_dev, parent);
 
 	for_each_child_of_node(node, child) {
+		unsigned int bin;
 
 		if (of_property_read_u32(child, "qcom,speed-bin", &bin))
 			continue;
@@ -1075,51 +781,9 @@ static int adreno_of_get_pwrlevels(struct adreno_device *adreno_dev,
 		}
 	}
 
-	KGSL_CORE_ERR("GPU speed_bin:%d mismatch for efused bin:%d\n",
-			adreno_dev->speed_bin, bin);
 	return -ENODEV;
 }
 
-static void
-l3_pwrlevel_probe(struct kgsl_device *device, struct device_node *node)
-{
-
-	struct device_node *pwrlevel_node, *child;
-
-	pwrlevel_node = of_find_node_by_name(node, "qcom,l3-pwrlevels");
-
-	if (pwrlevel_node == NULL)
-		return;
-
-	device->num_l3_pwrlevels = 0;
-
-	for_each_child_of_node(pwrlevel_node, child) {
-		unsigned int index;
-
-		if (of_property_read_u32(child, "reg", &index))
-			return;
-		if (index >= MAX_L3_LEVELS) {
-			dev_err(&device->pdev->dev, "L3 pwrlevel %d is out of range\n",
-					index);
-			continue;
-		}
-
-		if (index >= device->num_l3_pwrlevels)
-			device->num_l3_pwrlevels = index + 1;
-
-		if (of_property_read_u32(child, "qcom,l3-freq",
-				&device->l3_freq[index]))
-			return;
-	}
-
-	device->l3_clk = devm_clk_get(&device->pdev->dev, "l3_vote");
-
-	if (IS_ERR_OR_NULL(device->l3_clk)) {
-		dev_err(&device->pdev->dev, "Unable to get the l3_vote clock\n");
-		return;
-	}
-}
-
 static inline struct adreno_device *adreno_get_dev(struct platform_device *pdev)
 {
 	const struct of_device_id *of_id =
@@ -1128,12 +792,22 @@ static inline struct adreno_device *adreno_get_dev(struct platform_device *pdev)
 	return of_id ? (struct adreno_device *) of_id->data : NULL;
 }
 
+static struct {
+	unsigned int quirk;
+	const char *prop;
+} adreno_quirks[] = {
+	 { ADRENO_QUIRK_TWO_PASS_USE_WFI, "qcom,gpu-quirk-two-pass-use-wfi" },
+	 { ADRENO_QUIRK_IOMMU_SYNC, "qcom,gpu-quirk-iommu-sync" },
+	 { ADRENO_QUIRK_CRITICAL_PACKETS, "qcom,gpu-quirk-critical-packets" },
+	 { ADRENO_QUIRK_FAULT_DETECT_MASK, "qcom,gpu-quirk-fault-detect-mask" },
+};
+
 static int adreno_of_get_power(struct adreno_device *adreno_dev,
 		struct platform_device *pdev)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct device_node *node = pdev->dev.of_node;
-	struct resource *res;
+	int i;
 	unsigned int timeout;
 
 	if (of_property_read_string(node, "label", &pdev->name)) {
@@ -1144,40 +818,20 @@ static int adreno_of_get_power(struct adreno_device *adreno_dev,
 	if (adreno_of_read_property(node, "qcom,id", &pdev->id))
 		return -EINVAL;
 
-	/* Get starting physical address of device registers */
-	res = platform_get_resource_byname(device->pdev, IORESOURCE_MEM,
-					   device->iomemname);
-	if (res == NULL) {
-		KGSL_DRV_ERR(device, "platform_get_resource_byname failed\n");
-		return -EINVAL;
-	}
-	if (res->start == 0 || resource_size(res) == 0) {
-		KGSL_DRV_ERR(device, "dev %d invalid register region\n",
-			device->id);
-		return -EINVAL;
+	/* Set up quirks and other boolean options */
+	for (i = 0; i < ARRAY_SIZE(adreno_quirks); i++) {
+		if (of_property_read_bool(node, adreno_quirks[i].prop))
+			adreno_dev->quirks |= adreno_quirks[i].quirk;
 	}
 
-	device->reg_phys = res->start;
-	device->reg_len = resource_size(res);
-
 	if (adreno_of_get_pwrlevels(adreno_dev, node))
 		return -EINVAL;
 
-	/* Get context aware DCVS properties */
-	adreno_of_get_ca_aware_properties(adreno_dev, node);
-
-	l3_pwrlevel_probe(device, node);
-
 	/* get pm-qos-active-latency, set it to default if not found */
 	if (of_property_read_u32(node, "qcom,pm-qos-active-latency",
 		&device->pwrctrl.pm_qos_active_latency))
 		device->pwrctrl.pm_qos_active_latency = 501;
 
-	/* get pm-qos-cpu-mask-latency, set it to default if not found */
-	if (of_property_read_u32(node, "qcom,l2pc-cpu-mask-latency",
-		&device->pwrctrl.pm_qos_cpu_mask_latency))
-		device->pwrctrl.pm_qos_cpu_mask_latency = 501;
-
 	/* get pm-qos-wakeup-latency, set it to default if not found */
 	if (of_property_read_u32(node, "qcom,pm-qos-wakeup-latency",
 		&device->pwrctrl.pm_qos_wakeup_latency))
@@ -1188,6 +842,9 @@ static int adreno_of_get_power(struct adreno_device *adreno_dev,
 
 	device->pwrctrl.interval_timeout = msecs_to_jiffies(timeout);
 
+	device->pwrctrl.strtstp_sleepwake =
+		of_property_read_bool(node, "qcom,strtstp-sleepwake");
+
 	device->pwrctrl.bus_control = of_property_read_bool(node,
 		"qcom,bus-control");
 
@@ -1197,7 +854,7 @@ static int adreno_of_get_power(struct adreno_device *adreno_dev,
 	return 0;
 }
 
-#ifdef CONFIG_QCOM_OCMEM
+#ifdef CONFIG_MSM_OCMEM
 static int
 adreno_ocmem_malloc(struct adreno_device *adreno_dev)
 {
@@ -1240,85 +897,6 @@ adreno_ocmem_free(struct adreno_device *adreno_dev)
 }
 #endif
 
-static void adreno_cx_dbgc_probe(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct resource *res;
-
-	res = platform_get_resource_byname(device->pdev, IORESOURCE_MEM,
-					   "kgsl_3d0_cx_dbgc_memory");
-
-	if (res == NULL)
-		return;
-
-	adreno_dev->cx_dbgc_base = res->start - device->reg_phys;
-	adreno_dev->cx_dbgc_len = resource_size(res);
-	adreno_dev->cx_dbgc_virt = devm_ioremap(device->dev,
-					device->reg_phys +
-						adreno_dev->cx_dbgc_base,
-					adreno_dev->cx_dbgc_len);
-
-	if (adreno_dev->cx_dbgc_virt == NULL)
-		KGSL_DRV_WARN(device, "cx_dbgc ioremap failed\n");
-}
-
-static void adreno_efuse_read_soc_hw_rev(struct adreno_device *adreno_dev)
-{
-	unsigned int val;
-	unsigned int soc_hw_rev[3];
-	int ret;
-
-	if (of_property_read_u32_array(
-		KGSL_DEVICE(adreno_dev)->pdev->dev.of_node,
-		"qcom,soc-hw-rev-efuse", soc_hw_rev, 3))
-		return;
-
-	ret = adreno_efuse_map(adreno_dev);
-	if (ret) {
-		KGSL_CORE_ERR(
-			"Unable to map hardware revision fuse: ret=%d\n", ret);
-		return;
-	}
-
-	ret = adreno_efuse_read_u32(adreno_dev, soc_hw_rev[0], &val);
-	adreno_efuse_unmap(adreno_dev);
-
-	if (ret) {
-		KGSL_CORE_ERR(
-			"Unable to read hardware revision fuse: ret=%d\n", ret);
-		return;
-	}
-
-	adreno_dev->soc_hw_rev = (val >> soc_hw_rev[1]) & soc_hw_rev[2];
-}
-
-static bool adreno_is_gpu_disabled(struct adreno_device *adreno_dev)
-{
-	unsigned int row0;
-	unsigned int pte_row0_msb[3];
-	int ret;
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (of_property_read_u32_array(device->pdev->dev.of_node,
-		"qcom,gpu-disable-fuse", pte_row0_msb, 3))
-		return false;
-	/*
-	 * Read the fuse value to disable GPU driver if fuse
-	 * is blown. By default(fuse value is 0) GPU is enabled.
-	 */
-	if (adreno_efuse_map(adreno_dev))
-		return false;
-
-	ret = adreno_efuse_read_u32(adreno_dev, pte_row0_msb[0], &row0);
-	adreno_efuse_unmap(adreno_dev);
-
-	if (ret)
-		return false;
-
-	return (row0 >> pte_row0_msb[2]) &
-			pte_row0_msb[1] ? true : false;
-}
-
 static int adreno_probe(struct platform_device *pdev)
 {
 	struct kgsl_device *device;
@@ -1335,16 +913,6 @@ static int adreno_probe(struct platform_device *pdev)
 	device = KGSL_DEVICE(adreno_dev);
 	device->pdev = pdev;
 
-	if (adreno_is_gpu_disabled(adreno_dev)) {
-		pr_err("adreno: GPU is disabled on this device");
-		return -ENODEV;
-	}
-
-	/* Identify SOC hardware revision to be used */
-	adreno_efuse_read_soc_hw_rev(adreno_dev);
-
-	adreno_update_soc_hw_revision_quirks(adreno_dev, pdev);
-
 	/* Get the chip ID from the DT and set up target specific parameters */
 	adreno_identify_gpu(adreno_dev);
 
@@ -1355,21 +923,10 @@ static int adreno_probe(struct platform_device *pdev)
 	}
 
 	/*
-	 * Probe/init GMU after initial gpu power probe
-	 * Another part of GPU power probe in platform_probe
-	 * needs GMU initialized.
-	 */
-	status = gmu_probe(device);
-	if (status != 0 && status != -ENXIO) {
-		device->pdev = NULL;
-		return status;
-	}
-
-	/*
-	 * The SMMU APIs use unsigned long for virtual addresses which means
-	 * that we cannot use 64 bit virtual addresses on a 32 bit kernel even
-	 * though the hardware and the rest of the KGSL driver supports it.
-	 */
+	* The SMMU APIs use unsigned long for virtual addresses which means
+	* that we cannot use 64 bit virtual addresses on a 32 bit kernel even
+	* though the hardware and the rest of the KGSL driver supports it.
+	*/
 	if (adreno_support_64bit(adreno_dev))
 		device->mmu.features |= KGSL_MMU_64BIT;
 
@@ -1379,9 +936,6 @@ static int adreno_probe(struct platform_device *pdev)
 		return status;
 	}
 
-	/* Probe for the optional CX_DBGC block */
-	adreno_cx_dbgc_probe(device);
-
 	/*
 	 * qcom,iommu-secure-id is used to identify MMUs that can handle secure
 	 * content but that is only part of the story - the GPU also has to be
@@ -1393,10 +947,7 @@ static int adreno_probe(struct platform_device *pdev)
 	if (!ADRENO_FEATURE(adreno_dev, ADRENO_CONTENT_PROTECTION))
 		device->mmu.secured = false;
 
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_IOCOHERENT))
-		device->mmu.features |= KGSL_MMU_IO_COHERENT;
-
-	status = adreno_ringbuffer_probe(adreno_dev, nopreempt);
+	status = adreno_ringbuffer_init(adreno_dev, nopreempt);
 	if (status)
 		goto out;
 
@@ -1414,24 +965,6 @@ static int adreno_probe(struct platform_device *pdev)
 	/* Initialize coresight for the target */
 	adreno_coresight_init(adreno_dev);
 
-	/* Get the system cache slice descriptor for GPU */
-	adreno_dev->gpu_llc_slice = adreno_llc_getd(&pdev->dev, "gpu");
-	if (IS_ERR(adreno_dev->gpu_llc_slice)) {
-		KGSL_DRV_WARN(device,
-			"Failed to get GPU LLC slice descriptor (%ld)\n",
-			PTR_ERR(adreno_dev->gpu_llc_slice));
-		adreno_dev->gpu_llc_slice = NULL;
-	}
-
-	/* Get the system cache slice descriptor for GPU pagetables */
-	adreno_dev->gpuhtw_llc_slice = adreno_llc_getd(&pdev->dev, "gpuhtw");
-	if (IS_ERR(adreno_dev->gpuhtw_llc_slice)) {
-		KGSL_DRV_WARN(device,
-			"Failed to get gpuhtw LLC slice descriptor (%ld)\n",
-			PTR_ERR(adreno_dev->gpuhtw_llc_slice));
-		adreno_dev->gpuhtw_llc_slice = NULL;
-	}
-
 #ifdef CONFIG_INPUT
 	if (!device->pwrctrl.input_disable) {
 		adreno_input_handler.private = device;
@@ -1459,37 +992,34 @@ static int adreno_probe(struct platform_device *pdev)
 static void _adreno_free_memories(struct adreno_device *adreno_dev)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_firmware *pfp_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PFP);
-	struct adreno_firmware *pm4_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PM4);
 
-	if (test_bit(ADRENO_DEVICE_DRAWOBJ_PROFILE, &adreno_dev->priv))
-		kgsl_free_global(device, &adreno_dev->profile_buffer);
+	if (test_bit(ADRENO_DEVICE_CMDBATCH_PROFILE, &adreno_dev->priv))
+		kgsl_free_global(device, &adreno_dev->cmdbatch_profile_buffer);
 
 	/* Free local copies of firmware and other command streams */
-	kfree(pfp_fw->fwvirt);
-	pfp_fw->fwvirt = NULL;
+	kfree(adreno_dev->pfp_fw);
+	adreno_dev->pfp_fw = NULL;
 
-	kfree(pm4_fw->fwvirt);
-	pm4_fw->fwvirt = NULL;
+	kfree(adreno_dev->pm4_fw);
+	adreno_dev->pm4_fw = NULL;
 
 	kfree(adreno_dev->gpmu_cmds);
 	adreno_dev->gpmu_cmds = NULL;
 
-	kgsl_free_global(device, &pfp_fw->memdesc);
-	kgsl_free_global(device, &pm4_fw->memdesc);
+	kgsl_free_global(device, &adreno_dev->pm4);
+	kgsl_free_global(device, &adreno_dev->pfp);
 }
 
 static int adreno_remove(struct platform_device *pdev)
 {
 	struct adreno_device *adreno_dev = adreno_get_dev(pdev);
-	struct adreno_gpudev *gpudev;
+	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	struct kgsl_device *device;
 
 	if (adreno_dev == NULL)
 		return 0;
 
 	device = KGSL_DEVICE(adreno_dev);
-	gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 
 	if (gpudev->remove != NULL)
 		gpudev->remove(adreno_dev);
@@ -1506,12 +1036,6 @@ static int adreno_remove(struct platform_device *pdev)
 	adreno_coresight_remove(adreno_dev);
 	adreno_profile_close(adreno_dev);
 
-	/* Release the system cache slice descriptor */
-	if (adreno_dev->gpu_llc_slice)
-		adreno_llc_putd(adreno_dev->gpu_llc_slice);
-	if (adreno_dev->gpuhtw_llc_slice)
-		adreno_llc_putd(adreno_dev->gpuhtw_llc_slice);
-
 	kgsl_pwrscale_close(device);
 
 	adreno_dispatcher_close(adreno_dev);
@@ -1531,8 +1055,6 @@ static int adreno_remove(struct platform_device *pdev)
 	adreno_perfcounter_close(adreno_dev);
 	kgsl_device_platform_remove(device);
 
-	gmu_remove(device);
-
 	if (test_bit(ADRENO_DEVICE_PWRON_FIXUP, &adreno_dev->priv)) {
 		kgsl_free_global(device, &adreno_dev->pwron_fixup);
 		clear_bit(ADRENO_DEVICE_PWRON_FIXUP, &adreno_dev->priv);
@@ -1545,11 +1067,7 @@ static int adreno_remove(struct platform_device *pdev)
 static void adreno_fault_detect_init(struct adreno_device *adreno_dev)
 {
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	int i;
-
-	if (!(swfdetect ||
-			ADRENO_FEATURE(adreno_dev, ADRENO_SOFT_FAULT_DETECT)))
-		return;
+	int i, val = adreno_dev->fast_hang_detect;
 
 	/* Disable the fast hang detect bit until we know its a go */
 	adreno_dev->fast_hang_detect = 0;
@@ -1557,9 +1075,9 @@ static void adreno_fault_detect_init(struct adreno_device *adreno_dev)
 	adreno_ft_regs_num = (ARRAY_SIZE(adreno_ft_regs_default) +
 		gpudev->ft_perf_counters_count*2);
 
-	adreno_ft_regs = kcalloc(adreno_ft_regs_num, sizeof(unsigned int),
+	adreno_ft_regs = kzalloc(adreno_ft_regs_num * sizeof(unsigned int),
 		GFP_KERNEL);
-	adreno_ft_regs_val = kcalloc(adreno_ft_regs_num, sizeof(unsigned int),
+	adreno_ft_regs_val = kzalloc(adreno_ft_regs_num * sizeof(unsigned int),
 		GFP_KERNEL);
 
 	if (adreno_ft_regs == NULL || adreno_ft_regs_val == NULL) {
@@ -1578,7 +1096,8 @@ static void adreno_fault_detect_init(struct adreno_device *adreno_dev)
 
 	set_bit(ADRENO_DEVICE_SOFT_FAULT_DETECT, &adreno_dev->priv);
 
-	adreno_fault_detect_start(adreno_dev);
+	if (val)
+		adreno_fault_detect_start(adreno_dev);
 }
 
 static int adreno_init(struct kgsl_device *device)
@@ -1587,14 +1106,7 @@ static int adreno_init(struct kgsl_device *device)
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	int ret;
 
-	if (!adreno_is_a3xx(adreno_dev))
-		kgsl_sharedmem_set(device, &device->scratch, 0, 0,
-				device->scratch.size);
-
-	ret = kgsl_pwrctrl_change_state(device, KGSL_STATE_INIT);
-	if (ret)
-		return ret;
-
+	kgsl_pwrctrl_change_state(device, KGSL_STATE_INIT);
 	/*
 	 * initialization only needs to be done once initially until
 	 * device is shutdown
@@ -1613,12 +1125,9 @@ static int adreno_init(struct kgsl_device *device)
 		return ret;
 
 	/* Put the GPU in a responsive state */
-	if (ADRENO_GPUREV(adreno_dev) < 600) {
-		/* No need for newer generation architectures */
-		ret = kgsl_pwrctrl_change_state(device, KGSL_STATE_AWARE);
-		if (ret)
-			return ret;
-	}
+	ret = kgsl_pwrctrl_change_state(device, KGSL_STATE_AWARE);
+	if (ret)
+		return ret;
 
 	ret = adreno_iommu_init(adreno_dev);
 	if (ret)
@@ -1628,8 +1137,16 @@ static int adreno_init(struct kgsl_device *device)
 	adreno_fault_detect_init(adreno_dev);
 
 	/* Power down the device */
-	if (ADRENO_GPUREV(adreno_dev) < 600)
-		kgsl_pwrctrl_change_state(device, KGSL_STATE_SLUMBER);
+	kgsl_pwrctrl_change_state(device, KGSL_STATE_INIT);
+
+	/*
+	 * Enable the power on shader corruption fix
+	 * This is only applicable for 28nm targets
+	 */
+	if (adreno_is_a3xx(adreno_dev))
+		adreno_a3xx_pwron_fixup_init(adreno_dev);
+	else if ((adreno_is_a405(adreno_dev)) || (adreno_is_a420(adreno_dev)))
+		adreno_a4xx_pwron_fixup_init(adreno_dev);
 
 	if (gpudev->init != NULL)
 		gpudev->init(adreno_dev);
@@ -1653,23 +1170,32 @@ static int adreno_init(struct kgsl_device *device)
 		}
 	}
 
+	/* Adjust snapshot section sizes according to core */
+	if ((adreno_is_a330(adreno_dev) || adreno_is_a305b(adreno_dev))) {
+		gpudev->snapshot_data->sect_sizes->cp_pfp =
+					A320_SNAPSHOT_CP_STATE_SECTION_SIZE;
+		gpudev->snapshot_data->sect_sizes->roq =
+					A320_SNAPSHOT_ROQ_SECTION_SIZE;
+		gpudev->snapshot_data->sect_sizes->cp_merciu =
+					A320_SNAPSHOT_CP_MERCIU_SECTION_SIZE;
+	}
+
 	/*
-	 * Allocate a small chunk of memory for precise drawobj profiling for
+	 * Allocate a small chunk of memory for precise cmdbatch profiling for
 	 * those targets that have the always on timer
 	 */
 
 	if (!adreno_is_a3xx(adreno_dev)) {
 		int r = kgsl_allocate_global(device,
-			&adreno_dev->profile_buffer, PAGE_SIZE,
-			0, 0, "alwayson");
+			&adreno_dev->cmdbatch_profile_buffer, PAGE_SIZE, 0, 0);
 
-		adreno_dev->profile_index = 0;
+		adreno_dev->cmdbatch_profile_index = 0;
 
 		if (r == 0) {
-			set_bit(ADRENO_DEVICE_DRAWOBJ_PROFILE,
+			set_bit(ADRENO_DEVICE_CMDBATCH_PROFILE,
 				&adreno_dev->priv);
 			kgsl_sharedmem_set(device,
-				&adreno_dev->profile_buffer, 0, 0,
+				&adreno_dev->cmdbatch_profile_buffer, 0, 0,
 				PAGE_SIZE);
 		}
 
@@ -1695,9 +1221,6 @@ static bool regulators_left_on(struct kgsl_device *device)
 {
 	int i;
 
-	if (kgsl_gmu_isenabled(device))
-		return false;
-
 	for (i = 0; i < KGSL_MAX_REGULATORS; i++) {
 		struct kgsl_regulator *regulator =
 			&device->pwrctrl.regulators[i];
@@ -1712,92 +1235,76 @@ static bool regulators_left_on(struct kgsl_device *device)
 	return false;
 }
 
-static void _set_secvid(struct kgsl_device *device)
+static void _setup_throttling_counters(struct adreno_device *adreno_dev)
 {
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	static bool set;
+	int i, ret;
 
-	/* Program GPU contect protection init values */
-	if (device->mmu.secured && !set) {
-		if (adreno_is_a4xx(adreno_dev))
-			adreno_writereg(adreno_dev,
-				ADRENO_REG_RBBM_SECVID_TRUST_CONFIG, 0x2);
-		adreno_writereg(adreno_dev,
-				ADRENO_REG_RBBM_SECVID_TSB_CONTROL, 0x0);
+	if (!adreno_is_a540(adreno_dev))
+		return;
 
-		adreno_writereg64(adreno_dev,
-			ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE,
-			ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE_HI,
-			KGSL_IOMMU_SECURE_BASE(&device->mmu));
-		adreno_writereg(adreno_dev,
-			ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_SIZE,
-			KGSL_IOMMU_SECURE_SIZE);
-		if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_SECVID_SET_ONCE))
-			set = true;
-	}
-}
+	if (!ADRENO_FEATURE(adreno_dev, ADRENO_LM))
+		return;
 
-static int adreno_switch_to_unsecure_mode(struct adreno_device *adreno_dev,
-				struct adreno_ringbuffer *rb)
-{
-	unsigned int *cmds;
-	int ret;
+	for (i = 0; i < ADRENO_GPMU_THROTTLE_COUNTERS; i++) {
+		/* reset throttled cycles ivalue */
+		adreno_dev->busy_data.throttle_cycles[i] = 0;
 
-	cmds = adreno_ringbuffer_allocspace(rb, 2);
-	if (IS_ERR(cmds))
-		return PTR_ERR(cmds);
-	if (cmds == NULL)
-		return -ENOSPC;
+		if (adreno_dev->gpmu_throttle_counters[i] != 0)
+			continue;
+		ret = adreno_perfcounter_get(adreno_dev,
+			KGSL_PERFCOUNTER_GROUP_GPMU_PWR,
+			ADRENO_GPMU_THROTTLE_COUNTERS_BASE_REG + i,
+			&adreno_dev->gpmu_throttle_counters[i],
+			NULL,
+			PERFCOUNTER_FLAG_KERNEL);
+		WARN_ONCE(ret,  "Unable to get clock throttling counter %x\n",
+			ADRENO_GPMU_THROTTLE_COUNTERS_BASE_REG + i);
+	}
+}
 
-	cmds += cp_secure_mode(adreno_dev, cmds, 0);
+/* FW driven idle 10% throttle */
+#define IDLE_10PCT 0
+/* number of cycles when clock is throttled by 50% (CRC) */
+#define CRC_50PCT  1
+/* number of cycles when clock is throttled by more than 50% (CRC) */
+#define CRC_MORE50PCT 2
+/* number of cycles when clock is throttle by less than 50% (CRC) */
+#define CRC_LESS50PCT 3
 
-	ret = adreno_ringbuffer_submit_spin(rb, NULL, 2000);
-	if (ret)
-		adreno_spin_idle_debug(adreno_dev,
-				"Switch to unsecure failed to idle\n");
+static uint64_t _read_throttling_counters(struct adreno_device *adreno_dev)
+{
+	int i;
+	uint32_t th[ADRENO_GPMU_THROTTLE_COUNTERS];
+	struct adreno_busy_data *busy = &adreno_dev->busy_data;
 
-	return ret;
-}
+	if (!adreno_is_a540(adreno_dev))
+		return 0;
 
-int adreno_set_unsecured_mode(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb)
-{
-	int ret = 0;
+	if (!ADRENO_FEATURE(adreno_dev, ADRENO_LM))
+		return 0;
 
-	if (!adreno_is_a5xx(adreno_dev) && !adreno_is_a6xx(adreno_dev))
-		return -EINVAL;
+	for (i = 0; i < ADRENO_GPMU_THROTTLE_COUNTERS; i++) {
+		if (!adreno_dev->gpmu_throttle_counters[i])
+			return 0;
 
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_CRITICAL_PACKETS) &&
-			adreno_is_a5xx(adreno_dev)) {
-		ret = a5xx_critical_packet_submit(adreno_dev, rb);
-		if (ret)
-			return ret;
+		th[i] = counter_delta(KGSL_DEVICE(adreno_dev),
+			adreno_dev->gpmu_throttle_counters[i],
+			&busy->throttle_cycles[i]);
 	}
+	return th[CRC_50PCT] + th[CRC_LESS50PCT] / 3 +
+		(th[CRC_MORE50PCT] - th[IDLE_10PCT]) * 3;
 
-	/* GPU comes up in secured mode, make it unsecured by default */
-	if (adreno_dev->zap_loaded)
-		ret = adreno_switch_to_unsecure_mode(adreno_dev, rb);
-	else
-		adreno_writereg(adreno_dev,
-				ADRENO_REG_RBBM_SECVID_TRUST_CONTROL, 0x0);
-
-	return ret;
 }
 
-static void adreno_set_active_ctxs_null(struct adreno_device *adreno_dev)
+static void _update_threshold_count(struct adreno_device *adreno_dev,
+	uint64_t adj)
 {
-	int i;
-	struct adreno_ringbuffer *rb;
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		if (rb->drawctxt_active)
-			kgsl_context_put(&(rb->drawctxt_active->base));
-		rb->drawctxt_active = NULL;
-
-		kgsl_sharedmem_writel(KGSL_DEVICE(adreno_dev),
-			&rb->pagetable_desc, PT_INFO_OFFSET(current_rb_ptname),
-			0);
-	}
+	if (adreno_is_a530(adreno_dev))
+		kgsl_regread(KGSL_DEVICE(adreno_dev),
+			adreno_dev->lm_threshold_count,
+			&adreno_dev->lm_threshold_cross);
+	else if (adreno_is_a540(adreno_dev))
+		adreno_dev->lm_threshold_cross = adj;
 }
 
 /**
@@ -1820,13 +1327,11 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 	/* make sure ADRENO_DEVICE_STARTED is not set here */
 	BUG_ON(test_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv));
 
-	/* disallow l2pc during wake up to improve GPU wake up time */
-	kgsl_pwrctrl_update_l2pc(&adreno_dev->dev,
-			KGSL_L2PC_WAKEUP_TIMEOUT);
-
 	pm_qos_update_request(&device->pwrctrl.pm_qos_req_dma,
 			pmqos_wakeup_vote);
 
+	kgsl_cffdump_open(device);
+
 	regulator_left_on = regulators_left_on(device);
 
 	/* Clear any GPU faults that might have been left over */
@@ -1837,9 +1342,6 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 	if (status)
 		goto error_pwr_off;
 
-	/* Set any stale active contexts to NULL */
-	adreno_set_active_ctxs_null(adreno_dev);
-
 	/* Set the bit to indicate that we've just powered on */
 	set_bit(ADRENO_DEVICE_PWRON, &adreno_dev->priv);
 
@@ -1847,13 +1349,26 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 	if (regulator_left_on)
 		_soft_reset(adreno_dev);
 
-	adreno_ringbuffer_set_global(adreno_dev, 0);
-
 	status = kgsl_mmu_start(device);
 	if (status)
 		goto error_pwr_off;
 
-	_set_secvid(device);
+	/* Program GPU contect protection init values */
+	if (device->mmu.secured) {
+		if (adreno_is_a4xx(adreno_dev))
+			adreno_writereg(adreno_dev,
+				ADRENO_REG_RBBM_SECVID_TRUST_CONFIG, 0x2);
+		adreno_writereg(adreno_dev,
+				ADRENO_REG_RBBM_SECVID_TSB_CONTROL, 0x0);
+
+		adreno_writereg64(adreno_dev,
+			ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE,
+			ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE_HI,
+			KGSL_IOMMU_SECURE_BASE);
+		adreno_writereg(adreno_dev,
+			ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_SIZE,
+			KGSL_IOMMU_SECURE_SIZE);
+	}
 
 	status = adreno_ocmem_malloc(adreno_dev);
 	if (status) {
@@ -1861,15 +1376,6 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 		goto error_mmu_off;
 	}
 
-	/* Send OOB request to turn on the GX */
-	if (gpudev->oob_set) {
-		status = gpudev->oob_set(adreno_dev, OOB_GPU_SET_MASK,
-				OOB_GPU_CHECK_MASK,
-				OOB_GPU_CLEAR_MASK);
-		if (status)
-			goto error_mmu_off;
-	}
-
 	/* Enable 64 bit gpu addr if feature is set */
 	if (gpudev->enable_64bit &&
 			adreno_support_64bit(adreno_dev))
@@ -1882,13 +1388,14 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 			PERFCOUNTER_FLAG_KERNEL);
 
 		if (ret) {
-			WARN_ONCE(1, "Unable to get perf counters for DCVS\n");
+			KGSL_DRV_ERR(device,
+				"Unable to get the perf counters for DCVS\n");
 			adreno_dev->perfctr_pwr_lo = 0;
 		}
 	}
 
-
 	if (device->pwrctrl.bus_control) {
+
 		/* VBIF waiting for RAM */
 		if (adreno_dev->starved_ram_lo == 0) {
 			ret = adreno_perfcounter_get(adreno_dev,
@@ -1903,104 +1410,40 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 			}
 		}
 
-		if (adreno_has_gbif(adreno_dev)) {
-			if (adreno_dev->starved_ram_lo_ch1 == 0) {
-				ret = adreno_perfcounter_get(adreno_dev,
-					KGSL_PERFCOUNTER_GROUP_VBIF_PWR, 1,
-					&adreno_dev->starved_ram_lo_ch1, NULL,
-					PERFCOUNTER_FLAG_KERNEL);
-
-				if (ret) {
-					KGSL_DRV_ERR(device,
-						"Unable to get perf counters for bus DCVS\n");
-					adreno_dev->starved_ram_lo_ch1 = 0;
-				}
-			}
-
-			if (adreno_dev->ram_cycles_lo == 0) {
-				ret = adreno_perfcounter_get(adreno_dev,
-					KGSL_PERFCOUNTER_GROUP_VBIF,
-					GBIF_AXI0_READ_DATA_TOTAL_BEATS,
-					&adreno_dev->ram_cycles_lo, NULL,
-					PERFCOUNTER_FLAG_KERNEL);
-
-				if (ret) {
-					KGSL_DRV_ERR(device,
-						"Unable to get perf counters for bus DCVS\n");
-					adreno_dev->ram_cycles_lo = 0;
-				}
-			}
-
-			if (adreno_dev->ram_cycles_lo_ch1_read == 0) {
-				ret = adreno_perfcounter_get(adreno_dev,
-					KGSL_PERFCOUNTER_GROUP_VBIF,
-					GBIF_AXI1_READ_DATA_TOTAL_BEATS,
-					&adreno_dev->ram_cycles_lo_ch1_read,
-					NULL,
-					PERFCOUNTER_FLAG_KERNEL);
-
-				if (ret) {
-					KGSL_DRV_ERR(device,
-						"Unable to get perf counters for bus DCVS\n");
-					adreno_dev->ram_cycles_lo_ch1_read = 0;
-				}
-			}
-
-			if (adreno_dev->ram_cycles_lo_ch0_write == 0) {
-				ret = adreno_perfcounter_get(adreno_dev,
-					KGSL_PERFCOUNTER_GROUP_VBIF,
-					GBIF_AXI0_WRITE_DATA_TOTAL_BEATS,
-					&adreno_dev->ram_cycles_lo_ch0_write,
-					NULL,
-					PERFCOUNTER_FLAG_KERNEL);
-
-				if (ret) {
-					KGSL_DRV_ERR(device,
-						"Unable to get perf counters for bus DCVS\n");
-					adreno_dev->ram_cycles_lo_ch0_write = 0;
-				}
-			}
+		/* VBIF DDR cycles */
+		if (adreno_dev->ram_cycles_lo == 0) {
+			ret = adreno_perfcounter_get(adreno_dev,
+				KGSL_PERFCOUNTER_GROUP_VBIF,
+				VBIF_AXI_TOTAL_BEATS,
+				&adreno_dev->ram_cycles_lo, NULL,
+				PERFCOUNTER_FLAG_KERNEL);
 
-			if (adreno_dev->ram_cycles_lo_ch1_write == 0) {
-				ret = adreno_perfcounter_get(adreno_dev,
-					KGSL_PERFCOUNTER_GROUP_VBIF,
-					GBIF_AXI1_WRITE_DATA_TOTAL_BEATS,
-					&adreno_dev->ram_cycles_lo_ch1_write,
-					NULL,
-					PERFCOUNTER_FLAG_KERNEL);
-
-				if (ret) {
-					KGSL_DRV_ERR(device,
-						"Unable to get perf counters for bus DCVS\n");
-					adreno_dev->ram_cycles_lo_ch1_write = 0;
-				}
-			}
-		} else {
-			/* VBIF DDR cycles */
-			if (adreno_dev->ram_cycles_lo == 0) {
-				ret = adreno_perfcounter_get(adreno_dev,
-					KGSL_PERFCOUNTER_GROUP_VBIF,
-					VBIF_AXI_TOTAL_BEATS,
-					&adreno_dev->ram_cycles_lo, NULL,
-					PERFCOUNTER_FLAG_KERNEL);
-
-				if (ret) {
-					KGSL_DRV_ERR(device,
-						"Unable to get perf counters for bus DCVS\n");
-					adreno_dev->ram_cycles_lo = 0;
-				}
+			if (ret) {
+				KGSL_DRV_ERR(device,
+					"Unable to get perf counters for bus DCVS\n");
+				adreno_dev->ram_cycles_lo = 0;
 			}
 		}
 	}
 
 	/* Clear the busy_data stats - we're starting over from scratch */
 	adreno_dev->busy_data.gpu_busy = 0;
-	adreno_dev->busy_data.bif_ram_cycles = 0;
-	adreno_dev->busy_data.bif_ram_cycles_read_ch1 = 0;
-	adreno_dev->busy_data.bif_ram_cycles_write_ch0 = 0;
-	adreno_dev->busy_data.bif_ram_cycles_write_ch1 = 0;
-	adreno_dev->busy_data.bif_starved_ram = 0;
-	adreno_dev->busy_data.bif_starved_ram_ch1 = 0;
+	adreno_dev->busy_data.vbif_ram_cycles = 0;
+	adreno_dev->busy_data.vbif_starved_ram = 0;
+
+	if (adreno_is_a530(adreno_dev) && ADRENO_FEATURE(adreno_dev, ADRENO_LM)
+		&& adreno_dev->lm_threshold_count == 0) {
+
+		ret = adreno_perfcounter_get(adreno_dev,
+			KGSL_PERFCOUNTER_GROUP_GPMU_PWR, 27,
+			&adreno_dev->lm_threshold_count, NULL,
+			PERFCOUNTER_FLAG_KERNEL);
+		/* Ignore noncritical ret - used for debugfs */
+		if (ret)
+			adreno_dev->lm_threshold_count = 0;
+	}
+
+	_setup_throttling_counters(adreno_dev);
 
 	/* Restore performance counter registers with saved values */
 	adreno_perfcounter_restore(adreno_dev);
@@ -2008,14 +1451,6 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 	/* Start the GPU */
 	gpudev->start(adreno_dev);
 
-	/*
-	 * The system cache control registers
-	 * live on the CX rail. Hence need
-	 * reprogramming everytime the GPU
-	 * comes out of power collapse.
-	 */
-	adreno_llc_setup(device);
-
 	/* Re-initialize the coresight registers if applicable */
 	adreno_coresight_start(adreno_dev);
 
@@ -2028,7 +1463,13 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 
 	status = adreno_ringbuffer_start(adreno_dev, ADRENO_START_COLD);
 	if (status)
-		goto error_oob_clear;
+		goto error_mmu_off;
+
+	if (gpudev->hw_init) {
+		status = gpudev->hw_init(adreno_dev);
+		if (status)
+			goto error_mmu_off;
+	}
 
 	/* Start the dispatcher */
 	adreno_dispatcher_start(device);
@@ -2041,22 +1482,8 @@ static int _adreno_start(struct adreno_device *adreno_dev)
 		pm_qos_update_request(&device->pwrctrl.pm_qos_req_dma,
 				pmqos_active_vote);
 
-	/* Send OOB request to allow IFPC */
-	if (gpudev->oob_clear) {
-		gpudev->oob_clear(adreno_dev, OOB_GPU_CLEAR_MASK);
-
-		/* If we made it this far, the BOOT OOB was sent to the GMU */
-		if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_HFI_USE_REG))
-			gpudev->oob_clear(adreno_dev,
-					OOB_BOOT_SLUMBER_CLEAR_MASK);
-	}
-
 	return 0;
 
-error_oob_clear:
-	if (gpudev->oob_clear)
-		gpudev->oob_clear(adreno_dev, OOB_GPU_CLEAR_MASK);
-
 error_mmu_off:
 	kgsl_mmu_stop(&device->mmu);
 
@@ -2080,7 +1507,7 @@ static int _adreno_start(struct adreno_device *adreno_dev)
  * Power up the GPU and initialize it.  If priority is specified then elevate
  * the thread priority for the duration of the start operation
  */
-int adreno_start(struct kgsl_device *device, int priority)
+static int adreno_start(struct kgsl_device *device, int priority)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	int nice = task_nice(current);
@@ -2097,94 +1524,69 @@ int adreno_start(struct kgsl_device *device, int priority)
 	return ret;
 }
 
-static int adreno_stop(struct kgsl_device *device)
+/**
+ * adreno_vbif_clear_pending_transactions() - Clear transactions in VBIF pipe
+ * @device: Pointer to the device whose VBIF pipe is to be cleared
+ */
+static int adreno_vbif_clear_pending_transactions(struct kgsl_device *device)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	int error = 0;
+	unsigned int mask = gpudev->vbif_xin_halt_ctrl0_mask;
+	unsigned int val;
+	unsigned long wait_for_vbif;
+	int ret = 0;
+
+	adreno_writereg(adreno_dev, ADRENO_REG_VBIF_XIN_HALT_CTRL0, mask);
+	/* wait for the transactions to clear */
+	wait_for_vbif = jiffies + msecs_to_jiffies(100);
+	while (1) {
+		adreno_readreg(adreno_dev,
+			ADRENO_REG_VBIF_XIN_HALT_CTRL1, &val);
+		if ((val & mask) == mask)
+			break;
+		if (time_after(jiffies, wait_for_vbif)) {
+			KGSL_DRV_ERR(device,
+				"Wait limit reached for VBIF XIN Halt\n");
+			ret = -ETIMEDOUT;
+			break;
+		}
+	}
+	adreno_writereg(adreno_dev, ADRENO_REG_VBIF_XIN_HALT_CTRL0, 0);
+	return ret;
+}
+
+static int adreno_stop(struct kgsl_device *device)
+{
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 
 	if (!test_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv))
 		return 0;
 
-	/* Turn the power on one last time before stopping */
-	if (gpudev->oob_set) {
-		error = gpudev->oob_set(adreno_dev, OOB_GPU_SET_MASK,
-				OOB_GPU_CHECK_MASK,
-				OOB_GPU_CLEAR_MASK);
-		if (error) {
-			struct gmu_device *gmu = &device->gmu;
-
-			gpudev->oob_clear(adreno_dev, OOB_GPU_CLEAR_MASK);
-			if (gmu->gx_gdsc &&
-				regulator_is_enabled(gmu->gx_gdsc)) {
-				/* GPU is on. Try recovery */
-				set_bit(GMU_FAULT, &gmu->flags);
-				gmu_snapshot(device);
-				error = -EINVAL;
-			} else {
-				return error;
-			}
-		}
-	}
+	adreno_set_active_ctxs_null(adreno_dev);
 
 	adreno_dispatcher_stop(adreno_dev);
 
 	adreno_ringbuffer_stop(adreno_dev);
 
-	kgsl_pwrscale_update_stats(device);
-
 	adreno_irqctrl(adreno_dev, 0);
 
 	adreno_ocmem_free(adreno_dev);
 
-	if (adreno_dev->gpu_llc_slice)
-		adreno_llc_deactivate_slice(adreno_dev->gpu_llc_slice);
-	if (adreno_dev->gpuhtw_llc_slice)
-		adreno_llc_deactivate_slice(adreno_dev->gpuhtw_llc_slice);
-
 	/* Save active coresight registers if applicable */
 	adreno_coresight_stop(adreno_dev);
 
 	/* Save physical performance counter values before GPU power down*/
 	adreno_perfcounter_save(adreno_dev);
 
-	if (gpudev->oob_clear)
-		gpudev->oob_clear(adreno_dev, OOB_GPU_CLEAR_MASK);
-
-	/*
-	 * Saving perfcounters will use an OOB to put the GMU into
-	 * active state. Before continuing, we should wait for the
-	 * GMU to return to the lowest idle level. This is
-	 * because some idle level transitions require VBIF and MMU.
-	 */
-	if (!error && gpudev->wait_for_lowest_idle &&
-			gpudev->wait_for_lowest_idle(adreno_dev)) {
-		struct gmu_device *gmu = &device->gmu;
-
-		set_bit(GMU_FAULT, &gmu->flags);
-		gmu_snapshot(device);
-		/*
-		 * Assume GMU hang after 10ms without responding.
-		 * It shall be relative safe to clear vbif and stop
-		 * MMU later. Early return in adreno_stop function
-		 * will result in kernel panic in adreno_start
-		 */
-		error = -EINVAL;
-	}
-
 	adreno_vbif_clear_pending_transactions(device);
 
 	kgsl_mmu_stop(&device->mmu);
-
-	/*
-	 * At this point, MMU is turned off so we can safely
-	 * destroy any pending contexts and their pagetables
-	 */
-	adreno_set_active_ctxs_null(adreno_dev);
+	kgsl_cffdump_close(device);
 
 	clear_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv);
 
-	return error;
+	return 0;
 }
 
 static inline bool adreno_try_soft_reset(struct kgsl_device *device, int fault)
@@ -2215,9 +1617,14 @@ static inline bool adreno_try_soft_reset(struct kgsl_device *device, int fault)
 int adreno_reset(struct kgsl_device *device, int fault)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	int ret = -EINVAL;
 	int i = 0;
 
+	/* broadcast to HW - reset is coming */
+	if (gpudev->pre_reset)
+		gpudev->pre_reset(adreno_dev);
+
 	/* Try soft reset first */
 	if (adreno_try_soft_reset(device, fault)) {
 		/* Make sure VBIF is cleared before resetting */
@@ -2248,7 +1655,7 @@ int adreno_reset(struct kgsl_device *device, int fault)
 	if (ret)
 		return ret;
 
-	if (i != 0)
+	if (0 != i)
 		KGSL_DRV_WARN(device, "Device hard reset tried %d tries\n", i);
 
 	/*
@@ -2261,6 +1668,13 @@ int adreno_reset(struct kgsl_device *device, int fault)
 	else
 		kgsl_pwrctrl_change_state(device, KGSL_STATE_NAP);
 
+	/* Set the page table back to the default page table */
+	kgsl_mmu_set_pt(&device->mmu, device->mmu.defaultpagetable);
+	kgsl_sharedmem_writel(device,
+		&adreno_dev->ringbuffers[0].pagetable_desc,
+		offsetof(struct adreno_ringbuffer_pagetable_info,
+			current_global_ptname), 0);
+
 	return ret;
 }
 
@@ -2312,11 +1726,10 @@ static int adreno_getproperty(struct kgsl_device *device,
 				 * anything to mmap().
 				 */
 				shadowprop.gpuaddr =
-					(unsigned long)device->memstore.gpuaddr;
+					(unsigned int) device->memstore.gpuaddr;
 				shadowprop.size = device->memstore.size;
 				/* GSL needs this to be set, even if it
-				 * appears to be meaningless
-				 */
+				   appears to be meaningless */
 				shadowprop.flags = KGSL_FLAGS_INITIALIZED |
 					KGSL_FLAGS_PER_CONTEXT_TIMESTAMPS;
 			}
@@ -2328,54 +1741,6 @@ static int adreno_getproperty(struct kgsl_device *device,
 			status = 0;
 		}
 		break;
-	case KGSL_PROP_DEVICE_QDSS_STM:
-		{
-			struct kgsl_qdss_stm_prop qdssprop = {0};
-			struct kgsl_memdesc *qdss_desc =
-				kgsl_mmu_get_qdss_global_entry(device);
-
-			if (sizebytes != sizeof(qdssprop)) {
-				status = -EINVAL;
-				break;
-			}
-
-			if (qdss_desc) {
-				qdssprop.gpuaddr = qdss_desc->gpuaddr;
-				qdssprop.size = qdss_desc->size;
-			}
-
-			if (copy_to_user(value, &qdssprop,
-						sizeof(qdssprop))) {
-				status = -EFAULT;
-				break;
-			}
-			status = 0;
-		}
-		break;
-	case KGSL_PROP_DEVICE_QTIMER:
-		{
-			struct kgsl_qtimer_prop qtimerprop = {0};
-			struct kgsl_memdesc *qtimer_desc =
-				kgsl_mmu_get_qtimer_global_entry(device);
-
-			if (sizebytes != sizeof(qtimerprop)) {
-				status = -EINVAL;
-				break;
-			}
-
-			if (qtimer_desc) {
-				qtimerprop.gpuaddr = qtimer_desc->gpuaddr;
-				qtimerprop.size = qtimer_desc->size;
-			}
-
-			if (copy_to_user(value, &qtimerprop,
-						sizeof(qtimerprop))) {
-				status = -EFAULT;
-				break;
-			}
-			status = 0;
-		}
-		break;
 	case KGSL_PROP_MMU_ENABLE:
 		{
 			/* Report MMU only if we can handle paged memory */
@@ -2396,7 +1761,6 @@ static int adreno_getproperty(struct kgsl_device *device,
 	case KGSL_PROP_INTERRUPT_WAITS:
 		{
 			int int_waits = 1;
-
 			if (sizebytes != sizeof(int)) {
 				status = -EINVAL;
 				break;
@@ -2411,9 +1775,7 @@ static int adreno_getproperty(struct kgsl_device *device,
 	case KGSL_PROP_UCHE_GMEM_VADDR:
 		{
 			uint64_t gmem_vaddr = 0;
-
-			if (adreno_is_a5xx(adreno_dev) ||
-					adreno_is_a6xx(adreno_dev))
+			if (adreno_is_a5xx(adreno_dev))
 				gmem_vaddr = ADRENO_UCHE_GMEM_BASE;
 			if (sizebytes != sizeof(uint64_t)) {
 				status = -EINVAL;
@@ -2430,7 +1792,6 @@ static int adreno_getproperty(struct kgsl_device *device,
 	case KGSL_PROP_SP_GENERIC_MEM:
 		{
 			struct kgsl_sp_generic_mem sp_mem;
-
 			if (sizebytes != sizeof(sp_mem)) {
 				status = -EINVAL;
 				break;
@@ -2457,8 +1818,8 @@ static int adreno_getproperty(struct kgsl_device *device,
 			}
 			memset(&ucode, 0, sizeof(ucode));
 
-			ucode.pfp = adreno_dev->fw[ADRENO_FW_PFP].version;
-			ucode.pm4 = adreno_dev->fw[ADRENO_FW_PM4].version;
+			ucode.pfp = adreno_dev->pfp_fw_version;
+			ucode.pm4 = adreno_dev->pm4_fw_version;
 
 			if (copy_to_user(value, &ucode, sizeof(ucode))) {
 				status = -EFAULT;
@@ -2509,8 +1870,8 @@ static int adreno_getproperty(struct kgsl_device *device,
 
 			if (of_property_read_u32(device->pdev->dev.of_node,
 				"qcom,highest-bank-bit", &bit)) {
-				status = -EINVAL;
-				break;
+					status = -EINVAL;
+					break;
 			}
 
 			if (copy_to_user(value, &bit, sizeof(bit))) {
@@ -2520,68 +1881,6 @@ static int adreno_getproperty(struct kgsl_device *device,
 		}
 		status = 0;
 		break;
-	case KGSL_PROP_MIN_ACCESS_LENGTH:
-		{
-			unsigned int mal;
-
-			if (sizebytes < sizeof(unsigned int)) {
-				status = -EINVAL;
-				break;
-			}
-
-			if (of_property_read_u32(device->pdev->dev.of_node,
-				"qcom,min-access-length", &mal)) {
-				mal = 0;
-			}
-
-			if (copy_to_user(value, &mal, sizeof(mal))) {
-				status = -EFAULT;
-				break;
-			}
-		}
-		status = 0;
-		break;
-	case KGSL_PROP_UBWC_MODE:
-		{
-			unsigned int mode;
-
-			if (sizebytes < sizeof(unsigned int)) {
-				status = -EINVAL;
-				break;
-			}
-
-			if (of_property_read_u32(device->pdev->dev.of_node,
-				"qcom,ubwc-mode", &mode))
-				mode = 0;
-
-			if (copy_to_user(value, &mode, sizeof(mode))) {
-				status = -EFAULT;
-				break;
-			}
-		}
-		status = 0;
-		break;
-
-	case KGSL_PROP_DEVICE_BITNESS:
-	{
-		unsigned int bitness = 32;
-
-		if (sizebytes != sizeof(unsigned int)) {
-			status = -EINVAL;
-			break;
-		}
-		/* No of bits used by the GPU */
-		if (adreno_support_64bit(adreno_dev))
-			bitness = 48;
-
-		if (copy_to_user(value, &bitness,
-				sizeof(unsigned int))) {
-			status = -EFAULT;
-			break;
-		}
-		status = 0;
-	}
-	break;
 
 	default:
 		status = -EINVAL;
@@ -2633,39 +1932,7 @@ int adreno_set_constraint(struct kgsl_device *device,
 				context->pwr_constraint.sub_type);
 		context->pwr_constraint.type = KGSL_CONSTRAINT_NONE;
 		break;
-	case KGSL_CONSTRAINT_L3_PWRLEVEL: {
-		struct kgsl_device_constraint_pwrlevel pwr;
-
-		if (constraint->size != sizeof(pwr)) {
-			status = -EINVAL;
-			break;
-		}
 
-		if (copy_from_user(&pwr, (void __user *)constraint->data,
-					sizeof(pwr))) {
-			status = -EFAULT;
-			break;
-		}
-		if (pwr.level >= KGSL_CONSTRAINT_PWR_MAXLEVELS)
-			pwr.level = KGSL_CONSTRAINT_PWR_MAXLEVELS-1;
-
-		context->l3_pwr_constraint.type = KGSL_CONSTRAINT_L3_PWRLEVEL;
-		context->l3_pwr_constraint.sub_type = pwr.level;
-		trace_kgsl_user_pwrlevel_constraint(device, context->id,
-			context->l3_pwr_constraint.type,
-			context->l3_pwr_constraint.sub_type);
-		}
-		break;
-	case KGSL_CONSTRAINT_L3_NONE: {
-		unsigned int type = context->l3_pwr_constraint.type;
-
-		if (type == KGSL_CONSTRAINT_L3_PWRLEVEL)
-			trace_kgsl_user_pwrlevel_constraint(device, context->id,
-				KGSL_CONSTRAINT_L3_NONE,
-				context->l3_pwr_constraint.sub_type);
-		context->l3_pwr_constraint.type = KGSL_CONSTRAINT_L3_NONE;
-		}
-		break;
 	default:
 		status = -EINVAL;
 		break;
@@ -2719,7 +1986,7 @@ static int adreno_setproperty(struct kgsl_device_private *dev_priv,
 							KGSL_STATE_ACTIVE);
 				device->pwrctrl.ctrl_flags = KGSL_PWR_ON;
 				adreno_fault_detect_stop(adreno_dev);
-				kgsl_pwrscale_disable(device, true);
+				kgsl_pwrscale_disable(device);
 			}
 
 			mutex_unlock(&device->mutex);
@@ -2747,27 +2014,7 @@ static int adreno_setproperty(struct kgsl_device_private *dev_priv,
 
 			status = adreno_set_constraint(device, context,
 								&constraint);
-			kgsl_context_put(context);
-		}
-		break;
-	case KGSL_PROP_L3_PWR_CONSTRAINT: {
-			struct kgsl_device_constraint constraint;
-			struct kgsl_context *context;
-
-			if (sizebytes != sizeof(constraint))
-				break;
-			if (copy_from_user(&constraint, value,
-				sizeof(constraint))) {
-				status = -EFAULT;
-				break;
-			}
-			context = kgsl_context_get_owner(dev_priv,
-							constraint.context_id);
 
-			if (context == NULL)
-				break;
-			status = adreno_set_constraint(device, context,
-							&constraint);
 			kgsl_context_put(context);
 		}
 		break;
@@ -2791,18 +2038,7 @@ inline unsigned int adreno_irq_pending(struct adreno_device *adreno_dev)
 
 	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_INT_0_STATUS, &status);
 
-	/*
-	 * IRQ handler clears the RBBM INT0 status register immediately
-	 * entering the ISR before actually serving the interrupt because
-	 * of this we can't rely only on RBBM INT0 status only.
-	 * Use pending_irq_refcnt along with RBBM INT0 to correctly
-	 * determine whether any IRQ is pending or not.
-	 */
-	if ((status & gpudev->irq->mask) ||
-		atomic_read(&adreno_dev->pending_irq_refcnt))
-		return 1;
-	else
-		return 0;
+	return (status & gpudev->irq->mask) ? 1 : 0;
 }
 
 
@@ -2817,19 +2053,6 @@ bool adreno_hw_isidle(struct adreno_device *adreno_dev)
 {
 	const struct adreno_gpu_core *gpucore = adreno_dev->gpucore;
 	unsigned int reg_rbbm_status;
-	struct adreno_gpudev *gpudev  = ADRENO_GPU_DEVICE(adreno_dev);
-
-	/* if hw driver implements idle check - use it */
-	if (gpudev->hw_isidle)
-		return gpudev->hw_isidle(adreno_dev);
-
-	if (adreno_is_a540(adreno_dev))
-		/**
-		 * Due to CRC idle throttling GPU
-		 * idle hysteresys can take up to
-		 * 3usec for expire - account for it
-		 */
-		udelay(5);
 
 	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_STATUS,
 		&reg_rbbm_status);
@@ -2852,20 +2075,12 @@ bool adreno_hw_isidle(struct adreno_device *adreno_dev)
  * The GPU hardware is reset but we never pull power so we can skip
  * a lot of the standard adreno_stop/adreno_start sequence
  */
-int adreno_soft_reset(struct kgsl_device *device)
+static int adreno_soft_reset(struct kgsl_device *device)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	int ret;
 
-	if (gpudev->oob_set) {
-		ret = gpudev->oob_set(adreno_dev, OOB_GPU_SET_MASK,
-				OOB_GPU_CHECK_MASK,
-				OOB_GPU_CLEAR_MASK);
-		if (ret)
-			return ret;
-	}
-
 	kgsl_pwrctrl_change_state(device, KGSL_STATE_AWARE);
 	adreno_set_active_ctxs_null(adreno_dev);
 
@@ -2878,37 +2093,20 @@ int adreno_soft_reset(struct kgsl_device *device)
 	/* save physical performance counter values before GPU soft reset */
 	adreno_perfcounter_save(adreno_dev);
 
+	kgsl_cffdump_close(device);
 	/* Reset the GPU */
-	if (gpudev->soft_reset)
-		ret = gpudev->soft_reset(adreno_dev);
-	else
-		ret = _soft_reset(adreno_dev);
-	if (ret) {
-		if (gpudev->oob_clear)
-			gpudev->oob_clear(adreno_dev, OOB_GPU_CLEAR_MASK);
-		return ret;
-	}
+	_soft_reset(adreno_dev);
 
-	/* Clear the busy_data stats - we're starting over from scratch */
-	adreno_dev->busy_data.gpu_busy = 0;
-	adreno_dev->busy_data.bif_ram_cycles = 0;
-	adreno_dev->busy_data.bif_ram_cycles_read_ch1 = 0;
-	adreno_dev->busy_data.bif_ram_cycles_write_ch0 = 0;
-	adreno_dev->busy_data.bif_ram_cycles_write_ch1 = 0;
-	adreno_dev->busy_data.bif_starved_ram = 0;
-	adreno_dev->busy_data.bif_starved_ram_ch1 = 0;
-
-	/* Set the page table back to the default page table */
-	adreno_ringbuffer_set_global(adreno_dev, 0);
-	kgsl_mmu_set_pt(&device->mmu, device->mmu.defaultpagetable);
-
-	_set_secvid(device);
+	/* start of new CFF after reset */
+	kgsl_cffdump_open(device);
 
 	/* Enable 64 bit gpu addr if feature is set */
 	if (gpudev->enable_64bit &&
 			adreno_support_64bit(adreno_dev))
 		gpudev->enable_64bit(adreno_dev);
 
+	/* Restore physical performance counter values after soft reset */
+	adreno_perfcounter_restore(adreno_dev);
 
 	/* Reinitialize the GPU */
 	gpudev->start(adreno_dev);
@@ -2930,17 +2128,19 @@ int adreno_soft_reset(struct kgsl_device *device)
 		ret = adreno_ringbuffer_start(adreno_dev, ADRENO_START_WARM);
 	else
 		ret = adreno_ringbuffer_start(adreno_dev, ADRENO_START_COLD);
-	if (ret == 0) {
-		device->reset_counter++;
-		set_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv);
-	}
+	if (ret)
+		goto done;
 
-	/* Restore physical performance counter values after soft reset */
-	adreno_perfcounter_restore(adreno_dev);
+	if (gpudev->hw_init)
+		ret = gpudev->hw_init(adreno_dev);
+	if (ret)
+		goto done;
 
-	if (gpudev->oob_clear)
-		gpudev->oob_clear(adreno_dev, OOB_GPU_CLEAR_MASK);
+	device->reset_counter++;
+	/* device is back online */
+	set_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv);
 
+done:
 	return ret;
 }
 
@@ -2960,6 +2160,8 @@ bool adreno_isidle(struct kgsl_device *device)
 	if (!kgsl_state_is_awake(device))
 		return true;
 
+	adreno_get_rptr(ADRENO_CURRENT_RINGBUFFER(adreno_dev));
+
 	/*
 	 * wptr is updated when we add commands to ringbuffer, add a barrier
 	 * to make sure updated wptr is compared to rptr
@@ -2970,41 +2172,15 @@ bool adreno_isidle(struct kgsl_device *device)
 	 * ringbuffer is truly idle when all ringbuffers read and write
 	 * pointers are equal
 	 */
-
 	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		if (!adreno_rb_empty(rb))
-			return false;
+		if (rb->rptr != rb->wptr)
+			break;
 	}
 
-	return adreno_hw_isidle(adreno_dev);
-}
-
-/* Print some key registers if a spin-for-idle times out */
-void adreno_spin_idle_debug(struct adreno_device *adreno_dev,
-		const char *str)
-{
-	struct kgsl_device *device = &adreno_dev->dev;
-	unsigned int rptr, wptr;
-	unsigned int status, status3, intstatus;
-	unsigned int hwfault;
-
-	dev_err(device->dev, str);
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR, &rptr);
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
-
-	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_STATUS, &status);
-	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_STATUS3, &status3);
-	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_INT_0_STATUS, &intstatus);
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_HW_FAULT, &hwfault);
+	if (i == adreno_dev->num_ringbuffers)
+		return adreno_hw_isidle(adreno_dev);
 
-	dev_err(device->dev,
-		"rb=%d pos=%X/%X rbbm_status=%8.8X/%8.8X int_0_status=%8.8X\n",
-		adreno_dev->cur_rb->id, rptr, wptr, status, status3, intstatus);
-
-	dev_err(device->dev, " hwfault=%8.8X\n", hwfault);
-
-	kgsl_device_snapshot(device, NULL, adreno_gmu_gpu_fault(adreno_dev));
+	return false;
 }
 
 /**
@@ -3018,6 +2194,10 @@ int adreno_spin_idle(struct adreno_device *adreno_dev, unsigned int timeout)
 {
 	unsigned long wait = jiffies + msecs_to_jiffies(timeout);
 
+	kgsl_cffdump_regpoll(KGSL_DEVICE(adreno_dev),
+		adreno_getreg(adreno_dev, ADRENO_REG_RBBM_STATUS) << 2,
+		0x00000000, 0x80000000);
+
 	do {
 		/*
 		 * If we fault, stop waiting and return an error. The dispatcher
@@ -3053,7 +2233,6 @@ int adreno_spin_idle(struct adreno_device *adreno_dev, unsigned int timeout)
  * @device: Pointer to the KGSL device structure for the GPU
  *
  * Wait up to ADRENO_IDLE_TIMEOUT milliseconds for the GPU hardware to go quiet.
- * Caller must hold the device mutex, and must not hold the dispatcher mutex.
  */
 
 int adreno_idle(struct kgsl_device *device)
@@ -3066,8 +2245,7 @@ int adreno_idle(struct kgsl_device *device)
 	 * more commands to the hardware
 	 */
 
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return -EDEADLK;
+	BUG_ON(!mutex_is_locked(&device->mutex));
 
 	/* Check if we are already idle before idling dispatcher */
 	if (adreno_isidle(device))
@@ -3087,12 +2265,12 @@ int adreno_idle(struct kgsl_device *device)
  * adreno_drain() - Drain the dispatch queue
  * @device: Pointer to the KGSL device structure for the GPU
  *
- * Drain the dispatcher of existing drawobjs.  This halts
+ * Drain the dispatcher of existing command batches.  This halts
  * additional commands from being issued until the gate is completed.
  */
 static int adreno_drain(struct kgsl_device *device)
 {
-	reinit_completion(&device->halt_gate);
+	reinit_completion(&device->cmdbatch_gate);
 
 	return 0;
 }
@@ -3100,11 +2278,25 @@ static int adreno_drain(struct kgsl_device *device)
 /* Caller must hold the device mutex. */
 static int adreno_suspend_context(struct kgsl_device *device)
 {
+	int status = 0;
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+
 	/* process any profiling results that are available */
-	adreno_profile_process_results(ADRENO_DEVICE(device));
+	adreno_profile_process_results(adreno_dev);
+
+	status = adreno_idle(device);
+	if (status)
+		return status;
+	/* set the device to default pagetable */
+	kgsl_mmu_set_pt(&device->mmu, device->mmu.defaultpagetable);
+	kgsl_sharedmem_writel(device,
+		&adreno_dev->ringbuffers[0].pagetable_desc,
+		offsetof(struct adreno_ringbuffer_pagetable_info,
+			current_global_ptname), 0);
+	/* set ringbuffers to NULL ctxt */
+	adreno_set_active_ctxs_null(adreno_dev);
 
-	/* Wait for the device to go idle */
-	return adreno_idle(device);
+	return status;
 }
 
 /**
@@ -3121,24 +2313,16 @@ static void adreno_read(struct kgsl_device *device, void __iomem *base,
 		unsigned int mem_len)
 {
 
-	void __iomem *reg;
-
-	/* Make sure we're not reading from invalid memory */
-	if (WARN(offsetwords * sizeof(uint32_t) >= mem_len,
-		"Out of bounds register read: 0x%x/0x%x\n",
-			offsetwords, mem_len >> 2))
-		return;
-
-	reg = (base + (offsetwords << 2));
+	unsigned int __iomem *reg;
+	BUG_ON(offsetwords*sizeof(uint32_t) >= mem_len);
+	reg = (unsigned int __iomem *)(base + (offsetwords << 2));
 
 	if (!in_interrupt())
 		kgsl_pre_hwaccess(device);
 
+	/*ensure this read finishes before the next one.
+	 * i.e. act like normal readl() */
 	*value = __raw_readl(reg);
-	/*
-	 * ensure this read finishes before the next one.
-	 * i.e. act like normal readl()
-	 */
 	rmb();
 }
 
@@ -3171,119 +2355,24 @@ static void adreno_regwrite(struct kgsl_device *device,
 				unsigned int offsetwords,
 				unsigned int value)
 {
-	void __iomem *reg;
+	unsigned int __iomem *reg;
 
-	/* Make sure we're not writing to an invalid register */
-	if (WARN(offsetwords * sizeof(uint32_t) >= device->reg_len,
-		"Out of bounds register write: 0x%x/0x%x\n",
-			offsetwords, device->reg_len >> 2))
-		return;
+	BUG_ON(offsetwords*sizeof(uint32_t) >= device->reg_len);
 
 	if (!in_interrupt())
 		kgsl_pre_hwaccess(device);
 
 	trace_kgsl_regwrite(device, offsetwords, value);
 
-	reg = (device->reg_virt + (offsetwords << 2));
-
-	/*
-	 * ensure previous writes post before this one,
-	 * i.e. act like normal writel()
-	 */
-	wmb();
-	__raw_writel(value, reg);
-}
-
-static void adreno_gmu_regwrite(struct kgsl_device *device,
-				unsigned int offsetwords,
-				unsigned int value)
-{
-	void __iomem *reg;
-	struct gmu_device *gmu = &device->gmu;
-
-	trace_kgsl_regwrite(device, offsetwords, value);
-
-	offsetwords -= gmu->gmu2gpu_offset;
-	reg = gmu->reg_virt + (offsetwords << 2);
+	kgsl_cffdump_regwrite(device, offsetwords << 2, value);
+	reg = (unsigned int __iomem *)(device->reg_virt + (offsetwords << 2));
 
-	/*
-	 * ensure previous writes post before this one,
-	 * i.e. act like normal writel()
-	 */
+	/*ensure previous writes post before this one,
+	 * i.e. act like normal writel() */
 	wmb();
 	__raw_writel(value, reg);
 }
 
-static void adreno_gmu_regread(struct kgsl_device *device,
-				unsigned int offsetwords,
-				unsigned int *value)
-{
-	void __iomem *reg;
-	struct gmu_device *gmu = &device->gmu;
-
-	offsetwords -= gmu->gmu2gpu_offset;
-
-	reg = gmu->reg_virt + (offsetwords << 2);
-
-	*value = __raw_readl(reg);
-
-	/*
-	 * ensure this read finishes before the next one.
-	 * i.e. act like normal readl()
-	 */
-	rmb();
-}
-
-bool adreno_is_cx_dbgc_register(struct kgsl_device *device,
-		unsigned int offsetwords)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	return adreno_dev->cx_dbgc_virt &&
-		(offsetwords >= (adreno_dev->cx_dbgc_base >> 2)) &&
-		(offsetwords < (adreno_dev->cx_dbgc_base +
-				adreno_dev->cx_dbgc_len) >> 2);
-}
-
-void adreno_cx_dbgc_regread(struct kgsl_device *device,
-	unsigned int offsetwords, unsigned int *value)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	unsigned int cx_dbgc_offset;
-
-	if (!adreno_is_cx_dbgc_register(device, offsetwords))
-		return;
-
-	cx_dbgc_offset = (offsetwords << 2) - adreno_dev->cx_dbgc_base;
-	*value = __raw_readl(adreno_dev->cx_dbgc_virt + cx_dbgc_offset);
-
-	/*
-	 * ensure this read finishes before the next one.
-	 * i.e. act like normal readl()
-	 */
-	rmb();
-}
-
-void adreno_cx_dbgc_regwrite(struct kgsl_device *device,
-	unsigned int offsetwords, unsigned int value)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	unsigned int cx_dbgc_offset;
-
-	if (!adreno_is_cx_dbgc_register(device, offsetwords))
-		return;
-
-	cx_dbgc_offset = (offsetwords << 2) - adreno_dev->cx_dbgc_base;
-	trace_kgsl_regwrite(device, offsetwords, value);
-
-	/*
-	 * ensure previous writes post before this one,
-	 * i.e. act like normal writel()
-	 */
-	wmb();
-	__raw_writel(value, adreno_dev->cx_dbgc_virt + cx_dbgc_offset);
-}
-
 /**
  * adreno_waittimestamp - sleep while waiting for the specified timestamp
  * @device - pointer to a KGSL device structure
@@ -3382,7 +2471,14 @@ int adreno_rb_readtimestamp(struct adreno_device *adreno_dev,
 	int status = 0;
 	struct adreno_ringbuffer *rb = priv;
 
-	if (type == KGSL_TIMESTAMP_QUEUED)
+	/*
+	 * If user passed in a NULL pointer for timestamp, return without
+	 * doing anything.
+	 */
+	if (!timestamp)
+		return status;
+
+	if (KGSL_TIMESTAMP_QUEUED == type)
 		*timestamp = rb->timestamp;
 	else
 		status = __adreno_readtimestamp(adreno_dev,
@@ -3412,63 +2508,50 @@ static int adreno_readtimestamp(struct kgsl_device *device,
 {
 	int status = 0;
 	struct kgsl_context *context = priv;
+	unsigned int id = KGSL_CONTEXT_ID(context);
 
-	if (type == KGSL_TIMESTAMP_QUEUED) {
-		struct adreno_context *ctxt = ADRENO_CONTEXT(context);
+	BUG_ON(NULL == context || id >= KGSL_MEMSTORE_MAX);
+	/*
+	 * If user passed in a NULL pointer for timestamp, return without
+	 * doing anything.
+	 */
+	if (!timestamp)
+		return status;
 
-		*timestamp = ctxt->timestamp;
-	} else
+	if (KGSL_TIMESTAMP_QUEUED == type)
+		*timestamp = adreno_context_timestamp(context);
+	else
 		status = __adreno_readtimestamp(ADRENO_DEVICE(device),
 				context->id, type, timestamp);
 
 	return status;
 }
 
-/**
- * adreno_device_private_create(): Allocate an adreno_device_private structure
- */
-static struct kgsl_device_private *adreno_device_private_create(void)
+static inline s64 adreno_ticks_to_us(u32 ticks, u32 freq)
 {
-	struct adreno_device_private *adreno_priv =
-			kzalloc(sizeof(*adreno_priv), GFP_KERNEL);
-
-	if (adreno_priv) {
-		INIT_LIST_HEAD(&adreno_priv->perfcounter_list);
-		return &adreno_priv->dev_priv;
-	}
-	return NULL;
+	freq /= 1000000;
+	return ticks / freq;
 }
 
-/**
- * adreno_device_private_destroy(): Destroy an adreno_device_private structure
- * and release the perfcounters held by the kgsl fd.
- * @dev_priv: The kgsl device private structure
- */
-static void adreno_device_private_destroy(struct kgsl_device_private *dev_priv)
+static unsigned int counter_delta(struct kgsl_device *device,
+			unsigned int reg, unsigned int *counter)
 {
-	struct kgsl_device *device = dev_priv->device;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_device_private *adreno_priv =
-		container_of(dev_priv, struct adreno_device_private,
-		dev_priv);
-	struct adreno_perfcounter_list_node *p, *tmp;
+	unsigned int val;
+	unsigned int ret = 0;
 
-	mutex_lock(&device->mutex);
-	list_for_each_entry_safe(p, tmp, &adreno_priv->perfcounter_list, node) {
-		adreno_perfcounter_put(adreno_dev, p->groupid,
-					p->countable, PERFCOUNTER_FLAG_NONE);
-		list_del(&p->node);
-		kfree(p);
-	}
-	mutex_unlock(&device->mutex);
+	/* Read the value */
+	kgsl_regread(device, reg, &val);
 
-	kfree(adreno_priv);
-}
+	/* Return 0 for the first read */
+	if (*counter != 0) {
+		if (val < *counter)
+			ret = (0xFFFFFFFF - *counter) + val;
+		else
+			ret = val - *counter;
+	}
 
-static inline s64 adreno_ticks_to_us(u32 ticks, u32 freq)
-{
-	freq /= 1000000;
-	return ticks / freq;
+	*counter = val;
+	return ret;
 }
 
 /**
@@ -3482,7 +2565,6 @@ static void adreno_power_stats(struct kgsl_device *device,
 				struct kgsl_power_stats *stats)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev  = ADRENO_GPU_DEVICE(adreno_dev);
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
 	struct adreno_busy_data *busy = &adreno_dev->busy_data;
 	uint64_t adj = 0;
@@ -3496,20 +2578,10 @@ static void adreno_power_stats(struct kgsl_device *device,
 		gpu_busy = counter_delta(device, adreno_dev->perfctr_pwr_lo,
 			&busy->gpu_busy);
 
-		if (gpudev->read_throttling_counters) {
-			adj = gpudev->read_throttling_counters(adreno_dev);
-			gpu_busy += adj;
-		}
-
-		if (adreno_is_a6xx(adreno_dev)) {
-			/* clock sourced from XO */
-			stats->busy_time = gpu_busy * 10;
-			do_div(stats->busy_time, 192);
-		} else {
-			/* clock sourced from GFX3D */
-			stats->busy_time = adreno_ticks_to_us(gpu_busy,
-				kgsl_pwrctrl_active_freq(pwr));
-		}
+		adj = _read_throttling_counters(adreno_dev);
+		gpu_busy += adj;
+		stats->busy_time = adreno_ticks_to_us(gpu_busy,
+			kgsl_pwrctrl_active_freq(pwr));
 	}
 
 	if (device->pwrctrl.bus_control) {
@@ -3518,43 +2590,18 @@ static void adreno_power_stats(struct kgsl_device *device,
 		if (adreno_dev->ram_cycles_lo != 0)
 			ram_cycles = counter_delta(device,
 				adreno_dev->ram_cycles_lo,
-				&busy->bif_ram_cycles);
-
-		if (adreno_has_gbif(adreno_dev)) {
-			if (adreno_dev->ram_cycles_lo_ch1_read != 0)
-				ram_cycles += counter_delta(device,
-					adreno_dev->ram_cycles_lo_ch1_read,
-					&busy->bif_ram_cycles_read_ch1);
-
-			if (adreno_dev->ram_cycles_lo_ch0_write != 0)
-				ram_cycles += counter_delta(device,
-					adreno_dev->ram_cycles_lo_ch0_write,
-					&busy->bif_ram_cycles_write_ch0);
-
-			if (adreno_dev->ram_cycles_lo_ch1_write != 0)
-				ram_cycles += counter_delta(device,
-					adreno_dev->ram_cycles_lo_ch1_write,
-					&busy->bif_ram_cycles_write_ch1);
-		}
+				&busy->vbif_ram_cycles);
 
 		if (adreno_dev->starved_ram_lo != 0)
 			starved_ram = counter_delta(device,
 				adreno_dev->starved_ram_lo,
-				&busy->bif_starved_ram);
-
-		if (adreno_has_gbif(adreno_dev)) {
-			if (adreno_dev->starved_ram_lo_ch1 != 0)
-				starved_ram += counter_delta(device,
-					adreno_dev->starved_ram_lo_ch1,
-					&busy->bif_starved_ram_ch1);
-		}
+				&busy->vbif_starved_ram);
 
 		stats->ram_time = ram_cycles;
 		stats->ram_wait = starved_ram;
 	}
-	if (adreno_dev->lm_threshold_count &&
-			gpudev->count_throttles)
-		gpudev->count_throttles(adreno_dev, adj);
+	if (adreno_dev->lm_threshold_count)
+		_update_threshold_count(adreno_dev, adj);
 }
 
 static unsigned int adreno_gpuid(struct kgsl_device *device,
@@ -3562,16 +2609,13 @@ static unsigned int adreno_gpuid(struct kgsl_device *device,
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 
-	/*
-	 * Some applications need to know the chip ID too, so pass
-	 * that as a parameter
-	 */
+	/* Some applications need to know the chip ID too, so pass
+	 * that as a parameter */
 
 	if (chipid != NULL)
 		*chipid = adreno_dev->chipid;
 
-	/*
-	 * Standard KGSL gpuid format:
+	/* Standard KGSL gpuid format:
 	 * top word is 0x0002 for 2D or 0x0003 for 3D
 	 * Bottom word is core specific identifer
 	 */
@@ -3584,7 +2628,6 @@ static int adreno_regulator_enable(struct kgsl_device *device)
 	int ret = 0;
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct adreno_gpudev *gpudev  = ADRENO_GPU_DEVICE(adreno_dev);
-
 	if (gpudev->regulator_enable &&
 		!test_bit(ADRENO_DEVICE_GPU_REGULATOR_ENABLED,
 			&adreno_dev->priv)) {
@@ -3618,7 +2661,6 @@ static void adreno_regulator_disable(struct kgsl_device *device)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct adreno_gpudev *gpudev  = ADRENO_GPU_DEVICE(adreno_dev);
-
 	if (gpudev->regulator_disable &&
 		test_bit(ADRENO_DEVICE_GPU_REGULATOR_ENABLED,
 			&adreno_dev->priv)) {
@@ -3639,14 +2681,6 @@ static void adreno_pwrlevel_change_settings(struct kgsl_device *device,
 					postlevel, post);
 }
 
-static void adreno_clk_set_options(struct kgsl_device *device, const char *name,
-	struct clk *clk, bool on)
-{
-	if (ADRENO_GPU_DEVICE(ADRENO_DEVICE(device))->clk_set_options)
-		ADRENO_GPU_DEVICE(ADRENO_DEVICE(device))->clk_set_options(
-			ADRENO_DEVICE(device), name, clk, on);
-}
-
 static void adreno_iommu_sync(struct kgsl_device *device, bool sync)
 {
 	struct scm_desc desc = {0};
@@ -3710,24 +2744,10 @@ static void adreno_regulator_disable_poll(struct kgsl_device *device)
 	adreno_iommu_sync(device, false);
 }
 
-static void adreno_gpu_model(struct kgsl_device *device, char *str,
-				size_t bufsz)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	snprintf(str, bufsz, "Adreno%d%d%dv%d",
-			ADRENO_CHIPID_CORE(adreno_dev->chipid),
-			 ADRENO_CHIPID_MAJOR(adreno_dev->chipid),
-			 ADRENO_CHIPID_MINOR(adreno_dev->chipid),
-			 ADRENO_CHIPID_PATCH(adreno_dev->chipid) + 1);
-}
-
 static const struct kgsl_functable adreno_functable = {
 	/* Mandatory functions */
 	.regread = adreno_regread,
 	.regwrite = adreno_regwrite,
-	.gmu_regread = adreno_gmu_regread,
-	.gmu_regwrite = adreno_gmu_regwrite,
 	.idle = adreno_idle,
 	.isidle = adreno_isidle,
 	.suspend_context = adreno_suspend_context,
@@ -3738,7 +2758,7 @@ static const struct kgsl_functable adreno_functable = {
 	.getproperty_compat = adreno_getproperty_compat,
 	.waittimestamp = adreno_waittimestamp,
 	.readtimestamp = adreno_readtimestamp,
-	.queue_cmds = adreno_dispatcher_queue_cmds,
+	.issueibcmds = adreno_ringbuffer_issueibcmds,
 	.ioctl = adreno_ioctl,
 	.compat_ioctl = adreno_compat_ioctl,
 	.power_stats = adreno_power_stats,
@@ -3746,10 +2766,7 @@ static const struct kgsl_functable adreno_functable = {
 	.snapshot = adreno_snapshot,
 	.irq_handler = adreno_irq_handler,
 	.drain = adreno_drain,
-	.device_private_create = adreno_device_private_create,
-	.device_private_destroy = adreno_device_private_destroy,
 	/* Optional functions */
-	.snapshot_gmu = adreno_snapshot_gmu,
 	.drawctxt_create = adreno_drawctxt_create,
 	.drawctxt_detach = adreno_drawctxt_detach,
 	.drawctxt_destroy = adreno_drawctxt_destroy,
@@ -3763,11 +2780,6 @@ static const struct kgsl_functable adreno_functable = {
 	.regulator_disable = adreno_regulator_disable,
 	.pwrlevel_change_settings = adreno_pwrlevel_change_settings,
 	.regulator_disable_poll = adreno_regulator_disable_poll,
-	.clk_set_options = adreno_clk_set_options,
-	.gpu_model = adreno_gpu_model,
-	.stop_fault_timer = adreno_dispatcher_stop_fault_timer,
-	.dispatcher_halt = adreno_dispatcher_halt,
-	.dispatcher_unhalt = adreno_dispatcher_unhalt,
 };
 
 static struct platform_driver adreno_platform_driver = {
@@ -3784,7 +2796,7 @@ static struct platform_driver adreno_platform_driver = {
 	}
 };
 
-static const struct of_device_id busmon_match_table[] = {
+static struct of_device_id busmon_match_table[] = {
 	{ .compatible = "qcom,kgsl-busmon", .data = &device_3d0 },
 	{}
 };
@@ -3839,5 +2851,6 @@ module_init(kgsl_3d_init);
 module_exit(kgsl_3d_exit);
 
 MODULE_DESCRIPTION("3D Graphics driver");
+MODULE_VERSION("1.2");
 MODULE_LICENSE("GPL v2");
 MODULE_ALIAS("platform:kgsl_3d");
diff --git a/drivers/gpu/msm/adreno.h b/drivers/gpu/msm/adreno.h
index fc32fb9cf58a..18ae12d6f17b 100644
--- a/drivers/gpu/msm/adreno.h
+++ b/drivers/gpu/msm/adreno.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2008-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -23,11 +23,10 @@
 #include "adreno_perfcounter.h"
 #include <linux/stat.h>
 #include <linux/delay.h>
-#include "kgsl_gmu.h"
 
 #include "a4xx_reg.h"
 
-#ifdef CONFIG_QCOM_OCMEM
+#ifdef CONFIG_MSM_OCMEM
 #include <soc/qcom/ocmem.h>
 #endif
 
@@ -56,6 +55,9 @@
 /* ADRENO_GPUREV - Return the GPU ID for the given adreno_device */
 #define ADRENO_GPUREV(_a) ((_a)->gpucore->gpurev)
 
+/* ADRENO_GPUREV - Return the GPU patchid for the given adreno_device */
+#define ADRENO_PATCHID(_a) ((_a)->gpucore->patchid)
+
 /*
  * ADRENO_FEATURE - return true if the specified feature is supported by the GPU
  * core
@@ -77,17 +79,15 @@
 		  KGSL_CONTEXT_PREEMPT_STYLE_SHIFT)
 
 /*
- * return the dispatcher drawqueue in which the given drawobj should
+ * return the dispatcher cmdqueue in which the given cmdbatch should
  * be submitted
  */
-#define ADRENO_DRAWOBJ_DISPATCH_DRAWQUEUE(c)	\
+#define ADRENO_CMDBATCH_DISPATCH_CMDQUEUE(c)	\
 	(&((ADRENO_CONTEXT(c->context))->rb->dispatch_q))
 
-#define ADRENO_DRAWOBJ_RB(c)			\
+#define ADRENO_CMDBATCH_RB(c)			\
 	((ADRENO_CONTEXT(c->context))->rb)
 
-#define ADRENO_FW(a, f)		(&(a->fw[f]))
-
 /* Adreno core features */
 /* The core uses OCMEM for GMEM/binning memory */
 #define ADRENO_USES_OCMEM     BIT(0)
@@ -111,24 +111,12 @@
 #define ADRENO_64BIT BIT(9)
 /* The GPU supports retention for cpz registers */
 #define ADRENO_CPZ_RETENTION BIT(10)
-/* The core has soft fault detection available */
-#define ADRENO_SOFT_FAULT_DETECT BIT(11)
-/* The GMU supports RPMh for power management*/
-#define ADRENO_RPMH BIT(12)
-/* The GMU supports IFPC power management*/
-#define ADRENO_IFPC BIT(13)
-/* The GMU supports HW based NAP */
-#define ADRENO_HW_NAP BIT(14)
-/* The GMU supports min voltage*/
-#define ADRENO_MIN_VOLT BIT(15)
-/* The core supports IO-coherent memory */
-#define ADRENO_IOCOHERENT BIT(16)
 
 /*
  * Adreno GPU quirks - control bits for various workarounds
  */
 
-/* Set TWOPASSUSEWFI in PC_DBG_ECO_CNTL (5XX/6XX) */
+/* Set TWOPASSUSEWFI in PC_DBG_ECO_CNTL (5XX) */
 #define ADRENO_QUIRK_TWO_PASS_USE_WFI BIT(0)
 /* Lock/unlock mutex to sync with the IOMMU */
 #define ADRENO_QUIRK_IOMMU_SYNC BIT(1)
@@ -136,22 +124,6 @@
 #define ADRENO_QUIRK_CRITICAL_PACKETS BIT(2)
 /* Mask out RB1-3 activity signals from HW hang detection logic */
 #define ADRENO_QUIRK_FAULT_DETECT_MASK BIT(3)
-/* Disable RB sampler datapath clock gating optimization */
-#define ADRENO_QUIRK_DISABLE_RB_DP2CLOCKGATING BIT(4)
-/* Disable local memory(LM) feature to avoid corner case error */
-#define ADRENO_QUIRK_DISABLE_LMLOADKILL BIT(5)
-/* Allow HFI to use registers to send message to GMU */
-#define ADRENO_QUIRK_HFI_USE_REG BIT(6)
-/* Only set protected SECVID registers once */
-#define ADRENO_QUIRK_SECVID_SET_ONCE BIT(7)
-/*
- * Limit number of read and write transactions from
- * UCHE block to GBIF to avoid possible deadlock
- * between GBIF, SMMU and MEMNOC.
- */
-#define ADRENO_QUIRK_LIMIT_UCHE_GBIF_RW BIT(8)
-/* Select alternate secure context bank for mmu */
-#define ADRENO_QUIRK_MMU_SECURE_CB_ALT BIT(9)
 
 /* Flags to control command packet settings */
 #define KGSL_CMD_FLAGS_NONE             0
@@ -160,6 +132,7 @@
 #define KGSL_CMD_FLAGS_WFI              BIT(2)
 #define KGSL_CMD_FLAGS_PROFILE		BIT(3)
 #define KGSL_CMD_FLAGS_PWRON_FIXUP      BIT(4)
+#define KGSL_CMD_FLAGS_MEMLIST          BIT(5)
 
 /* Command identifiers */
 #define KGSL_CONTEXT_TO_MEM_IDENTIFIER	0x2EADBEEF
@@ -167,30 +140,24 @@
 #define KGSL_CMD_INTERNAL_IDENTIFIER	0x2EEDD00D
 #define KGSL_START_OF_IB_IDENTIFIER	0x2EADEABE
 #define KGSL_END_OF_IB_IDENTIFIER	0x2ABEDEAD
+#define KGSL_END_OF_FRAME_IDENTIFIER	0x2E0F2E0F
+#define KGSL_NOP_IB_IDENTIFIER	        0x20F20F20
 #define KGSL_START_OF_PROFILE_IDENTIFIER	0x2DEFADE1
 #define KGSL_END_OF_PROFILE_IDENTIFIER	0x2DEFADE2
 #define KGSL_PWRON_FIXUP_IDENTIFIER	0x2AFAFAFA
 
-/* Number of times to try hard reset */
-#define NUM_TIMES_RESET_RETRY 5
+#define ADRENO_ISTORE_START 0x5000 /* Istore offset */
 
-/* Number of times to try loading of zap shader */
-#define ZAP_RETRY_MAX	5
-
-/* Number of times to poll the AHB fence in ISR */
-#define FENCE_RETRY_MAX 100
+#define ADRENO_NUM_CTX_SWITCH_ALLOWED_BEFORE_DRAW	50
 
 /* One cannot wait forever for the core to idle, so set an upper limit to the
  * amount of time to wait for the core to go idle
  */
+
 #define ADRENO_IDLE_TIMEOUT (20 * 1000)
 
 #define ADRENO_UCHE_GMEM_BASE	0x100000
 
-#define ADRENO_FW_PFP 0
-#define ADRENO_FW_SQE 0
-#define ADRENO_FW_PM4 1
-
 enum adreno_gpurev {
 	ADRENO_REV_UNKNOWN = 0,
 	ADRENO_REV_A304 = 304,
@@ -206,17 +173,11 @@ enum adreno_gpurev {
 	ADRENO_REV_A418 = 418,
 	ADRENO_REV_A420 = 420,
 	ADRENO_REV_A430 = 430,
-	ADRENO_REV_A504 = 504,
 	ADRENO_REV_A505 = 505,
 	ADRENO_REV_A506 = 506,
-	ADRENO_REV_A508 = 508,
 	ADRENO_REV_A510 = 510,
-	ADRENO_REV_A512 = 512,
 	ADRENO_REV_A530 = 530,
 	ADRENO_REV_A540 = 540,
-	ADRENO_REV_A615 = 615,
-	ADRENO_REV_A616 = 616,
-	ADRENO_REV_A630 = 630,
 };
 
 #define ADRENO_START_WARM 0
@@ -227,19 +188,11 @@ enum adreno_gpurev {
 #define ADRENO_TIMEOUT_FAULT BIT(2)
 #define ADRENO_IOMMU_PAGE_FAULT BIT(3)
 #define ADRENO_PREEMPT_FAULT BIT(4)
-#define ADRENO_GMU_FAULT BIT(5)
-#define ADRENO_CTX_DETATCH_TIMEOUT_FAULT BIT(6)
 
 #define ADRENO_SPTP_PC_CTRL 0
 #define ADRENO_PPD_CTRL     1
 #define ADRENO_LM_CTRL      2
 #define ADRENO_HWCG_CTRL    3
-#define ADRENO_THROTTLING_CTRL 4
-
-/* VBIF,  GBIF halt request and ack mask */
-#define GBIF_HALT_REQUEST       0x1E0
-#define VBIF_RESET_ACK_MASK     0x00f0
-#define VBIF_RESET_ACK_TIMEOUT  100
 
 /* number of throttle counters for DCVS adjustment */
 #define ADRENO_GPMU_THROTTLE_COUNTERS 4
@@ -248,107 +201,13 @@ enum adreno_gpurev {
 
 struct adreno_gpudev;
 
-/* Time to allow preemption to complete (in ms) */
-#define ADRENO_PREEMPT_TIMEOUT 10000
-
-#define ADRENO_INT_BIT(a, _bit) (((a)->gpucore->gpudev->int_bits) ? \
-		(adreno_get_int(a, _bit) < 0 ? 0 : \
-		BIT(adreno_get_int(a, _bit))) : 0)
-
-/**
- * enum adreno_preempt_states
- * ADRENO_PREEMPT_NONE: No preemption is scheduled
- * ADRENO_PREEMPT_START: The S/W has started
- * ADRENO_PREEMPT_TRIGGERED: A preeempt has been triggered in the HW
- * ADRENO_PREEMPT_FAULTED: The preempt timer has fired
- * ADRENO_PREEMPT_PENDING: The H/W has signaled preemption complete
- * ADRENO_PREEMPT_COMPLETE: Preemption could not be finished in the IRQ handler,
- * worker has been scheduled
- */
-enum adreno_preempt_states {
-	ADRENO_PREEMPT_NONE = 0,
-	ADRENO_PREEMPT_START,
-	ADRENO_PREEMPT_TRIGGERED,
-	ADRENO_PREEMPT_FAULTED,
-	ADRENO_PREEMPT_PENDING,
-	ADRENO_PREEMPT_COMPLETE,
-};
-
-/**
- * struct adreno_preemption
- * @state: The current state of preemption
- * @counters: Memory descriptor for the memory where the GPU writes the
- * preemption counters on switch
- * @timer: A timer to make sure preemption doesn't stall
- * @work: A work struct for the preemption worker (for 5XX)
- * @token_submit: Indicates if a preempt token has been submitted in
- * current ringbuffer (for 4XX)
- * preempt_level: The level of preemption (for 6XX)
- * skipsaverestore: To skip saverestore during L1 preemption (for 6XX)
- * usesgmem: enable GMEM save/restore across preemption (for 6XX)
- * count: Track the number of preemptions triggered
- */
-struct adreno_preemption {
-	atomic_t state;
-	struct kgsl_memdesc counters;
-	struct timer_list timer;
-	struct work_struct work;
-	bool token_submit;
-	unsigned int preempt_level;
-	bool skipsaverestore;
-	bool usesgmem;
-	unsigned int count;
-};
-
-
 struct adreno_busy_data {
 	unsigned int gpu_busy;
-	unsigned int bif_ram_cycles;
-	unsigned int bif_ram_cycles_read_ch1;
-	unsigned int bif_ram_cycles_write_ch0;
-	unsigned int bif_ram_cycles_write_ch1;
-	unsigned int bif_starved_ram;
-	unsigned int bif_starved_ram_ch1;
+	unsigned int vbif_ram_cycles;
+	unsigned int vbif_starved_ram;
 	unsigned int throttle_cycles[ADRENO_GPMU_THROTTLE_COUNTERS];
 };
 
-/**
- * struct adreno_firmware - Struct holding fw details
- * @fwvirt: Buffer which holds the ucode
- * @size: Size of ucode buffer
- * @version: Version of ucode
- * @memdesc: Memory descriptor which holds ucode buffer info
- */
-struct adreno_firmware {
-	unsigned int *fwvirt;
-	size_t size;
-	unsigned int version;
-	struct kgsl_memdesc memdesc;
-};
-
-/**
- * struct adreno_perfcounter_list_node - struct to store perfcounters
- * allocated by a process on a kgsl fd.
- * @groupid: groupid of the allocated perfcounter
- * @countable: countable assigned to the allocated perfcounter
- * @node: list node for perfcounter_list of a process
- */
-struct adreno_perfcounter_list_node {
-	unsigned int groupid;
-	unsigned int countable;
-	struct list_head node;
-};
-
-/**
- * struct adreno_device_private - Adreno private structure per fd
- * @dev_priv: the kgsl device private structure
- * @perfcounter_list: list of perfcounters used by the process
- */
-struct adreno_device_private {
-	struct kgsl_device_private dev_priv;
-	struct list_head perfcounter_list;
-};
-
 /**
  * struct adreno_gpu_core - A specific GPU core definition
  * @gpurev: Unique GPU revision identifier
@@ -389,7 +248,6 @@ struct adreno_gpu_core {
 	unsigned long features;
 	const char *pm4fw_name;
 	const char *pfpfw_name;
-	const char *sqefw_name;
 	const char *zap_name;
 	struct adreno_gpudev *gpudev;
 	size_t gmem_size;
@@ -455,27 +313,14 @@ enum gpu_coresight_sources {
  * @pwron_fixup_dwords: Number of dwords in the command buffer
  * @input_work: Work struct for turning on the GPU after a touch event
  * @busy_data: Struct holding GPU VBIF busy stats
- * @ram_cycles_lo: Number of DDR clock cycles for the monitor session (Only
- * DDR channel 0 read cycles in case of GBIF)
- * @ram_cycles_lo_ch1_read: Number of DDR channel 1 Read clock cycles for
- * the monitor session
- * @ram_cycles_lo_ch0_write: Number of DDR channel 0 Write clock cycles for
- * the monitor session
- * @ram_cycles_lo_ch1_write: Number of DDR channel 0 Write clock cycles for
- * the monitor session
- * @starved_ram_lo: Number of cycles VBIF/GBIF is stalled by DDR (Only channel 0
- * stall cycles in case of GBIF)
- * @starved_ram_lo_ch1: Number of cycles GBIF is stalled by DDR channel 1
- * @perfctr_pwr_lo: GPU busy cycles
+ * @ram_cycles_lo: Number of DDR clock cycles for the monitor session
+ * @perfctr_pwr_lo: Number of cycles VBIF is stalled by DDR
  * @halt: Atomic variable to check whether the GPU is currently halted
- * @pending_irq_refcnt: Atomic variable to keep track of running IRQ handlers
  * @ctx_d_debugfs: Context debugfs node
  * @pwrctrl_flag: Flag to hold adreno specific power attributes
- * @profile_buffer: Memdesc holding the drawobj profiling buffer
- * @profile_index: Index to store the start/stop ticks in the profiling
+ * @cmdbatch_profile_buffer: Memdesc holding the cmdbatch profiling buffer
+ * @cmdbatch_profile_index: Index to store the start/stop ticks in the profiling
  * buffer
- * @pwrup_reglist: Memdesc holding the power up register list
- * which is used by CP during preemption and IFPC
  * @sp_local_gpuaddr: Base GPU virtual address for SP local memory
  * @sp_pvt_gpuaddr: Base GPU virtual address for SP private memory
  * @lm_fw: The LM firmware handle
@@ -488,14 +333,6 @@ enum gpu_coresight_sources {
  * @csdev: Pointer to a coresight device (if applicable)
  * @gpmu_throttle_counters - counteers for number of throttled clocks
  * @irq_storm_work: Worker to handle possible interrupt storms
- * @active_list: List to track active contexts
- * @active_list_lock: Lock to protect active_list
- * @gpu_llc_slice: GPU system cache slice descriptor
- * @gpu_llc_slice_enable: To enable the GPU system cache slice or not
- * @gpuhtw_llc_slice: GPU pagetables system cache slice descriptor
- * @gpuhtw_llc_slice_enable: To enable the GPUHTW system cache slice or not
- * @zap_loaded: Used to track if zap was successfully loaded or not
- * @soc_hw_rev: Indicate which SOC hardware revision to use
  */
 struct adreno_device {
 	struct kgsl_device dev;    /* Must be first field in this struct */
@@ -503,11 +340,15 @@ struct adreno_device {
 	unsigned int chipid;
 	unsigned long gmem_base;
 	unsigned long gmem_size;
-	unsigned long cx_dbgc_base;
-	unsigned int cx_dbgc_len;
-	void __iomem *cx_dbgc_virt;
 	const struct adreno_gpu_core *gpucore;
-	struct adreno_firmware fw[2];
+	unsigned int *pfp_fw;
+	size_t pfp_fw_size;
+	unsigned int pfp_fw_version;
+	struct kgsl_memdesc pfp;
+	unsigned int *pm4_fw;
+	size_t pm4_fw_size;
+	unsigned int pm4_fw_version;
+	struct kgsl_memdesc pm4;
 	size_t gpmu_cmds_size;
 	unsigned int *gpmu_cmds;
 	struct adreno_ringbuffer ringbuffers[KGSL_PRIORITY_MAX_RB_LEVELS];
@@ -527,26 +368,20 @@ struct adreno_device {
 	struct work_struct input_work;
 	struct adreno_busy_data busy_data;
 	unsigned int ram_cycles_lo;
-	unsigned int ram_cycles_lo_ch1_read;
-	unsigned int ram_cycles_lo_ch0_write;
-	unsigned int ram_cycles_lo_ch1_write;
 	unsigned int starved_ram_lo;
-	unsigned int starved_ram_lo_ch1;
 	unsigned int perfctr_pwr_lo;
 	atomic_t halt;
-	atomic_t pending_irq_refcnt;
 	struct dentry *ctx_d_debugfs;
 	unsigned long pwrctrl_flag;
 
-	struct kgsl_memdesc profile_buffer;
-	unsigned int profile_index;
-	struct kgsl_memdesc pwrup_reglist;
+	struct kgsl_memdesc cmdbatch_profile_buffer;
+	unsigned int cmdbatch_profile_index;
 	uint64_t sp_local_gpuaddr;
 	uint64_t sp_pvt_gpuaddr;
 	const struct firmware *lm_fw;
 	uint32_t *lm_sequence;
 	uint32_t lm_size;
-	struct adreno_preemption preempt;
+	struct kgsl_memdesc preemption_counters;
 	struct work_struct gpmu_work;
 	uint32_t lm_leakage;
 	uint32_t lm_limit;
@@ -556,19 +391,9 @@ struct adreno_device {
 	unsigned int speed_bin;
 	unsigned int quirks;
 
-	struct coresight_device *csdev[GPU_CORESIGHT_MAX];
+	struct coresight_device *csdev;
 	uint32_t gpmu_throttle_counters[ADRENO_GPMU_THROTTLE_COUNTERS];
 	struct work_struct irq_storm_work;
-
-	struct list_head active_list;
-	spinlock_t active_list_lock;
-
-	void *gpu_llc_slice;
-	bool gpu_llc_slice_enable;
-	void *gpuhtw_llc_slice;
-	bool gpuhtw_llc_slice_enable;
-	unsigned int zap_loaded;
-	unsigned int soc_hw_rev;
 };
 
 /**
@@ -583,7 +408,7 @@ struct adreno_device {
  * @ADRENO_DEVICE_STARTED - Set if the device start sequence is in progress
  * @ADRENO_DEVICE_FAULT - Set if the device is currently in fault (and shouldn't
  * send any more commands to the ringbuffer)
- * @ADRENO_DEVICE_DRAWOBJ_PROFILE - Set if the device supports drawobj
+ * @ADRENO_DEVICE_CMDBATCH_PROFILE - Set if the device supports command batch
  * profiling via the ALWAYSON counter
  * @ADRENO_DEVICE_PREEMPTION - Turn on/off preemption
  * @ADRENO_DEVICE_SOFT_FAULT_DETECT - Set if soft fault detect is enabled
@@ -592,7 +417,6 @@ struct adreno_device {
  * attached and enabled
  * @ADRENO_DEVICE_CACHE_FLUSH_TS_SUSPENDED - Set if a CACHE_FLUSH_TS irq storm
  * is in progress
- * @ADRENO_DEVICE_HARD_RESET - Set if soft reset fails and hard reset is needed
  */
 enum adreno_device_flags {
 	ADRENO_DEVICE_PWRON = 0,
@@ -602,34 +426,32 @@ enum adreno_device_flags {
 	ADRENO_DEVICE_HANG_INTR = 4,
 	ADRENO_DEVICE_STARTED = 5,
 	ADRENO_DEVICE_FAULT = 6,
-	ADRENO_DEVICE_DRAWOBJ_PROFILE = 7,
+	ADRENO_DEVICE_CMDBATCH_PROFILE = 7,
 	ADRENO_DEVICE_GPU_REGULATOR_ENABLED = 8,
 	ADRENO_DEVICE_PREEMPTION = 9,
 	ADRENO_DEVICE_SOFT_FAULT_DETECT = 10,
 	ADRENO_DEVICE_GPMU_INITIALIZED = 11,
 	ADRENO_DEVICE_ISDB_ENABLED = 12,
 	ADRENO_DEVICE_CACHE_FLUSH_TS_SUSPENDED = 13,
-	ADRENO_DEVICE_HARD_RESET = 14,
-	ADRENO_DEVICE_CORESIGHT_CX = 16,
 };
 
 /**
- * struct adreno_drawobj_profile_entry - a single drawobj entry in the
+ * struct adreno_cmdbatch_profile_entry - a single command batch entry in the
  * kernel profiling buffer
- * @started: Number of GPU ticks at start of the drawobj
- * @retired: Number of GPU ticks at the end of the drawobj
+ * @started: Number of GPU ticks at start of the command batch
+ * @retired: Number of GPU ticks at the end of the command batch
  */
-struct adreno_drawobj_profile_entry {
+struct adreno_cmdbatch_profile_entry {
 	uint64_t started;
 	uint64_t retired;
 };
 
-#define ADRENO_DRAWOBJ_PROFILE_COUNT \
-	(PAGE_SIZE / sizeof(struct adreno_drawobj_profile_entry))
+#define ADRENO_CMDBATCH_PROFILE_COUNT \
+	(PAGE_SIZE / sizeof(struct adreno_cmdbatch_profile_entry))
 
-#define ADRENO_DRAWOBJ_PROFILE_OFFSET(_index, _member) \
-	 ((_index) * sizeof(struct adreno_drawobj_profile_entry) \
-	  + offsetof(struct adreno_drawobj_profile_entry, _member))
+#define ADRENO_CMDBATCH_PROFILE_OFFSET(_index, _member) \
+	 ((_index) * sizeof(struct adreno_cmdbatch_profile_entry) \
+	  + offsetof(struct adreno_cmdbatch_profile_entry, _member))
 
 
 /**
@@ -646,8 +468,6 @@ enum adreno_regs {
 	ADRENO_REG_CP_WFI_PEND_CTR,
 	ADRENO_REG_CP_RB_BASE,
 	ADRENO_REG_CP_RB_BASE_HI,
-	ADRENO_REG_CP_RB_RPTR_ADDR_LO,
-	ADRENO_REG_CP_RB_RPTR_ADDR_HI,
 	ADRENO_REG_CP_RB_RPTR,
 	ADRENO_REG_CP_RB_WPTR,
 	ADRENO_REG_CP_CNTL,
@@ -678,13 +498,6 @@ enum adreno_regs {
 	ADRENO_REG_CP_PROTECT_REG_0,
 	ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_LO,
 	ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_HI,
-	ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_LO,
-	ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_HI,
-	ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_LO,
-	ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_HI,
-	ADRENO_REG_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_LO,
-	ADRENO_REG_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_HI,
-	ADRENO_REG_CP_PREEMPT_LEVEL_STATUS,
 	ADRENO_REG_RBBM_STATUS,
 	ADRENO_REG_RBBM_STATUS3,
 	ADRENO_REG_RBBM_PERFCTR_CTL,
@@ -705,13 +518,11 @@ enum adreno_regs {
 	ADRENO_REG_VPC_DEBUG_RAM_READ,
 	ADRENO_REG_PA_SC_AA_CONFIG,
 	ADRENO_REG_SQ_GPR_MANAGEMENT,
-	ADRENO_REG_SQ_INST_STORE_MANAGEMENT,
+	ADRENO_REG_SQ_INST_STORE_MANAGMENT,
 	ADRENO_REG_TP0_CHICKEN,
 	ADRENO_REG_RBBM_RBBM_CTL,
 	ADRENO_REG_UCHE_INVALIDATE0,
 	ADRENO_REG_UCHE_INVALIDATE1,
-	ADRENO_REG_RBBM_PERFCTR_RBBM_0_LO,
-	ADRENO_REG_RBBM_PERFCTR_RBBM_0_HI,
 	ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_LO,
 	ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_HI,
 	ADRENO_REG_RBBM_SECVID_TRUST_CONTROL,
@@ -722,42 +533,12 @@ enum adreno_regs {
 	ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE,
 	ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE_HI,
 	ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_SIZE,
-	ADRENO_REG_RBBM_GPR0_CNTL,
-	ADRENO_REG_RBBM_VBIF_GX_RESET_STATUS,
 	ADRENO_REG_VBIF_XIN_HALT_CTRL0,
 	ADRENO_REG_VBIF_XIN_HALT_CTRL1,
 	ADRENO_REG_VBIF_VERSION,
-	ADRENO_REG_GBIF_HALT,
-	ADRENO_REG_GBIF_HALT_ACK,
-	ADRENO_REG_GMU_AO_AHB_FENCE_CTRL,
-	ADRENO_REG_GMU_AO_INTERRUPT_EN,
-	ADRENO_REG_GMU_AO_HOST_INTERRUPT_CLR,
-	ADRENO_REG_GMU_AO_HOST_INTERRUPT_STATUS,
-	ADRENO_REG_GMU_AO_HOST_INTERRUPT_MASK,
-	ADRENO_REG_GMU_PWR_COL_KEEPALIVE,
-	ADRENO_REG_GMU_AHB_FENCE_STATUS,
-	ADRENO_REG_GMU_RPMH_POWER_STATE,
-	ADRENO_REG_GMU_HFI_CTRL_STATUS,
-	ADRENO_REG_GMU_HFI_VERSION_INFO,
-	ADRENO_REG_GMU_HFI_SFR_ADDR,
-	ADRENO_REG_GMU_GMU2HOST_INTR_CLR,
-	ADRENO_REG_GMU_GMU2HOST_INTR_INFO,
-	ADRENO_REG_GMU_GMU2HOST_INTR_MASK,
-	ADRENO_REG_GMU_HOST2GMU_INTR_SET,
-	ADRENO_REG_GMU_HOST2GMU_INTR_CLR,
-	ADRENO_REG_GMU_HOST2GMU_INTR_RAW_INFO,
-	ADRENO_REG_GMU_NMI_CONTROL_STATUS,
-	ADRENO_REG_GMU_CM3_CFG,
-	ADRENO_REG_GMU_RBBM_INT_UNMASKED_STATUS,
-	ADRENO_REG_GPMU_POWER_COUNTER_ENABLE,
 	ADRENO_REG_REGISTER_MAX,
 };
 
-enum adreno_int_bits {
-	ADRENO_INT_RBBM_AHB_ERROR,
-	ADRENO_INT_BITS_MAX,
-};
-
 /**
  * adreno_reg_offsets: Holds array of register offsets
  * @offsets: Offset array of size defined by enum adreno_regs
@@ -773,7 +554,6 @@ struct adreno_reg_offsets {
 #define ADRENO_REG_UNUSED	0xFFFFFFFF
 #define ADRENO_REG_SKIP	0xFFFFFFFE
 #define ADRENO_REG_DEFINE(_offset, _reg) [_offset] = _reg
-#define ADRENO_INT_DEFINE(_offset, _val) ADRENO_REG_DEFINE(_offset, _val)
 
 /*
  * struct adreno_vbif_data - Describes vbif register value pair
@@ -792,7 +572,7 @@ struct adreno_vbif_data {
  * @vbif: Array of reg value pairs for vbif registers
  */
 struct adreno_vbif_platform {
-	int (*devfunc)(struct adreno_device *);
+	int(*devfunc)(struct adreno_device *);
 	const struct adreno_vbif_data *vbif;
 };
 
@@ -800,13 +580,11 @@ struct adreno_vbif_platform {
  * struct adreno_vbif_snapshot_registers - Holds an array of vbif registers
  * listed for snapshot dump for a particular core
  * @version: vbif version
- * @mask: vbif revision mask
  * @registers: vbif registers listed for snapshot dump
  * @count: count of vbif registers listed for snapshot
  */
 struct adreno_vbif_snapshot_registers {
 	const unsigned int version;
-	const unsigned int mask;
 	const unsigned int *registers;
 	const int count;
 };
@@ -838,7 +616,7 @@ ssize_t adreno_coresight_store_register(struct device *dev,
 
 #define ADRENO_CORESIGHT_ATTR(_attrname, _reg) \
 	struct adreno_coresight_attr coresight_attr_##_attrname  = { \
-		__ATTR(_attrname, 0644, \
+		__ATTR(_attrname, S_IRUGO | S_IWUSR, \
 		adreno_coresight_show_register, \
 		adreno_coresight_store_register), \
 		(_reg), }
@@ -848,13 +626,11 @@ ssize_t adreno_coresight_store_register(struct device *dev,
  * @registers - Array of GPU specific registers to configure trace bus output
  * @count - Number of registers in the array
  * @groups - Pointer to an attribute list of control files
- * @atid - The unique ATID value of the coresight device
  */
 struct adreno_coresight {
 	struct adreno_coresight_register *registers;
 	unsigned int count;
 	const struct attribute_group **groups;
-	unsigned int atid;
 };
 
 
@@ -907,28 +683,21 @@ struct adreno_snapshot_data {
 	struct adreno_snapshot_sizes *sect_sizes;
 };
 
-enum adreno_cp_marker_type {
-	IFPC_DISABLE,
-	IFPC_ENABLE,
-	IB1LIST_START,
-	IB1LIST_END,
-};
-
 struct adreno_gpudev {
 	/*
 	 * These registers are in a different location on different devices,
 	 * so define them in the structure and use them as variables.
 	 */
 	const struct adreno_reg_offsets *reg_offsets;
-	unsigned int *const int_bits;
 	const struct adreno_ft_perf_counters *ft_perf_counters;
 	unsigned int ft_perf_counters_count;
 
 	struct adreno_perfcounters *perfcounters;
-	const struct adreno_invalid_countables *invalid_countables;
+	const struct adreno_invalid_countables
+			*invalid_countables;
 	struct adreno_snapshot_data *snapshot_data;
 
-	struct adreno_coresight *coresight[GPU_CORESIGHT_MAX];
+	struct adreno_coresight *coresight;
 
 	struct adreno_irq *irq;
 	int num_prio_levels;
@@ -936,12 +705,13 @@ struct adreno_gpudev {
 	/* GPU specific function hooks */
 	void (*irq_trace)(struct adreno_device *, unsigned int status);
 	void (*snapshot)(struct adreno_device *, struct kgsl_snapshot *);
-	void (*snapshot_gmu)(struct adreno_device *, struct kgsl_snapshot *);
 	void (*platform_setup)(struct adreno_device *);
 	void (*init)(struct adreno_device *);
 	void (*remove)(struct adreno_device *);
-	int (*rb_start)(struct adreno_device *, unsigned int start_type);
+	int (*rb_init)(struct adreno_device *, struct adreno_ringbuffer *);
+	int (*hw_init)(struct adreno_device *);
 	int (*microcode_read)(struct adreno_device *);
+	int (*microcode_load)(struct adreno_device *, unsigned int start_type);
 	void (*perfcounter_init)(struct adreno_device *);
 	void (*perfcounter_close)(struct adreno_device *);
 	void (*start)(struct adreno_device *);
@@ -951,55 +721,25 @@ struct adreno_gpudev {
 	void (*pwrlevel_change_settings)(struct adreno_device *,
 				unsigned int prelevel, unsigned int postlevel,
 				bool post);
-	uint64_t (*read_throttling_counters)(struct adreno_device *);
-	void (*count_throttles)(struct adreno_device *, uint64_t adj);
-	int (*enable_pwr_counters)(struct adreno_device *,
-				unsigned int counter);
-	unsigned int (*preemption_pre_ibsubmit)(
-				struct adreno_device *adreno_dev,
-				struct adreno_ringbuffer *rb,
-				unsigned int *cmds,
-				struct kgsl_context *context);
-	int (*preemption_yield_enable)(unsigned int *);
-	unsigned int (*set_marker)(unsigned int *cmds,
-				enum adreno_cp_marker_type type);
-	unsigned int (*preemption_post_ibsubmit)(
-				struct adreno_device *adreno_dev,
-				unsigned int *cmds);
+	int (*preemption_pre_ibsubmit)(struct adreno_device *,
+				struct adreno_ringbuffer *, unsigned int *,
+				struct kgsl_context *, uint64_t cond_addr,
+				struct kgsl_memobj_node *);
+	int (*preemption_post_ibsubmit)(struct adreno_device *,
+				struct adreno_ringbuffer *, unsigned int *,
+				struct kgsl_context *);
+	int (*preemption_token)(struct adreno_device *,
+				struct adreno_ringbuffer *, unsigned int *,
+				uint64_t gpuaddr);
 	int (*preemption_init)(struct adreno_device *);
 	void (*preemption_schedule)(struct adreno_device *);
-	int (*preemption_context_init)(struct kgsl_context *);
-	void (*preemption_context_destroy)(struct kgsl_context *);
 	void (*enable_64bit)(struct adreno_device *);
-	void (*clk_set_options)(struct adreno_device *,
-				const char *, struct clk *, bool on);
-	void (*llc_configure_gpu_scid)(struct adreno_device *adreno_dev);
-	void (*llc_configure_gpuhtw_scid)(struct adreno_device *adreno_dev);
-	void (*llc_enable_overrides)(struct adreno_device *adreno_dev);
 	void (*pre_reset)(struct adreno_device *);
-	int (*oob_set)(struct adreno_device *adreno_dev, unsigned int set_mask,
-				unsigned int check_mask,
-				unsigned int clear_mask);
-	void (*oob_clear)(struct adreno_device *adreno_dev,
-				unsigned int clear_mask);
-	void (*gpu_keepalive)(struct adreno_device *adreno_dev,
-			bool state);
-	int (*rpmh_gpu_pwrctrl)(struct adreno_device *, unsigned int ops,
-				unsigned int arg1, unsigned int arg2);
-	bool (*hw_isidle)(struct adreno_device *);
-	int (*wait_for_lowest_idle)(struct adreno_device *);
-	int (*wait_for_gmu_idle)(struct adreno_device *);
-	const char *(*iommu_fault_block)(struct adreno_device *adreno_dev,
-				unsigned int fsynr1);
-	int (*reset)(struct kgsl_device *, int fault);
-	int (*soft_reset)(struct adreno_device *);
-	bool (*gx_is_on)(struct adreno_device *);
-	bool (*sptprac_is_on)(struct adreno_device *);
-	unsigned int (*ccu_invalidate)(struct adreno_device *adreno_dev,
-				unsigned int *cmds);
-	int (*perfcounter_update)(struct adreno_device *adreno_dev,
-				struct adreno_perfcount_register *reg,
-				bool update_reg);
+};
+
+struct log_field {
+	bool show;
+	const char *display;
 };
 
 /**
@@ -1008,7 +748,7 @@ struct adreno_gpudev {
  * @KGSL_FT_REPLAY: Replay the faulting command
  * @KGSL_FT_SKIPIB: Skip the faulting indirect buffer
  * @KGSL_FT_SKIPFRAME: Skip the frame containing the faulting IB
- * @KGSL_FT_DISABLE: Tells the dispatcher to disable FT for the command obj
+ * @KGSL_FT_DISABLE: Tells the dispatcher to disable FT for the command batch
  * @KGSL_FT_TEMP_DISABLE: Disables FT for all commands
  * @KGSL_FT_THROTTLE: Disable the context if it faults too often
  * @KGSL_FT_SKIPCMD: Skip the command containing the faulting IB
@@ -1025,7 +765,7 @@ enum kgsl_ft_policy_bits {
 	/* KGSL_FT_MAX_BITS is used to calculate the mask */
 	KGSL_FT_MAX_BITS,
 	/* Internal bits - set during GFT */
-	/* Skip the PM dump on replayed command obj's */
+	/* Skip the PM dump on replayed command batches */
 	KGSL_FT_SKIP_PMDUMP = 31,
 };
 
@@ -1083,13 +823,10 @@ extern unsigned int *adreno_ft_regs_val;
 extern struct adreno_gpudev adreno_a3xx_gpudev;
 extern struct adreno_gpudev adreno_a4xx_gpudev;
 extern struct adreno_gpudev adreno_a5xx_gpudev;
-extern struct adreno_gpudev adreno_a6xx_gpudev;
 
 extern int adreno_wake_nice;
 extern unsigned int adreno_wake_timeout;
 
-int adreno_start(struct kgsl_device *device, int priority);
-int adreno_soft_reset(struct kgsl_device *device);
 long adreno_ioctl(struct kgsl_device_private *dev_priv,
 		unsigned int cmd, unsigned long arg);
 
@@ -1097,11 +834,6 @@ long adreno_ioctl_helper(struct kgsl_device_private *dev_priv,
 		unsigned int cmd, unsigned long arg,
 		const struct kgsl_ioctl *cmds, int len);
 
-int a5xx_critical_packet_submit(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb);
-int adreno_set_unsecured_mode(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb);
-void adreno_spin_idle_debug(struct adreno_device *adreno_dev, const char *str);
 int adreno_spin_idle(struct adreno_device *device, unsigned int timeout);
 int adreno_idle(struct kgsl_device *device);
 bool adreno_isidle(struct kgsl_device *device);
@@ -1118,14 +850,14 @@ void adreno_snapshot(struct kgsl_device *device,
 		struct kgsl_snapshot *snapshot,
 		struct kgsl_context *context);
 
-void adreno_snapshot_gmu(struct kgsl_device *device,
-		struct kgsl_snapshot *snapshot);
-
 int adreno_reset(struct kgsl_device *device, int fault);
 
 void adreno_fault_skipcmd_detached(struct adreno_device *adreno_dev,
 					 struct adreno_context *drawctxt,
-					 struct kgsl_drawobj *drawobj);
+					 struct kgsl_cmdbatch *cmdbatch);
+
+int adreno_a3xx_pwron_fixup_init(struct adreno_device *adreno_dev);
+int adreno_a4xx_pwron_fixup_init(struct adreno_device *adreno_dev);
 
 int adreno_coresight_init(struct adreno_device *adreno_dev);
 
@@ -1157,14 +889,6 @@ int adreno_efuse_map(struct adreno_device *adreno_dev);
 int adreno_efuse_read_u32(struct adreno_device *adreno_dev, unsigned int offset,
 		unsigned int *val);
 void adreno_efuse_unmap(struct adreno_device *adreno_dev);
-void adreno_efuse_speed_bin_array(struct adreno_device *adreno_dev);
-
-bool adreno_is_cx_dbgc_register(struct kgsl_device *device,
-		unsigned int offset);
-void adreno_cx_dbgc_regread(struct kgsl_device *adreno_device,
-		unsigned int offsetwords, unsigned int *value);
-void adreno_cx_dbgc_regwrite(struct kgsl_device *device,
-		unsigned int offsetwords, unsigned int value);
 
 #define ADRENO_TARGET(_name, _id) \
 static inline int adreno_is_##_name(struct adreno_device *adreno_dev) \
@@ -1230,12 +954,9 @@ static inline int adreno_is_a5xx(struct adreno_device *adreno_dev)
 			ADRENO_GPUREV(adreno_dev) < 600;
 }
 
-ADRENO_TARGET(a504, ADRENO_REV_A504)
 ADRENO_TARGET(a505, ADRENO_REV_A505)
 ADRENO_TARGET(a506, ADRENO_REV_A506)
-ADRENO_TARGET(a508, ADRENO_REV_A508)
 ADRENO_TARGET(a510, ADRENO_REV_A510)
-ADRENO_TARGET(a512, ADRENO_REV_A512)
 ADRENO_TARGET(a530, ADRENO_REV_A530)
 ADRENO_TARGET(a540, ADRENO_REV_A540)
 
@@ -1257,9 +978,9 @@ static inline int adreno_is_a530v3(struct adreno_device *adreno_dev)
 		(ADRENO_CHIPID_PATCH(adreno_dev->chipid) == 2);
 }
 
-static inline int adreno_is_a504_to_a506(struct adreno_device *adreno_dev)
+static inline int adreno_is_a505_or_a506(struct adreno_device *adreno_dev)
 {
-	return ADRENO_GPUREV(adreno_dev) >= 504 &&
+	return ADRENO_GPUREV(adreno_dev) >= 505 &&
 			ADRENO_GPUREV(adreno_dev) <= 506;
 }
 
@@ -1268,33 +989,17 @@ static inline int adreno_is_a540v1(struct adreno_device *adreno_dev)
 	return (ADRENO_GPUREV(adreno_dev) == ADRENO_REV_A540) &&
 		(ADRENO_CHIPID_PATCH(adreno_dev->chipid) == 0);
 }
-
-static inline int adreno_is_a540v2(struct adreno_device *adreno_dev)
-{
-	return (ADRENO_GPUREV(adreno_dev) == ADRENO_REV_A540) &&
-		(ADRENO_CHIPID_PATCH(adreno_dev->chipid) == 1);
-}
-
-static inline int adreno_is_a6xx(struct adreno_device *adreno_dev)
-{
-	return ADRENO_GPUREV(adreno_dev) >= 600 &&
-			ADRENO_GPUREV(adreno_dev) < 700;
-}
-
-ADRENO_TARGET(a615, ADRENO_REV_A615)
-ADRENO_TARGET(a616, ADRENO_REV_A616)
-ADRENO_TARGET(a630, ADRENO_REV_A630)
-
-static inline int adreno_is_a630v1(struct adreno_device *adreno_dev)
-{
-	return (ADRENO_GPUREV(adreno_dev) == ADRENO_REV_A630) &&
-		(ADRENO_CHIPID_PATCH(adreno_dev->chipid) == 0);
-}
-
-static inline int adreno_is_a630v2(struct adreno_device *adreno_dev)
+/**
+ * adreno_context_timestamp() - Return the last queued timestamp for the context
+ * @k_ctxt: Pointer to the KGSL context to query
+ *
+ * Return the last queued context for the given context. This is used to verify
+ * that incoming requests are not using an invalid (unsubmitted) timestamp
+ */
+static inline int adreno_context_timestamp(struct kgsl_context *k_ctxt)
 {
-	return (ADRENO_GPUREV(adreno_dev) == ADRENO_REV_A630) &&
-		(ADRENO_CHIPID_PATCH(adreno_dev->chipid) == 1);
+	struct adreno_context *drawctxt = ADRENO_CONTEXT(k_ctxt);
+	return drawctxt->timestamp;
 }
 
 /*
@@ -1308,8 +1013,8 @@ static inline bool adreno_checkreg_off(struct adreno_device *adreno_dev,
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 
 	if (offset_name >= ADRENO_REG_REGISTER_MAX ||
-		gpudev->reg_offsets->offsets[offset_name] == ADRENO_REG_UNUSED)
-		return false;
+		ADRENO_REG_UNUSED == gpudev->reg_offsets->offsets[offset_name])
+			BUG();
 
 	/*
 	 * GPU register programming is kept common as much as possible
@@ -1319,7 +1024,7 @@ static inline bool adreno_checkreg_off(struct adreno_device *adreno_dev,
 	 * Common programming programs 64bit register but upper 32 bits
 	 * are skipped in a4xx and a3xx using ADRENO_REG_SKIP.
 	 */
-	if (gpudev->reg_offsets->offsets[offset_name] == ADRENO_REG_SKIP)
+	if (ADRENO_REG_SKIP == gpudev->reg_offsets->offsets[offset_name])
 		return false;
 
 	return true;
@@ -1336,7 +1041,6 @@ static inline void adreno_readreg(struct adreno_device *adreno_dev,
 				enum adreno_regs offset_name, unsigned int *val)
 {
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
 	if (adreno_checkreg_off(adreno_dev, offset_name))
 		kgsl_regread(KGSL_DEVICE(adreno_dev),
 				gpudev->reg_offsets->offsets[offset_name], val);
@@ -1355,7 +1059,6 @@ static inline void adreno_writereg(struct adreno_device *adreno_dev,
 				enum adreno_regs offset_name, unsigned int val)
 {
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
 	if (adreno_checkreg_off(adreno_dev, offset_name))
 		kgsl_regwrite(KGSL_DEVICE(adreno_dev),
 				gpudev->reg_offsets->offsets[offset_name], val);
@@ -1371,65 +1074,11 @@ static inline unsigned int adreno_getreg(struct adreno_device *adreno_dev,
 				enum adreno_regs offset_name)
 {
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
 	if (!adreno_checkreg_off(adreno_dev, offset_name))
 		return ADRENO_REG_REGISTER_MAX;
 	return gpudev->reg_offsets->offsets[offset_name];
 }
 
-/*
- * adreno_read_gmureg() - Read a GMU register by getting its offset from the
- * offset array defined in gpudev node
- * @adreno_dev:		Pointer to the the adreno device
- * @offset_name:	The register enum that is to be read
- * @val:		Register value read is placed here
- */
-static inline void adreno_read_gmureg(struct adreno_device *adreno_dev,
-				enum adreno_regs offset_name, unsigned int *val)
-{
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	if (adreno_checkreg_off(adreno_dev, offset_name))
-		kgsl_gmu_regread(KGSL_DEVICE(adreno_dev),
-				gpudev->reg_offsets->offsets[offset_name], val);
-	else
-		*val = 0;
-}
-
-/*
- * adreno_write_gmureg() - Write a GMU register by getting its offset from the
- * offset array defined in gpudev node
- * @adreno_dev:		Pointer to the the adreno device
- * @offset_name:	The register enum that is to be written
- * @val:		Value to write
- */
-static inline void adreno_write_gmureg(struct adreno_device *adreno_dev,
-				enum adreno_regs offset_name, unsigned int val)
-{
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	if (adreno_checkreg_off(adreno_dev, offset_name))
-		kgsl_gmu_regwrite(KGSL_DEVICE(adreno_dev),
-				gpudev->reg_offsets->offsets[offset_name], val);
-}
-
-/*
- * adreno_get_int() - Returns the offset value of an interrupt bit from
- * the interrupt bit array in the gpudev node
- * @adreno_dev:		Pointer to the the adreno device
- * @bit_name:		The interrupt bit enum whose bit is returned
- */
-static inline unsigned int adreno_get_int(struct adreno_device *adreno_dev,
-				enum adreno_int_bits bit_name)
-{
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	if (bit_name >= ADRENO_INT_BITS_MAX)
-		return -ERANGE;
-
-	return gpudev->int_bits[bit_name];
-}
-
 /**
  * adreno_gpu_fault() - Return the current state of the GPU
  * @adreno_dev: A pointer to the adreno_device to query
@@ -1439,7 +1088,6 @@ static inline unsigned int adreno_get_int(struct adreno_device *adreno_dev,
  */
 static inline unsigned int adreno_gpu_fault(struct adreno_device *adreno_dev)
 {
-	/* make sure we're reading the latest value */
 	smp_rmb();
 	return atomic_read(&adreno_dev->dispatcher.fault);
 }
@@ -1455,15 +1103,9 @@ static inline void adreno_set_gpu_fault(struct adreno_device *adreno_dev,
 {
 	/* only set the fault bit w/o overwriting other bits */
 	atomic_add(state, &adreno_dev->dispatcher.fault);
-
-	/* make sure other CPUs see the update */
 	smp_wmb();
 }
 
-static inline bool adreno_gmu_gpu_fault(struct adreno_device *adreno_dev)
-{
-	return adreno_gpu_fault(adreno_dev) & ADRENO_GMU_FAULT;
-}
 
 /**
  * adreno_clear_gpu_fault() - Clear the GPU fault register
@@ -1475,8 +1117,6 @@ static inline bool adreno_gmu_gpu_fault(struct adreno_device *adreno_dev)
 static inline void adreno_clear_gpu_fault(struct adreno_device *adreno_dev)
 {
 	atomic_set(&adreno_dev->dispatcher.fault, 0);
-
-	/* make sure other CPUs see the update */
 	smp_wmb();
 }
 
@@ -1486,7 +1126,6 @@ static inline void adreno_clear_gpu_fault(struct adreno_device *adreno_dev)
  */
 static inline int adreno_gpu_halt(struct adreno_device *adreno_dev)
 {
-	/* make sure we're reading the latest value */
 	smp_rmb();
 	return atomic_read(&adreno_dev->halt);
 }
@@ -1499,8 +1138,6 @@ static inline int adreno_gpu_halt(struct adreno_device *adreno_dev)
 static inline void adreno_clear_gpu_halt(struct adreno_device *adreno_dev)
 {
 	atomic_set(&adreno_dev->halt, 0);
-
-	/* make sure other CPUs see the update */
 	smp_wmb();
 }
 
@@ -1519,10 +1156,8 @@ static inline void adreno_get_gpu_halt(struct adreno_device *adreno_dev)
  */
 static inline void adreno_put_gpu_halt(struct adreno_device *adreno_dev)
 {
-	/* Make sure the refcount is good */
-	int ret = atomic_dec_if_positive(&adreno_dev->halt);
-
-	WARN(ret < 0, "GPU halt refcount unbalanced\n");
+	if (atomic_dec_return(&adreno_dev->halt) < 0)
+		BUG();
 }
 
 
@@ -1576,13 +1211,11 @@ static inline void adreno_set_protected_registers(
 	unsigned int base =
 		adreno_getreg(adreno_dev, ADRENO_REG_CP_PROTECT_REG_0);
 	unsigned int offset = *index;
-	unsigned int max_slots = adreno_dev->gpucore->num_protected_regs ?
-				adreno_dev->gpucore->num_protected_regs : 16;
 
-	/* Do we have a free slot? */
-	if (WARN(*index >= max_slots, "Protected register slots full: %d/%d\n",
-					*index, max_slots))
-		return;
+	if (adreno_dev->gpucore->num_protected_regs)
+		BUG_ON(*index >= adreno_dev->gpucore->num_protected_regs);
+	else
+		BUG_ON(*index >= 16);
 
 	/*
 	 * On A4XX targets with more than 16 protected mode registers
@@ -1603,8 +1236,8 @@ static inline void adreno_set_protected_registers(
 
 #ifdef CONFIG_DEBUG_FS
 void adreno_debugfs_init(struct adreno_device *adreno_dev);
-void adreno_context_debugfs_init(struct adreno_device *adreno_dev,
-				struct adreno_context *ctx);
+void adreno_context_debugfs_init(struct adreno_device *,
+				struct adreno_context *);
 #else
 static inline void adreno_debugfs_init(struct adreno_device *adreno_dev) { }
 static inline void adreno_context_debugfs_init(struct adreno_device *device,
@@ -1623,10 +1256,10 @@ static inline void adreno_context_debugfs_init(struct adreno_device *device,
 static inline int adreno_compare_pm4_version(struct adreno_device *adreno_dev,
 	unsigned int version)
 {
-	if (adreno_dev->fw[ADRENO_FW_PM4].version == version)
+	if (adreno_dev->pm4_fw_version == version)
 		return 0;
 
-	return (adreno_dev->fw[ADRENO_FW_PM4].version > version) ? 1 : -1;
+	return (adreno_dev->pm4_fw_version > version) ? 1 : -1;
 }
 
 /**
@@ -1640,10 +1273,10 @@ static inline int adreno_compare_pm4_version(struct adreno_device *adreno_dev,
 static inline int adreno_compare_pfp_version(struct adreno_device *adreno_dev,
 	unsigned int version)
 {
-	if (adreno_dev->fw[ADRENO_FW_PFP].version == version)
+	if (adreno_dev->pfp_fw_version == version)
 		return 0;
 
-	return (adreno_dev->fw[ADRENO_FW_PFP].version > version) ? 1 : -1;
+	return (adreno_dev->pfp_fw_version > version) ? 1 : -1;
 }
 
 /*
@@ -1658,32 +1291,34 @@ static inline int adreno_bootstrap_ucode(struct adreno_device *adreno_dev)
 }
 
 /**
- * adreno_in_preempt_state() - Check if preemption state is equal to given state
+ * adreno_preempt_state() - Check if preemption state is equal to given state
  * @adreno_dev: Device whose preemption state is checked
  * @state: State to compare against
  */
-static inline bool adreno_in_preempt_state(struct adreno_device *adreno_dev,
-			enum adreno_preempt_states state)
+static inline unsigned int adreno_preempt_state(
+			struct adreno_device *adreno_dev,
+			enum adreno_dispatcher_preempt_states state)
 {
-	return atomic_read(&adreno_dev->preempt.state) == state;
+	return atomic_read(&adreno_dev->dispatcher.preemption_state) ==
+		state;
 }
+
 /**
- * adreno_set_preempt_state() - Set the specified preemption state
- * @adreno_dev: Device to change preemption state
- * @state: State to set
+ * adreno_get_rptr() - Get the current ringbuffer read pointer
+ * @rb: Pointer the ringbuffer to query
+ *
+ * Get the current read pointer from the GPU register.
  */
-static inline void adreno_set_preempt_state(struct adreno_device *adreno_dev,
-		enum adreno_preempt_states state)
+static inline unsigned int
+adreno_get_rptr(struct adreno_ringbuffer *rb)
 {
-	/*
-	 * atomic_set doesn't use barriers, so we need to do it ourselves.  One
-	 * before...
-	 */
-	smp_wmb();
-	atomic_set(&adreno_dev->preempt.state, state);
+	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
+	if (adreno_dev->cur_rb == rb &&
+		adreno_preempt_state(adreno_dev,
+			ADRENO_DISPATCHER_PREEMPT_CLEAR))
+		adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR, &(rb->rptr));
 
-	/* ... and one after */
-	smp_wmb();
+	return rb->rptr;
 }
 
 static inline bool adreno_is_preemption_enabled(
@@ -1691,6 +1326,7 @@ static inline bool adreno_is_preemption_enabled(
 {
 	return test_bit(ADRENO_DEVICE_PREEMPTION, &adreno_dev->priv);
 }
+
 /**
  * adreno_ctx_get_rb() - Return the ringbuffer that a context should
  * use based on priority
@@ -1703,7 +1339,6 @@ static inline struct adreno_ringbuffer *adreno_ctx_get_rb(
 {
 	struct kgsl_context *context;
 	int level;
-
 	if (!drawctxt)
 		return NULL;
 
@@ -1728,6 +1363,25 @@ static inline struct adreno_ringbuffer *adreno_ctx_get_rb(
 		return &(adreno_dev->ringbuffers[
 				adreno_dev->num_ringbuffers - 1]);
 }
+/*
+ * adreno_set_active_ctxs_null() - Put back reference to any active context
+ * and set the active context to NULL
+ * @adreno_dev: The adreno device
+ */
+static inline void adreno_set_active_ctxs_null(struct adreno_device *adreno_dev)
+{
+	int i;
+	struct adreno_ringbuffer *rb;
+	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
+		if (rb->drawctxt_active)
+			kgsl_context_put(&(rb->drawctxt_active->base));
+		rb->drawctxt_active = NULL;
+		kgsl_sharedmem_writel(KGSL_DEVICE(adreno_dev),
+			&rb->pagetable_desc,
+			offsetof(struct adreno_ringbuffer_pagetable_info,
+				current_rb_ptname), 0);
+	}
+}
 
 /*
  * adreno_compare_prio_level() - Compares 2 priority levels based on enum values
@@ -1748,13 +1402,6 @@ void adreno_readreg64(struct adreno_device *adreno_dev,
 void adreno_writereg64(struct adreno_device *adreno_dev,
 		enum adreno_regs lo, enum adreno_regs hi, uint64_t val);
 
-unsigned int adreno_get_rptr(struct adreno_ringbuffer *rb);
-
-static inline bool adreno_rb_empty(struct adreno_ringbuffer *rb)
-{
-	return (adreno_get_rptr(rb) == rb->wptr);
-}
-
 static inline bool adreno_soft_fault_detect(struct adreno_device *adreno_dev)
 {
 	return adreno_dev->fast_hang_detect &&
@@ -1784,220 +1431,4 @@ static inline bool adreno_support_64bit(struct adreno_device *adreno_dev)
 }
 #endif /*BITS_PER_LONG*/
 
-static inline void adreno_ringbuffer_set_global(
-		struct adreno_device *adreno_dev, int name)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	kgsl_sharedmem_writel(device,
-		&adreno_dev->ringbuffers[0].pagetable_desc,
-		PT_INFO_OFFSET(current_global_ptname), name);
-}
-
-static inline void adreno_ringbuffer_set_pagetable(struct adreno_ringbuffer *rb,
-		struct kgsl_pagetable *pt)
-{
-	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned long flags;
-
-	spin_lock_irqsave(&rb->preempt_lock, flags);
-
-	kgsl_sharedmem_writel(device, &rb->pagetable_desc,
-		PT_INFO_OFFSET(current_rb_ptname), pt->name);
-
-	kgsl_sharedmem_writeq(device, &rb->pagetable_desc,
-		PT_INFO_OFFSET(ttbr0), kgsl_mmu_pagetable_get_ttbr0(pt));
-
-	kgsl_sharedmem_writel(device, &rb->pagetable_desc,
-		PT_INFO_OFFSET(contextidr),
-		kgsl_mmu_pagetable_get_contextidr(pt));
-
-	spin_unlock_irqrestore(&rb->preempt_lock, flags);
-}
-
-static inline bool is_power_counter_overflow(struct adreno_device *adreno_dev,
-	unsigned int reg, unsigned int prev_val, unsigned int *perfctr_pwr_hi)
-{
-	unsigned int val;
-	bool ret = false;
-
-	/*
-	 * If prev_val is zero, it is first read after perf counter reset.
-	 * So set perfctr_pwr_hi register to zero.
-	 */
-	if (prev_val == 0) {
-		*perfctr_pwr_hi = 0;
-		return ret;
-	}
-	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_PERFCTR_RBBM_0_HI, &val);
-	if (val > *perfctr_pwr_hi) {
-		*perfctr_pwr_hi = val;
-		ret = true;
-	}
-	return ret;
-}
-
-static inline unsigned int counter_delta(struct kgsl_device *device,
-			unsigned int reg, unsigned int *counter)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	unsigned int val;
-	unsigned int ret = 0;
-	bool overflow = true;
-	static unsigned int perfctr_pwr_hi;
-	unsigned int prev_perfctr_pwr_hi = 0;
-
-	/* Read the value */
-	kgsl_regread(device, reg, &val);
-
-	if (adreno_is_a5xx(adreno_dev) && reg == adreno_getreg
-		(adreno_dev, ADRENO_REG_RBBM_PERFCTR_RBBM_0_LO)) {
-		prev_perfctr_pwr_hi = perfctr_pwr_hi;
-		overflow = is_power_counter_overflow(adreno_dev, reg,
-				*counter, &perfctr_pwr_hi);
-	}
-
-	/* Return 0 for the first read */
-	if (*counter != 0) {
-		if (val >= *counter) {
-			ret = val - *counter;
-		} else if (overflow == true) {
-			ret = (0xFFFFFFFF - *counter) + val;
-		} else {
-			/*
-			 * Since KGSL got abnormal value from the counter,
-			 * We will drop the value from being accumulated.
-			 */
-			KGSL_DRV_ERR_RATELIMIT(device,
-				"Abnormal value:0x%llx (0x%llx) from perf counter : 0x%x\n",
-				val | ((uint64_t)perfctr_pwr_hi << 32),
-				*counter |
-					((uint64_t)prev_perfctr_pwr_hi << 32),
-				reg);
-		}
-	}
-
-	*counter = val;
-	return ret;
-}
-
-static inline int adreno_perfcntr_active_oob_get(
-		struct adreno_device *adreno_dev)
-{
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	int ret;
-
-	ret = kgsl_active_count_get(KGSL_DEVICE(adreno_dev));
-	if (ret)
-		return ret;
-
-	if (gpudev->oob_set) {
-		ret = gpudev->oob_set(adreno_dev, OOB_PERFCNTR_SET_MASK,
-				OOB_PERFCNTR_CHECK_MASK,
-				OOB_PERFCNTR_CLEAR_MASK);
-		if (ret) {
-			adreno_set_gpu_fault(adreno_dev, ADRENO_GMU_FAULT);
-			adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
-			kgsl_active_count_put(KGSL_DEVICE(adreno_dev));
-		}
-	}
-
-	return ret;
-}
-
-static inline void adreno_perfcntr_active_oob_put(
-		struct adreno_device *adreno_dev)
-{
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	if (gpudev->oob_clear)
-		gpudev->oob_clear(adreno_dev, OOB_PERFCNTR_CLEAR_MASK);
-
-	kgsl_active_count_put(KGSL_DEVICE(adreno_dev));
-}
-
-static inline bool adreno_has_gbif(struct adreno_device *adreno_dev)
-{
-	if (adreno_is_a615(adreno_dev) || adreno_is_a616(adreno_dev))
-		return true;
-	else
-		return false;
-}
-
-/**
- * adreno_wait_for_vbif_halt_ack() - wait for VBIF acknowledgment
- * for given HALT request.
- * @ack_reg: register offset to wait for acknowledge
- */
-static inline int adreno_wait_for_vbif_halt_ack(struct kgsl_device *device,
-	int ack_reg, unsigned int mask)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	unsigned long wait_for_vbif;
-	unsigned int val;
-	int ret = 0;
-
-	/* wait for the transactions to clear */
-	wait_for_vbif = jiffies + msecs_to_jiffies(VBIF_RESET_ACK_TIMEOUT);
-	while (1) {
-		adreno_readreg(adreno_dev, ack_reg,
-			&val);
-		if ((val & mask) == mask)
-			break;
-		if (time_after(jiffies, wait_for_vbif)) {
-			KGSL_DRV_ERR(device,
-				"Wait limit reached for VBIF XIN Halt\n");
-			ret = -ETIMEDOUT;
-			break;
-		}
-	}
-
-	return ret;
-}
-
-/**
- * adreno_vbif_clear_pending_transactions() - Clear transactions in VBIF pipe
- * @device: Pointer to the device whose VBIF pipe is to be cleared
- */
-static inline int adreno_vbif_clear_pending_transactions(
-	struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	unsigned int mask = gpudev->vbif_xin_halt_ctrl0_mask;
-	int ret = 0;
-
-	if (adreno_has_gbif(adreno_dev)) {
-		/*
-		 * Halt GBIF GX first and then CX part.
-		 * Need to release CX Halt explicitly in case of SW_RESET.
-		 * GX Halt release will be taken care by SW_RESET internally.
-		 */
-		if (gpudev->gx_is_on(adreno_dev)) {
-			adreno_writereg(adreno_dev, ADRENO_REG_RBBM_GPR0_CNTL,
-					GBIF_HALT_REQUEST);
-			ret = adreno_wait_for_vbif_halt_ack(device,
-					ADRENO_REG_RBBM_VBIF_GX_RESET_STATUS,
-					VBIF_RESET_ACK_MASK);
-			if (ret)
-				return ret;
-		}
-
-		adreno_writereg(adreno_dev, ADRENO_REG_GBIF_HALT, mask);
-		ret = adreno_wait_for_vbif_halt_ack(device,
-				ADRENO_REG_GBIF_HALT_ACK, mask);
-	} else {
-		adreno_writereg(adreno_dev, ADRENO_REG_VBIF_XIN_HALT_CTRL0,
-			mask);
-		ret = adreno_wait_for_vbif_halt_ack(device,
-				ADRENO_REG_VBIF_XIN_HALT_CTRL1, mask);
-		adreno_writereg(adreno_dev, ADRENO_REG_VBIF_XIN_HALT_CTRL0, 0);
-	}
-	return ret;
-}
-
-int adreno_gmu_fenced_write(struct adreno_device *adreno_dev,
-	enum adreno_regs offset, unsigned int val,
-	unsigned int fence_mask);
 #endif /*__ADRENO_H */
diff --git a/drivers/gpu/msm/adreno_a3xx.c b/drivers/gpu/msm/adreno_a3xx.c
index e1f32e8c1419..298e9c83f993 100644
--- a/drivers/gpu/msm/adreno_a3xx.c
+++ b/drivers/gpu/msm/adreno_a3xx.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -18,6 +18,7 @@
 #include "kgsl.h"
 #include "adreno.h"
 #include "kgsl_sharedmem.h"
+#include "kgsl_cffdump.h"
 #include "a3xx_reg.h"
 #include "adreno_a3xx.h"
 #include "adreno_a4xx.h"
@@ -26,7 +27,6 @@
 #include "adreno_trace.h"
 #include "adreno_pm4types.h"
 #include "adreno_perfcounter.h"
-#include "adreno_snapshot.h"
 
 /*
  * Define registers for a3xx that contain addresses used by the
@@ -150,51 +150,8 @@ static const unsigned int _a3xx_pwron_fixup_fs_instructions[] = {
 	0x00000000, 0x03000000, 0x00000000, 0x00000000,
 };
 
-static void a3xx_efuse_speed_bin(struct adreno_device *adreno_dev)
-{
-	unsigned int val;
-	unsigned int speed_bin[3];
-	struct kgsl_device *device = &adreno_dev->dev;
-
-	if (of_get_property(device->pdev->dev.of_node,
-		"qcom,gpu-speed-bin-vectors", NULL)) {
-		adreno_efuse_speed_bin_array(adreno_dev);
-		return;
-	}
-
-	if (of_property_read_u32_array(device->pdev->dev.of_node,
-		"qcom,gpu-speed-bin", speed_bin, 3))
-		return;
-
-	adreno_efuse_read_u32(adreno_dev, speed_bin[0], &val);
-
-	adreno_dev->speed_bin = (val & speed_bin[1]) >> speed_bin[2];
-}
-
-static const struct {
-	int (*check)(struct adreno_device *adreno_dev);
-	void (*func)(struct adreno_device *adreno_dev);
-} a3xx_efuse_funcs[] = {
-	{ adreno_is_a306a, a3xx_efuse_speed_bin },
-};
-
-static void a3xx_check_features(struct adreno_device *adreno_dev)
-{
-	unsigned int i;
-
-	if (adreno_efuse_map(adreno_dev))
-		return;
-
-	for (i = 0; i < ARRAY_SIZE(a3xx_efuse_funcs); i++) {
-		if (a3xx_efuse_funcs[i].check(adreno_dev))
-			a3xx_efuse_funcs[i].func(adreno_dev);
-	}
-
-	adreno_efuse_unmap(adreno_dev);
-}
-
 /**
- * _a3xx_pwron_fixup() - Initialize a special command buffer to run a
+ * adreno_a3xx_pwron_fixup_init() - Initalize a special command buffer to run a
  * post-power collapse shader workaround
  * @adreno_dev: Pointer to a adreno_device struct
  *
@@ -204,7 +161,7 @@ static void a3xx_check_features(struct adreno_device *adreno_dev)
  *
  * Returns: 0 on success or negative on error
  */
-static int _a3xx_pwron_fixup(struct adreno_device *adreno_dev)
+int adreno_a3xx_pwron_fixup_init(struct adreno_device *adreno_dev)
 {
 	unsigned int *cmds;
 	int count = ARRAY_SIZE(_a3xx_pwron_fixup_fs_instructions);
@@ -216,7 +173,7 @@ static int _a3xx_pwron_fixup(struct adreno_device *adreno_dev)
 
 	ret = kgsl_allocate_global(KGSL_DEVICE(adreno_dev),
 		&adreno_dev->pwron_fixup, PAGE_SIZE,
-		KGSL_MEMFLAGS_GPUREADONLY, 0, "pwron_fixup");
+		KGSL_MEMFLAGS_GPUREADONLY, 0);
 
 	if (ret)
 		return ret;
@@ -646,12 +603,17 @@ static void a3xx_platform_setup(struct adreno_device *adreno_dev)
 		gpudev->vbif_xin_halt_ctrl0_mask =
 				A30X_VBIF_XIN_HALT_CTRL0_MASK;
 	}
-
-	/* Check efuse bits for various capabilties */
-	a3xx_check_features(adreno_dev);
 }
 
-static int a3xx_send_me_init(struct adreno_device *adreno_dev,
+/*
+ * a3xx_rb_init() - Initialize ringbuffer
+ * @adreno_dev: Pointer to adreno device
+ * @rb: Pointer to the ringbuffer of device
+ *
+ * Submit commands for ME initialization, common function shared between
+ * a3xx devices
+ */
+static int a3xx_rb_init(struct adreno_device *adreno_dev,
 			 struct adreno_ringbuffer *rb)
 {
 	unsigned int *cmds;
@@ -691,64 +653,12 @@ static int a3xx_send_me_init(struct adreno_device *adreno_dev,
 		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
 		dev_err(device->dev, "CP initialization failed to idle\n");
-		kgsl_device_snapshot(device, NULL, false);
-	}
-
-	return ret;
-}
-
-static int a3xx_rb_start(struct adreno_device *adreno_dev,
-			 unsigned int start_type)
-{
-	struct adreno_ringbuffer *rb = ADRENO_CURRENT_RINGBUFFER(adreno_dev);
-	int ret;
-
-	/*
-	 * The size of the ringbuffer in the hardware is the log2
-	 * representation of the size in quadwords (sizedwords / 2).
-	 * Also disable the host RPTR shadow register as it might be unreliable
-	 * in certain circumstances.
-	 */
-
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_CNTL,
-		(ilog2(KGSL_RB_DWORDS >> 1) & 0x3F) |
-		(1 << 27));
-
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_BASE,
-			rb->buffer_desc.gpuaddr);
-
-	ret = a3xx_microcode_load(adreno_dev, start_type);
-	if (ret == 0) {
-		/* clear ME_HALT to start micro engine */
-		adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_CNTL, 0);
-
-		ret = a3xx_send_me_init(adreno_dev, rb);
+		kgsl_device_snapshot(device, NULL);
 	}
 
 	return ret;
 }
 
-/*
- * a3xx_init() - Initialize gpu specific data
- * @adreno_dev: Pointer to adreno device
- */
-static void a3xx_init(struct adreno_device *adreno_dev)
-{
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	_a3xx_pwron_fixup(adreno_dev);
-
-	/* Adjust snapshot section sizes according to core */
-	if ((adreno_is_a330(adreno_dev) || adreno_is_a305b(adreno_dev))) {
-		gpudev->snapshot_data->sect_sizes->cp_pfp =
-					A320_SNAPSHOT_CP_STATE_SECTION_SIZE;
-		gpudev->snapshot_data->sect_sizes->roq =
-					A320_SNAPSHOT_ROQ_SECTION_SIZE;
-		gpudev->snapshot_data->sect_sizes->cp_merciu =
-					A320_SNAPSHOT_CP_MERCIU_SECTION_SIZE;
-	}
-}
-
 /*
  * a3xx_err_callback() - Call back for a3xx error interrupts
  * @adreno_dev: Pointer to device
@@ -926,8 +836,8 @@ static const struct adreno_vbif_data a306_vbif[] = {
 
 static const struct adreno_vbif_data a306a_vbif[] = {
 	{ A3XX_VBIF_ROUND_ROBIN_QOS_ARB, 0x0003 },
-	{ A3XX_VBIF_OUT_RD_LIM_CONF0, 0x00000010 },
-	{ A3XX_VBIF_OUT_WR_LIM_CONF0, 0x00000010 },
+	{ A3XX_VBIF_OUT_RD_LIM_CONF0, 0x0000000A },
+	{ A3XX_VBIF_OUT_WR_LIM_CONF0, 0x0000000A },
 	{0, 0},
 };
 
@@ -1338,7 +1248,7 @@ static void a3xx_protect_init(struct adreno_device *adreno_dev)
 	iommu_regs = kgsl_mmu_get_prot_regs(&device->mmu);
 	if (iommu_regs)
 		adreno_set_protected_registers(adreno_dev, &index,
-				iommu_regs->base, ilog2(iommu_regs->range));
+				iommu_regs->base, iommu_regs->range);
 }
 
 static void a3xx_start(struct adreno_device *adreno_dev)
@@ -1356,10 +1266,8 @@ static void a3xx_start(struct adreno_device *adreno_dev)
 	kgsl_regwrite(device, A3XX_RBBM_SP_HYST_CNT, 0x10);
 	kgsl_regwrite(device, A3XX_RBBM_WAIT_IDLE_CLOCKS_CTL, 0x10);
 
-	/*
-	 * Enable the RBBM error reporting bits.  This lets us get
-	 * useful information on failure
-	 */
+	/* Enable the RBBM error reporting bits.  This lets us get
+	   useful information on failure */
 
 	kgsl_regwrite(device, A3XX_RBBM_AHB_CTL0, 0x00000001);
 
@@ -1369,10 +1277,8 @@ static void a3xx_start(struct adreno_device *adreno_dev)
 	/* Turn on the power counters */
 	kgsl_regwrite(device, A3XX_RBBM_RBBM_CTL, 0x00030000);
 
-	/*
-	 * Turn on hang detection - this spews a lot of useful information
-	 * into the RBBM registers on a hang
-	 */
+	/* Turn on hang detection - this spews a lot of useful information
+	 * into the RBBM registers on a hang */
 	if (adreno_is_a330v2(adreno_dev)) {
 		set_bit(ADRENO_DEVICE_HANG_INTR, &adreno_dev->priv);
 		gpudev->irq->mask |= (1 << A3XX_INT_MISC_HANG_DETECT);
@@ -1474,10 +1380,6 @@ static struct adreno_coresight a3xx_coresight = {
 	.groups = a3xx_coresight_groups,
 };
 
-static unsigned int a3xx_int_bits[ADRENO_INT_BITS_MAX] = {
-	ADRENO_INT_DEFINE(ADRENO_INT_RBBM_AHB_ERROR, A3XX_INT_RBBM_AHB_ERROR),
-};
-
 /* Register offset defines for A3XX */
 static unsigned int a3xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_ME_RAM_WADDR, A3XX_CP_ME_RAM_WADDR),
@@ -1530,8 +1432,8 @@ static unsigned int a3xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 	ADRENO_REG_DEFINE(ADRENO_REG_PA_SC_AA_CONFIG, A3XX_PA_SC_AA_CONFIG),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PM_OVERRIDE2, A3XX_RBBM_PM_OVERRIDE2),
 	ADRENO_REG_DEFINE(ADRENO_REG_SQ_GPR_MANAGEMENT, A3XX_SQ_GPR_MANAGEMENT),
-	ADRENO_REG_DEFINE(ADRENO_REG_SQ_INST_STORE_MANAGEMENT,
-				A3XX_SQ_INST_STORE_MANAGEMENT),
+	ADRENO_REG_DEFINE(ADRENO_REG_SQ_INST_STORE_MANAGMENT,
+				A3XX_SQ_INST_STORE_MANAGMENT),
 	ADRENO_REG_DEFINE(ADRENO_REG_TP0_CHICKEN, A3XX_TP0_CHICKEN),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_RBBM_CTL, A3XX_RBBM_RBBM_CTL),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SW_RESET_CMD, A3XX_RBBM_SW_RESET_CMD),
@@ -1539,10 +1441,6 @@ static unsigned int a3xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 			A3XX_UCHE_CACHE_INVALIDATE0_REG),
 	ADRENO_REG_DEFINE(ADRENO_REG_UCHE_INVALIDATE1,
 			A3XX_UCHE_CACHE_INVALIDATE1_REG),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_RBBM_0_LO,
-			A3XX_RBBM_PERFCTR_RBBM_0_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_RBBM_0_HI,
-			A3XX_RBBM_PERFCTR_RBBM_0_HI),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_LO,
 				A3XX_RBBM_PERFCTR_LOAD_VALUE_LO),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_HI,
@@ -1606,10 +1504,8 @@ static int _load_firmware(struct kgsl_device *device, const char *fwfile,
 int a3xx_microcode_read(struct adreno_device *adreno_dev)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_firmware *pm4_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PM4);
-	struct adreno_firmware *pfp_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PFP);
 
-	if (pm4_fw->fwvirt == NULL) {
+	if (adreno_dev->pm4_fw == NULL) {
 		int len;
 		void *ptr;
 
@@ -1630,12 +1526,12 @@ int a3xx_microcode_read(struct adreno_device *adreno_dev)
 			return -ENOMEM;
 		}
 
-		pm4_fw->size = len / sizeof(uint32_t);
-		pm4_fw->fwvirt = ptr;
-		pm4_fw->version = pm4_fw->fwvirt[1];
+		adreno_dev->pm4_fw_size = len / sizeof(uint32_t);
+		adreno_dev->pm4_fw = ptr;
+		adreno_dev->pm4_fw_version = adreno_dev->pm4_fw[1];
 	}
 
-	if (pfp_fw->fwvirt == NULL) {
+	if (adreno_dev->pfp_fw == NULL) {
 		int len;
 		void *ptr;
 
@@ -1655,9 +1551,9 @@ int a3xx_microcode_read(struct adreno_device *adreno_dev)
 			return -ENOMEM;
 		}
 
-		pfp_fw->size = len / sizeof(uint32_t);
-		pfp_fw->fwvirt = ptr;
-		pfp_fw->version = pfp_fw->fwvirt[1];
+		adreno_dev->pfp_fw_size = len / sizeof(uint32_t);
+		adreno_dev->pfp_fw = ptr;
+		adreno_dev->pfp_fw_version = adreno_dev->pfp_fw[5];
 	}
 
 	return 0;
@@ -1679,7 +1575,7 @@ static inline void load_pm4_ucode(struct adreno_device *adreno_dev,
 	adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_RAM_WADDR, addr);
 	for (i = start; i < end; i++)
 		adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_RAM_DATA,
-				adreno_dev->fw[ADRENO_FW_PM4].fwvirt[i]);
+					adreno_dev->pm4_fw[i]);
 }
 /**
  * load_pfp_ucode() - Load pfp ucode
@@ -1698,7 +1594,7 @@ static inline void load_pfp_ucode(struct adreno_device *adreno_dev,
 	adreno_writereg(adreno_dev, ADRENO_REG_CP_PFP_UCODE_ADDR, addr);
 	for (i = start; i < end; i++)
 		adreno_writereg(adreno_dev, ADRENO_REG_CP_PFP_UCODE_DATA,
-				adreno_dev->fw[ADRENO_FW_PFP].fwvirt[i]);
+						adreno_dev->pfp_fw[i]);
 }
 
 /**
@@ -1728,8 +1624,6 @@ static int _ringbuffer_bootstrap_ucode(struct adreno_device *adreno_dev,
 	int i = 0;
 	int ret;
 	unsigned int pm4_size, pm4_idx, pm4_addr, pfp_size, pfp_idx, pfp_addr;
-	struct adreno_firmware *pfp_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PFP);
-	struct adreno_firmware *pm4_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PM4);
 
 	/* Only bootstrap jump tables of ucode */
 	if (load_jt) {
@@ -1745,8 +1639,8 @@ static int _ringbuffer_bootstrap_ucode(struct adreno_device *adreno_dev,
 		pfp_addr = 0;
 	}
 
-	pm4_size = (pm4_fw->size - pm4_idx);
-	pfp_size = (pfp_fw->size - pfp_idx);
+	pm4_size = (adreno_dev->pm4_fw_size - pm4_idx);
+	pfp_size = (adreno_dev->pfp_fw_size - pfp_idx);
 
 	bootstrap_size = (pm4_size + pfp_size + 5);
 
@@ -1805,10 +1699,10 @@ static int _ringbuffer_bootstrap_ucode(struct adreno_device *adreno_dev,
 	 * from the ring buffer to the ME.
 	 */
 	if (adreno_is_a4xx(adreno_dev)) {
-		for (i = pm4_idx; i < pm4_fw->size; i++)
-			*cmds++ = pm4_fw->fwvirt[i];
-		for (i = pfp_idx; i < pfp_fw->size; i++)
-			*cmds++ = pfp_fw->fwvirt[i];
+		for (i = pm4_idx; i < adreno_dev->pm4_fw_size; i++)
+			*cmds++ = adreno_dev->pm4_fw[i];
+		for (i = pfp_idx; i < adreno_dev->pfp_fw_size; i++)
+			*cmds++ = adreno_dev->pfp_fw[i];
 
 		*cmds++ = cp_type3_packet(CP_REG_RMW, 3);
 		*cmds++ = 0x20000000 + A4XX_CP_RB_WPTR;
@@ -1817,14 +1711,14 @@ static int _ringbuffer_bootstrap_ucode(struct adreno_device *adreno_dev,
 		*cmds++ = cp_type3_packet(CP_INTERRUPT, 1);
 		*cmds++ = 0;
 
-		rb->_wptr = rb->_wptr - 2;
+		rb->wptr = rb->wptr - 2;
 		adreno_ringbuffer_submit(rb, NULL);
-		rb->_wptr = rb->_wptr + 2;
+		rb->wptr = rb->wptr + 2;
 	} else {
-		for (i = pfp_idx; i < pfp_fw->size; i++)
-			*cmds++ = pfp_fw->fwvirt[i];
-		for (i = pm4_idx; i < pm4_fw->size; i++)
-			*cmds++ = pm4_fw->fwvirt[i];
+		for (i = pfp_idx; i < adreno_dev->pfp_fw_size; i++)
+			*cmds++ = adreno_dev->pfp_fw[i];
+		for (i = pm4_idx; i < adreno_dev->pm4_fw_size; i++)
+			*cmds++ = adreno_dev->pm4_fw[i];
 		adreno_ringbuffer_submit(rb, NULL);
 	}
 
@@ -1833,7 +1727,7 @@ static int _ringbuffer_bootstrap_ucode(struct adreno_device *adreno_dev,
 
 	if (ret) {
 		KGSL_DRV_ERR(device, "microcode bootstrap failed to idle\n");
-		kgsl_device_snapshot(device, NULL, false);
+		kgsl_device_snapshot(device, NULL);
 	}
 
 	/* Clear the chicken bit for speed up on A430 and its derivatives */
@@ -1849,8 +1743,6 @@ int a3xx_microcode_load(struct adreno_device *adreno_dev,
 {
 	int status;
 	struct adreno_ringbuffer *rb = ADRENO_CURRENT_RINGBUFFER(adreno_dev);
-	size_t pm4_size = adreno_dev->fw[ADRENO_FW_PM4].size;
-	size_t pfp_size = adreno_dev->fw[ADRENO_FW_PFP].size;
 
 	if (start_type == ADRENO_START_COLD) {
 		/* If bootstrapping if supported to load ucode */
@@ -1879,10 +1771,12 @@ int a3xx_microcode_load(struct adreno_device *adreno_dev,
 
 		} else {
 			/* load the CP ucode using AHB writes */
-			load_pm4_ucode(adreno_dev, 1, pm4_size, 0);
+			load_pm4_ucode(adreno_dev, 1, adreno_dev->pm4_fw_size,
+				0);
 
 			/* load the prefetch parser ucode using AHB writes */
-			load_pfp_ucode(adreno_dev, 1, pfp_size, 0);
+			load_pfp_ucode(adreno_dev, 1, adreno_dev->pfp_fw_size,
+				0);
 		}
 	} else if (start_type == ADRENO_START_WARM) {
 			/* If bootstrapping if supported to load jump tables */
@@ -1895,14 +1789,16 @@ int a3xx_microcode_load(struct adreno_device *adreno_dev,
 			/* load the CP jump tables using AHB writes */
 			load_pm4_ucode(adreno_dev,
 				adreno_dev->gpucore->pm4_jt_idx,
-				pm4_size, adreno_dev->gpucore->pm4_jt_addr);
+				adreno_dev->pm4_fw_size,
+				adreno_dev->gpucore->pm4_jt_addr);
 
 			/*
 			 * load the prefetch parser jump tables using AHB writes
 			 */
 			load_pfp_ucode(adreno_dev,
 				adreno_dev->gpucore->pfp_jt_idx,
-				pfp_size, adreno_dev->gpucore->pfp_jt_addr);
+				adreno_dev->pfp_fw_size,
+				adreno_dev->gpucore->pfp_jt_addr);
 		}
 	} else
 		return -EINVAL;
@@ -1912,7 +1808,6 @@ int a3xx_microcode_load(struct adreno_device *adreno_dev,
 
 struct adreno_gpudev adreno_a3xx_gpudev = {
 	.reg_offsets = &a3xx_reg_offsets,
-	.int_bits = a3xx_int_bits,
 	.ft_perf_counters = a3xx_ft_perf_counters,
 	.ft_perf_counters_count = ARRAY_SIZE(a3xx_ft_perf_counters),
 	.perfcounters = &a3xx_perfcounters,
@@ -1922,12 +1817,12 @@ struct adreno_gpudev adreno_a3xx_gpudev = {
 	.num_prio_levels = 1,
 	.vbif_xin_halt_ctrl0_mask = A3XX_VBIF_XIN_HALT_CTRL0_MASK,
 	.platform_setup = a3xx_platform_setup,
-	.rb_start = a3xx_rb_start,
-	.init = a3xx_init,
+	.rb_init = a3xx_rb_init,
 	.microcode_read = a3xx_microcode_read,
+	.microcode_load = a3xx_microcode_load,
 	.perfcounter_init = a3xx_perfcounter_init,
 	.perfcounter_close = a3xx_perfcounter_close,
 	.start = a3xx_start,
 	.snapshot = a3xx_snapshot,
-	.coresight = {&a3xx_coresight},
+	.coresight = &a3xx_coresight,
 };
diff --git a/drivers/gpu/msm/adreno_a3xx.h b/drivers/gpu/msm/adreno_a3xx.h
index 11596b8bf7aa..4ab1236020e8 100644
--- a/drivers/gpu/msm/adreno_a3xx.h
+++ b/drivers/gpu/msm/adreno_a3xx.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -13,34 +13,6 @@
 #ifndef __A3XX_H
 #define __A3XX_H
 
-#include "a3xx_reg.h"
-
-#define A3XX_IRQ_FLAGS \
-	{ BIT(A3XX_INT_RBBM_GPU_IDLE), "RBBM_GPU_IDLE" }, \
-	{ BIT(A3XX_INT_RBBM_AHB_ERROR), "RBBM_AHB_ERR" }, \
-	{ BIT(A3XX_INT_RBBM_REG_TIMEOUT), "RBBM_REG_TIMEOUT" }, \
-	{ BIT(A3XX_INT_RBBM_ME_MS_TIMEOUT), "RBBM_ME_MS_TIMEOUT" }, \
-	{ BIT(A3XX_INT_RBBM_PFP_MS_TIMEOUT), "RBBM_PFP_MS_TIMEOUT" }, \
-	{ BIT(A3XX_INT_RBBM_ATB_BUS_OVERFLOW), "RBBM_ATB_BUS_OVERFLOW" }, \
-	{ BIT(A3XX_INT_VFD_ERROR), "RBBM_VFD_ERROR" }, \
-	{ BIT(A3XX_INT_CP_SW_INT), "CP_SW" }, \
-	{ BIT(A3XX_INT_CP_T0_PACKET_IN_IB), "CP_T0_PACKET_IN_IB" }, \
-	{ BIT(A3XX_INT_CP_OPCODE_ERROR), "CP_OPCODE_ERROR" }, \
-	{ BIT(A3XX_INT_CP_RESERVED_BIT_ERROR), "CP_RESERVED_BIT_ERROR" }, \
-	{ BIT(A3XX_INT_CP_HW_FAULT), "CP_HW_FAULT" }, \
-	{ BIT(A3XX_INT_CP_DMA), "CP_DMA" }, \
-	{ BIT(A3XX_INT_CP_IB2_INT), "CP_IB2_INT" }, \
-	{ BIT(A3XX_INT_CP_IB1_INT), "CP_IB1_INT" }, \
-	{ BIT(A3XX_INT_CP_RB_INT), "CP_RB_INT" }, \
-	{ BIT(A3XX_INT_CP_REG_PROTECT_FAULT), "CP_REG_PROTECT_FAULT" }, \
-	{ BIT(A3XX_INT_CP_RB_DONE_TS), "CP_RB_DONE_TS" }, \
-	{ BIT(A3XX_INT_CP_VS_DONE_TS), "CP_VS_DONE_TS" }, \
-	{ BIT(A3XX_INT_CP_PS_DONE_TS), "CP_PS_DONE_TS" }, \
-	{ BIT(A3XX_INT_CACHE_FLUSH_TS), "CACHE_FLUSH_TS" }, \
-	{ BIT(A3XX_INT_CP_AHB_ERROR_HALT), "CP_AHB_ERROR_HALT" }, \
-	{ BIT(A3XX_INT_MISC_HANG_DETECT), "MISC_HANG_DETECT" }, \
-	{ BIT(A3XX_INT_UCHE_OOB_ACCESS), "UCHE_OOB_ACCESS" }
-
 unsigned int a3xx_irq_pending(struct adreno_device *adreno_dev);
 
 int a3xx_microcode_read(struct adreno_device *adreno_dev);
diff --git a/drivers/gpu/msm/adreno_a3xx_snapshot.c b/drivers/gpu/msm/adreno_a3xx_snapshot.c
index 240dcdbbf1cd..48af1fe353f5 100644
--- a/drivers/gpu/msm/adreno_a3xx_snapshot.c
+++ b/drivers/gpu/msm/adreno_a3xx_snapshot.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -95,7 +95,6 @@ static void _rbbm_debug_bus_read(struct kgsl_device *device,
 	unsigned int block_id, unsigned int index, unsigned int *val)
 {
 	unsigned int block = (block_id << 8) | 1 << 16;
-
 	kgsl_regwrite(device, A3XX_RBBM_DEBUG_BUS_CTL, block | index);
 	kgsl_regread(device, A3XX_RBBM_DEBUG_BUS_DATA_STATUS, val);
 }
diff --git a/drivers/gpu/msm/adreno_a4xx.c b/drivers/gpu/msm/adreno_a4xx.c
index 432e98dbed94..60603850298c 100644
--- a/drivers/gpu/msm/adreno_a4xx.c
+++ b/drivers/gpu/msm/adreno_a4xx.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -26,8 +26,6 @@
 #include "adreno_perfcounter.h"
 
 #define SP_TP_PWR_ON BIT(20)
-/* A4XX_RBBM_CLOCK_CTL_IP */
-#define CNTL_IP_SW_COLLAPSE		BIT(0)
 
 /*
  * Define registers for a4xx that contain addresses used by the
@@ -180,6 +178,111 @@ static const struct adreno_vbif_platform a4xx_vbif_platforms[] = {
 	{ adreno_is_a418, a430_vbif },
 };
 
+/* a4xx_preemption_start() - Setup state to start preemption */
+static void a4xx_preemption_start(struct adreno_device *adreno_dev,
+		struct adreno_ringbuffer *rb)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	uint32_t val;
+
+	/*
+	 * Setup scratch registers from which the GPU will program the
+	 * registers required to start execution of new ringbuffer
+	 * set ringbuffer address
+	 */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG8,
+		rb->buffer_desc.gpuaddr);
+	kgsl_regread(device, A4XX_CP_RB_CNTL, &val);
+	/* scratch REG9 corresponds to CP_RB_CNTL register */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG9, val);
+	/* scratch REG10 corresponds to rptr address */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG10, 0);
+	/* scratch REG11 corresponds to rptr */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG11, rb->rptr);
+	/* scratch REG12 corresponds to wptr */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG12, rb->wptr);
+	/*
+	 * scratch REG13 corresponds to  IB1_BASE,
+	 * 0 since we do not do switches in between IB's
+	 */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG13, 0);
+	/* scratch REG14 corresponds to IB1_BUFSZ */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG14, 0);
+	/* scratch REG15 corresponds to IB2_BASE */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG15, 0);
+	/* scratch REG16 corresponds to  IB2_BUFSZ */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG16, 0);
+	/* scratch REG17 corresponds to GPR11 */
+	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG17, rb->gpr11);
+}
+
+/* a4xx_preemption_save() - Save the state after preemption is done */
+static void a4xx_preemption_save(struct adreno_device *adreno_dev,
+		struct adreno_ringbuffer *rb)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	kgsl_regread(device, A4XX_CP_SCRATCH_REG18, &rb->rptr);
+	kgsl_regread(device, A4XX_CP_SCRATCH_REG23, &rb->gpr11);
+}
+
+static int a4xx_preemption_token(struct adreno_device *adreno_dev,
+			struct adreno_ringbuffer *rb, unsigned int *cmds,
+			uint64_t gpuaddr)
+{
+	unsigned int *cmds_orig = cmds;
+
+	/* Turn on preemption flag */
+	/* preemption token - fill when pt switch command size is known */
+	*cmds++ = cp_type3_packet(CP_PREEMPT_TOKEN, 3);
+	*cmds++ = (uint)gpuaddr;
+	*cmds++ = 1;
+	/* generate interrupt on preemption completion */
+	*cmds++ = 1 << CP_PREEMPT_ORDINAL_INTERRUPT;
+
+	return cmds - cmds_orig;
+
+}
+
+static int a4xx_preemption_pre_ibsubmit(
+			struct adreno_device *adreno_dev,
+			struct adreno_ringbuffer *rb, unsigned int *cmds,
+			struct kgsl_context *context, uint64_t cond_addr,
+			struct kgsl_memobj_node *ib)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	unsigned int *cmds_orig = cmds;
+	int exec_ib = 0;
+
+	cmds += a4xx_preemption_token(adreno_dev, rb, cmds,
+				device->memstore.gpuaddr +
+				KGSL_MEMSTORE_OFFSET(context->id, preempted));
+
+	if (ib)
+		exec_ib = 1;
+
+	*cmds++ = cp_type3_packet(CP_COND_EXEC, 4);
+	*cmds++ = cond_addr;
+	*cmds++ = cond_addr;
+	*cmds++ = 1;
+	*cmds++ = 7 + exec_ib * 3;
+	if (exec_ib) {
+		*cmds++ = cp_type3_packet(CP_INDIRECT_BUFFER_PFE, 2);
+		*cmds++ = ib->gpuaddr;
+		*cmds++ = (unsigned int) ib->size >> 2;
+	}
+	/* clear preemption flag */
+	*cmds++ = cp_type3_packet(CP_MEM_WRITE, 2);
+	*cmds++ = cond_addr;
+	*cmds++ = 0;
+	*cmds++ = cp_type3_packet(CP_WAIT_MEM_WRITES, 1);
+	*cmds++ = 0;
+	*cmds++ = cp_type3_packet(CP_WAIT_FOR_ME, 1);
+	*cmds++ = 0;
+
+	return cmds - cmds_orig;
+}
+
 /*
  * a4xx_is_sptp_idle() - A430 SP/TP should be off to be considered idle
  * @adreno_dev: The adreno device pointer
@@ -202,131 +305,6 @@ static bool a4xx_is_sptp_idle(struct adreno_device *adreno_dev)
 	return !(reg & SP_TP_PWR_ON);
 }
 
-/*
- * a4xx_enable_hwcg() - Program the clock control registers
- * @device: The adreno device pointer
- */
-static void a4xx_enable_hwcg(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP0, 0x02222202);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP1, 0x02222202);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP2, 0x02222202);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP3, 0x02222202);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP0, 0x00002222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP1, 0x00002222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP2, 0x00002222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP3, 0x00002222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP0, 0x0E739CE7);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP1, 0x0E739CE7);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP2, 0x0E739CE7);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP3, 0x0E739CE7);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP0, 0x00111111);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP1, 0x00111111);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP2, 0x00111111);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP3, 0x00111111);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP0, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP1, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP2, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP3, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP0, 0x00222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP1, 0x00222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP2, 0x00222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP3, 0x00222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP0, 0x00000104);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP1, 0x00000104);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP2, 0x00000104);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP3, 0x00000104);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP0, 0x00000081);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP1, 0x00000081);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP2, 0x00000081);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP3, 0x00000081);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_UCHE, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_UCHE, 0x02222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL3_UCHE, 0x00000000);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL4_UCHE, 0x00000000);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_UCHE, 0x00004444);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_UCHE, 0x00001112);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB0, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB1, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB2, 0x22222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB3, 0x22222222);
-	/* Disable L1 clocking in A420 due to CCU issues with it */
-	if (adreno_is_a420(adreno_dev)) {
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB0, 0x00002020);
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB1, 0x00002020);
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB2, 0x00002020);
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB3, 0x00002020);
-	} else {
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB0, 0x00022020);
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB1, 0x00022020);
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB2, 0x00022020);
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB3, 0x00022020);
-	}
-	/* No CCU for A405 */
-	if (!adreno_is_a405(adreno_dev)) {
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_CTL_MARB_CCU0, 0x00000922);
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_CTL_MARB_CCU1, 0x00000922);
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_CTL_MARB_CCU2, 0x00000922);
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_CTL_MARB_CCU3, 0x00000922);
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU0, 0x00000000);
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU1, 0x00000000);
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU2, 0x00000000);
-		kgsl_regwrite(device,
-			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU3, 0x00000000);
-		kgsl_regwrite(device,
-				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_0,
-				0x00000001);
-		kgsl_regwrite(device,
-				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_1,
-				0x00000001);
-		kgsl_regwrite(device,
-				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_2,
-				0x00000001);
-		kgsl_regwrite(device,
-				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_3,
-				0x00000001);
-	}
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_MODE_GPC, 0x02222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_GPC, 0x04100104);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_GPC, 0x00022222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_COM_DCOM, 0x00000022);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_COM_DCOM, 0x0000010F);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_COM_DCOM, 0x00000022);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TSE_RAS_RBBM, 0x00222222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00004104);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00000222);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_HLSQ, 0x00000000);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_HLSQ, 0x00000000);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_HLSQ, 0x00220000);
-	/*
-	 * Due to a HW timing issue, top level HW clock gating is causing
-	 * register read/writes to be dropped in adreno a430.
-	 * This timing issue started happening because of SP/TP power collapse.
-	 * On targets that do not have SP/TP PC there is no timing issue.
-	 * The HW timing issue could be fixed by
-	 * a) disabling SP/TP power collapse
-	 * b) or disabling HW clock gating.
-	 * Disabling HW clock gating + NAP enabled combination has
-	 * minimal power impact. So this option is chosen over disabling
-	 * SP/TP power collapse.
-	 * Revisions of A430 which chipid 2 and above do not have the issue.
-	 */
-	if (adreno_is_a430(adreno_dev) &&
-		(ADRENO_CHIPID_PATCH(adreno_dev->chipid) < 2))
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL, 0);
-	else
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL, 0xAAAAAAAA);
-	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2, 0);
-}
 /*
  * a4xx_regulator_enable() - Enable any necessary HW regulators
  * @adreno_dev: The adreno device pointer
@@ -339,12 +317,8 @@ static int a4xx_regulator_enable(struct adreno_device *adreno_dev)
 	unsigned int reg;
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
-	if (!(adreno_is_a430(adreno_dev) || adreno_is_a418(adreno_dev))) {
-		/* Halt the sp_input_clk at HM level */
-		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL, 0x00000055);
-		a4xx_enable_hwcg(device);
+	if (!(adreno_is_a430(adreno_dev) || adreno_is_a418(adreno_dev)))
 		return 0;
-	}
 
 	/* Set the default register values; set SW_COLLAPSE to 0 */
 	kgsl_regwrite(device, A4XX_RBBM_POWER_CNTL_IP, 0x778000);
@@ -352,13 +326,6 @@ static int a4xx_regulator_enable(struct adreno_device *adreno_dev)
 		udelay(5);
 		kgsl_regread(device, A4XX_RBBM_POWER_STATUS, &reg);
 	} while (!(reg & SP_TP_PWR_ON));
-
-	/* Disable SP clock */
-	kgsl_regrmw(device, A4XX_RBBM_CLOCK_CTL_IP, CNTL_IP_SW_COLLAPSE, 0);
-	/* Enable hardware clockgating */
-	a4xx_enable_hwcg(device);
-	/* Enable SP clock */
-	kgsl_regrmw(device, A4XX_RBBM_CLOCK_CTL_IP, CNTL_IP_SW_COLLAPSE, 1);
 	return 0;
 }
 
@@ -466,6 +433,131 @@ static void a4xx_pwrlevel_change_settings(struct adreno_device *adreno_dev,
 		pre = 0;
 }
 
+/*
+ * a4xx_enable_hwcg() - Program the clock control registers
+ * @device: The adreno device pointer
+ */
+static void a4xx_enable_hwcg(struct kgsl_device *device)
+{
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP0, 0x02222202);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP1, 0x02222202);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP2, 0x02222202);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TP3, 0x02222202);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP0, 0x00002222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP1, 0x00002222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP2, 0x00002222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_TP3, 0x00002222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP0, 0x0E739CE7);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP1, 0x0E739CE7);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP2, 0x0E739CE7);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TP3, 0x0E739CE7);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP0, 0x00111111);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP1, 0x00111111);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP2, 0x00111111);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TP3, 0x00111111);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP0, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP1, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP2, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_SP3, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP0, 0x00222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP1, 0x00222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP2, 0x00222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_SP3, 0x00222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP0, 0x00000104);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP1, 0x00000104);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP2, 0x00000104);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_SP3, 0x00000104);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP0, 0x00000081);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP1, 0x00000081);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP2, 0x00000081);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_SP3, 0x00000081);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_UCHE, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_UCHE, 0x02222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL3_UCHE, 0x00000000);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL4_UCHE, 0x00000000);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_UCHE, 0x00004444);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_UCHE, 0x00001112);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB0, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB1, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB2, 0x22222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_RB3, 0x22222222);
+	/* Disable L1 clocking in A420 due to CCU issues with it */
+	if (adreno_is_a420(adreno_dev)) {
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB0, 0x00002020);
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB1, 0x00002020);
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB2, 0x00002020);
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB3, 0x00002020);
+	} else {
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB0, 0x00022020);
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB1, 0x00022020);
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB2, 0x00022020);
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2_RB3, 0x00022020);
+	}
+	/* No CCU for A405 */
+	if (!adreno_is_a405(adreno_dev)) {
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_CTL_MARB_CCU0, 0x00000922);
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_CTL_MARB_CCU1, 0x00000922);
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_CTL_MARB_CCU2, 0x00000922);
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_CTL_MARB_CCU3, 0x00000922);
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU0, 0x00000000);
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU1, 0x00000000);
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU2, 0x00000000);
+		kgsl_regwrite(device,
+			A4XX_RBBM_CLOCK_HYST_RB_MARB_CCU3, 0x00000000);
+		kgsl_regwrite(device,
+				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_0,
+				0x00000001);
+		kgsl_regwrite(device,
+				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_1,
+				0x00000001);
+		kgsl_regwrite(device,
+				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_2,
+				0x00000001);
+		kgsl_regwrite(device,
+				A4XX_RBBM_CLOCK_DELAY_RB_MARB_CCU_L1_3,
+				0x00000001);
+	}
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_MODE_GPC, 0x02222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_GPC, 0x04100104);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_GPC, 0x00022222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_COM_DCOM, 0x00000022);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_COM_DCOM, 0x0000010F);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_COM_DCOM, 0x00000022);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_TSE_RAS_RBBM, 0x00222222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00004104);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00000222);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL_HLSQ , 0x00000000);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_HYST_HLSQ, 0x00000000);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_DELAY_HLSQ, 0x00220000);
+	/*
+	 * Due to a HW timing issue, top level HW clock gating is causing
+	 * register read/writes to be dropped in adreno a430.
+	 * This timing issue started happening because of SP/TP power collapse.
+	 * On targets that do not have SP/TP PC there is no timing issue.
+	 * The HW timing issue could be fixed by
+	 * a) disabling SP/TP power collapse
+	 * b) or disabling HW clock gating.
+	 * Disabling HW clock gating + NAP enabled combination has
+	 * minimal power impact. So this option is chosen over disabling
+	 * SP/TP power collapse.
+	 * Revisions of A430 which chipid 2 and above do not have the issue.
+	 */
+	if (adreno_is_a430(adreno_dev) &&
+		(ADRENO_CHIPID_PATCH(adreno_dev->chipid) < 2))
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL, 0);
+	else
+		kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL, 0xAAAAAAAA);
+	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2, 0);
+}
+
 /**
  * a4xx_protect_init() - Initializes register protection on a4xx
  * @adreno_dev: Pointer to the device structure
@@ -524,7 +616,7 @@ static void a4xx_protect_init(struct adreno_device *adreno_dev)
 	iommu_regs = kgsl_mmu_get_prot_regs(&device->mmu);
 	if (iommu_regs)
 		adreno_set_protected_registers(adreno_dev, &index,
-				iommu_regs->base, ilog2(iommu_regs->range));
+				iommu_regs->base, iommu_regs->range);
 }
 
 static struct adreno_snapshot_sizes a4xx_snap_sizes = {
@@ -610,13 +702,13 @@ static void a4xx_start(struct adreno_device *adreno_dev)
 				0x00000441);
 	}
 
+	a4xx_enable_hwcg(device);
 	/*
 	 * For A420 set RBBM_CLOCK_DELAY_HLSQ.CGC_HLSQ_TP_EARLY_CYC >= 2
 	 * due to timing issue with HLSQ_TP_CLK_EN
 	 */
 	if (adreno_is_a420(adreno_dev)) {
 		unsigned int val;
-
 		kgsl_regread(device, A4XX_RBBM_CLOCK_DELAY_HLSQ, &val);
 		val &= ~A4XX_CGC_HLSQ_TP_EARLY_CYC_MASK;
 		val |= 2 << A4XX_CGC_HLSQ_TP_EARLY_CYC_SHIFT;
@@ -631,8 +723,6 @@ static void a4xx_start(struct adreno_device *adreno_dev)
 		gpudev->vbif_xin_halt_ctrl0_mask =
 			A405_VBIF_XIN_HALT_CTRL0_MASK;
 
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-
 	a4xx_protect_init(adreno_dev);
 }
 
@@ -695,7 +785,6 @@ static void a4xx_err_callback(struct adreno_device *adreno_dev, int bit)
 	case A4XX_INT_CP_HW_FAULT:
 	{
 		struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
 		kgsl_regread(device, A4XX_CP_HW_FAULT, &reg);
 		KGSL_DRV_CRIT_RATELIMIT(device,
 			"CP | Ringbuffer HW fault | status=%x\n", reg);
@@ -741,10 +830,6 @@ static void a4xx_err_callback(struct adreno_device *adreno_dev, int bit)
 	}
 }
 
-static unsigned int a4xx_int_bits[ADRENO_INT_BITS_MAX] = {
-	ADRENO_INT_DEFINE(ADRENO_INT_RBBM_AHB_ERROR, A4XX_INT_RBBM_AHB_ERROR),
-};
-
 /* Register offset defines for A4XX, in order of enum adreno_regs */
 static unsigned int a4xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_ME_RAM_WADDR, A4XX_CP_ME_RAM_WADDR),
@@ -754,7 +839,6 @@ static unsigned int a4xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_WFI_PEND_CTR, A4XX_CP_WFI_PEND_CTR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_BASE, A4XX_CP_RB_BASE),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_BASE_HI, ADRENO_REG_SKIP),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR_ADDR_LO, A4XX_CP_RB_RPTR_ADDR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR, A4XX_CP_RB_RPTR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_WPTR, A4XX_CP_RB_WPTR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_CNTL, A4XX_CP_CNTL),
@@ -808,10 +892,6 @@ static unsigned int a4xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SW_RESET_CMD, A4XX_RBBM_SW_RESET_CMD),
 	ADRENO_REG_DEFINE(ADRENO_REG_UCHE_INVALIDATE0, A4XX_UCHE_INVALIDATE0),
 	ADRENO_REG_DEFINE(ADRENO_REG_UCHE_INVALIDATE1, A4XX_UCHE_INVALIDATE1),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_RBBM_0_LO,
-				A4XX_RBBM_PERFCTR_RBBM_0_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_RBBM_0_HI,
-				A4XX_RBBM_PERFCTR_RBBM_0_HI),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_LO,
 				A4XX_RBBM_PERFCTR_LOAD_VALUE_LO),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_HI,
@@ -1359,7 +1439,7 @@ static const unsigned int _a4xx_pwron_fixup_fs_instructions[] = {
 };
 
 /**
- * _a4xx_pwron_fixup() - Initialize a special command buffer to run a
+ * adreno_a4xx_pwron_fixup_init() - Initalize a special command buffer to run a
  * post-power collapse shader workaround
  * @adreno_dev: Pointer to a adreno_device struct
  *
@@ -1369,7 +1449,7 @@ static const unsigned int _a4xx_pwron_fixup_fs_instructions[] = {
  *
  * Returns: 0 on success or negative on error
  */
-static int _a4xx_pwron_fixup(struct adreno_device *adreno_dev)
+int adreno_a4xx_pwron_fixup_init(struct adreno_device *adreno_dev)
 {
 	unsigned int *cmds;
 	unsigned int count = ARRAY_SIZE(_a4xx_pwron_fixup_fs_instructions);
@@ -1378,11 +1458,11 @@ static int _a4xx_pwron_fixup(struct adreno_device *adreno_dev)
 
 	/* Return if the fixup is already in place */
 	if (test_bit(ADRENO_DEVICE_PWRON_FIXUP, &adreno_dev->priv))
-		return 0;
+			return 0;
 
 	ret = kgsl_allocate_global(KGSL_DEVICE(adreno_dev),
 		&adreno_dev->pwron_fixup, PAGE_SIZE,
-		KGSL_MEMFLAGS_GPUREADONLY, 0, "pwron_fixup");
+		KGSL_MEMFLAGS_GPUREADONLY, 0);
 
 	if (ret)
 		return ret;
@@ -1480,17 +1560,23 @@ static int _a4xx_pwron_fixup(struct adreno_device *adreno_dev)
 	return 0;
 }
 
-/*
- * a4xx_init() - Initialize gpu specific data
- * @adreno_dev: Pointer to adreno device
- */
-static void a4xx_init(struct adreno_device *adreno_dev)
+static int a4xx_hw_init(struct adreno_device *adreno_dev)
 {
-	if ((adreno_is_a405(adreno_dev)) || (adreno_is_a420(adreno_dev)))
-		_a4xx_pwron_fixup(adreno_dev);
+	a4xx_enable_pc(adreno_dev);
+	a4xx_enable_ppd(adreno_dev);
+
+	return 0;
 }
 
-static int a4xx_send_me_init(struct adreno_device *adreno_dev,
+/*
+ * a4xx_rb_init() - Initialize ringbuffer
+ * @adreno_dev: Pointer to adreno device
+ * @rb: Pointer to the ringbuffer of device
+ *
+ * Submit commands for ME initialization, common function shared between
+ * a4xx devices
+ */
+static int a4xx_rb_init(struct adreno_device *adreno_dev,
 			 struct adreno_ringbuffer *rb)
 {
 	unsigned int *cmds;
@@ -1539,55 +1625,7 @@ static int a4xx_send_me_init(struct adreno_device *adreno_dev,
 		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
 		dev_err(device->dev, "CP initialization failed to idle\n");
-		kgsl_device_snapshot(device, NULL, false);
-	}
-
-	return ret;
-}
-
-/*
- * a4xx_rb_start() - Start the ringbuffer
- * @adreno_dev: Pointer to adreno device
- * @start_type: Warm or cold start
- */
-static int a4xx_rb_start(struct adreno_device *adreno_dev,
-			 unsigned int start_type)
-{
-	struct adreno_ringbuffer *rb = ADRENO_CURRENT_RINGBUFFER(adreno_dev);
-	struct kgsl_device *device = &adreno_dev->dev;
-	uint64_t addr;
-	int ret;
-
-	addr = SCRATCH_RPTR_GPU_ADDR(device, rb->id);
-
-	adreno_writereg64(adreno_dev, ADRENO_REG_CP_RB_RPTR_ADDR_LO,
-			ADRENO_REG_CP_RB_RPTR_ADDR_HI, addr);
-
-	/*
-	 * The size of the ringbuffer in the hardware is the log2
-	 * representation of the size in quadwords (sizedwords / 2).
-	 * Also disable the host RPTR shadow register as it might be unreliable
-	 * in certain circumstances.
-	 */
-
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_CNTL,
-			((ilog2(4) << 8) & 0x1F00) |
-			(ilog2(KGSL_RB_DWORDS >> 1) & 0x3F));
-
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_BASE,
-			  rb->buffer_desc.gpuaddr);
-
-	ret = a3xx_microcode_load(adreno_dev, start_type);
-	if (ret)
-		return ret;
-
-	/* clear ME_HALT to start micro engine */
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_CNTL, 0);
-
-	ret = a4xx_send_me_init(adreno_dev, rb);
-	if (ret == 0) {
-		a4xx_enable_pc(adreno_dev);
-		a4xx_enable_ppd(adreno_dev);
+		kgsl_device_snapshot(device, NULL);
 	}
 
 	return ret;
@@ -1682,19 +1720,6 @@ static struct adreno_coresight a4xx_coresight = {
 	.groups = a4xx_coresight_groups,
 };
 
-static void a4xx_preempt_callback(struct adreno_device *adreno_dev, int bit)
-{
-	if (atomic_read(&adreno_dev->preempt.state) != ADRENO_PREEMPT_TRIGGERED)
-		return;
-
-	trace_adreno_hw_preempt_trig_to_comp_int(adreno_dev->cur_rb,
-			      adreno_dev->next_rb,
-			      adreno_get_rptr(adreno_dev->cur_rb),
-			      adreno_get_rptr(adreno_dev->next_rb));
-
-	adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
-}
-
 #define A4XX_INT_MASK \
 	((1 << A4XX_INT_RBBM_AHB_ERROR) |		\
 	 (1 << A4XX_INT_RBBM_REG_TIMEOUT) |		\
@@ -1732,7 +1757,7 @@ static struct adreno_irq_funcs a4xx_irq_funcs[32] = {
 	/* 6 - RBBM_ATB_ASYNC_OVERFLOW */
 	ADRENO_IRQ_CALLBACK(a4xx_err_callback),
 	ADRENO_IRQ_CALLBACK(NULL), /* 7 - RBBM_GPC_ERR */
-	ADRENO_IRQ_CALLBACK(a4xx_preempt_callback), /* 8 - CP_SW */
+	ADRENO_IRQ_CALLBACK(adreno_dispatcher_preempt_callback), /* 8 - CP_SW */
 	ADRENO_IRQ_CALLBACK(a4xx_err_callback), /* 9 - CP_OPCODE_ERROR */
 	/* 10 - CP_RESERVED_BIT_ERROR */
 	ADRENO_IRQ_CALLBACK(a4xx_err_callback),
@@ -1773,9 +1798,435 @@ static struct adreno_snapshot_data a4xx_snapshot_data = {
 	.sect_sizes = &a4xx_snap_sizes,
 };
 
+#define ADRENO_RB_PREEMPT_TOKEN_DWORDS		125
+
+static int a4xx_submit_preempt_token(struct adreno_ringbuffer *rb,
+					struct adreno_ringbuffer *incoming_rb)
+{
+	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	unsigned int *ringcmds, *start;
+	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	int ptname;
+	struct kgsl_pagetable *pt;
+	int pt_switch_sizedwords = 0, total_sizedwords = 20;
+	unsigned link[ADRENO_RB_PREEMPT_TOKEN_DWORDS];
+	uint i;
+
+	if (incoming_rb->preempted_midway) {
+
+		kgsl_sharedmem_readl(&incoming_rb->pagetable_desc,
+			&ptname, offsetof(
+			struct adreno_ringbuffer_pagetable_info,
+			current_rb_ptname));
+		pt = kgsl_mmu_get_pt_from_ptname(&(device->mmu),
+			ptname);
+		/*
+		 * always expect a valid pt, else pt refcounting is
+		 * messed up or current pt tracking has a bug which
+		 * could lead to eventual disaster
+		 */
+		BUG_ON(!pt);
+		/* set the ringbuffer for incoming RB */
+		pt_switch_sizedwords =
+			adreno_iommu_set_pt_generate_cmds(incoming_rb,
+							&link[0], pt);
+		total_sizedwords += pt_switch_sizedwords;
+	}
+
+	/*
+	 *  Allocate total_sizedwords space in RB, this is the max space
+	 *  required.
+	 */
+	ringcmds = adreno_ringbuffer_allocspace(rb, total_sizedwords);
+
+	if (IS_ERR(ringcmds))
+		return PTR_ERR(ringcmds);
+
+	start = ringcmds;
+
+	*ringcmds++ = cp_packet(adreno_dev, CP_SET_PROTECTED_MODE, 1);
+	*ringcmds++ = 0;
+
+	if (incoming_rb->preempted_midway) {
+		for (i = 0; i < pt_switch_sizedwords; i++)
+			*ringcmds++ = link[i];
+	}
+
+	*ringcmds++ = cp_register(adreno_dev, adreno_getreg(adreno_dev,
+			ADRENO_REG_CP_PREEMPT_DISABLE), 1);
+	*ringcmds++ = 0;
+
+	*ringcmds++ = cp_packet(adreno_dev, CP_SET_PROTECTED_MODE, 1);
+	*ringcmds++ = 1;
+
+	ringcmds += gpudev->preemption_token(adreno_dev, rb, ringcmds,
+				device->memstore.gpuaddr +
+				KGSL_MEMSTORE_RB_OFFSET(rb, preempted));
+
+	if ((uint)(ringcmds - start) > total_sizedwords) {
+		KGSL_DRV_ERR(device, "Insufficient rb size allocated\n");
+		BUG();
+	}
+
+	/*
+	 * If we have commands less than the space reserved in RB
+	 *  adjust the wptr accordingly
+	 */
+	rb->wptr = rb->wptr - (total_sizedwords - (uint)(ringcmds - start));
+
+	/* submit just the preempt token */
+	mb();
+	kgsl_pwrscale_busy(device);
+	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR, rb->wptr);
+	return 0;
+}
+
+/**
+ * a4xx_preempt_trig_state() - Schedule preemption in TRIGGERRED
+ * state
+ * @adreno_dev: Device which is in TRIGGERRED state
+ */
+static void a4xx_preempt_trig_state(
+			struct adreno_device *adreno_dev)
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	unsigned int rbbase, val;
+
+	/*
+	 * Hardware not yet idle means that preemption interrupt
+	 * may still occur, nothing to do here until interrupt signals
+	 * completion of preemption, just return here
+	 */
+	if (!adreno_hw_isidle(adreno_dev))
+		return;
+
+	/*
+	 * We just changed states, reschedule dispatcher to change
+	 * preemption states
+	 */
+	if (ADRENO_DISPATCHER_PREEMPT_TRIGGERED !=
+		atomic_read(&dispatcher->preemption_state)) {
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+
+	/*
+	 * H/W is idle and we did not get a preemption interrupt, may
+	 * be device went idle w/o encountering any preempt token or
+	 * we already preempted w/o interrupt
+	 */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_BASE, &rbbase);
+	 /* Did preemption occur, if so then change states and return */
+	if (rbbase != adreno_dev->cur_rb->buffer_desc.gpuaddr) {
+		adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT_DEBUG, &val);
+		if (val && rbbase == adreno_dev->next_rb->buffer_desc.gpuaddr) {
+			KGSL_DRV_INFO(device,
+			"Preemption completed without interrupt\n");
+			trace_adreno_hw_preempt_trig_to_comp(adreno_dev->cur_rb,
+					adreno_dev->next_rb);
+			atomic_set(&dispatcher->preemption_state,
+				ADRENO_DISPATCHER_PREEMPT_COMPLETE);
+			adreno_dispatcher_schedule(device);
+			return;
+		}
+		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+		/* reschedule dispatcher to take care of the fault */
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+	/*
+	 * Check if preempt token was submitted after preemption trigger, if so
+	 * then preemption should have occurred, since device is already idle it
+	 * means something went wrong - trigger FT
+	 */
+	if (dispatcher->preempt_token_submit) {
+		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+		/* reschedule dispatcher to take care of the fault */
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+	/*
+	 * Preempt token was not submitted after preemption trigger so device
+	 * may have gone idle before preemption could occur, if there are
+	 * commands that got submitted to current RB after triggering preemption
+	 * then submit them as those commands may have a preempt token in them
+	 */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR,
+			&adreno_dev->cur_rb->rptr);
+	if (adreno_dev->cur_rb->rptr != adreno_dev->cur_rb->wptr) {
+		/*
+		 * Memory barrier before informing the
+		 * hardware of new commands
+		 */
+		mb();
+		kgsl_pwrscale_busy(device);
+		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR,
+			adreno_dev->cur_rb->wptr);
+		return;
+	}
+
+	/* Submit preempt token to make preemption happen */
+	if (adreno_drawctxt_switch(adreno_dev, adreno_dev->cur_rb, NULL, 0))
+		BUG();
+	if (a4xx_submit_preempt_token(adreno_dev->cur_rb,
+						adreno_dev->next_rb))
+		BUG();
+	dispatcher->preempt_token_submit = 1;
+	adreno_dev->cur_rb->wptr_preempt_end = adreno_dev->cur_rb->wptr;
+	trace_adreno_hw_preempt_token_submit(adreno_dev->cur_rb,
+						adreno_dev->next_rb);
+}
+
+/**
+ * a4xx_preempt_clear_state() - Schedule preemption in
+ * CLEAR state. Preemption can be issued in this state.
+ * @adreno_dev: Device which is in CLEAR state
+ */
+static void a4xx_preempt_clear_state(
+			struct adreno_device *adreno_dev)
+
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct adreno_dispatcher_cmdqueue *dispatch_tempq;
+	struct kgsl_cmdbatch *cmdbatch;
+	struct adreno_ringbuffer *highest_busy_rb;
+	int switch_low_to_high;
+	int ret;
+
+	/* Device not awake means there is nothing to do */
+	if (!kgsl_state_is_awake(device))
+		return;
+
+	/* keep updating the current rptr when preemption is clear */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR,
+			&(adreno_dev->cur_rb->rptr));
+
+	highest_busy_rb = adreno_dispatcher_get_highest_busy_rb(adreno_dev);
+	if (!highest_busy_rb)
+		return;
+
+	switch_low_to_high = adreno_compare_prio_level(
+					highest_busy_rb->id,
+					adreno_dev->cur_rb->id);
+
+	/* already current then return */
+	if (!switch_low_to_high)
+		return;
+
+	if (switch_low_to_high < 0) {
+		/*
+		 * if switching to lower priority make sure that the rptr and
+		 * wptr are equal, when the lower rb is not starved
+		 */
+		if (adreno_dev->cur_rb->rptr != adreno_dev->cur_rb->wptr)
+			return;
+		/*
+		 * switch to default context because when we switch back
+		 * to higher context then its not known which pt will
+		 * be current, so by making it default here the next
+		 * commands submitted will set the right pt
+		 */
+		ret = adreno_drawctxt_switch(adreno_dev,
+				adreno_dev->cur_rb,
+				NULL, 0);
+		/*
+		 * lower priority RB has to wait until space opens up in
+		 * higher RB
+		 */
+		if (ret)
+			return;
+
+		adreno_writereg(adreno_dev,
+			ADRENO_REG_CP_PREEMPT_DISABLE, 1);
+	}
+
+	/*
+	 * setup registers to do the switch to highest priority RB
+	 * which is not empty or may be starving away(poor thing)
+	 */
+	a4xx_preemption_start(adreno_dev, highest_busy_rb);
+
+	/* turn on IOMMU as the preemption may trigger pt switch */
+	kgsl_mmu_enable_clk(&device->mmu);
+
+	atomic_set(&dispatcher->preemption_state,
+			ADRENO_DISPATCHER_PREEMPT_TRIGGERED);
+
+	adreno_dev->next_rb = highest_busy_rb;
+	mod_timer(&dispatcher->preempt_timer, jiffies +
+		msecs_to_jiffies(ADRENO_DISPATCH_PREEMPT_TIMEOUT));
+
+	trace_adreno_hw_preempt_clear_to_trig(adreno_dev->cur_rb,
+						adreno_dev->next_rb);
+	/* issue PREEMPT trigger */
+	adreno_writereg(adreno_dev, ADRENO_REG_CP_PREEMPT, 1);
+	/*
+	 * IOMMU clock can be safely switched off after the timestamp
+	 * of the first command in the new rb
+	 */
+	dispatch_tempq = &adreno_dev->next_rb->dispatch_q;
+	if (dispatch_tempq->head != dispatch_tempq->tail)
+		cmdbatch = dispatch_tempq->cmd_q[dispatch_tempq->head];
+	else
+		cmdbatch = NULL;
+	if (cmdbatch)
+		adreno_ringbuffer_mmu_disable_clk_on_ts(device,
+			adreno_dev->next_rb,
+			cmdbatch->global_ts);
+	else
+		adreno_ringbuffer_mmu_disable_clk_on_ts(device,
+			adreno_dev->next_rb, adreno_dev->next_rb->timestamp);
+	/* submit preempt token packet to ensure preemption */
+	if (switch_low_to_high < 0) {
+		ret = a4xx_submit_preempt_token(
+			adreno_dev->cur_rb, adreno_dev->next_rb);
+		/*
+		 * unexpected since we are submitting this when rptr = wptr,
+		 * this was checked above already
+		 */
+		BUG_ON(ret);
+		dispatcher->preempt_token_submit = 1;
+		adreno_dev->cur_rb->wptr_preempt_end = adreno_dev->cur_rb->wptr;
+	} else {
+		dispatcher->preempt_token_submit = 0;
+		adreno_dispatcher_schedule(device);
+		adreno_dev->cur_rb->wptr_preempt_end = 0xFFFFFFFF;
+	}
+}
+
+/**
+ * a4xx_preempt_complete_state() - Schedule preemption in
+ * COMPLETE state
+ * @adreno_dev: Device which is in COMPLETE state
+ */
+static void a4xx_preempt_complete_state(
+			struct adreno_device *adreno_dev)
+
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct adreno_dispatcher_cmdqueue *dispatch_q;
+	unsigned int wptr, rbbase;
+	unsigned int val, val1;
+
+	del_timer_sync(&dispatcher->preempt_timer);
+
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &val);
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT_DEBUG, &val1);
+
+	if (val || !val1) {
+		KGSL_DRV_ERR(device,
+		"Invalid state after preemption CP_PREEMPT: %08x, CP_PREEMPT_DEBUG: %08x\n",
+		val, val1);
+		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_BASE, &rbbase);
+	if (rbbase != adreno_dev->next_rb->buffer_desc.gpuaddr) {
+		KGSL_DRV_ERR(device,
+		"RBBASE incorrect after preemption, expected %x got %016llx\b",
+		rbbase,
+		adreno_dev->next_rb->buffer_desc.gpuaddr);
+		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+
+	a4xx_preemption_save(adreno_dev, adreno_dev->cur_rb);
+
+	dispatch_q = &(adreno_dev->cur_rb->dispatch_q);
+	/* new RB is the current RB */
+	trace_adreno_hw_preempt_comp_to_clear(adreno_dev->next_rb,
+						adreno_dev->cur_rb);
+	adreno_dev->prev_rb = adreno_dev->cur_rb;
+	adreno_dev->cur_rb = adreno_dev->next_rb;
+	adreno_dev->cur_rb->preempted_midway = 0;
+	adreno_dev->cur_rb->wptr_preempt_end = 0xFFFFFFFF;
+	adreno_dev->next_rb = NULL;
+	if (adreno_disp_preempt_fair_sched) {
+		/* starved rb is now scheduled so unhalt dispatcher */
+		if (ADRENO_DISPATCHER_RB_STARVE_TIMER_ELAPSED ==
+			adreno_dev->cur_rb->starve_timer_state)
+			adreno_put_gpu_halt(adreno_dev);
+		adreno_dev->cur_rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_SCHEDULED;
+		adreno_dev->cur_rb->sched_timer = jiffies;
+		/*
+		 * If the outgoing RB is has commands then set the
+		 * busy time for it
+		 */
+		if (adreno_dev->prev_rb->rptr != adreno_dev->prev_rb->wptr) {
+			adreno_dev->prev_rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_INIT;
+			adreno_dev->prev_rb->sched_timer = jiffies;
+		} else {
+			adreno_dev->prev_rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT;
+		}
+	}
+	atomic_set(&dispatcher->preemption_state,
+		ADRENO_DISPATCHER_PREEMPT_CLEAR);
+	if (adreno_compare_prio_level(adreno_dev->prev_rb->id,
+				adreno_dev->cur_rb->id) < 0) {
+		if (adreno_dev->prev_rb->wptr_preempt_end !=
+			adreno_dev->prev_rb->rptr)
+			adreno_dev->prev_rb->preempted_midway = 1;
+	} else if (adreno_dev->prev_rb->wptr_preempt_end !=
+		adreno_dev->prev_rb->rptr) {
+		BUG();
+	}
+	/* submit wptr if required for new rb */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
+	if (adreno_dev->cur_rb->wptr != wptr) {
+		kgsl_pwrscale_busy(device);
+		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR,
+					adreno_dev->cur_rb->wptr);
+	}
+	/* clear preemption register */
+	adreno_writereg(adreno_dev, ADRENO_REG_CP_PREEMPT_DEBUG, 0);
+	adreno_preempt_process_dispatch_queue(adreno_dev, dispatch_q);
+}
+
+static void a4xx_preemption_schedule(
+				struct adreno_device *adreno_dev)
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	if (!adreno_is_preemption_enabled(adreno_dev))
+		return;
+
+	mutex_lock(&device->mutex);
+
+	switch (atomic_read(&dispatcher->preemption_state)) {
+	case ADRENO_DISPATCHER_PREEMPT_CLEAR:
+		a4xx_preempt_clear_state(adreno_dev);
+		break;
+	case ADRENO_DISPATCHER_PREEMPT_TRIGGERED:
+		a4xx_preempt_trig_state(adreno_dev);
+		/*
+		 * if we transitioned to next state then fall-through
+		 * processing to next state
+		 */
+		if (!adreno_preempt_state(adreno_dev,
+			ADRENO_DISPATCHER_PREEMPT_COMPLETE))
+			break;
+	case ADRENO_DISPATCHER_PREEMPT_COMPLETE:
+		a4xx_preempt_complete_state(adreno_dev);
+		break;
+	default:
+		BUG();
+	}
+
+	mutex_unlock(&device->mutex);
+}
+
 struct adreno_gpudev adreno_a4xx_gpudev = {
 	.reg_offsets = &a4xx_reg_offsets,
-	.int_bits = a4xx_int_bits,
 	.ft_perf_counters = a4xx_ft_perf_counters,
 	.ft_perf_counters_count = ARRAY_SIZE(a4xx_ft_perf_counters),
 	.perfcounters = &a4xx_perfcounters,
@@ -1787,10 +2238,11 @@ struct adreno_gpudev adreno_a4xx_gpudev = {
 
 	.perfcounter_init = a4xx_perfcounter_init,
 	.perfcounter_close = a4xx_perfcounter_close,
-	.rb_start = a4xx_rb_start,
-	.init = a4xx_init,
+	.rb_init = a4xx_rb_init,
+	.hw_init = a4xx_hw_init,
 	.microcode_read = a3xx_microcode_read,
-	.coresight = {&a4xx_coresight},
+	.microcode_load = a3xx_microcode_load,
+	.coresight = &a4xx_coresight,
 	.start = a4xx_start,
 	.snapshot = a4xx_snapshot,
 	.is_sptp_idle = a4xx_is_sptp_idle,
@@ -1798,6 +2250,6 @@ struct adreno_gpudev adreno_a4xx_gpudev = {
 	.regulator_enable = a4xx_regulator_enable,
 	.regulator_disable = a4xx_regulator_disable,
 	.preemption_pre_ibsubmit = a4xx_preemption_pre_ibsubmit,
+	.preemption_token = a4xx_preemption_token,
 	.preemption_schedule = a4xx_preemption_schedule,
-	.preemption_init = a4xx_preemption_init,
 };
diff --git a/drivers/gpu/msm/adreno_a4xx.h b/drivers/gpu/msm/adreno_a4xx.h
index 5dabc26fd34f..93e54e82a48c 100644
--- a/drivers/gpu/msm/adreno_a4xx.h
+++ b/drivers/gpu/msm/adreno_a4xx.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -14,48 +14,6 @@
 #ifndef _ADRENO_A4XX_H_
 #define _ADRENO_A4XX_H_
 
-#include "a4xx_reg.h"
-
-#define A4XX_IRQ_FLAGS \
-	{ BIT(A4XX_INT_RBBM_GPU_IDLE), "RBBM_GPU_IDLE" }, \
-	{ BIT(A4XX_INT_RBBM_REG_TIMEOUT), "RBBM_REG_TIMEOUT" }, \
-	{ BIT(A4XX_INT_RBBM_ME_MS_TIMEOUT), "RBBM_ME_MS_TIMEOUT" }, \
-	{ BIT(A4XX_INT_RBBM_PFP_MS_TIMEOUT), "RBBM_PFP_MS_TIMEOUT" }, \
-	{ BIT(A4XX_INT_RBBM_ETS_MS_TIMEOUT), "RBBM_ETS_MS_TIMEOUT" }, \
-	{ BIT(A4XX_INT_RBBM_ASYNC_OVERFLOW), "RBBM_ASYNC_OVERFLOW" }, \
-	{ BIT(A4XX_INT_RBBM_GPC_ERR), "RBBM_GPC_ERR" }, \
-	{ BIT(A4XX_INT_CP_SW), "CP_SW" }, \
-	{ BIT(A4XX_INT_CP_OPCODE_ERROR), "CP_OPCODE_ERROR" }, \
-	{ BIT(A4XX_INT_CP_RESERVED_BIT_ERROR), "CP_RESERVED_BIT_ERROR" }, \
-	{ BIT(A4XX_INT_CP_HW_FAULT), "CP_HW_FAULT" }, \
-	{ BIT(A4XX_INT_CP_DMA), "CP_DMA" }, \
-	{ BIT(A4XX_INT_CP_IB2_INT), "CP_IB2_INT" }, \
-	{ BIT(A4XX_INT_CP_IB1_INT), "CP_IB1_INT" }, \
-	{ BIT(A4XX_INT_CP_RB_INT), "CP_RB_INT" }, \
-	{ BIT(A4XX_INT_CP_REG_PROTECT_FAULT), "CP_REG_PROTECT_FAULT" }, \
-	{ BIT(A4XX_INT_CP_RB_DONE_TS), "CP_RB_DONE_TS" }, \
-	{ BIT(A4XX_INT_CP_VS_DONE_TS), "CP_VS_DONE_TS" }, \
-	{ BIT(A4XX_INT_CP_PS_DONE_TS), "CP_PS_DONE_TS" }, \
-	{ BIT(A4XX_INT_CACHE_FLUSH_TS), "CACHE_FLUSH_TS" }, \
-	{ BIT(A4XX_INT_CP_AHB_ERROR_HALT), "CP_AHB_ERROR_HALT" }, \
-	{ BIT(A4XX_INT_RBBM_ATB_BUS_OVERFLOW), "RBBM_ATB_BUS_OVERFLOW" }, \
-	{ BIT(A4XX_INT_MISC_HANG_DETECT), "MISC_HANG_DETECT" }, \
-	{ BIT(A4XX_INT_UCHE_OOB_ACCESS), "UCHE_OOB_ACCESS" }, \
-	{ BIT(A4XX_INT_RBBM_DPM_CALC_ERR), "RBBM_DPM_CALC_ERR" }, \
-	{ BIT(A4XX_INT_RBBM_DPM_EPOCH_ERR), "RBBM_DPM_CALC_ERR" }, \
-	{ BIT(A4XX_INT_RBBM_DPM_THERMAL_YELLOW_ERR), \
-		"RBBM_DPM_THERMAL_YELLOW_ERR" }, \
-	{ BIT(A4XX_INT_RBBM_DPM_THERMAL_RED_ERR), "RBBM_DPM_THERMAL_RED_ERR" }
-
-unsigned int a4xx_preemption_pre_ibsubmit(struct adreno_device *adreno_dev,
-			struct adreno_ringbuffer *rb,
-			unsigned int *cmds,
-			struct kgsl_context *context);
-
-void a4xx_preemption_schedule(struct adreno_device *adreno_dev);
-
-int a4xx_preemption_init(struct adreno_device *adreno_dev);
-
 void a4xx_snapshot(struct adreno_device *adreno_dev,
 		struct kgsl_snapshot *snapshot);
 
diff --git a/drivers/gpu/msm/adreno_a4xx_preempt.c b/drivers/gpu/msm/adreno_a4xx_preempt.c
deleted file mode 100644
index 058ac9c2cf4b..000000000000
--- a/drivers/gpu/msm/adreno_a4xx_preempt.c
+++ /dev/null
@@ -1,573 +0,0 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#include "adreno.h"
-#include "adreno_a4xx.h"
-#include "adreno_trace.h"
-#include "adreno_pm4types.h"
-
-#define ADRENO_RB_PREEMPT_TOKEN_DWORDS		125
-
-static void a4xx_preemption_timer(unsigned long data)
-{
-	struct adreno_device *adreno_dev = (struct adreno_device *) data;
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int cur_rptr = adreno_get_rptr(adreno_dev->cur_rb);
-	unsigned int next_rptr = adreno_get_rptr(adreno_dev->next_rb);
-
-	KGSL_DRV_ERR(device,
-		"Preemption timed out. cur_rb rptr/wptr %x/%x id %d, next_rb rptr/wptr %x/%x id %d, disp_state: %d\n",
-		cur_rptr, adreno_dev->cur_rb->wptr, adreno_dev->cur_rb->id,
-		next_rptr, adreno_dev->next_rb->wptr, adreno_dev->next_rb->id,
-		atomic_read(&adreno_dev->preempt.state));
-
-	adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-	adreno_dispatcher_schedule(device);
-}
-
-static unsigned int a4xx_preemption_token(struct adreno_device *adreno_dev,
-			unsigned int *cmds, uint64_t gpuaddr)
-{
-	unsigned int *cmds_orig = cmds;
-
-	/* Turn on preemption flag */
-	/* preemption token - fill when pt switch command size is known */
-	*cmds++ = cp_type3_packet(CP_PREEMPT_TOKEN, 3);
-	*cmds++ = (uint)gpuaddr;
-	*cmds++ = 1;
-	/* generate interrupt on preemption completion */
-	*cmds++ = 1 << CP_PREEMPT_ORDINAL_INTERRUPT;
-
-	return (unsigned int) (cmds - cmds_orig);
-}
-
-unsigned int a4xx_preemption_pre_ibsubmit(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb, unsigned int *cmds,
-		struct kgsl_context *context)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int *cmds_orig = cmds;
-	unsigned int cond_addr = device->memstore.gpuaddr +
-		MEMSTORE_ID_GPU_ADDR(device, context->id, preempted);
-
-	cmds += a4xx_preemption_token(adreno_dev, cmds, cond_addr);
-
-	*cmds++ = cp_type3_packet(CP_COND_EXEC, 4);
-	*cmds++ = cond_addr;
-	*cmds++ = cond_addr;
-	*cmds++ = 1;
-	*cmds++ = 7;
-
-	/* clear preemption flag */
-	*cmds++ = cp_type3_packet(CP_MEM_WRITE, 2);
-	*cmds++ = cond_addr;
-	*cmds++ = 0;
-	*cmds++ = cp_type3_packet(CP_WAIT_MEM_WRITES, 1);
-	*cmds++ = 0;
-	*cmds++ = cp_type3_packet(CP_WAIT_FOR_ME, 1);
-	*cmds++ = 0;
-
-	return (unsigned int) (cmds - cmds_orig);
-}
-
-
-static void a4xx_preemption_start(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	uint32_t val;
-
-	/*
-	 * Setup scratch registers from which the GPU will program the
-	 * registers required to start execution of new ringbuffer
-	 * set ringbuffer address
-	 */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG8,
-		rb->buffer_desc.gpuaddr);
-	kgsl_regread(device, A4XX_CP_RB_CNTL, &val);
-	/* scratch REG9 corresponds to CP_RB_CNTL register */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG9, val);
-	/* scratch REG10 corresponds to rptr address */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG10,
-		SCRATCH_RPTR_GPU_ADDR(device, rb->id));
-	/* scratch REG11 corresponds to rptr */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG11, adreno_get_rptr(rb));
-	/* scratch REG12 corresponds to wptr */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG12, rb->wptr);
-	/*
-	 * scratch REG13 corresponds to  IB1_BASE,
-	 * 0 since we do not do switches in between IB's
-	 */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG13, 0);
-	/* scratch REG14 corresponds to IB1_BUFSZ */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG14, 0);
-	/* scratch REG15 corresponds to IB2_BASE */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG15, 0);
-	/* scratch REG16 corresponds to  IB2_BUFSZ */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG16, 0);
-	/* scratch REG17 corresponds to GPR11 */
-	kgsl_regwrite(device, A4XX_CP_SCRATCH_REG17, rb->gpr11);
-}
-
-static void a4xx_preemption_save(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	kgsl_regread(device, A4XX_CP_SCRATCH_REG23, &rb->gpr11);
-}
-
-
-static int a4xx_submit_preempt_token(struct adreno_ringbuffer *rb,
-					struct adreno_ringbuffer *incoming_rb)
-{
-	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int *ringcmds, *start;
-	int ptname;
-	struct kgsl_pagetable *pt;
-	int pt_switch_sizedwords = 0, total_sizedwords = 20;
-	unsigned int link[ADRENO_RB_PREEMPT_TOKEN_DWORDS];
-	uint i;
-
-	if (incoming_rb->preempted_midway) {
-
-		kgsl_sharedmem_readl(&incoming_rb->pagetable_desc,
-			&ptname, PT_INFO_OFFSET(current_rb_ptname));
-		pt = kgsl_mmu_get_pt_from_ptname(&(device->mmu),
-			ptname);
-		if (IS_ERR_OR_NULL(pt))
-			return (pt == NULL) ? -ENOENT : PTR_ERR(pt);
-		/* set the ringbuffer for incoming RB */
-		pt_switch_sizedwords =
-			adreno_iommu_set_pt_generate_cmds(incoming_rb,
-							&link[0], pt);
-		total_sizedwords += pt_switch_sizedwords;
-	}
-
-	/*
-	 *  Allocate total_sizedwords space in RB, this is the max space
-	 *  required.
-	 */
-	ringcmds = adreno_ringbuffer_allocspace(rb, total_sizedwords);
-
-	if (IS_ERR(ringcmds))
-		return PTR_ERR(ringcmds);
-
-	start = ringcmds;
-
-	*ringcmds++ = cp_packet(adreno_dev, CP_SET_PROTECTED_MODE, 1);
-	*ringcmds++ = 0;
-
-	if (incoming_rb->preempted_midway) {
-		for (i = 0; i < pt_switch_sizedwords; i++)
-			*ringcmds++ = link[i];
-	}
-
-	*ringcmds++ = cp_register(adreno_dev, adreno_getreg(adreno_dev,
-			ADRENO_REG_CP_PREEMPT_DISABLE), 1);
-	*ringcmds++ = 0;
-
-	*ringcmds++ = cp_packet(adreno_dev, CP_SET_PROTECTED_MODE, 1);
-	*ringcmds++ = 1;
-
-	ringcmds += a4xx_preemption_token(adreno_dev, ringcmds,
-				device->memstore.gpuaddr +
-				MEMSTORE_RB_OFFSET(rb, preempted));
-
-	if ((uint)(ringcmds - start) > total_sizedwords)
-		KGSL_DRV_ERR(device, "Insufficient rb size allocated\n");
-
-	/*
-	 * If we have commands less than the space reserved in RB
-	 *  adjust the wptr accordingly
-	 */
-	rb->wptr = rb->wptr - (total_sizedwords - (uint)(ringcmds - start));
-
-	/* submit just the preempt token */
-	mb();
-	kgsl_pwrscale_busy(device);
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR, rb->wptr);
-	return 0;
-}
-
-static void a4xx_preempt_trig_state(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int rbbase, val;
-	int ret;
-
-	/*
-	 * Hardware not yet idle means that preemption interrupt
-	 * may still occur, nothing to do here until interrupt signals
-	 * completion of preemption, just return here
-	 */
-	if (!adreno_hw_isidle(adreno_dev))
-		return;
-
-	/*
-	 * We just changed states, reschedule dispatcher to change
-	 * preemption states
-	 */
-
-	if (atomic_read(&adreno_dev->preempt.state) !=
-		ADRENO_PREEMPT_TRIGGERED) {
-		adreno_dispatcher_schedule(device);
-		return;
-	}
-
-	/*
-	 * H/W is idle and we did not get a preemption interrupt, may
-	 * be device went idle w/o encountering any preempt token or
-	 * we already preempted w/o interrupt
-	 */
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_BASE, &rbbase);
-	 /* Did preemption occur, if so then change states and return */
-	if (rbbase != adreno_dev->cur_rb->buffer_desc.gpuaddr) {
-		adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT_DEBUG, &val);
-		if (val && rbbase == adreno_dev->next_rb->buffer_desc.gpuaddr) {
-			KGSL_DRV_INFO(device,
-			"Preemption completed without interrupt\n");
-			trace_adreno_hw_preempt_trig_to_comp(adreno_dev->cur_rb,
-					adreno_dev->next_rb,
-					adreno_get_rptr(adreno_dev->cur_rb),
-					adreno_get_rptr(adreno_dev->next_rb));
-			adreno_set_preempt_state(adreno_dev,
-				ADRENO_PREEMPT_COMPLETE);
-			adreno_dispatcher_schedule(device);
-			return;
-		}
-		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-		/* reschedule dispatcher to take care of the fault */
-		adreno_dispatcher_schedule(device);
-		return;
-	}
-	/*
-	 * Check if preempt token was submitted after preemption trigger, if so
-	 * then preemption should have occurred, since device is already idle it
-	 * means something went wrong - trigger FT
-	 */
-	if (adreno_dev->preempt.token_submit) {
-		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-		/* reschedule dispatcher to take care of the fault */
-		adreno_dispatcher_schedule(device);
-		return;
-	}
-	/*
-	 * Preempt token was not submitted after preemption trigger so device
-	 * may have gone idle before preemption could occur, if there are
-	 * commands that got submitted to current RB after triggering preemption
-	 * then submit them as those commands may have a preempt token in them
-	 */
-	if (!adreno_rb_empty(adreno_dev->cur_rb)) {
-		/*
-		 * Memory barrier before informing the
-		 * hardware of new commands
-		 */
-		mb();
-		kgsl_pwrscale_busy(device);
-		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR,
-			adreno_dev->cur_rb->wptr);
-		return;
-	}
-
-	/* Submit preempt token to make preemption happen */
-	ret = adreno_drawctxt_switch(adreno_dev, adreno_dev->cur_rb,
-		NULL, 0);
-	if (ret)
-		KGSL_DRV_ERR(device,
-			"Unable to switch context to NULL: %d\n", ret);
-
-	ret = a4xx_submit_preempt_token(adreno_dev->cur_rb,
-						adreno_dev->next_rb);
-	if (ret)
-		KGSL_DRV_ERR(device,
-			"Unable to submit preempt token: %d\n", ret);
-
-	adreno_dev->preempt.token_submit = true;
-	adreno_dev->cur_rb->wptr_preempt_end = adreno_dev->cur_rb->wptr;
-	trace_adreno_hw_preempt_token_submit(adreno_dev->cur_rb,
-			adreno_dev->next_rb,
-			adreno_get_rptr(adreno_dev->cur_rb),
-			adreno_get_rptr(adreno_dev->next_rb));
-}
-
-static struct adreno_ringbuffer *a4xx_next_ringbuffer(
-		struct adreno_device *adreno_dev)
-{
-	struct adreno_ringbuffer *rb, *next = NULL;
-	int i;
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		if (!adreno_rb_empty(rb) && next == NULL) {
-			next = rb;
-			continue;
-		}
-
-		if (!adreno_disp_preempt_fair_sched)
-			continue;
-
-		switch (rb->starve_timer_state) {
-		case ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT:
-			if (!adreno_rb_empty(rb) &&
-				adreno_dev->cur_rb != rb) {
-				rb->starve_timer_state =
-				ADRENO_DISPATCHER_RB_STARVE_TIMER_INIT;
-				rb->sched_timer = jiffies;
-			}
-			break;
-		case ADRENO_DISPATCHER_RB_STARVE_TIMER_INIT:
-			if (time_after(jiffies, rb->sched_timer +
-				msecs_to_jiffies(
-					adreno_dispatch_starvation_time))) {
-				rb->starve_timer_state =
-				ADRENO_DISPATCHER_RB_STARVE_TIMER_ELAPSED;
-				/* halt dispatcher to remove starvation */
-				adreno_get_gpu_halt(adreno_dev);
-			}
-			break;
-		case ADRENO_DISPATCHER_RB_STARVE_TIMER_SCHEDULED:
-			/*
-			 * If the RB has not been running for the minimum
-			 * time slice then allow it to run
-			 */
-			if (!adreno_rb_empty(rb) && time_before(jiffies,
-				adreno_dev->cur_rb->sched_timer +
-				msecs_to_jiffies(adreno_dispatch_time_slice)))
-				next = rb;
-			else
-				rb->starve_timer_state =
-				ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT;
-			break;
-		case ADRENO_DISPATCHER_RB_STARVE_TIMER_ELAPSED:
-		default:
-			break;
-		}
-	}
-
-	return next;
-}
-
-static void a4xx_preempt_clear_state(struct adreno_device *adreno_dev)
-
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_ringbuffer *highest_busy_rb;
-	int switch_low_to_high;
-	int ret;
-
-	/* Device not awake means there is nothing to do */
-	if (!kgsl_state_is_awake(device))
-		return;
-
-	highest_busy_rb = a4xx_next_ringbuffer(adreno_dev);
-	if (!highest_busy_rb || highest_busy_rb == adreno_dev->cur_rb)
-		return;
-
-	switch_low_to_high = adreno_compare_prio_level(
-					highest_busy_rb->id,
-					adreno_dev->cur_rb->id);
-
-	if (switch_low_to_high < 0) {
-		/*
-		 * if switching to lower priority make sure that the rptr and
-		 * wptr are equal, when the lower rb is not starved
-		 */
-		if (!adreno_rb_empty(adreno_dev->cur_rb))
-			return;
-		/*
-		 * switch to default context because when we switch back
-		 * to higher context then its not known which pt will
-		 * be current, so by making it default here the next
-		 * commands submitted will set the right pt
-		 */
-		ret = adreno_drawctxt_switch(adreno_dev,
-				adreno_dev->cur_rb,
-				NULL, 0);
-		/*
-		 * lower priority RB has to wait until space opens up in
-		 * higher RB
-		 */
-		if (ret) {
-			KGSL_DRV_ERR(device,
-				"Unable to switch context to NULL: %d",
-				ret);
-
-			return;
-		}
-
-		adreno_writereg(adreno_dev,
-			ADRENO_REG_CP_PREEMPT_DISABLE, 1);
-	}
-
-	/*
-	 * setup registers to do the switch to highest priority RB
-	 * which is not empty or may be starving away(poor thing)
-	 */
-	a4xx_preemption_start(adreno_dev, highest_busy_rb);
-
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_TRIGGERED);
-
-	adreno_dev->next_rb = highest_busy_rb;
-	mod_timer(&adreno_dev->preempt.timer, jiffies +
-		msecs_to_jiffies(ADRENO_PREEMPT_TIMEOUT));
-
-	trace_adreno_hw_preempt_clear_to_trig(adreno_dev->cur_rb,
-			adreno_dev->next_rb,
-			adreno_get_rptr(adreno_dev->cur_rb),
-			adreno_get_rptr(adreno_dev->next_rb));
-	/* issue PREEMPT trigger */
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_PREEMPT, 1);
-
-	/* submit preempt token packet to ensure preemption */
-	if (switch_low_to_high < 0) {
-		ret = a4xx_submit_preempt_token(
-			adreno_dev->cur_rb, adreno_dev->next_rb);
-		KGSL_DRV_ERR(device,
-			"Unable to submit preempt token: %d\n", ret);
-		adreno_dev->preempt.token_submit = true;
-		adreno_dev->cur_rb->wptr_preempt_end = adreno_dev->cur_rb->wptr;
-	} else {
-		adreno_dev->preempt.token_submit = false;
-		adreno_dispatcher_schedule(device);
-		adreno_dev->cur_rb->wptr_preempt_end = 0xFFFFFFFF;
-	}
-}
-
-static void a4xx_preempt_complete_state(struct adreno_device *adreno_dev)
-
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int wptr, rbbase;
-	unsigned int val, val1;
-	unsigned int prevrptr;
-
-	del_timer_sync(&adreno_dev->preempt.timer);
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &val);
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT_DEBUG, &val1);
-
-	if (val || !val1) {
-		KGSL_DRV_ERR(device,
-		"Invalid state after preemption CP_PREEMPT: %08x, CP_PREEMPT_DEBUG: %08x\n",
-		val, val1);
-		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-		adreno_dispatcher_schedule(device);
-		return;
-	}
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_BASE, &rbbase);
-	if (rbbase != adreno_dev->next_rb->buffer_desc.gpuaddr) {
-		KGSL_DRV_ERR(device,
-		"RBBASE incorrect after preemption, expected %x got %016llx\b",
-		rbbase,
-		adreno_dev->next_rb->buffer_desc.gpuaddr);
-		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-		adreno_dispatcher_schedule(device);
-		return;
-	}
-
-	a4xx_preemption_save(adreno_dev, adreno_dev->cur_rb);
-
-	/* new RB is the current RB */
-	trace_adreno_hw_preempt_comp_to_clear(adreno_dev->next_rb,
-			adreno_dev->cur_rb,
-			adreno_get_rptr(adreno_dev->next_rb),
-			adreno_get_rptr(adreno_dev->cur_rb));
-
-	adreno_dev->prev_rb = adreno_dev->cur_rb;
-	adreno_dev->cur_rb = adreno_dev->next_rb;
-	adreno_dev->cur_rb->preempted_midway = 0;
-	adreno_dev->cur_rb->wptr_preempt_end = 0xFFFFFFFF;
-	adreno_dev->next_rb = NULL;
-
-	if (adreno_disp_preempt_fair_sched) {
-		/* starved rb is now scheduled so unhalt dispatcher */
-		if (ADRENO_DISPATCHER_RB_STARVE_TIMER_ELAPSED ==
-			adreno_dev->cur_rb->starve_timer_state)
-			adreno_put_gpu_halt(adreno_dev);
-		adreno_dev->cur_rb->starve_timer_state =
-				ADRENO_DISPATCHER_RB_STARVE_TIMER_SCHEDULED;
-		adreno_dev->cur_rb->sched_timer = jiffies;
-		/*
-		 * If the outgoing RB is has commands then set the
-		 * busy time for it
-		 */
-		if (!adreno_rb_empty(adreno_dev->prev_rb)) {
-			adreno_dev->prev_rb->starve_timer_state =
-				ADRENO_DISPATCHER_RB_STARVE_TIMER_INIT;
-			adreno_dev->prev_rb->sched_timer = jiffies;
-		} else {
-			adreno_dev->prev_rb->starve_timer_state =
-				ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT;
-		}
-	}
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-
-	prevrptr = adreno_get_rptr(adreno_dev->prev_rb);
-
-	if (adreno_compare_prio_level(adreno_dev->prev_rb->id,
-				adreno_dev->cur_rb->id) < 0) {
-		if (adreno_dev->prev_rb->wptr_preempt_end != prevrptr)
-			adreno_dev->prev_rb->preempted_midway = 1;
-	}
-
-	/* submit wptr if required for new rb */
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
-	if (adreno_dev->cur_rb->wptr != wptr) {
-		kgsl_pwrscale_busy(device);
-		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR,
-					adreno_dev->cur_rb->wptr);
-	}
-	/* clear preemption register */
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_PREEMPT_DEBUG, 0);
-}
-
-void a4xx_preemption_schedule(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return;
-
-	mutex_lock(&device->mutex);
-
-	switch (atomic_read(&adreno_dev->preempt.state)) {
-	case ADRENO_PREEMPT_NONE:
-		a4xx_preempt_clear_state(adreno_dev);
-		break;
-	case ADRENO_PREEMPT_TRIGGERED:
-		a4xx_preempt_trig_state(adreno_dev);
-		/*
-		 * if we transitioned to next state then fall-through
-		 * processing to next state
-		 */
-		if (!adreno_in_preempt_state(adreno_dev,
-			ADRENO_PREEMPT_COMPLETE))
-			break;
-	case ADRENO_PREEMPT_COMPLETE:
-		a4xx_preempt_complete_state(adreno_dev);
-		break;
-	default:
-		break;
-	}
-
-	mutex_unlock(&device->mutex);
-}
-
-int a4xx_preemption_init(struct adreno_device *adreno_dev)
-{
-	setup_timer(&adreno_dev->preempt.timer, a4xx_preemption_timer,
-		(unsigned long) adreno_dev);
-
-	return 0;
-}
diff --git a/drivers/gpu/msm/adreno_a4xx_snapshot.c b/drivers/gpu/msm/adreno_a4xx_snapshot.c
index b6737d4359a9..b07e970aae32 100644
--- a/drivers/gpu/msm/adreno_a4xx_snapshot.c
+++ b/drivers/gpu/msm/adreno_a4xx_snapshot.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -168,15 +168,15 @@ static const unsigned int a4xx_vbif_ver_20050000_registers[] = {
 
 static const struct adreno_vbif_snapshot_registers
 					a4xx_vbif_snapshot_registers[] = {
-	{ 0x20000000, 0xFFFF0000, a4xx_vbif_ver_20000000_registers,
+	{ 0x20000000, a4xx_vbif_ver_20000000_registers,
 				ARRAY_SIZE(a4xx_vbif_ver_20000000_registers)/2},
-	{ 0x20020000, 0xFFFF0000, a4xx_vbif_ver_20020000_registers,
+	{ 0x20020000, a4xx_vbif_ver_20020000_registers,
 				ARRAY_SIZE(a4xx_vbif_ver_20020000_registers)/2},
-	{ 0x20050000, 0xFFFF0000, a4xx_vbif_ver_20050000_registers,
+	{ 0x20050000, a4xx_vbif_ver_20050000_registers,
 				ARRAY_SIZE(a4xx_vbif_ver_20050000_registers)/2},
-	{ 0x20070000, 0xFFFF0000, a4xx_vbif_ver_20020000_registers,
+	{ 0x20070000, a4xx_vbif_ver_20020000_registers,
 				ARRAY_SIZE(a4xx_vbif_ver_20020000_registers)/2},
-	{ 0x20090000, 0xFFFF0000, a4xx_vbif_ver_20050000_registers,
+	{ 0x20090000, a4xx_vbif_ver_20050000_registers,
 				ARRAY_SIZE(a4xx_vbif_ver_20050000_registers)/2},
 };
 
@@ -319,7 +319,7 @@ static void a4xx_rbbm_debug_bus_read(struct kgsl_device *device,
  * a4xx_snapshot_vbif_debugbus() - Dump the VBIF debug data
  * @device: Device pointer for which the debug data is dumped
  * @buf: Pointer to the memory where the data is dumped
- * @remain: Amount of bytes remaining in snapshot
+ * @remain: Amout of bytes remaining in snapshot
  * @priv: Pointer to debug bus block
  *
  * Returns the number of bytes dumped
@@ -534,6 +534,9 @@ void a4xx_snapshot(struct adreno_device *adreno_dev,
 	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL, 0);
 	kgsl_regwrite(device, A4XX_RBBM_CLOCK_CTL2, 0);
 
+	/* Turn on MMU clocks since we read MMU registers */
+	kgsl_mmu_enable_clk(&device->mmu);
+
 	/* Master set of (non debug) registers */
 
 	SNAPSHOT_REGISTERS(device, snapshot, a4xx_registers);
@@ -551,6 +554,8 @@ void a4xx_snapshot(struct adreno_device *adreno_dev,
 		a4xx_vbif_snapshot_registers,
 		ARRAY_SIZE(a4xx_vbif_snapshot_registers));
 
+	kgsl_mmu_disable_clk(&device->mmu);
+
 	kgsl_snapshot_indexed_registers(device, snapshot,
 		A4XX_CP_STATE_DEBUG_INDEX, A4XX_CP_STATE_DEBUG_DATA,
 		0, snap_data->sect_sizes->cp_pfp);
diff --git a/drivers/gpu/msm/adreno_a5xx.c b/drivers/gpu/msm/adreno_a5xx.c
index 68a7c2a2f987..c67de761d9c8 100644
--- a/drivers/gpu/msm/adreno_a5xx.c
+++ b/drivers/gpu/msm/adreno_a5xx.c
@@ -14,7 +14,6 @@
 #include <soc/qcom/subsystem_restart.h>
 #include <soc/qcom/scm.h>
 #include <linux/pm_opp.h>
-#include <linux/clk/qcom.h>
 
 #include "adreno.h"
 #include "a5xx_reg.h"
@@ -27,9 +26,9 @@
 #include "kgsl_sharedmem.h"
 #include "kgsl_log.h"
 #include "kgsl.h"
-#include "kgsl_trace.h"
 #include "adreno_a5xx_packets.h"
 
+static int zap_ucode_loaded;
 static int critical_packet_constructed;
 
 static struct kgsl_memdesc crit_pkts;
@@ -39,6 +38,9 @@ static struct kgsl_memdesc crit_pkts_refbuf1;
 static struct kgsl_memdesc crit_pkts_refbuf2;
 static struct kgsl_memdesc crit_pkts_refbuf3;
 
+void a5xx_snapshot(struct adreno_device *adreno_dev,
+		struct kgsl_snapshot *snapshot);
+
 static const struct adreno_vbif_data a530_vbif[] = {
 	{A5XX_VBIF_ROUND_ROBIN_QOS_ARB, 0x00000003},
 	{0, 0},
@@ -53,20 +55,24 @@ static const struct adreno_vbif_data a540_vbif[] = {
 static const struct adreno_vbif_platform a5xx_vbif_platforms[] = {
 	{ adreno_is_a540, a540_vbif },
 	{ adreno_is_a530, a530_vbif },
-	{ adreno_is_a512, a540_vbif },
 	{ adreno_is_a510, a530_vbif },
-	{ adreno_is_a508, a530_vbif },
-	{ adreno_is_a504, a530_vbif },
 	{ adreno_is_a505, a530_vbif },
 	{ adreno_is_a506, a530_vbif },
 };
 
+#define PREEMPT_RECORD(_field) \
+		offsetof(struct a5xx_cp_preemption_record, _field)
+
+#define PREEMPT_SMMU_RECORD(_field) \
+		offsetof(struct a5xx_cp_smmu_info, _field)
+
 static void a5xx_irq_storm_worker(struct work_struct *work);
 static int _read_fw2_block_header(uint32_t *header, uint32_t remain,
 	uint32_t id, uint32_t major, uint32_t minor);
 static void a5xx_gpmu_reset(struct work_struct *work);
 static int a5xx_gpmu_init(struct adreno_device *adreno_dev);
 
+
 /**
  * Number of times to check if the regulator enabled before
  * giving up and returning failure.
@@ -77,7 +83,7 @@ static int a5xx_gpmu_init(struct adreno_device *adreno_dev);
  * Number of times to check if the GPMU firmware is initialized before
  * giving up and returning failure.
  */
-#define GPMU_FW_INIT_RETRY 5000
+#define GPMU_FW_INIT_RETRY 100
 
 #define A530_QFPROM_RAW_PTE_ROW0_MSB 0x134
 #define A530_QFPROM_RAW_PTE_ROW2_MSB 0x144
@@ -122,36 +128,12 @@ static void a530_efuse_speed_bin(struct adreno_device *adreno_dev)
 	adreno_dev->speed_bin = (val & speed_bin[1]) >> speed_bin[2];
 }
 
-static void a5xx_efuse_speed_bin(struct adreno_device *adreno_dev)
-{
-	unsigned int val;
-	unsigned int speed_bin[3];
-	struct kgsl_device *device = &adreno_dev->dev;
-
-	if (of_get_property(device->pdev->dev.of_node,
-			"qcom,gpu-speed-bin-vectors", NULL)) {
-		adreno_efuse_speed_bin_array(adreno_dev);
-		return;
-	}
-
-	if (!of_property_read_u32_array(device->pdev->dev.of_node,
-			"qcom,gpu-speed-bin", speed_bin, 3)) {
-		adreno_efuse_read_u32(adreno_dev, speed_bin[0], &val);
-		adreno_dev->speed_bin = (val & speed_bin[1]) >> speed_bin[2];
-		return;
-	}
-}
-
 static const struct {
 	int (*check)(struct adreno_device *adreno_dev);
 	void (*func)(struct adreno_device *adreno_dev);
 } a5xx_efuse_funcs[] = {
 	{ adreno_is_a530, a530_efuse_leakage },
 	{ adreno_is_a530, a530_efuse_speed_bin },
-	{ adreno_is_a504, a5xx_efuse_speed_bin },
-	{ adreno_is_a505, a5xx_efuse_speed_bin },
-	{ adreno_is_a512, a530_efuse_speed_bin },
-	{ adreno_is_a508, a530_efuse_speed_bin },
 };
 
 static void a5xx_check_features(struct adreno_device *adreno_dev)
@@ -169,12 +151,278 @@ static void a5xx_check_features(struct adreno_device *adreno_dev)
 	adreno_efuse_unmap(adreno_dev);
 }
 
+/*
+ * a5xx_preemption_start() - Setup state to start preemption
+ */
+static void a5xx_preemption_start(struct adreno_device *adreno_dev,
+		struct adreno_ringbuffer *rb)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
+	uint64_t ttbr0;
+	uint32_t contextidr;
+	struct kgsl_pagetable *pt;
+	bool switch_default_pt = true;
+
+	kgsl_sharedmem_writel(device, &rb->preemption_desc,
+		PREEMPT_RECORD(wptr), rb->wptr);
+	kgsl_regwrite(device, A5XX_CP_CONTEXT_SWITCH_RESTORE_ADDR_LO,
+		lower_32_bits(rb->preemption_desc.gpuaddr));
+	kgsl_regwrite(device, A5XX_CP_CONTEXT_SWITCH_RESTORE_ADDR_HI,
+		upper_32_bits(rb->preemption_desc.gpuaddr));
+	kgsl_sharedmem_readq(&rb->pagetable_desc, &ttbr0,
+		offsetof(struct adreno_ringbuffer_pagetable_info, ttbr0));
+	kgsl_sharedmem_readl(&rb->pagetable_desc, &contextidr,
+		offsetof(struct adreno_ringbuffer_pagetable_info, contextidr));
+
+	spin_lock(&kgsl_driver.ptlock);
+	list_for_each_entry(pt, &kgsl_driver.pagetable_list, list) {
+		if (kgsl_mmu_pagetable_get_ttbr0(pt) == ttbr0) {
+			switch_default_pt = false;
+			break;
+		}
+	}
+	spin_unlock(&kgsl_driver.ptlock);
+
+	if (switch_default_pt) {
+		ttbr0 = kgsl_mmu_pagetable_get_ttbr0(
+				device->mmu.defaultpagetable);
+		contextidr = kgsl_mmu_pagetable_get_contextidr(
+				device->mmu.defaultpagetable);
+	}
+
+	kgsl_sharedmem_writeq(device, &iommu->smmu_info,
+		offsetof(struct a5xx_cp_smmu_info, ttbr0), ttbr0);
+	kgsl_sharedmem_writel(device, &iommu->smmu_info,
+		offsetof(struct a5xx_cp_smmu_info, context_idr), contextidr);
+}
+
+/*
+ * a5xx_preemption_save() - Save the state after preemption is done
+ */
+static void a5xx_preemption_save(struct adreno_device *adreno_dev,
+		struct adreno_ringbuffer *rb)
+{
+	/* save the rptr from ctxrecord here */
+	kgsl_sharedmem_readl(&rb->preemption_desc, &rb->rptr,
+		PREEMPT_RECORD(rptr));
+}
+
+#ifdef CONFIG_QCOM_KGSL_IOMMU
+static int a5xx_preemption_iommu_init(struct adreno_device *adreno_dev)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
+
+	/* Allocate mem for storing preemption smmu record */
+	return kgsl_allocate_global(device, &iommu->smmu_info, PAGE_SIZE,
+		KGSL_MEMFLAGS_GPUREADONLY, KGSL_MEMDESC_PRIVILEGED);
+}
+#else
+static int a5xx_preemption_iommu_init(struct adreno_device *adreno_dev)
+{
+	return -ENODEV;
+}
+#endif
+
+static int a5xx_preemption_init(struct adreno_device *adreno_dev)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct adreno_ringbuffer *rb;
+	int ret;
+	unsigned int i;
+	uint64_t addr;
+
+	/* We are dependent on IOMMU to make preemption go on the CP side */
+	if (kgsl_mmu_get_mmutype(device) != KGSL_MMU_TYPE_IOMMU)
+		return -ENODEV;
+
+	/* Allocate mem for storing preemption counters */
+	ret = kgsl_allocate_global(device, &adreno_dev->preemption_counters,
+		adreno_dev->num_ringbuffers *
+		A5XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE, 0, 0);
+	if (ret)
+		return ret;
+
+	addr = adreno_dev->preemption_counters.gpuaddr;
+
+	/* Allocate mem for storing preemption switch record */
+	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
+		ret = kgsl_allocate_global(device,
+			&rb->preemption_desc, A5XX_CP_CTXRECORD_SIZE_IN_BYTES,
+			0, KGSL_MEMDESC_PRIVILEGED);
+		if (ret)
+			return ret;
+
+		/* Initialize the context switch record here */
+		kgsl_sharedmem_writel(device, &rb->preemption_desc,
+			PREEMPT_RECORD(magic), A5XX_CP_CTXRECORD_MAGIC_REF);
+		kgsl_sharedmem_writel(device, &rb->preemption_desc,
+			PREEMPT_RECORD(info), 0);
+		kgsl_sharedmem_writel(device, &rb->preemption_desc,
+			PREEMPT_RECORD(data), 0);
+		kgsl_sharedmem_writel(device, &rb->preemption_desc,
+			PREEMPT_RECORD(cntl), 0x0800000C);
+		kgsl_sharedmem_writel(device, &rb->preemption_desc,
+			PREEMPT_RECORD(rptr), 0);
+		kgsl_sharedmem_writel(device, &rb->preemption_desc,
+			PREEMPT_RECORD(wptr), 0);
+		kgsl_sharedmem_writeq(device, &rb->preemption_desc,
+			PREEMPT_RECORD(rbase),
+			adreno_dev->ringbuffers[i].buffer_desc.gpuaddr);
+		kgsl_sharedmem_writeq(device, &rb->preemption_desc,
+			PREEMPT_RECORD(counter), addr);
+
+		addr += A5XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE;
+	}
+
+	return a5xx_preemption_iommu_init(adreno_dev);
+}
+
+/*
+ * a5xx_preemption_token() - Preempt token on a5xx
+ * PM4 commands for preempt token on a5xx. These commands are
+ * submitted to ringbuffer to trigger preemption.
+ */
+static int a5xx_preemption_token(struct adreno_device *adreno_dev,
+			struct adreno_ringbuffer *rb, unsigned int *cmds,
+			uint64_t gpuaddr)
+{
+	unsigned int *cmds_orig = cmds;
+
+	/* Enable yield in RB only */
+	*cmds++ = cp_type7_packet(CP_YIELD_ENABLE, 1);
+	*cmds++ = 1;
+
+	*cmds++ = cp_type7_packet(CP_CONTEXT_SWITCH_YIELD, 4);
+	cmds += cp_gpuaddr(adreno_dev, cmds, gpuaddr);
+	*cmds++ = 1;
+	/* generate interrupt on preemption completion */
+	*cmds++ = 1;
+
+	return cmds - cmds_orig;
+
+}
+
+/*
+ * a5xx_preemption_pre_ibsubmit() - Below PM4 commands are
+ * added at the beginning of every cmdbatch submission.
+ */
+static int a5xx_preemption_pre_ibsubmit(
+			struct adreno_device *adreno_dev,
+			struct adreno_ringbuffer *rb, unsigned int *cmds,
+			struct kgsl_context *context, uint64_t cond_addr,
+			struct kgsl_memobj_node *ib)
+{
+	unsigned int *cmds_orig = cmds;
+	uint64_t gpuaddr = rb->preemption_desc.gpuaddr;
+	unsigned int preempt_style = 0;
+
+	if (context) {
+		/*
+		 * Preemption from secure to unsecure needs Zap shader to be
+		 * run to clear all secure content. CP does not know during
+		 * preemption if it is switching between secure and unsecure
+		 * contexts so restrict Secure contexts to be preempted at
+		 * ringbuffer level.
+		 */
+		if (context->flags & KGSL_CONTEXT_SECURE)
+			preempt_style = KGSL_CONTEXT_PREEMPT_STYLE_RINGBUFFER;
+		else
+			preempt_style = ADRENO_PREEMPT_STYLE(context->flags);
+	}
+
+	/*
+	 * CP_PREEMPT_ENABLE_GLOBAL(global preemption) can only be set by KMD
+	 * in ringbuffer.
+	 * 1) set global preemption to 0x0 to disable global preemption.
+	 *    Only RB level preemption is allowed in this mode
+	 * 2) Set global preemption to defer(0x2) for finegrain preemption.
+	 *    when global preemption is set to defer(0x2),
+	 *    CP_PREEMPT_ENABLE_LOCAL(local preemption) determines the
+	 *    preemption point. Local preemption
+	 *    can be enabled by both UMD(within IB) and KMD.
+	 */
+	*cmds++ = cp_type7_packet(CP_PREEMPT_ENABLE_GLOBAL, 1);
+	*cmds++ = ((preempt_style == KGSL_CONTEXT_PREEMPT_STYLE_FINEGRAIN)
+				? 2 : 0);
+
+	/* Turn CP protection OFF */
+	*cmds++ = cp_type7_packet(CP_SET_PROTECTED_MODE, 1);
+	*cmds++ = 0;
+
+	/*
+	 * CP during context switch will save context switch info to
+	 * a5xx_cp_preemption_record pointed by CONTEXT_SWITCH_SAVE_ADDR
+	 */
+	*cmds++ = cp_type4_packet(A5XX_CP_CONTEXT_SWITCH_SAVE_ADDR_LO, 1);
+	*cmds++ = lower_32_bits(gpuaddr);
+	*cmds++ = cp_type4_packet(A5XX_CP_CONTEXT_SWITCH_SAVE_ADDR_HI, 1);
+	*cmds++ = upper_32_bits(gpuaddr);
+
+	/* Turn CP protection ON */
+	*cmds++ = cp_type7_packet(CP_SET_PROTECTED_MODE, 1);
+	*cmds++ = 1;
+
+	/*
+	 * Enable local preemption for finegrain preemption in case of
+	 * a misbehaving IB
+	 */
+	if (preempt_style == KGSL_CONTEXT_PREEMPT_STYLE_FINEGRAIN) {
+		*cmds++ = cp_type7_packet(CP_PREEMPT_ENABLE_LOCAL, 1);
+		*cmds++ = 1;
+	} else {
+		*cmds++ = cp_type7_packet(CP_PREEMPT_ENABLE_LOCAL, 1);
+		*cmds++ = 0;
+	}
+
+	/* Enable CP_CONTEXT_SWITCH_YIELD packets in the IB2s */
+	*cmds++ = cp_type7_packet(CP_YIELD_ENABLE, 1);
+	*cmds++ = 2;
+
+	return cmds - cmds_orig;
+}
+
+/*
+ * a5xx_preemption_post_ibsubmit() - Below PM4 commands are
+ * added after every cmdbatch submission.
+ */
+static int a5xx_preemption_post_ibsubmit(
+			struct adreno_device *adreno_dev,
+			struct adreno_ringbuffer *rb, unsigned int *cmds,
+			struct kgsl_context *context)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	unsigned int *cmds_orig = cmds;
+	unsigned int ctx_id = context ? context->id : 0;
+
+	/*
+	 * SRM -- set render mode (ex binning, direct render etc)
+	 * SRM is set by UMD usually at start of IB to tell CP the type of
+	 * preemption.
+	 * KMD needs to set SRM to NULL to indicate CP that rendering is
+	 * done by IB.
+	 */
+	*cmds++ = cp_type7_packet(CP_SET_RENDER_MODE, 5);
+	*cmds++ = 0;
+	*cmds++ = 0;
+	*cmds++ = 0;
+	*cmds++ = 0;
+	*cmds++ = 0;
+
+	cmds += a5xx_preemption_token(adreno_dev, rb, cmds,
+				device->memstore.gpuaddr +
+				KGSL_MEMSTORE_OFFSET(ctx_id, preempted));
+
+	return cmds - cmds_orig;
+}
+
 static void a5xx_platform_setup(struct adreno_device *adreno_dev)
 {
 	uint64_t addr;
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 
-	if (adreno_is_a504_to_a506(adreno_dev) || adreno_is_a508(adreno_dev)) {
+	if (adreno_is_a505_or_a506(adreno_dev)) {
 		gpudev->snapshot_data->sect_sizes->cp_meq = 32;
 		gpudev->snapshot_data->sect_sizes->cp_merciu = 1024;
 		gpudev->snapshot_data->sect_sizes->roq = 256;
@@ -190,9 +438,6 @@ static void a5xx_platform_setup(struct adreno_device *adreno_dev)
 		/* A510 has 3 XIN ports in VBIF */
 		gpudev->vbif_xin_halt_ctrl0_mask =
 				A510_VBIF_XIN_HALT_CTRL0_MASK;
-	} else if (adreno_is_a540(adreno_dev) ||
-		adreno_is_a512(adreno_dev)) {
-		gpudev->snapshot_data->sect_sizes->cp_merciu = 1024;
 	}
 
 	/* Calculate SP local and private mem addresses */
@@ -215,8 +460,6 @@ static void a5xx_critical_packet_destroy(struct adreno_device *adreno_dev)
 	kgsl_free_global(&adreno_dev->dev, &crit_pkts_refbuf2);
 	kgsl_free_global(&adreno_dev->dev, &crit_pkts_refbuf3);
 
-	kgsl_iommu_unmap_global_secure_pt_entry(KGSL_DEVICE(adreno_dev),
-			&crit_pkts_refbuf0);
 	kgsl_sharedmem_free(&crit_pkts_refbuf0);
 
 }
@@ -245,8 +488,7 @@ static int a5xx_critical_packet_construct(struct adreno_device *adreno_dev)
 
 	ret = kgsl_allocate_global(&adreno_dev->dev,
 					&crit_pkts, PAGE_SIZE,
-					KGSL_MEMFLAGS_GPUREADONLY,
-					0, "crit_pkts");
+					KGSL_MEMFLAGS_GPUREADONLY, 0);
 	if (ret)
 		return ret;
 
@@ -255,26 +497,24 @@ static int a5xx_critical_packet_construct(struct adreno_device *adreno_dev)
 	if (ret)
 		return ret;
 
-	ret = kgsl_iommu_map_global_secure_pt_entry(&adreno_dev->dev,
+	kgsl_add_global_secure_entry(&adreno_dev->dev,
 					&crit_pkts_refbuf0);
-	if (ret)
-		return ret;
 
 	ret = kgsl_allocate_global(&adreno_dev->dev,
 					&crit_pkts_refbuf1,
-					PAGE_SIZE, 0, 0, "crit_pkts_refbuf1");
+					PAGE_SIZE, 0, 0);
 	if (ret)
 		return ret;
 
 	ret = kgsl_allocate_global(&adreno_dev->dev,
 					&crit_pkts_refbuf2,
-					PAGE_SIZE, 0, 0, "crit_pkts_refbuf2");
+					PAGE_SIZE, 0, 0);
 	if (ret)
 		return ret;
 
 	ret = kgsl_allocate_global(&adreno_dev->dev,
 					&crit_pkts_refbuf3,
-					PAGE_SIZE, 0, 0, "crit_pkts_refbuf3");
+					PAGE_SIZE, 0, 0);
 	if (ret)
 		return ret;
 
@@ -319,13 +559,8 @@ static void a5xx_init(struct adreno_device *adreno_dev)
 
 	INIT_WORK(&adreno_dev->irq_storm_work, a5xx_irq_storm_worker);
 
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_CRITICAL_PACKETS)) {
-		int ret;
-
-		ret = a5xx_critical_packet_construct(adreno_dev);
-		if (ret)
-			a5xx_critical_packet_destroy(adreno_dev);
-	}
+	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_CRITICAL_PACKETS))
+		a5xx_critical_packet_construct(adreno_dev);
 
 	a5xx_crashdump_init(adreno_dev);
 }
@@ -386,7 +621,7 @@ static void a5xx_protect_init(struct adreno_device *adreno_dev)
 	iommu_regs = kgsl_mmu_get_prot_regs(&device->mmu);
 	if (iommu_regs)
 		adreno_set_protected_registers(adreno_dev, &index,
-				iommu_regs->base, ilog2(iommu_regs->range));
+				iommu_regs->base, iommu_regs->range);
 }
 
 /*
@@ -435,43 +670,6 @@ static int _poll_gdsc_status(struct adreno_device *adreno_dev,
 	return 0;
 }
 
-static void a5xx_restore_isense_regs(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int reg, i, ramp = GPMU_ISENSE_SAVE;
-	static unsigned int isense_regs[6] = {0xFFFF}, isense_reg_addr[] = {
-		A5XX_GPU_CS_DECIMAL_ALIGN,
-		A5XX_GPU_CS_SENSOR_PARAM_CORE_1,
-		A5XX_GPU_CS_SENSOR_PARAM_CORE_2,
-		A5XX_GPU_CS_SW_OV_FUSE_EN,
-		A5XX_GPU_CS_ENDPOINT_CALIBRATION_DONE,
-		A5XX_GPMU_TEMP_SENSOR_CONFIG};
-
-	if (!adreno_is_a540(adreno_dev))
-		return;
-
-	/* read signature */
-	kgsl_regread(device, ramp++, &reg);
-
-	if (reg == 0xBABEFACE) {
-		/* store memory locations in buffer */
-		for (i = 0; i < ARRAY_SIZE(isense_regs); i++)
-			kgsl_regread(device, ramp + i, isense_regs + i);
-
-		/* clear signature */
-		kgsl_regwrite(device, GPMU_ISENSE_SAVE, 0x0);
-	}
-
-	/* if we never stored memory locations - do nothing */
-	if (isense_regs[0] == 0xFFFF)
-		return;
-
-	/* restore registers from memory */
-	for (i = 0; i < ARRAY_SIZE(isense_reg_addr); i++)
-		kgsl_regwrite(device, isense_reg_addr[i], isense_regs[i]);
-
-}
-
 /*
  * a5xx_regulator_enable() - Enable any necessary HW regulators
  * @adreno_dev: The adreno device pointer
@@ -483,15 +681,8 @@ static int a5xx_regulator_enable(struct adreno_device *adreno_dev)
 {
 	unsigned int ret;
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (!(adreno_is_a530(adreno_dev) || adreno_is_a540(adreno_dev))) {
-		/* Halt the sp_input_clk at HM level */
-		kgsl_regwrite(device, A5XX_RBBM_CLOCK_CNTL, 0x00000055);
-		a5xx_hwcg_set(adreno_dev, true);
-		/* Turn on sp_input_clk at HM level */
-		kgsl_regrmw(device, A5XX_RBBM_CLOCK_CNTL, 0xFF, 0);
+	if (!(adreno_is_a530(adreno_dev) || adreno_is_a540(adreno_dev)))
 		return 0;
-	}
 
 	/*
 	 * Turn on smaller power domain first to reduce voltage droop.
@@ -513,16 +704,6 @@ static int a5xx_regulator_enable(struct adreno_device *adreno_dev)
 		return ret;
 	}
 
-	/* Disable SP clock */
-	kgsl_regrmw(device, A5XX_GPMU_GPMU_SP_CLOCK_CONTROL,
-		CNTL_IP_CLK_ENABLE, 0);
-	/* Enable hardware clockgating */
-	a5xx_hwcg_set(adreno_dev, true);
-	/* Enable SP clock */
-	kgsl_regrmw(device, A5XX_GPMU_GPMU_SP_CLOCK_CONTROL,
-		CNTL_IP_CLK_ENABLE, 1);
-
-	a5xx_restore_isense_regs(adreno_dev);
 	return 0;
 }
 
@@ -539,9 +720,6 @@ static void a5xx_regulator_disable(struct adreno_device *adreno_dev)
 	unsigned int reg;
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
-	if (adreno_is_a512(adreno_dev) || adreno_is_a508(adreno_dev))
-		return;
-
 	/* If feature is not supported or not enabled */
 	if (!ADRENO_FEATURE(adreno_dev, ADRENO_SPTP_PC) ||
 		!test_bit(ADRENO_SPTP_PC_CTRL, &adreno_dev->pwrctrl_flag)) {
@@ -726,7 +904,7 @@ static int _load_gpmu_firmware(struct adreno_device *adreno_dev)
 	if (cmd_size > GPMU_INST_RAM_SIZE) {
 		KGSL_CORE_ERR(
 			"GPMU firmware block size is larger than RAM size\n");
-		goto err;
+		 goto err;
 	}
 
 	/* Everything is cool, so create some commands */
@@ -743,7 +921,6 @@ static int _gpmu_send_init_cmds(struct adreno_device *adreno_dev)
 	struct adreno_ringbuffer *rb = adreno_dev->cur_rb;
 	uint32_t *cmds;
 	uint32_t size = adreno_dev->gpmu_cmds_size;
-	int ret;
 
 	if (size == 0 || adreno_dev->gpmu_cmds == NULL)
 		return -EINVAL;
@@ -756,13 +933,7 @@ static int _gpmu_send_init_cmds(struct adreno_device *adreno_dev)
 
 	/* Copy to the RB the predefined fw sequence cmds */
 	memcpy(cmds, adreno_dev->gpmu_cmds, size << 2);
-
-	ret = adreno_ringbuffer_submit_spin(rb, NULL, 2000);
-	if (ret != 0)
-		adreno_spin_idle_debug(adreno_dev,
-				"gpmu initialization failed to idle\n");
-
-	return ret;
+	return adreno_ringbuffer_submit_spin(rb, NULL, 2000);
 }
 
 /*
@@ -782,8 +953,10 @@ static int a5xx_gpmu_start(struct adreno_device *adreno_dev)
 		return 0;
 
 	ret = _gpmu_send_init_cmds(adreno_dev);
-	if (ret)
+	if (ret) {
+		KGSL_CORE_ERR("Failed to program the GPMU: %d\n", ret);
 		return ret;
+	}
 
 	if (adreno_is_a530(adreno_dev)) {
 		/* GPMU clock gating setup */
@@ -852,17 +1025,14 @@ static const struct kgsl_hwcg_reg a50x_hwcg_regs[] = {
 	{A5XX_RBBM_CLOCK_DELAY_TP0, 0x11111111},
 	{A5XX_RBBM_CLOCK_DELAY2_TP0, 0x11111111},
 	{A5XX_RBBM_CLOCK_DELAY3_TP0, 0x00001111},
-	{A5XX_RBBM_CLOCK_CNTL2_UCHE, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL3_UCHE, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL4_UCHE, 0x00222222},
 	{A5XX_RBBM_CLOCK_CNTL_UCHE, 0x22222222},
-	{A5XX_RBBM_CLOCK_HYST_UCHE, 0x00FFFFF4},
+	{A5XX_RBBM_CLOCK_HYST_UCHE, 0x00444444},
 	{A5XX_RBBM_CLOCK_DELAY_UCHE, 0x00000002},
 	{A5XX_RBBM_CLOCK_CNTL_RB0, 0x22222222},
 	{A5XX_RBBM_CLOCK_CNTL2_RB0, 0x00222222},
 	{A5XX_RBBM_CLOCK_CNTL_CCU0, 0x00022220},
 	{A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},
-	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},
+	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00555555},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},
 	{A5XX_RBBM_CLOCK_HYST_RAC, 0x07444044},
 	{A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_0, 0x00000002},
@@ -919,7 +1089,7 @@ static const struct kgsl_hwcg_reg a510_hwcg_regs[] = {
 	{A5XX_RBBM_CLOCK_CNTL_CCU0, 0x00022220},
 	{A5XX_RBBM_CLOCK_CNTL_CCU1, 0x00022220},
 	{A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},
-	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},
+	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00555555},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU1, 0x04040404},
 	{A5XX_RBBM_CLOCK_HYST_RAC, 0x07444044},
@@ -1010,7 +1180,7 @@ static const struct kgsl_hwcg_reg a530_hwcg_regs[] = {
 	{A5XX_RBBM_CLOCK_CNTL_CCU2, 0x00022220},
 	{A5XX_RBBM_CLOCK_CNTL_CCU3, 0x00022220},
 	{A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},
-	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},
+	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00555555},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU1, 0x04040404},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU2, 0x04040404},
@@ -1106,7 +1276,7 @@ static const struct kgsl_hwcg_reg a540_hwcg_regs[] = {
 	{A5XX_RBBM_CLOCK_CNTL_CCU2, 0x00022220},
 	{A5XX_RBBM_CLOCK_CNTL_CCU3, 0x00022220},
 	{A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},
-	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},
+	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00555555},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU1, 0x04040404},
 	{A5XX_RBBM_CLOCK_HYST_RB_CCU2, 0x04040404},
@@ -1132,65 +1302,6 @@ static const struct kgsl_hwcg_reg a540_hwcg_regs[] = {
 	{A5XX_RBBM_CLOCK_HYST_GPMU, 0x00000004}
 };
 
-static const struct kgsl_hwcg_reg a512_hwcg_regs[] = {
-	{A5XX_RBBM_CLOCK_CNTL_SP0, 0x02222222},
-	{A5XX_RBBM_CLOCK_CNTL_SP1, 0x02222222},
-	{A5XX_RBBM_CLOCK_CNTL2_SP0, 0x02222220},
-	{A5XX_RBBM_CLOCK_CNTL2_SP1, 0x02222220},
-	{A5XX_RBBM_CLOCK_HYST_SP0, 0x0000F3CF},
-	{A5XX_RBBM_CLOCK_HYST_SP1, 0x0000F3CF},
-	{A5XX_RBBM_CLOCK_DELAY_SP0, 0x00000080},
-	{A5XX_RBBM_CLOCK_DELAY_SP1, 0x00000080},
-	{A5XX_RBBM_CLOCK_CNTL_TP0, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL_TP1, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL2_TP0, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL2_TP1, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL3_TP0, 0x00002222},
-	{A5XX_RBBM_CLOCK_CNTL3_TP1, 0x00002222},
-	{A5XX_RBBM_CLOCK_HYST_TP0, 0x77777777},
-	{A5XX_RBBM_CLOCK_HYST_TP1, 0x77777777},
-	{A5XX_RBBM_CLOCK_HYST2_TP0, 0x77777777},
-	{A5XX_RBBM_CLOCK_HYST2_TP1, 0x77777777},
-	{A5XX_RBBM_CLOCK_HYST3_TP0, 0x00007777},
-	{A5XX_RBBM_CLOCK_HYST3_TP1, 0x00007777},
-	{A5XX_RBBM_CLOCK_DELAY_TP0, 0x11111111},
-	{A5XX_RBBM_CLOCK_DELAY_TP1, 0x11111111},
-	{A5XX_RBBM_CLOCK_DELAY2_TP0, 0x11111111},
-	{A5XX_RBBM_CLOCK_DELAY2_TP1, 0x11111111},
-	{A5XX_RBBM_CLOCK_DELAY3_TP0, 0x00001111},
-	{A5XX_RBBM_CLOCK_DELAY3_TP1, 0x00001111},
-	{A5XX_RBBM_CLOCK_CNTL_UCHE, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL2_UCHE, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL3_UCHE, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL4_UCHE, 0x00222222},
-	{A5XX_RBBM_CLOCK_HYST_UCHE, 0x00444444},
-	{A5XX_RBBM_CLOCK_DELAY_UCHE, 0x00000002},
-	{A5XX_RBBM_CLOCK_CNTL_RB0, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL_RB1, 0x22222222},
-	{A5XX_RBBM_CLOCK_CNTL2_RB0, 0x00222222},
-	{A5XX_RBBM_CLOCK_CNTL2_RB1, 0x00222222},
-	{A5XX_RBBM_CLOCK_CNTL_CCU0, 0x00022220},
-	{A5XX_RBBM_CLOCK_CNTL_CCU1, 0x00022220},
-	{A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},
-	{A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},
-	{A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},
-	{A5XX_RBBM_CLOCK_HYST_RB_CCU1, 0x04040404},
-	{A5XX_RBBM_CLOCK_HYST_RAC, 0x07444044},
-	{A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_0, 0x00000002},
-	{A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_1, 0x00000002},
-	{A5XX_RBBM_CLOCK_DELAY_RAC, 0x00010011},
-	{A5XX_RBBM_CLOCK_CNTL_TSE_RAS_RBBM, 0x04222222},
-	{A5XX_RBBM_CLOCK_MODE_GPC, 0x02222222},
-	{A5XX_RBBM_CLOCK_MODE_VFD, 0x00002222},
-	{A5XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00000000},
-	{A5XX_RBBM_CLOCK_HYST_GPC, 0x04104004},
-	{A5XX_RBBM_CLOCK_HYST_VFD, 0x00000000},
-	{A5XX_RBBM_CLOCK_DELAY_HLSQ, 0x00000000},
-	{A5XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00004000},
-	{A5XX_RBBM_CLOCK_DELAY_GPC, 0x00000200},
-	{A5XX_RBBM_CLOCK_DELAY_VFD, 0x00002222},
-};
-
 static const struct {
 	int (*devfunc)(struct adreno_device *adreno_dev);
 	const struct kgsl_hwcg_reg *regs;
@@ -1198,12 +1309,9 @@ static const struct {
 } a5xx_hwcg_registers[] = {
 	{ adreno_is_a540, a540_hwcg_regs, ARRAY_SIZE(a540_hwcg_regs) },
 	{ adreno_is_a530, a530_hwcg_regs, ARRAY_SIZE(a530_hwcg_regs) },
-	{ adreno_is_a512, a512_hwcg_regs, ARRAY_SIZE(a512_hwcg_regs) },
 	{ adreno_is_a510, a510_hwcg_regs, ARRAY_SIZE(a510_hwcg_regs) },
-	{ adreno_is_a504, a50x_hwcg_regs, ARRAY_SIZE(a50x_hwcg_regs) },
 	{ adreno_is_a505, a50x_hwcg_regs, ARRAY_SIZE(a50x_hwcg_regs) },
 	{ adreno_is_a506, a50x_hwcg_regs, ARRAY_SIZE(a50x_hwcg_regs) },
-	{ adreno_is_a508, a50x_hwcg_regs, ARRAY_SIZE(a50x_hwcg_regs) },
 };
 
 void a5xx_hwcg_set(struct adreno_device *adreno_dev, bool on)
@@ -1366,6 +1474,7 @@ static void _load_regfile(struct adreno_device *adreno_dev)
 	KGSL_PWR_ERR(device,
 		"Register file failed to load sz=%d bsz=%llu header=%d\n",
 		fw_size, block_size, ret);
+	return;
 }
 
 static int _execute_reg_sequence(struct adreno_device *adreno_dev,
@@ -1376,13 +1485,13 @@ static int _execute_reg_sequence(struct adreno_device *adreno_dev,
 
 	/* todo double check the reg writes */
 	while ((cur - opcode) < length) {
-		if (cur[0] == 1 && (length - (cur - opcode) >= 4)) {
+		if (cur[0] == 1 && ((cur + 4) - opcode) <= length) {
 			/* Write a 32 bit value to a 64 bit reg */
 			reg = cur[2];
 			reg = (reg << 32) | cur[1];
 			kgsl_regwrite(KGSL_DEVICE(adreno_dev), reg, cur[3]);
 			cur += 4;
-		} else if (cur[0] == 2 && (length - (cur - opcode) >= 5)) {
+		} else if (cur[0] == 2 && ((cur + 5) - opcode) <= length) {
 			/* Write a 64 bit value to a 64 bit reg */
 			reg = cur[2];
 			reg = (reg << 32) | cur[1];
@@ -1390,7 +1499,7 @@ static int _execute_reg_sequence(struct adreno_device *adreno_dev,
 			val = (val << 32) | cur[3];
 			kgsl_regwrite(KGSL_DEVICE(adreno_dev), reg, val);
 			cur += 5;
-		} else if (cur[0] == 3 && (length - (cur - opcode) >= 2)) {
+		} else if (cur[0] == 3 && ((cur + 2) - opcode) <= length) {
 			/* Delay for X usec */
 			udelay(cur[1]);
 			cur += 2;
@@ -1523,42 +1632,230 @@ static void a530_lm_enable(struct adreno_device *adreno_dev)
 			adreno_is_a530v2(adreno_dev) ? 0x00060011 : 0x00000011);
 }
 
-static void a540_lm_init(struct adreno_device *adreno_dev)
+static int isense_cot(struct adreno_device *adreno_dev)
 {
+	unsigned int r, ret;
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	uint32_t agc_lm_config = AGC_BCL_DISABLED |
-		((ADRENO_CHIPID_PATCH(adreno_dev->chipid) & 0x3)
-		<< AGC_GPU_VERSION_SHIFT);
-	unsigned int r;
-
-	if (!test_bit(ADRENO_THROTTLING_CTRL, &adreno_dev->pwrctrl_flag))
-		agc_lm_config |= AGC_THROTTLE_DISABLE;
 
-	if (lm_on(adreno_dev)) {
-		agc_lm_config |=
-			AGC_LM_CONFIG_ENABLE_GPMU_ADAPTIVE |
-			AGC_LM_CONFIG_ISENSE_ENABLE;
 
-		kgsl_regread(device, A5XX_GPMU_TEMP_SENSOR_CONFIG, &r);
+	kgsl_regwrite(device, A5XX_GPU_CS_AMP_CALIBRATION_CONTROL1,
+		AMP_SW_TRIM_START);
 
-		if ((r & GPMU_ISENSE_STATUS) == GPMU_ISENSE_END_POINT_CAL_ERR) {
-			KGSL_CORE_ERR(
-				"GPMU: ISENSE end point calibration failure\n");
-			agc_lm_config |= AGC_LM_CONFIG_ENABLE_ERROR;
-		}
+	for (ret = 0; ret < AMP_CALIBRATION_TIMEOUT; ret++) {
+		kgsl_regread(device, A5XX_GPU_CS_SENSOR_GENERAL_STATUS, &r);
+		if (r & SS_AMPTRIM_DONE)
+			break;
+		udelay(10);
 	}
 
-	kgsl_regwrite(device, AGC_MSG_STATE, 0x80000001);
-	kgsl_regwrite(device, AGC_MSG_COMMAND, AGC_POWER_CONFIG_PRODUCTION_ID);
-	(void) _write_voltage_table(adreno_dev, AGC_MSG_PAYLOAD);
-	kgsl_regwrite(device, AGC_MSG_PAYLOAD + AGC_LM_CONFIG, agc_lm_config);
-	kgsl_regwrite(device, AGC_MSG_PAYLOAD + AGC_LEVEL_CONFIG,
-		(unsigned int) ~(GENMASK(LM_DCVS_LIMIT, 0) |
-				GENMASK(16+LM_DCVS_LIMIT, 16)));
+	if (ret == AMP_CALIBRATION_TIMEOUT)
+		return -ETIMEDOUT;
 
-	kgsl_regwrite(device, AGC_MSG_PAYLOAD_SIZE,
-		(AGC_LEVEL_CONFIG + 1) * sizeof(uint32_t));
-	kgsl_regwrite(device, AGC_INIT_MSG_MAGIC, AGC_INIT_MSG_VALUE);
+	if (adreno_is_a540v1(adreno_dev)) {
+		/* HM */
+		kgsl_regread(device, A5XX_GPU_CS_AMP_CALIBRATION_STATUS1_0, &r);
+		if (r & AMP_CALIBRATION_ERR)
+			return -EIO;
+	}
+	/* SPTP */
+	kgsl_regread(device, A5XX_GPU_CS_AMP_CALIBRATION_STATUS1_2, &r);
+	if (r & AMP_CALIBRATION_ERR)
+		return -EIO;
+	/* RAC */
+	kgsl_regread(device, A5XX_GPU_CS_AMP_CALIBRATION_STATUS1_4, &r);
+	if (r & AMP_CALIBRATION_ERR)
+		return -EIO;
+
+	return 0;
+}
+
+static int isense_enable(struct adreno_device *adreno_dev)
+{
+	unsigned int r;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	kgsl_regwrite(device, A5XX_GPU_CS_ENABLE_REG,
+		adreno_is_a540v1(adreno_dev) ? 7 : 6);
+	udelay(2);
+	kgsl_regread(device, A5XX_GPU_CS_SENSOR_GENERAL_STATUS, &r);
+	if ((r & CS_PWR_ON_STATUS) == 0) {
+		KGSL_CORE_ERR("GPMU: ISENSE enabling failure\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static void isense_disable(struct adreno_device *adreno_dev)
+{
+	unsigned int r;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	kgsl_regwrite(device, A5XX_GPU_CS_ENABLE_REG, 0);
+	udelay(1);
+	kgsl_regread(device, A5XX_GPU_CS_SENSOR_GENERAL_STATUS, &r);
+	if ((r & CS_PWR_ON_STATUS) != 0)
+		KGSL_CORE_ERR("GPMU: ISENSE disabling failure\n");
+}
+
+static bool isense_is_enabled(struct adreno_device *adreno_dev)
+{
+	unsigned int r;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	kgsl_regread(device, A5XX_GPU_CS_SENSOR_GENERAL_STATUS, &r);
+	return r & CS_PWR_ON_STATUS;
+}
+
+static bool llm_is_enabled(struct adreno_device *adreno_dev)
+{
+	unsigned int r;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	kgsl_regread(device, A5XX_GPMU_TEMP_SENSOR_CONFIG, &r);
+	return r & (GPMU_BCL_ENABLED | GPMU_LLM_ENABLED);
+}
+
+
+static void sleep_llm(struct adreno_device *adreno_dev)
+{
+	unsigned int r, retry;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	if (!llm_is_enabled(adreno_dev))
+		return;
+
+	kgsl_regread(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_CTRL, &r);
+
+	if ((r & STATE_OF_CHILD) == 0) {
+		/* If both children are on, sleep CHILD_O1 first */
+		kgsl_regrmw(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_CTRL,
+			STATE_OF_CHILD, STATE_OF_CHILD_01 | IDLE_FULL_LM_SLEEP);
+		/* Wait for IDLE_FULL_ACK before continuing */
+		for (retry = 0; retry < 5; retry++) {
+			udelay(1);
+			kgsl_regread(device,
+				A5XX_GPMU_GPMU_LLM_GLM_SLEEP_STATUS, &r);
+			if (r & IDLE_FULL_ACK)
+				break;
+		}
+
+		if (retry == 5)
+			KGSL_CORE_ERR("GPMU: LLM failed to idle: 0x%X\n", r);
+	}
+
+	/* Now turn off both children */
+	kgsl_regrmw(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_CTRL,
+		0, STATE_OF_CHILD | IDLE_FULL_LM_SLEEP);
+
+	/* wait for WAKEUP_ACK to be zero */
+	for (retry = 0; retry < 5; retry++) {
+		udelay(1);
+		kgsl_regread(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_STATUS, &r);
+		if ((r & WAKEUP_ACK) == 0)
+			break;
+	}
+
+	if (retry == 5)
+		KGSL_CORE_ERR("GPMU: LLM failed to sleep: 0x%X\n", r);
+}
+
+static void wake_llm(struct adreno_device *adreno_dev)
+{
+	unsigned int r, retry;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	if (!llm_is_enabled(adreno_dev))
+		return;
+
+	kgsl_regrmw(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_CTRL,
+		STATE_OF_CHILD, STATE_OF_CHILD_01);
+
+	if (((device->pwrctrl.num_pwrlevels - 2) -
+		device->pwrctrl.active_pwrlevel) <= LM_DCVS_LIMIT)
+		return;
+
+	udelay(1);
+
+	/* Turn on all children */
+	kgsl_regrmw(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_CTRL,
+		STATE_OF_CHILD | IDLE_FULL_LM_SLEEP, 0);
+
+	/* Wait for IDLE_FULL_ACK to be zero and WAKEUP_ACK to be set */
+	for (retry = 0; retry < 5; retry++) {
+		udelay(1);
+		kgsl_regread(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_STATUS, &r);
+		if ((r & (WAKEUP_ACK | IDLE_FULL_ACK)) == WAKEUP_ACK)
+			break;
+	}
+
+	if (retry == 5)
+		KGSL_CORE_ERR("GPMU: LLM failed to wake: 0x%X\n", r);
+}
+
+static bool llm_is_awake(struct adreno_device *adreno_dev)
+{
+	unsigned int r;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	kgsl_regread(device, A5XX_GPMU_GPMU_LLM_GLM_SLEEP_STATUS, &r);
+	return r & WAKEUP_ACK;
+}
+
+static void a540_lm_init(struct adreno_device *adreno_dev)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	uint32_t agc_lm_config =
+		AGC_LM_CONFIG_ENABLE_GPMU_ADAPTIVE | AGC_THROTTLE_SEL_DCS;
+	unsigned int r, i;
+
+	if (!lm_on(adreno_dev))
+		return;
+
+	agc_lm_config |= ((ADRENO_CHIPID_PATCH(adreno_dev->chipid) | 0x3)
+		<< AGC_GPU_VERSION_SHIFT);
+
+	kgsl_regread(device, A5XX_GPMU_TEMP_SENSOR_CONFIG, &r);
+	if (r & GPMU_BCL_ENABLED)
+		agc_lm_config |= AGC_BCL_ENABLED;
+
+	if (r & GPMU_LLM_ENABLED)
+		agc_lm_config |= AGC_LLM_ENABLED;
+
+	if ((r & GPMU_ISENSE_STATUS) == GPMU_ISENSE_END_POINT_CAL_ERR) {
+		KGSL_CORE_ERR("GPMU: ISENSE end point calibration failure\n");
+		agc_lm_config |= AGC_LM_CONFIG_ENABLE_ERROR;
+		goto start_agc;
+	}
+
+	if (!isense_enable(adreno_dev)) {
+		agc_lm_config |= AGC_LM_CONFIG_ENABLE_ERROR;
+		goto start_agc;
+	}
+
+	for (i = 0; i < AMP_CALIBRATION_RETRY_CNT; i++)
+		if (isense_cot(adreno_dev))
+			cpu_relax();
+		else
+			break;
+
+	if (i == AMP_CALIBRATION_RETRY_CNT) {
+		KGSL_CORE_ERR("GPMU: ISENSE cold trimming failure\n");
+		agc_lm_config |= AGC_LM_CONFIG_ENABLE_ERROR;
+	}
+
+start_agc:
+	kgsl_regwrite(device, AGC_MSG_STATE, 0x80000001);
+	kgsl_regwrite(device, AGC_MSG_COMMAND, AGC_POWER_CONFIG_PRODUCTION_ID);
+	(void) _write_voltage_table(adreno_dev, AGC_MSG_PAYLOAD);
+	kgsl_regwrite(device, AGC_MSG_PAYLOAD + AGC_LM_CONFIG, agc_lm_config);
+	kgsl_regwrite(device, AGC_MSG_PAYLOAD + AGC_LEVEL_CONFIG,
+		(unsigned int) (~GENMASK(LM_DCVS_LIMIT, 0) |
+				~GENMASK(16+LM_DCVS_LIMIT, 16)));
+
+	kgsl_regwrite(device, AGC_MSG_PAYLOAD_SIZE,
+		(AGC_LEVEL_CONFIG + 1) * sizeof(uint32_t));
+	kgsl_regwrite(device, AGC_INIT_MSG_MAGIC, AGC_INIT_MSG_VALUE);
 
 	kgsl_regwrite(device, A5XX_GPMU_GPMU_VOLTAGE,
 		(0x80000000 | device->pwrctrl.active_pwrlevel));
@@ -1566,8 +1863,7 @@ static void a540_lm_init(struct adreno_device *adreno_dev)
 	kgsl_regwrite(device, A5XX_GPMU_GPMU_PWR_THRESHOLD,
 		PWR_THRESHOLD_VALID | lm_limit(adreno_dev));
 
-	kgsl_regwrite(device, A5XX_GPMU_GPMU_VOLTAGE_INTR_EN_MASK,
-		VOLTAGE_INTR_EN);
+	wake_llm(adreno_dev);
 }
 
 
@@ -1588,7 +1884,7 @@ static void a5xx_lm_init(struct adreno_device *adreno_dev)
 static int gpmu_set_level(struct adreno_device *adreno_dev, unsigned int val)
 {
 	unsigned int reg;
-	int retry = 100;
+	int retry = 20;
 
 	kgsl_regwrite(KGSL_DEVICE(adreno_dev), A5XX_GPMU_GPMU_VOLTAGE, val);
 
@@ -1614,10 +1910,7 @@ static void a5xx_pwrlevel_change_settings(struct adreno_device *adreno_dev,
 {
 	int on = 0;
 
-	/*
-	 * On pre A540 HW only call through if PPD or LMx
-	 * is supported and enabled
-	 */
+	/* Only call through if PPD or LM is supported and enabled */
 	if (ADRENO_FEATURE(adreno_dev, ADRENO_PPD) &&
 		test_bit(ADRENO_PPD_CTRL, &adreno_dev->pwrctrl_flag))
 		on = ADRENO_PPD;
@@ -1626,12 +1919,6 @@ static void a5xx_pwrlevel_change_settings(struct adreno_device *adreno_dev,
 		test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag))
 		on = ADRENO_LM;
 
-	/* On 540+ HW call through unconditionally as long as GPMU is enabled */
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_GPMU)) {
-		if (adreno_is_a540(adreno_dev))
-			on = ADRENO_GPMU;
-	}
-
 	if (!on)
 		return;
 
@@ -1646,102 +1933,6 @@ static void a5xx_pwrlevel_change_settings(struct adreno_device *adreno_dev,
 	}
 }
 
-static void a5xx_clk_set_options(struct adreno_device *adreno_dev,
-	const char *name, struct clk *clk, bool on)
-{
-
-	if (!adreno_is_a540(adreno_dev) && !adreno_is_a512(adreno_dev) &&
-		!adreno_is_a508(adreno_dev))
-		return;
-
-	/* Handle clock settings for GFX PSCBCs */
-	if (on) {
-		if (!strcmp(name, "mem_iface_clk")) {
-			clk_set_flags(clk, CLKFLAG_NORETAIN_PERIPH);
-			clk_set_flags(clk, CLKFLAG_NORETAIN_MEM);
-		} else if (!strcmp(name, "core_clk")) {
-			clk_set_flags(clk, CLKFLAG_RETAIN_PERIPH);
-			clk_set_flags(clk, CLKFLAG_RETAIN_MEM);
-		}
-	} else {
-		if (!strcmp(name, "core_clk")) {
-			clk_set_flags(clk, CLKFLAG_NORETAIN_PERIPH);
-			clk_set_flags(clk, CLKFLAG_NORETAIN_MEM);
-		}
-	}
-}
-
-static void a5xx_count_throttles(struct adreno_device *adreno_dev,
-		uint64_t adj)
-{
-	if (adreno_is_a530(adreno_dev))
-		kgsl_regread(KGSL_DEVICE(adreno_dev),
-				adreno_dev->lm_threshold_count,
-				&adreno_dev->lm_threshold_cross);
-	else if (adreno_is_a540(adreno_dev))
-		adreno_dev->lm_threshold_cross = adj;
-}
-
-static int a5xx_enable_pwr_counters(struct adreno_device *adreno_dev,
-		unsigned int counter)
-{
-	/*
-	 * On 5XX we have to emulate the PWR counters which are physically
-	 * missing. Program countable 6 on RBBM_PERFCTR_RBBM_0 as a substitute
-	 * for PWR:1. Don't emulate PWR:0 as nobody uses it and we don't want
-	 * to take away too many of the generic RBBM counters.
-	 */
-
-	if (counter == 0)
-		return -EINVAL;
-
-	kgsl_regwrite(KGSL_DEVICE(adreno_dev), A5XX_RBBM_PERFCTR_RBBM_SEL_0, 6);
-
-	return 0;
-}
-
-/* FW driven idle 10% throttle */
-#define IDLE_10PCT 0
-/* number of cycles when clock is throttled by 50% (CRC) */
-#define CRC_50PCT  1
-/* number of cycles when clock is throttled by more than 50% (CRC) */
-#define CRC_MORE50PCT 2
-/* number of cycles when clock is throttle by less than 50% (CRC) */
-#define CRC_LESS50PCT 3
-
-static uint64_t a5xx_read_throttling_counters(struct adreno_device *adreno_dev)
-{
-	int i, adj;
-	uint32_t th[ADRENO_GPMU_THROTTLE_COUNTERS];
-	struct adreno_busy_data *busy = &adreno_dev->busy_data;
-
-	if (!adreno_is_a540(adreno_dev))
-		return 0;
-
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_GPMU))
-		return 0;
-
-	if (!test_bit(ADRENO_THROTTLING_CTRL, &adreno_dev->pwrctrl_flag))
-		return 0;
-
-	for (i = 0; i < ADRENO_GPMU_THROTTLE_COUNTERS; i++) {
-		if (!adreno_dev->gpmu_throttle_counters[i])
-			return 0;
-
-		th[i] = counter_delta(KGSL_DEVICE(adreno_dev),
-				adreno_dev->gpmu_throttle_counters[i],
-				&busy->throttle_cycles[i]);
-	}
-	adj = th[CRC_MORE50PCT] - th[IDLE_10PCT];
-	adj = th[CRC_50PCT] + th[CRC_LESS50PCT] / 3 + (adj < 0 ? 0 : adj) * 3;
-
-	trace_kgsl_clock_throttling(
-		th[IDLE_10PCT], th[CRC_50PCT],
-		th[CRC_MORE50PCT], th[CRC_LESS50PCT],
-		adj);
-	return adj;
-}
-
 static void a5xx_enable_64bit(struct adreno_device *adreno_dev)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
@@ -1760,6 +1951,16 @@ static void a5xx_enable_64bit(struct adreno_device *adreno_dev)
 	kgsl_regwrite(device, A5XX_RBBM_SECVID_TSB_ADDR_MODE_CNTL, 0x1);
 }
 
+static void a5xx_pre_reset(struct adreno_device *adreno_dev)
+{
+	if (adreno_is_a540(adreno_dev) && lm_on(adreno_dev)) {
+		if (llm_is_awake(adreno_dev))
+			sleep_llm(adreno_dev);
+		if (isense_is_enabled(adreno_dev))
+			isense_disable(adreno_dev);
+	}
+}
+
 /*
  * a5xx_gpmu_reset() - Re-enable GPMU based power features and restart GPMU
  * @work: Pointer to the work struct for gpmu reset
@@ -1794,47 +1995,17 @@ static void a5xx_gpmu_reset(struct work_struct *work)
 	if (a5xx_regulator_enable(adreno_dev))
 		goto out;
 
+	a5xx_pre_reset(adreno_dev);
+
 	/* Soft reset of the GPMU block */
 	kgsl_regwrite(device, A5XX_RBBM_BLOCK_SW_RESET_CMD, BIT(16));
 
-	/* GPU comes up in secured mode, make it unsecured by default */
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_CONTENT_PROTECTION))
-		kgsl_regwrite(device, A5XX_RBBM_SECVID_TRUST_CNTL, 0x0);
-
-
 	a5xx_gpmu_init(adreno_dev);
 
 out:
 	mutex_unlock(&device->mutex);
 }
 
-static void _setup_throttling_counters(struct adreno_device *adreno_dev)
-{
-	int i, ret;
-
-	if (!adreno_is_a540(adreno_dev))
-		return;
-
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_GPMU))
-		return;
-
-	for (i = 0; i < ADRENO_GPMU_THROTTLE_COUNTERS; i++) {
-		/* reset throttled cycles ivalue */
-		adreno_dev->busy_data.throttle_cycles[i] = 0;
-
-		if (adreno_dev->gpmu_throttle_counters[i] != 0)
-			continue;
-		ret = adreno_perfcounter_get(adreno_dev,
-			KGSL_PERFCOUNTER_GROUP_GPMU_PWR,
-			ADRENO_GPMU_THROTTLE_COUNTERS_BASE_REG + i,
-			&adreno_dev->gpmu_throttle_counters[i],
-			NULL,
-			PERFCOUNTER_FLAG_KERNEL);
-		WARN_ONCE(ret,  "Unable to get clock throttling counter %x\n",
-			ADRENO_GPMU_THROTTLE_COUNTERS_BASE_REG + i);
-	}
-}
-
 /*
  * a5xx_start() - Device start
  * @adreno_dev: Pointer to adreno device
@@ -1844,23 +2015,12 @@ static void _setup_throttling_counters(struct adreno_device *adreno_dev)
 static void a5xx_start(struct adreno_device *adreno_dev)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	unsigned int bit;
-	int ret;
-
-	if (adreno_is_a530(adreno_dev) && ADRENO_FEATURE(adreno_dev, ADRENO_LM)
-			&& adreno_dev->lm_threshold_count == 0) {
-
-		ret = adreno_perfcounter_get(adreno_dev,
-			KGSL_PERFCOUNTER_GROUP_GPMU_PWR, 27,
-			&adreno_dev->lm_threshold_count, NULL,
-			PERFCOUNTER_FLAG_KERNEL);
-		/* Ignore noncritical ret - used for debugfs */
-		if (ret)
-			adreno_dev->lm_threshold_count = 0;
-	}
-
-	_setup_throttling_counters(adreno_dev);
+	unsigned int i, bit;
+	struct adreno_ringbuffer *rb;
+	uint64_t def_ttbr0;
+	uint32_t contextidr;
 
 	adreno_vbif_start(adreno_dev, a5xx_vbif_platforms,
 			ARRAY_SIZE(a5xx_vbif_platforms));
@@ -1908,11 +2068,11 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 		set_bit(ADRENO_DEVICE_HANG_INTR, &adreno_dev->priv);
 		gpudev->irq->mask |= (1 << A5XX_INT_MISC_HANG_DETECT);
 		/*
-		 * Set hang detection threshold to 4 million cycles
-		 * (0x3FFFF*16)
+		 * Set hang detection threshold to 1 million cycles
+		 * (0xFFFF*16)
 		 */
 		kgsl_regwrite(device, A5XX_RBBM_INTERFACE_HANG_INT_CNTL,
-					  (1 << 30) | 0x3FFFF);
+					  (1 << 30) | 0xFFFF);
 	}
 
 
@@ -1947,7 +2107,7 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 	 * Below CP registers are 0x0 by default, program init
 	 * values based on a5xx flavor.
 	 */
-	if (adreno_is_a504_to_a506(adreno_dev) || adreno_is_a508(adreno_dev)) {
+	if (adreno_is_a505_or_a506(adreno_dev)) {
 		kgsl_regwrite(device, A5XX_CP_MEQ_THRESHOLDS, 0x20);
 		kgsl_regwrite(device, A5XX_CP_MERCIU_SIZE, 0x400);
 		kgsl_regwrite(device, A5XX_CP_ROQ_THRESHOLDS_2, 0x40000030);
@@ -1957,11 +2117,6 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 		kgsl_regwrite(device, A5XX_CP_MERCIU_SIZE, 0x20);
 		kgsl_regwrite(device, A5XX_CP_ROQ_THRESHOLDS_2, 0x40000030);
 		kgsl_regwrite(device, A5XX_CP_ROQ_THRESHOLDS_1, 0x20100D0A);
-	} else if (adreno_is_a540(adreno_dev) || adreno_is_a512(adreno_dev)) {
-		kgsl_regwrite(device, A5XX_CP_MEQ_THRESHOLDS, 0x40);
-		kgsl_regwrite(device, A5XX_CP_MERCIU_SIZE, 0x400);
-		kgsl_regwrite(device, A5XX_CP_ROQ_THRESHOLDS_2, 0x80000060);
-		kgsl_regwrite(device, A5XX_CP_ROQ_THRESHOLDS_1, 0x40201B16);
 	} else {
 		kgsl_regwrite(device, A5XX_CP_MEQ_THRESHOLDS, 0x40);
 		kgsl_regwrite(device, A5XX_CP_MERCIU_SIZE, 0x40);
@@ -1973,10 +2128,10 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 	 * vtxFifo and primFifo thresholds default values
 	 * are different.
 	 */
-	if (adreno_is_a504_to_a506(adreno_dev) || adreno_is_a508(adreno_dev))
+	if (adreno_is_a505_or_a506(adreno_dev))
 		kgsl_regwrite(device, A5XX_PC_DBG_ECO_CNTL,
 						(0x100 << 11 | 0x100 << 22));
-	else if (adreno_is_a510(adreno_dev) || adreno_is_a512(adreno_dev))
+	else if (adreno_is_a510(adreno_dev))
 		kgsl_regwrite(device, A5XX_PC_DBG_ECO_CNTL,
 						(0x200 << 11 | 0x200 << 22));
 	else
@@ -2005,18 +2160,6 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 			kgsl_regrmw(device, A5XX_PC_DBG_ECO_CNTL, 0, (1 << 8));
 	}
 
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_DISABLE_RB_DP2CLOCKGATING)) {
-		/*
-		 * Disable RB sampler datapath DP2 clock gating
-		 * optimization for 1-SP GPU's, by default it is enabled.
-		 */
-		kgsl_regrmw(device, A5XX_RB_DBG_ECO_CNT, 0, (1 << 9));
-	}
-	/*
-	 * Disable UCHE global filter as SP can invalidate/flush
-	 * independently
-	 */
-	kgsl_regwrite(device, A5XX_UCHE_MODE_CNTL, BIT(29));
 	/* Set the USE_RETENTION_FLOPS chicken bit */
 	kgsl_regwrite(device, A5XX_CP_CHICKEN_DBG, 0x02000000);
 
@@ -2024,10 +2167,10 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 	if (test_bit(ADRENO_DEVICE_ISDB_ENABLED, &adreno_dev->priv)) {
 		if (!kgsl_active_count_get(device)) {
 			/*
-			 * Disable ME/PFP split timeouts when the debugger is
-			 * enabled because the CP doesn't know when a shader is
-			 * in active debug
-			 */
+			* Disable ME/PFP split timeouts when the debugger is
+			* enabled because the CP doesn't know when a shader is
+			* in active debug
+			*/
 			kgsl_regwrite(device, A5XX_RBBM_AHB_CNTL1, 0x06FFFFFF);
 
 			/* Force the SP0/SP1 clocks on to enable ISDB */
@@ -2049,6 +2192,8 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 	} else {
 		/* if not in ISDB mode enable ME/PFP split notification */
 		kgsl_regwrite(device, A5XX_RBBM_AHB_CNTL1, 0xA6FFFFFF);
+		/* enable HWCG */
+		a5xx_hwcg_set(adreno_dev, true);
 	}
 
 	kgsl_regwrite(device, A5XX_RBBM_AHB_CNTL2, 0x0000003F);
@@ -2068,42 +2213,66 @@ static void a5xx_start(struct adreno_device *adreno_dev)
 
 			kgsl_regwrite(device, A5XX_TPL1_MODE_CNTL, bit << 7);
 			kgsl_regwrite(device, A5XX_RB_MODE_CNTL, bit << 1);
-			if (adreno_is_a540(adreno_dev) ||
-				adreno_is_a512(adreno_dev))
+
+			if (adreno_is_a540(adreno_dev))
 				kgsl_regwrite(device, A5XX_UCHE_DBG_ECO_CNTL_2,
 					bit);
 		}
 
 	}
 
-	/* Disable All flat shading optimization */
-	kgsl_regrmw(device, A5XX_VPC_DBG_ECO_CNTL, 0, 0x1 << 10);
+	if (adreno_is_preemption_enabled(adreno_dev)) {
+		struct kgsl_pagetable *pt = device->mmu.defaultpagetable;
 
-	/*
-	 * VPC corner case with local memory load kill leads to corrupt
-	 * internal state. Normal Disable does not work for all a5x chips.
-	 * So do the following setting to disable it.
-	 */
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_DISABLE_LMLOADKILL)) {
-		kgsl_regrmw(device, A5XX_VPC_DBG_ECO_CNTL, 0, 0x1 << 23);
-		kgsl_regrmw(device, A5XX_HLSQ_DBG_ECO_CNTL, 0x1 << 18, 0);
+		def_ttbr0 = kgsl_mmu_pagetable_get_ttbr0(pt);
+		contextidr = kgsl_mmu_pagetable_get_contextidr(pt);
+
+		/* Initialize the context switch record here */
+		kgsl_sharedmem_writel(device, &iommu->smmu_info,
+				PREEMPT_SMMU_RECORD(magic),
+				A5XX_CP_SMMU_INFO_MAGIC_REF);
+		kgsl_sharedmem_writeq(device, &iommu->smmu_info,
+				PREEMPT_SMMU_RECORD(ttbr0), def_ttbr0);
+		/*
+		 * The CP doesn't actually use the asid field, so
+		 * put a bad value into it until it is removed from
+		 * the preemption record.
+		 */
+		kgsl_sharedmem_writeq(device, &iommu->smmu_info,
+				PREEMPT_SMMU_RECORD(asid),
+				0xdecafbad);
+		kgsl_sharedmem_writeq(device, &iommu->smmu_info,
+				PREEMPT_SMMU_RECORD(context_idr),
+				contextidr);
+		adreno_writereg64(adreno_dev,
+				ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_LO,
+				ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_HI,
+				iommu->smmu_info.gpuaddr);
+
+		FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
+			kgsl_sharedmem_writel(device, &rb->preemption_desc,
+				PREEMPT_RECORD(rptr), 0);
+			kgsl_sharedmem_writel(device, &rb->preemption_desc,
+				PREEMPT_RECORD(wptr), 0);
+			kgsl_sharedmem_writeq(device, &rb->pagetable_desc,
+			  offsetof(struct adreno_ringbuffer_pagetable_info,
+			  ttbr0), def_ttbr0);
+		}
 	}
 
-	a5xx_preemption_start(adreno_dev);
 	a5xx_protect_init(adreno_dev);
 }
 
-/*
- * Follow the ME_INIT sequence with a preemption yield to allow the GPU to move
- * to a different ringbuffer, if desired
- */
 static int _preemption_init(
 			struct adreno_device *adreno_dev,
 			struct adreno_ringbuffer *rb, unsigned int *cmds,
 			struct kgsl_context *context)
 {
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	unsigned int *cmds_orig = cmds;
 	uint64_t gpuaddr = rb->preemption_desc.gpuaddr;
+	uint64_t gpuaddr_token = device->memstore.gpuaddr +
+				KGSL_MEMSTORE_OFFSET(0, preempted);
 
 	/* Turn CP protection OFF */
 	*cmds++ = cp_type7_packet(CP_SET_PROTECTED_MODE, 1);
@@ -2132,14 +2301,36 @@ static int _preemption_init(
 	*cmds++ = 1;
 
 	*cmds++ = cp_type7_packet(CP_CONTEXT_SWITCH_YIELD, 4);
-	cmds += cp_gpuaddr(adreno_dev, cmds, 0x0);
-	*cmds++ = 0;
+	cmds += cp_gpuaddr(adreno_dev, cmds, gpuaddr_token);
+	*cmds++ = 1;
 	/* generate interrupt on preemption completion */
 	*cmds++ = 1;
 
 	return cmds - cmds_orig;
 }
 
+/* Print some key registers if a spin-for-idle times out */
+static void spin_idle_debug(struct kgsl_device *device)
+{
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+	unsigned int rptr, wptr;
+	unsigned int status, status3, intstatus;
+	unsigned int hwfault;
+
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR, &rptr);
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
+
+	kgsl_regread(device, A5XX_RBBM_STATUS, &status);
+	kgsl_regread(device, A5XX_RBBM_STATUS3, &status3);
+	kgsl_regread(device, A5XX_RBBM_INT_0_STATUS, &intstatus);
+	kgsl_regread(device, A5XX_CP_HW_FAULT, &hwfault);
+
+	dev_err(device->dev,
+		" rb=%X/%X rbbm_status=%8.8X/%8.8X int_0_status=%8.8X\n",
+		rptr, wptr, status, status3, intstatus);
+	dev_err(device->dev, " hwfault=%8.8X\n", hwfault);
+}
+
 static int a5xx_post_start(struct adreno_device *adreno_dev)
 {
 	int ret;
@@ -2171,19 +2362,30 @@ static int a5xx_post_start(struct adreno_device *adreno_dev)
 	if (adreno_is_preemption_enabled(adreno_dev))
 		cmds += _preemption_init(adreno_dev, rb, cmds, NULL);
 
-	rb->_wptr = rb->_wptr - (42 - (cmds - start));
+	rb->wptr = rb->wptr - (42 - (cmds - start));
+
+	if (cmds == start)
+		return 0;
 
 	ret = adreno_ringbuffer_submit_spin(rb, NULL, 2000);
-	if (ret)
-		adreno_spin_idle_debug(adreno_dev,
-				"hw initialization failed to idle\n");
+	if (ret) {
+		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
-	return ret;
+		KGSL_DRV_ERR(device, "hw initialization failed to idle\n");
+		kgsl_device_snapshot(device, NULL);
+		return ret;
+	}
+	return 0;
 }
 
 static int a5xx_gpmu_init(struct adreno_device *adreno_dev)
 {
 	int ret;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+
+	/* GPU comes up in secured mode, make it unsecured by default */
+	if (!ADRENO_FEATURE(adreno_dev, ADRENO_CONTENT_PROTECTION))
+		kgsl_regwrite(device, A5XX_RBBM_SECVID_TRUST_CNTL, 0x0);
 
 	/* Set up LM before initializing the GPMU */
 	a5xx_lm_init(adreno_dev);
@@ -2200,77 +2402,44 @@ static int a5xx_gpmu_init(struct adreno_device *adreno_dev)
 	return 0;
 }
 
+
 /*
- * a5xx_microcode_load() - Load microcode
+ * a5xx_hw_init() - Initialize GPU HW using PM4 cmds
  * @adreno_dev: Pointer to adreno device
+ *
+ * Submit PM4 commands for HW initialization,
  */
-static int a5xx_microcode_load(struct adreno_device *adreno_dev)
+static int a5xx_hw_init(struct adreno_device *adreno_dev)
 {
-	void *ptr;
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_firmware *pm4_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PM4);
-	struct adreno_firmware *pfp_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PFP);
-	uint64_t gpuaddr;
-	int ret = 0, zap_retry = 0;
+	int ret = a5xx_gpmu_init(adreno_dev);
 
-	gpuaddr = pm4_fw->memdesc.gpuaddr;
-	kgsl_regwrite(device, A5XX_CP_PM4_INSTR_BASE_LO,
-				lower_32_bits(gpuaddr));
-	kgsl_regwrite(device, A5XX_CP_PM4_INSTR_BASE_HI,
-				upper_32_bits(gpuaddr));
-
-	gpuaddr = pfp_fw->memdesc.gpuaddr;
-	kgsl_regwrite(device, A5XX_CP_PFP_INSTR_BASE_LO,
-				lower_32_bits(gpuaddr));
-	kgsl_regwrite(device, A5XX_CP_PFP_INSTR_BASE_HI,
-				upper_32_bits(gpuaddr));
-
-	/*
-	 * Do not invoke to load zap shader if MMU does
-	 * not support secure mode.
-	 */
-	if (!device->mmu.secured)
-		return 0;
+	if (!ret)
+		ret = a5xx_post_start(adreno_dev);
 
-	/*
-	 * Resume call to write the zap shader base address into the
-	 * appropriate register,
-	 * skip if retention is supported for the CPZ register
-	 */
-	if (adreno_dev->zap_loaded && !(ADRENO_FEATURE(adreno_dev,
-		ADRENO_CPZ_RETENTION))) {
-		struct scm_desc desc = {0};
+	return ret;
+}
 
-		desc.args[0] = 0;
-		desc.args[1] = 13;
-		desc.arginfo = SCM_ARGS(2);
+static int a5xx_switch_to_unsecure_mode(struct adreno_device *adreno_dev,
+				struct adreno_ringbuffer *rb)
+{
+	unsigned int *cmds;
+	int ret;
 
-		ret = scm_call2(SCM_SIP_FNID(SCM_SVC_BOOT, 0xA), &desc);
-		if (ret) {
-			pr_err("SCM resume call failed with error %d\n", ret);
-			return ret;
-		}
+	cmds = adreno_ringbuffer_allocspace(rb, 2);
+	if (IS_ERR(cmds))
+		return PTR_ERR(cmds);
+	if (cmds == NULL)
+		return -ENOSPC;
 
-	}
+	cmds += cp_secure_mode(adreno_dev, cmds, 0);
 
-	/* Load the zap shader firmware through PIL if its available */
-	if (adreno_dev->gpucore->zap_name && !adreno_dev->zap_loaded) {
-		/*
-		 * subsystem_get() may return -EAGAIN in case system is busy
-		 * and unable to load the firmware. So keep trying since this
-		 * is not a fatal error.
-		 */
-		do {
-			ret = 0;
-			ptr = subsystem_get(adreno_dev->gpucore->zap_name);
+	ret = adreno_ringbuffer_submit_spin(rb, NULL, 2000);
+	if (ret != 0) {
+		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
-			/* Return error if the zap shader cannot be loaded */
-			if (IS_ERR_OR_NULL(ptr)) {
-				ret = (ptr == NULL) ? -ENODEV : PTR_ERR(ptr);
-				ptr = NULL;
-			} else
-				adreno_dev->zap_loaded = 1;
-		} while ((ret == -EAGAIN) && (zap_retry++ < ZAP_RETRY_MAX));
+		dev_err(device->dev, "Switch to unsecure failed to idle\n");
+		spin_idle_debug(device);
+		kgsl_device_snapshot(device, NULL);
 	}
 
 	return ret;
@@ -2279,26 +2448,12 @@ static int a5xx_microcode_load(struct adreno_device *adreno_dev)
 static int _me_init_ucode_workarounds(struct adreno_device *adreno_dev)
 {
 	switch (ADRENO_GPUREV(adreno_dev)) {
-	case ADRENO_REV_A510:
-		return 0x00000001; /* Ucode workaround for token end syncs */
-	case ADRENO_REV_A504:
 	case ADRENO_REV_A505:
 	case ADRENO_REV_A506:
+	case ADRENO_REV_A510:
+		return 0x00000001; /* Ucode workaround for token end syncs */
 	case ADRENO_REV_A530:
-		/*
-		 * Ucode workarounds for token end syncs,
-		 * WFI after every direct-render 3D mode draw and
-		 * WFI after every 2D Mode 3 draw.
-		 */
-		return 0x0000000B;
-	case ADRENO_REV_A540:
-		/*
-		 * WFI after every direct-render 3D mode draw and
-		 * WFI after every 2D Mode 3 draw. This is needed
-		 * only on a540v1.
-		 */
-		if (adreno_is_a540v1(adreno_dev))
-			return 0x0000000A;
+		return 0x00000003; /* Ucode default workarounds */
 	default:
 		return 0x00000000; /* No ucode workarounds enabled */
 	}
@@ -2377,7 +2532,7 @@ static void _set_ordinals(struct adreno_device *adreno_dev,
 		*cmds++ = 0x0;
 }
 
-int a5xx_critical_packet_submit(struct adreno_device *adreno_dev,
+static int a5xx_critical_packet_submit(struct adreno_device *adreno_dev,
 					struct adreno_ringbuffer *rb)
 {
 	unsigned int *cmds;
@@ -2395,21 +2550,26 @@ int a5xx_critical_packet_submit(struct adreno_device *adreno_dev,
 	*cmds++ = crit_pkts_dwords;
 
 	ret = adreno_ringbuffer_submit_spin(rb, NULL, 20);
-	if (ret)
-		adreno_spin_idle_debug(adreno_dev,
+	if (ret != 0) {
+		struct kgsl_device *device = &adreno_dev->dev;
+
+		dev_err(device->dev,
 			"Critical packet submission failed to idle\n");
+		spin_idle_debug(device);
+		kgsl_device_snapshot(device, NULL);
+	}
 
 	return ret;
 }
 
 /*
- * a5xx_send_me_init() - Initialize ringbuffer
+ * a5xx_rb_init() - Initialize ringbuffer
  * @adreno_dev: Pointer to adreno device
  * @rb: Pointer to the ringbuffer of device
  *
  * Submit commands for ME initialization,
  */
-static int a5xx_send_me_init(struct adreno_device *adreno_dev,
+static int a5xx_rb_init(struct adreno_device *adreno_dev,
 			 struct adreno_ringbuffer *rb)
 {
 	unsigned int *cmds;
@@ -2426,71 +2586,30 @@ static int a5xx_send_me_init(struct adreno_device *adreno_dev,
 	_set_ordinals(adreno_dev, cmds, 8);
 
 	ret = adreno_ringbuffer_submit_spin(rb, NULL, 2000);
-	if (ret)
-		adreno_spin_idle_debug(adreno_dev,
-				"CP initialization failed to idle\n");
-
-	return ret;
-}
-
-/*
- * a5xx_rb_start() - Start the ringbuffer
- * @adreno_dev: Pointer to adreno device
- * @start_type: Warm or cold start
- */
-static int a5xx_rb_start(struct adreno_device *adreno_dev,
-			 unsigned int start_type)
-{
-	struct adreno_ringbuffer *rb = ADRENO_CURRENT_RINGBUFFER(adreno_dev);
-	struct kgsl_device *device = &adreno_dev->dev;
-	uint64_t addr;
-	int ret;
-
-	addr = SCRATCH_RPTR_GPU_ADDR(device, rb->id);
-
-	adreno_writereg64(adreno_dev, ADRENO_REG_CP_RB_RPTR_ADDR_LO,
-			ADRENO_REG_CP_RB_RPTR_ADDR_HI, addr);
-
-	/*
-	 * The size of the ringbuffer in the hardware is the log2
-	 * representation of the size in quadwords (sizedwords / 2).
-	 * Also disable the host RPTR shadow register as it might be unreliable
-	 * in certain circumstances.
-	 */
-
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_CNTL,
-		A5XX_CP_RB_CNTL_DEFAULT);
-
-	adreno_writereg64(adreno_dev, ADRENO_REG_CP_RB_BASE,
-			ADRENO_REG_CP_RB_BASE_HI, rb->buffer_desc.gpuaddr);
-
-	ret = a5xx_microcode_load(adreno_dev);
-	if (ret)
-		return ret;
+	if (ret != 0) {
+		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
-	/* clear ME_HALT to start micro engine */
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_CNTL, 0);
+		dev_err(device->dev, "CP initialization failed to idle\n");
+		spin_idle_debug(device);
+		kgsl_device_snapshot(device, NULL);
+	}
 
-	ret = a5xx_send_me_init(adreno_dev, rb);
-	if (ret)
-		return ret;
+	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_CRITICAL_PACKETS)) {
+		ret = a5xx_critical_packet_submit(adreno_dev, rb);
+		if (ret)
+			return ret;
+	}
 
 	/* GPU comes up in secured mode, make it unsecured by default */
-	ret = adreno_set_unsecured_mode(adreno_dev, rb);
-	if (ret)
-		return ret;
+	if (ADRENO_FEATURE(adreno_dev, ADRENO_CONTENT_PROTECTION))
+		ret = a5xx_switch_to_unsecure_mode(adreno_dev, rb);
 
-	ret = a5xx_gpmu_init(adreno_dev);
-	if (ret)
-		return ret;
-
-	a5xx_post_start(adreno_dev);
-
-	return 0;
+	return ret;
 }
 
 static int _load_firmware(struct kgsl_device *device, const char *fwfile,
-			struct adreno_firmware *firmware)
+			  struct kgsl_memdesc *ucode, size_t *ucode_size,
+			  unsigned int *ucode_version)
 {
 	const struct firmware *fw = NULL;
 	int ret;
@@ -2503,15 +2622,15 @@ static int _load_firmware(struct kgsl_device *device, const char *fwfile,
 		return ret;
 	}
 
-	ret = kgsl_allocate_global(device, &firmware->memdesc, fw->size - 4,
-				KGSL_MEMFLAGS_GPUREADONLY, 0, "ucode");
+	ret = kgsl_allocate_global(device, ucode, fw->size - 4,
+				KGSL_MEMFLAGS_GPUREADONLY, 0);
 
 	if (ret)
 		goto done;
 
-	memcpy(firmware->memdesc.hostptr, &fw->data[4], fw->size - 4);
-	firmware->size = (fw->size - 4) / sizeof(uint32_t);
-	firmware->version = *(unsigned int *)&fw->data[4];
+	memcpy(ucode->hostptr, &fw->data[4], fw->size - 4);
+	*ucode_size = (fw->size - 4) / sizeof(uint32_t);
+	*ucode_version = *(unsigned int *)&fw->data[4];
 
 done:
 	release_firmware(fw);
@@ -2526,22 +2645,18 @@ static int _load_firmware(struct kgsl_device *device, const char *fwfile,
 static int a5xx_microcode_read(struct adreno_device *adreno_dev)
 {
 	int ret;
-	struct adreno_firmware *pm4_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PM4);
-	struct adreno_firmware *pfp_fw = ADRENO_FW(adreno_dev, ADRENO_FW_PFP);
 
-	if (pm4_fw->memdesc.hostptr == NULL) {
-		ret = _load_firmware(KGSL_DEVICE(adreno_dev),
-				 adreno_dev->gpucore->pm4fw_name, pm4_fw);
-		if (ret)
-			return ret;
-	}
+	ret = _load_firmware(KGSL_DEVICE(adreno_dev),
+			 adreno_dev->gpucore->pm4fw_name, &adreno_dev->pm4,
+			 &adreno_dev->pm4_fw_size, &adreno_dev->pm4_fw_version);
+	if (ret)
+		return ret;
 
-	if (pfp_fw->memdesc.hostptr == NULL) {
-		ret = _load_firmware(KGSL_DEVICE(adreno_dev),
-				 adreno_dev->gpucore->pfpfw_name, pfp_fw);
-		if (ret)
-			return ret;
-	}
+	ret = _load_firmware(KGSL_DEVICE(adreno_dev),
+			 adreno_dev->gpucore->pfpfw_name, &adreno_dev->pfp,
+			 &adreno_dev->pfp_fw_size, &adreno_dev->pfp_fw_version);
+	if (ret)
+		return ret;
 
 	ret = _load_gpmu_firmware(adreno_dev);
 	if (ret)
@@ -2552,6 +2667,66 @@ static int a5xx_microcode_read(struct adreno_device *adreno_dev)
 	return ret;
 }
 
+/*
+ * a5xx_microcode_load() - Load microcode
+ * @adreno_dev: Pointer to adreno device
+ * @start_type: type of device start cold/warm
+ */
+static int a5xx_microcode_load(struct adreno_device *adreno_dev,
+				unsigned int start_type)
+{
+	void *ptr;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	uint64_t gpuaddr;
+
+	gpuaddr = adreno_dev->pm4.gpuaddr;
+	kgsl_regwrite(device, A5XX_CP_PM4_INSTR_BASE_LO,
+				lower_32_bits(gpuaddr));
+	kgsl_regwrite(device, A5XX_CP_PM4_INSTR_BASE_HI,
+				upper_32_bits(gpuaddr));
+
+	gpuaddr = adreno_dev->pfp.gpuaddr;
+	kgsl_regwrite(device, A5XX_CP_PFP_INSTR_BASE_LO,
+				lower_32_bits(gpuaddr));
+	kgsl_regwrite(device, A5XX_CP_PFP_INSTR_BASE_HI,
+				upper_32_bits(gpuaddr));
+
+	/*
+	 * Resume call to write the zap shader base address into the
+	 * appropriate register,
+	 * skip if retention is supported for the CPZ register
+	 */
+	if (zap_ucode_loaded && !(ADRENO_FEATURE(adreno_dev,
+		ADRENO_CPZ_RETENTION))) {
+		int ret;
+		struct scm_desc desc = {0};
+
+		desc.args[0] = 0;
+		desc.args[1] = 13;
+		desc.arginfo = SCM_ARGS(2);
+
+		ret = scm_call2(SCM_SIP_FNID(SCM_SVC_BOOT, 0xA), &desc);
+		if (ret) {
+			pr_err("SCM resume call failed with error %d\n", ret);
+			return ret;
+		}
+
+	}
+
+	/* Load the zap shader firmware through PIL if its available */
+	if (adreno_dev->gpucore->zap_name && !zap_ucode_loaded) {
+		ptr = subsystem_get(adreno_dev->gpucore->zap_name);
+
+		/* Return error if the zap shader cannot be loaded */
+		if (IS_ERR_OR_NULL(ptr))
+			return (ptr == NULL) ? -ENODEV : PTR_ERR(ptr);
+
+		zap_ucode_loaded = 1;
+	}
+
+	return 0;
+}
+
 static struct adreno_perfcount_register a5xx_perfcounters_cp[] = {
 	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A5XX_RBBM_PERFCTR_CP_0_LO,
 		A5XX_RBBM_PERFCTR_CP_0_HI, 0, A5XX_CP_PERFCTR_CP_SEL_0 },
@@ -2827,10 +3002,10 @@ static struct adreno_perfcount_register a5xx_perfcounters_alwayson[] = {
 /*
  * 5XX targets don't really have physical PERFCTR_PWR registers - we emulate
  * them using similar performance counters from the RBBM block. The difference
- * between using this group and the RBBM group is that the RBBM counters are
+ * betweeen using this group and the RBBM group is that the RBBM counters are
  * reloaded after a power collapse which is not how the PWR counters behaved on
  * legacy hardware. In order to limit the disruption on the rest of the system
- * we go out of our way to ensure backwards compatibility. Since RBBM counters
+ * we go out of our way to ensure backwards compatability. Since RBBM counters
  * are in short supply, we don't emulate PWR:0 which nobody uses - mark it as
  * broken.
  */
@@ -2988,19 +3163,11 @@ static struct adreno_ft_perf_counters a5xx_ft_perf_counters[] = {
 	{KGSL_PERFCOUNTER_GROUP_TSE, A5XX_TSE_INPUT_PRIM_NUM},
 };
 
-static unsigned int a5xx_int_bits[ADRENO_INT_BITS_MAX] = {
-	ADRENO_INT_DEFINE(ADRENO_INT_RBBM_AHB_ERROR, A5XX_INT_RBBM_AHB_ERROR),
-};
-
 /* Register offset defines for A5XX, in order of enum adreno_regs */
 static unsigned int a5xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_WFI_PEND_CTR, A5XX_CP_WFI_PEND_CTR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_BASE, A5XX_CP_RB_BASE),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_BASE_HI, A5XX_CP_RB_BASE_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR_ADDR_LO,
-			A5XX_CP_RB_RPTR_ADDR_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR_ADDR_HI,
-			A5XX_CP_RB_RPTR_ADDR_HI),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR, A5XX_CP_RB_RPTR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_WPTR, A5XX_CP_RB_WPTR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_CNTL, A5XX_CP_CNTL),
@@ -3021,7 +3188,6 @@ static unsigned int a5xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_MEQ_ADDR, A5XX_CP_MEQ_DBG_ADDR),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_MEQ_DATA, A5XX_CP_MEQ_DBG_DATA),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_PROTECT_REG_0, A5XX_CP_PROTECT_REG_0),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_HW_FAULT, A5XX_CP_HW_FAULT),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_PREEMPT, A5XX_CP_CONTEXT_SWITCH_CNTL),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_PREEMPT_DEBUG, ADRENO_REG_SKIP),
 	ADRENO_REG_DEFINE(ADRENO_REG_CP_PREEMPT_DISABLE, ADRENO_REG_SKIP),
@@ -3051,10 +3217,6 @@ static unsigned int a5xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 		ADRENO_REG_DEFINE(ADRENO_REG_RBBM_BLOCK_SW_RESET_CMD2,
 					  A5XX_RBBM_BLOCK_SW_RESET_CMD2),
 	ADRENO_REG_DEFINE(ADRENO_REG_UCHE_INVALIDATE0, A5XX_UCHE_INVALIDATE0),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_RBBM_0_LO,
-				A5XX_RBBM_PERFCTR_RBBM_0_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_RBBM_0_HI,
-				A5XX_RBBM_PERFCTR_RBBM_0_HI),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_LO,
 				A5XX_RBBM_PERFCTR_LOAD_VALUE_LO),
 	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_HI,
@@ -3081,8 +3243,6 @@ static unsigned int a5xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
 				A5XX_VBIF_XIN_HALT_CTRL1),
 	ADRENO_REG_DEFINE(ADRENO_REG_VBIF_VERSION,
 				A5XX_VBIF_VERSION),
-	ADRENO_REG_DEFINE(ADRENO_REG_GPMU_POWER_COUNTER_ENABLE,
-				A5XX_GPMU_POWER_COUNTER_ENABLE),
 };
 
 static const struct adreno_reg_offsets a5xx_reg_offsets = {
@@ -3227,6 +3387,7 @@ static void a5xx_irq_storm_worker(struct work_struct *work)
 	mutex_unlock(&device->mutex);
 
 	/* Reschedule just to make sure everything retires */
+	kgsl_schedule_work(&device->event_work);
 	adreno_dispatcher_schedule(device);
 }
 
@@ -3276,7 +3437,7 @@ static void a5xx_cp_callback(struct adreno_device *adreno_dev, int bit)
 		prev = cur;
 	}
 
-	a5xx_preemption_trigger(adreno_dev);
+	kgsl_schedule_work(&device->event_work);
 	adreno_dispatcher_schedule(device);
 }
 
@@ -3329,11 +3490,11 @@ static void a5xx_gpmu_int_callback(struct adreno_device *adreno_dev, int bit)
 }
 
 /*
- * a5xx_gpc_err_int_callback() - Isr for GPC error interrupts
- * @adreno_dev: Pointer to device
- * @bit: Interrupt bit
- */
-static void a5xx_gpc_err_int_callback(struct adreno_device *adreno_dev, int bit)
+* a5x_gpc_err_int_callback() - Isr for GPC error interrupts
+* @adreno_dev: Pointer to device
+* @bit: Interrupt bit
+*/
+void a5x_gpc_err_int_callback(struct adreno_device *adreno_dev, int bit)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
@@ -3343,7 +3504,7 @@ static void a5xx_gpc_err_int_callback(struct adreno_device *adreno_dev, int bit)
 	 * with help of register dump.
 	 */
 
-	KGSL_DRV_CRIT_RATELIMIT(device, "RBBM: GPC error\n");
+	KGSL_DRV_CRIT(device, "RBBM: GPC error\n");
 	adreno_irqctrl(adreno_dev, 0);
 
 	/* Trigger a fault in the dispatcher - this will effect a restart */
@@ -3360,6 +3521,9 @@ static void a5xx_gpc_err_int_callback(struct adreno_device *adreno_dev, int bit)
 	 (1 << A5XX_INT_RBBM_ATB_ASYNC_OVERFLOW) |		\
 	 (1 << A5XX_INT_RBBM_GPC_ERROR) |		\
 	 (1 << A5XX_INT_CP_HW_ERROR) |	\
+	 (1 << A5XX_INT_CP_IB1) |			\
+	 (1 << A5XX_INT_CP_IB2) |			\
+	 (1 << A5XX_INT_CP_RB) |			\
 	 (1 << A5XX_INT_CP_CACHE_FLUSH_TS) |		\
 	 (1 << A5XX_INT_RBBM_ATB_BUS_OVERFLOW) |	\
 	 (1 << A5XX_INT_UCHE_OOB_ACCESS) |		\
@@ -3381,8 +3545,8 @@ static struct adreno_irq_funcs a5xx_irq_funcs[32] = {
 	ADRENO_IRQ_CALLBACK(a5xx_err_callback),
 	/* 6 - RBBM_ATB_ASYNC_OVERFLOW */
 	ADRENO_IRQ_CALLBACK(a5xx_err_callback),
-	ADRENO_IRQ_CALLBACK(a5xx_gpc_err_int_callback), /* 7 - GPC_ERR */
-	ADRENO_IRQ_CALLBACK(a5xx_preempt_callback),/* 8 - CP_SW */
+	ADRENO_IRQ_CALLBACK(a5x_gpc_err_int_callback), /* 7 - GPC_ERR */
+	ADRENO_IRQ_CALLBACK(adreno_dispatcher_preempt_callback),/* 8 - CP_SW */
 	ADRENO_IRQ_CALLBACK(a5xx_cp_hw_err_callback), /* 9 - CP_HW_ERROR */
 	/* 10 - CP_CCU_FLUSH_DEPTH_TS */
 	ADRENO_IRQ_CALLBACK(NULL),
@@ -3390,9 +3554,9 @@ static struct adreno_irq_funcs a5xx_irq_funcs[32] = {
 	ADRENO_IRQ_CALLBACK(NULL),
 	 /* 12 - CP_CCU_RESOLVE_TS */
 	ADRENO_IRQ_CALLBACK(NULL),
-	ADRENO_IRQ_CALLBACK(NULL), /* 13 - CP_IB2_INT */
-	ADRENO_IRQ_CALLBACK(NULL), /* 14 - CP_IB1_INT */
-	ADRENO_IRQ_CALLBACK(NULL), /* 15 - CP_RB_INT */
+	ADRENO_IRQ_CALLBACK(adreno_cp_callback), /* 13 - CP_IB2_INT */
+	ADRENO_IRQ_CALLBACK(adreno_cp_callback), /* 14 - CP_IB1_INT */
+	ADRENO_IRQ_CALLBACK(adreno_cp_callback), /* 15 - CP_RB_INT */
 	/* 16 - CCP_UNUSED_1 */
 	ADRENO_IRQ_CALLBACK(NULL),
 	ADRENO_IRQ_CALLBACK(NULL), /* 17 - CP_RB_DONE_TS */
@@ -3629,12 +3793,328 @@ static struct adreno_coresight a5xx_coresight = {
 	.groups = a5xx_coresight_groups,
 };
 
+/**
+ * a5xx_preempt_trig_state() - Schedule preemption in TRIGGERRED
+ * state
+ * @adreno_dev: Device which is in TRIGGERRED state
+ */
+static void a5xx_preempt_trig_state(
+			struct adreno_device *adreno_dev)
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	unsigned int preempt_busy;
+	uint64_t rbbase;
+
+	/*
+	 * triggered preemption, check for busy bits, if not set go to complete
+	 * bit 0: When high indicates CP is not done with preemption.
+	 * bit 4: When high indicates that the CP is actively switching between
+	 *        application contexts.
+	 * Check both the bits to make sure CP is done with preemption.
+	 */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &preempt_busy);
+	if (!(preempt_busy & 0x11)) {
+
+		adreno_readreg64(adreno_dev, ADRENO_REG_CP_RB_BASE,
+				 ADRENO_REG_CP_RB_BASE_HI, &rbbase);
+		/* Did preemption occur, if so then change states and return */
+		if (rbbase != adreno_dev->cur_rb->buffer_desc.gpuaddr) {
+			if (rbbase ==
+				adreno_dev->next_rb->buffer_desc.gpuaddr) {
+				KGSL_DRV_INFO(device,
+				"Preemption completed without interrupt\n");
+				trace_adreno_hw_preempt_trig_to_comp(
+					adreno_dev->cur_rb,
+					adreno_dev->next_rb);
+				atomic_set(&dispatcher->preemption_state,
+					ADRENO_DISPATCHER_PREEMPT_COMPLETE);
+			} else {
+				/*
+				 * Something wrong with preemption.
+				 * Set fault and reschedule dispatcher to take
+				 * care of fault.
+				 */
+				adreno_set_gpu_fault(adreno_dev,
+					ADRENO_PREEMPT_FAULT);
+			}
+			adreno_dispatcher_schedule(device);
+			return;
+		}
+	}
+
+	/*
+	 * Preemption is still happening.
+	 * Hardware not yet idle means that preemption interrupt
+	 * may still occur, nothing to do here until interrupt signals
+	 * completion of preemption, just return here
+	 */
+	if (!adreno_hw_isidle(adreno_dev))
+		return;
+
+	/*
+	 * We just changed states, reschedule dispatcher to change
+	 * preemption states
+	 */
+	if (ADRENO_DISPATCHER_PREEMPT_TRIGGERED !=
+		atomic_read(&dispatcher->preemption_state)) {
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+
+
+	adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+
+	/* reschedule dispatcher to take care of the fault */
+	adreno_dispatcher_schedule(device);
+}
+
+/**
+ * a5xx_preempt_clear_state() - Schedule preemption in CLEAR
+ * state. Preemption can be issued in this state.
+ * @adreno_dev: Device which is in CLEAR state
+ */
+static void a5xx_preempt_clear_state(
+			struct adreno_device *adreno_dev)
+
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct adreno_ringbuffer *highest_busy_rb;
+	int switch_low_to_high;
+	int ret;
+
+	/* Device not awake means there is nothing to do */
+	if (!kgsl_state_is_awake(device))
+		return;
+
+	/* keep updating the current rptr when preemption is clear */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR,
+			&(adreno_dev->cur_rb->rptr));
+
+	highest_busy_rb = adreno_dispatcher_get_highest_busy_rb(adreno_dev);
+	if (!highest_busy_rb)
+		return;
+
+	switch_low_to_high = adreno_compare_prio_level(
+		highest_busy_rb->id, adreno_dev->cur_rb->id);
+
+	/* already current then return */
+	if (!switch_low_to_high)
+		return;
+
+	if (switch_low_to_high < 0) {
+
+		if (!adreno_hw_isidle(adreno_dev)) {
+			adreno_dispatcher_schedule(device);
+			return;
+		}
+
+		/*
+		 * if switching to lower priority make sure that the rptr and
+		 * wptr are equal, when the lower rb is not starved
+		 */
+		if (adreno_dev->cur_rb->rptr != adreno_dev->cur_rb->wptr)
+			return;
+		/*
+		 * switch to default context because when we switch back
+		 * to higher context then its not known which pt will
+		 * be current, so by making it default here the next
+		 * commands submitted will set the right pt
+		 */
+		ret = adreno_drawctxt_switch(adreno_dev,
+				adreno_dev->cur_rb,
+				NULL, 0);
+		/*
+		 * lower priority RB has to wait until space opens up in
+		 * higher RB
+		 */
+		if (ret)
+			return;
+	}
+
+	/* rptr could be updated in drawctxt switch above, update it here */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR,
+			&(adreno_dev->cur_rb->rptr));
+
+	/* turn on IOMMU as the preemption may trigger pt switch */
+	kgsl_mmu_enable_clk(&device->mmu);
+
+	/*
+	 * setup memory to do the switch to highest priority RB
+	 * which is not empty or may be starving away(poor thing)
+	 */
+	a5xx_preemption_start(adreno_dev, highest_busy_rb);
+
+	atomic_set(&dispatcher->preemption_state,
+			ADRENO_DISPATCHER_PREEMPT_TRIGGERED);
+
+	adreno_dev->next_rb = highest_busy_rb;
+	mod_timer(&dispatcher->preempt_timer, jiffies +
+		msecs_to_jiffies(ADRENO_DISPATCH_PREEMPT_TIMEOUT));
+
+	trace_adreno_hw_preempt_clear_to_trig(adreno_dev->cur_rb,
+						adreno_dev->next_rb);
+	/* issue PREEMPT trigger */
+	adreno_writereg(adreno_dev, ADRENO_REG_CP_PREEMPT, 1);
+
+	adreno_dispatcher_schedule(device);
+}
+
+/**
+ * a5xx_preempt_complete_state() - Schedule preemption in
+ * COMPLETE state
+ * @adreno_dev: Device which is in COMPLETE state
+ */
+static void a5xx_preempt_complete_state(
+			struct adreno_device *adreno_dev)
+
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct adreno_dispatcher_cmdqueue *dispatch_q;
+	uint64_t rbbase;
+	unsigned int wptr;
+	unsigned int val;
+	static unsigned long wait_for_preemption_complete;
+
+	del_timer_sync(&dispatcher->preempt_timer);
+
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &val);
+
+	if (val) {
+		/*
+		 * Wait for 50ms for preemption state to be updated by CP
+		 * before triggering hang
+		 */
+		if (wait_for_preemption_complete == 0)
+			wait_for_preemption_complete = jiffies +
+						msecs_to_jiffies(50);
+		if (time_after(jiffies, wait_for_preemption_complete)) {
+			wait_for_preemption_complete = 0;
+			KGSL_DRV_ERR(device,
+			"Invalid state after preemption CP_PREEMPT:%08x STOP:%1x BUSY:%1x\n",
+					 val, (val & 0x1), (val & 0x10)>>4);
+			adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+		}
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+
+	wait_for_preemption_complete = 0;
+	adreno_readreg64(adreno_dev, ADRENO_REG_CP_RB_BASE,
+				ADRENO_REG_CP_RB_BASE_HI, &rbbase);
+	if (rbbase != adreno_dev->next_rb->buffer_desc.gpuaddr) {
+		KGSL_DRV_ERR(device,
+		"RBBASE incorrect after preemption, expected %016llx got %016llx\b",
+		rbbase,
+		adreno_dev->next_rb->buffer_desc.gpuaddr);
+		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+		adreno_dispatcher_schedule(device);
+		return;
+	}
+
+	a5xx_preemption_save(adreno_dev, adreno_dev->cur_rb);
+
+	dispatch_q = &(adreno_dev->cur_rb->dispatch_q);
+	/* new RB is the current RB */
+	trace_adreno_hw_preempt_comp_to_clear(adreno_dev->next_rb,
+						adreno_dev->cur_rb);
+	adreno_dev->prev_rb = adreno_dev->cur_rb;
+	adreno_dev->cur_rb = adreno_dev->next_rb;
+	adreno_dev->cur_rb->preempted_midway = 0;
+	adreno_dev->cur_rb->wptr_preempt_end = 0xFFFFFFFF;
+	adreno_dev->next_rb = NULL;
+
+	if (adreno_disp_preempt_fair_sched) {
+		/* starved rb is now scheduled so unhalt dispatcher */
+		if (ADRENO_DISPATCHER_RB_STARVE_TIMER_ELAPSED ==
+			adreno_dev->cur_rb->starve_timer_state)
+			adreno_put_gpu_halt(adreno_dev);
+		adreno_dev->cur_rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_SCHEDULED;
+		adreno_dev->cur_rb->sched_timer = jiffies;
+		/*
+		 * If the outgoing RB is has commands then set the
+		 * busy time for it
+		 */
+		if (adreno_dev->prev_rb->rptr != adreno_dev->prev_rb->wptr) {
+			adreno_dev->prev_rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_INIT;
+			adreno_dev->prev_rb->sched_timer = jiffies;
+		} else {
+			adreno_dev->prev_rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT;
+		}
+	}
+	adreno_ringbuffer_mmu_disable_clk_on_ts(device, adreno_dev->cur_rb,
+						adreno_dev->cur_rb->timestamp);
+
+	atomic_set(&dispatcher->preemption_state,
+		ADRENO_DISPATCHER_PREEMPT_CLEAR);
+
+	/* submit wptr if required for new rb */
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
+	if (adreno_dev->cur_rb->wptr != wptr) {
+		kgsl_pwrscale_busy(device);
+		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR,
+					adreno_dev->cur_rb->wptr);
+	}
+
+	adreno_preempt_process_dispatch_queue(adreno_dev, dispatch_q);
+}
+
+static void a5xx_preemption_schedule(
+				struct adreno_device *adreno_dev)
+{
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct adreno_ringbuffer *rb;
+	int i = 0;
+
+	if (!adreno_is_preemption_enabled(adreno_dev))
+		return;
+
+	mutex_lock(&device->mutex);
+
+	/*
+	 * This barrier is needed for most updated preemption_state
+	 * to be read.
+	 */
+	smp_mb();
+
+	if (KGSL_STATE_ACTIVE == device->state)
+		FOR_EACH_RINGBUFFER(adreno_dev, rb, i)
+			rb->rptr = adreno_get_rptr(rb);
+
+	switch (atomic_read(&dispatcher->preemption_state)) {
+	case ADRENO_DISPATCHER_PREEMPT_CLEAR:
+		a5xx_preempt_clear_state(adreno_dev);
+		break;
+	case ADRENO_DISPATCHER_PREEMPT_TRIGGERED:
+		a5xx_preempt_trig_state(adreno_dev);
+		/*
+		 * if we transitioned to next state then fall-through
+		 * processing to next state
+		 */
+		if (!adreno_preempt_state(adreno_dev,
+			ADRENO_DISPATCHER_PREEMPT_COMPLETE))
+			break;
+	case ADRENO_DISPATCHER_PREEMPT_COMPLETE:
+		a5xx_preempt_complete_state(adreno_dev);
+		break;
+	default:
+		BUG();
+	}
+
+	mutex_unlock(&device->mutex);
+}
+
 struct adreno_gpudev adreno_a5xx_gpudev = {
 	.reg_offsets = &a5xx_reg_offsets,
-	.int_bits = a5xx_int_bits,
 	.ft_perf_counters = a5xx_ft_perf_counters,
 	.ft_perf_counters_count = ARRAY_SIZE(a5xx_ft_perf_counters),
-	.coresight = {&a5xx_coresight},
+	.coresight = &a5xx_coresight,
 	.start = a5xx_start,
 	.snapshot = a5xx_snapshot,
 	.irq = &a5xx_irq,
@@ -3644,24 +4124,22 @@ struct adreno_gpudev adreno_a5xx_gpudev = {
 	.platform_setup = a5xx_platform_setup,
 	.init = a5xx_init,
 	.remove = a5xx_remove,
-	.rb_start = a5xx_rb_start,
+	.rb_init = a5xx_rb_init,
+	.hw_init = a5xx_hw_init,
 	.microcode_read = a5xx_microcode_read,
+	.microcode_load = a5xx_microcode_load,
 	.perfcounters = &a5xx_perfcounters,
 	.vbif_xin_halt_ctrl0_mask = A5XX_VBIF_XIN_HALT_CTRL0_MASK,
 	.is_sptp_idle = a5xx_is_sptp_idle,
 	.regulator_enable = a5xx_regulator_enable,
 	.regulator_disable = a5xx_regulator_disable,
 	.pwrlevel_change_settings = a5xx_pwrlevel_change_settings,
-	.read_throttling_counters = a5xx_read_throttling_counters,
-	.count_throttles = a5xx_count_throttles,
-	.enable_pwr_counters = a5xx_enable_pwr_counters,
 	.preemption_pre_ibsubmit = a5xx_preemption_pre_ibsubmit,
-	.preemption_yield_enable =
-				a5xx_preemption_yield_enable,
 	.preemption_post_ibsubmit =
-			a5xx_preemption_post_ibsubmit,
+				a5xx_preemption_post_ibsubmit,
+	.preemption_token = a5xx_preemption_token,
 	.preemption_init = a5xx_preemption_init,
 	.preemption_schedule = a5xx_preemption_schedule,
 	.enable_64bit = a5xx_enable_64bit,
-	.clk_set_options = a5xx_clk_set_options,
+	.pre_reset =  a5xx_pre_reset,
 };
diff --git a/drivers/gpu/msm/adreno_a5xx.h b/drivers/gpu/msm/adreno_a5xx.h
index 3d89d73df317..9425eddb517f 100644
--- a/drivers/gpu/msm/adreno_a5xx.h
+++ b/drivers/gpu/msm/adreno_a5xx.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2015-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2015-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -14,45 +14,9 @@
 #ifndef _ADRENO_A5XX_H_
 #define _ADRENO_A5XX_H_
 
-#include "a5xx_reg.h"
-
-#define A5XX_IRQ_FLAGS \
-	{ BIT(A5XX_INT_RBBM_GPU_IDLE), "RBBM_GPU_IDLE" }, \
-	{ BIT(A5XX_INT_RBBM_AHB_ERROR), "RBBM_AHB_ERR" }, \
-	{ BIT(A5XX_INT_RBBM_TRANSFER_TIMEOUT), "RBBM_TRANSFER_TIMEOUT" }, \
-	{ BIT(A5XX_INT_RBBM_ME_MS_TIMEOUT), "RBBM_ME_MS_TIMEOUT" }, \
-	{ BIT(A5XX_INT_RBBM_PFP_MS_TIMEOUT), "RBBM_PFP_MS_TIMEOUT" }, \
-	{ BIT(A5XX_INT_RBBM_ETS_MS_TIMEOUT), "RBBM_ETS_MS_TIMEOUT" }, \
-	{ BIT(A5XX_INT_RBBM_ATB_ASYNC_OVERFLOW), "RBBM_ATB_ASYNC_OVERFLOW" }, \
-	{ BIT(A5XX_INT_RBBM_GPC_ERROR), "RBBM_GPC_ERR" }, \
-	{ BIT(A5XX_INT_CP_SW), "CP_SW" }, \
-	{ BIT(A5XX_INT_CP_HW_ERROR), "CP_OPCODE_ERROR" }, \
-	{ BIT(A5XX_INT_CP_CCU_FLUSH_DEPTH_TS), "CP_CCU_FLUSH_DEPTH_TS" }, \
-	{ BIT(A5XX_INT_CP_CCU_FLUSH_COLOR_TS), "CP_CCU_FLUSH_COLOR_TS" }, \
-	{ BIT(A5XX_INT_CP_CCU_RESOLVE_TS), "CP_CCU_RESOLVE_TS" }, \
-	{ BIT(A5XX_INT_CP_IB2), "CP_IB2_INT" }, \
-	{ BIT(A5XX_INT_CP_IB1), "CP_IB1_INT" }, \
-	{ BIT(A5XX_INT_CP_RB), "CP_RB_INT" }, \
-	{ BIT(A5XX_INT_CP_UNUSED_1), "CP_UNUSED_1" }, \
-	{ BIT(A5XX_INT_CP_RB_DONE_TS), "CP_RB_DONE_TS" }, \
-	{ BIT(A5XX_INT_CP_WT_DONE_TS), "CP_WT_DONE_TS" }, \
-	{ BIT(A5XX_INT_UNKNOWN_1), "UNKNOWN_1" }, \
-	{ BIT(A5XX_INT_CP_CACHE_FLUSH_TS), "CP_CACHE_FLUSH_TS" }, \
-	{ BIT(A5XX_INT_UNUSED_2), "UNUSED_2" }, \
-	{ BIT(A5XX_INT_RBBM_ATB_BUS_OVERFLOW), "RBBM_ATB_BUS_OVERFLOW" }, \
-	{ BIT(A5XX_INT_MISC_HANG_DETECT), "MISC_HANG_DETECT" }, \
-	{ BIT(A5XX_INT_UCHE_OOB_ACCESS), "UCHE_OOB_ACCESS" }, \
-	{ BIT(A5XX_INT_UCHE_TRAP_INTR), "UCHE_TRAP_INTR" }, \
-	{ BIT(A5XX_INT_DEBBUS_INTR_0), "DEBBUS_INTR_0" }, \
-	{ BIT(A5XX_INT_DEBBUS_INTR_1), "DEBBUS_INTR_1" }, \
-	{ BIT(A5XX_INT_GPMU_VOLTAGE_DROOP), "GPMU_VOLTAGE_DROOP" }, \
-	{ BIT(A5XX_INT_GPMU_FIRMWARE), "GPMU_FIRMWARE" }, \
-	{ BIT(A5XX_INT_ISDB_CPU_IRQ), "ISDB_CPU_IRQ" }, \
-	{ BIT(A5XX_INT_ISDB_UNDER_DEBUG), "ISDB_UNDER_DEBUG" }
-
 #define A5XX_CP_CTXRECORD_MAGIC_REF     0x27C4BAFCUL
 /* Size of each CP preemption record */
-#define A5XX_CP_CTXRECORD_SIZE_IN_BYTES     0x10000
+#define A5XX_CP_CTXRECORD_SIZE_IN_BYTES     0x100000
 /* Size of the preemption counter block (in bytes) */
 #define A5XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE   (16 * 4)
 
@@ -112,8 +76,6 @@ void a5xx_crashdump_init(struct adreno_device *adreno_dev);
 
 void a5xx_hwcg_set(struct adreno_device *adreno_dev, bool on);
 
-#define A5XX_CP_RB_CNTL_DEFAULT (((ilog2(4) << 8) & 0x1F00) | \
-		(ilog2(KGSL_RB_DWORDS >> 1) & 0x3F))
 /* GPMU interrupt multiplexor */
 #define FW_INTR_INFO			(0)
 #define LLM_ACK_ERR_INTR		(1)
@@ -135,41 +97,56 @@ void a5xx_hwcg_set(struct adreno_device *adreno_dev, bool on);
 /* A5XX_GPMU_GPMU_LLM_GLM_SLEEP_CTRL */
 #define STATE_OF_CHILD			GENMASK(5, 4)
 #define STATE_OF_CHILD_01		BIT(4)
-#define STATE_OF_CHILD_11		(BIT(4) | BIT(5))
 #define IDLE_FULL_LM_SLEEP		BIT(0)
 
 /* A5XX_GPMU_GPMU_LLM_GLM_SLEEP_STATUS */
 #define WAKEUP_ACK			BIT(1)
 #define IDLE_FULL_ACK			BIT(0)
 
-/* A5XX_GPMU_GPMU_ISENSE_CTRL */
-#define	ISENSE_CGC_EN_DISABLE		BIT(0)
-
 /* A5XX_GPMU_TEMP_SENSOR_CONFIG */
 #define GPMU_BCL_ENABLED		BIT(4)
 #define GPMU_LLM_ENABLED		BIT(9)
+#define GPMU_LMH_ENABLED		BIT(8)
 #define GPMU_ISENSE_STATUS		GENMASK(3, 0)
 #define GPMU_ISENSE_END_POINT_CAL_ERR	BIT(0)
 
+/* A5XX_GPU_CS_AMP_CALIBRATION_CONTROL1 */
+#define AMP_SW_TRIM_START		BIT(0)
+
+/* A5XX_GPU_CS_SENSOR_GENERAL_STATUS */
+#define SS_AMPTRIM_DONE			BIT(11)
+#define CS_PWR_ON_STATUS		BIT(10)
+
+/* A5XX_GPU_CS_AMP_CALIBRATION_STATUS*_* */
+#define AMP_OUT_OF_RANGE_ERR		BIT(4)
+#define AMP_CHECK_TIMEOUT_ERR		BIT(3)
+#define AMP_OFFSET_CHECK_MAX_ERR	BIT(2)
+#define AMP_OFFSET_CHECK_MIN_ERR	BIT(1)
+
+#define AMP_CALIBRATION_ERR (AMP_OFFSET_CHECK_MIN_ERR | \
+		AMP_OFFSET_CHECK_MAX_ERR | AMP_OUT_OF_RANGE_ERR)
+
 #define AMP_CALIBRATION_RETRY_CNT	3
 #define AMP_CALIBRATION_TIMEOUT		6
 
-/* A5XX_GPMU_GPMU_VOLTAGE_INTR_EN_MASK */
-#define VOLTAGE_INTR_EN			BIT(0)
-
 /* A5XX_GPMU_GPMU_PWR_THRESHOLD */
 #define PWR_THRESHOLD_VALID		0x80000000
-
-/* A5XX_GPMU_GPMU_SP_CLOCK_CONTROL */
-#define CNTL_IP_CLK_ENABLE		BIT(0)
 /* AGC */
 #define AGC_INIT_BASE			A5XX_GPMU_DATA_RAM_BASE
+#define AGC_RVOUS_MAGIC			(AGC_INIT_BASE + 0)
+#define AGC_KMD_GPMU_ADDR		(AGC_INIT_BASE + 1)
+#define AGC_KMD_GPMU_BYTES		(AGC_INIT_BASE + 2)
+#define AGC_GPMU_KMD_ADDR		(AGC_INIT_BASE + 3)
+#define AGC_GPMU_KMD_BYTES		(AGC_INIT_BASE + 4)
 #define AGC_INIT_MSG_MAGIC		(AGC_INIT_BASE + 5)
+#define AGC_RESERVED			(AGC_INIT_BASE + 6)
 #define AGC_MSG_BASE			(AGC_INIT_BASE + 7)
 
 #define AGC_MSG_STATE			(AGC_MSG_BASE + 0)
 #define AGC_MSG_COMMAND			(AGC_MSG_BASE + 1)
+#define AGC_MSG_RETURN			(AGC_MSG_BASE + 2)
 #define AGC_MSG_PAYLOAD_SIZE		(AGC_MSG_BASE + 3)
+#define AGC_MSG_MAX_RETURN_SIZE		(AGC_MSG_BASE + 4)
 #define AGC_MSG_PAYLOAD			(AGC_MSG_BASE + 5)
 
 #define AGC_INIT_MSG_VALUE		0xBABEFACE
@@ -177,24 +154,29 @@ void a5xx_hwcg_set(struct adreno_device *adreno_dev, bool on);
 
 #define AGC_LM_CONFIG			(136/4)
 #define AGC_LM_CONFIG_ENABLE_GPMU_ADAPTIVE (1)
+#define AGC_LM_CONFIG_ENABLE_GPMU_LEGACY   (2)
+#define AGC_LM_CONFIG_ENABLE_GPMU_LLM	(3)
 
+#define AGC_LM_CONFIG_ENABLE_ISENSE	(1 << 4)
+#define AGC_LM_CONFIG_ENABLE_DPM	(2 << 4)
 #define AGC_LM_CONFIG_ENABLE_ERROR	(3 << 4)
-#define AGC_LM_CONFIG_ISENSE_ENABLE     (1 << 4)
 
+#define AGC_THROTTLE_SEL_CRC		(0 << 8)
 #define AGC_THROTTLE_SEL_DCS		(1 << 8)
-#define AGC_THROTTLE_DISABLE            (2 << 8)
-
 
 #define AGC_LLM_ENABLED			(1 << 16)
 #define	AGC_GPU_VERSION_MASK		GENMASK(18, 17)
 #define AGC_GPU_VERSION_SHIFT		17
-#define AGC_BCL_DISABLED		(1 << 24)
+#define AGC_BCL_ENABLED			(1 << 24)
 
 
 #define AGC_LEVEL_CONFIG		(140/4)
+#define AGC_LEVEL_CONFIG_SENSOR_DISABLE	GENMASK(15, 0)
+#define AGC_LEVEL_CONFIG_LMDISABLE	GENMASK(31, 16)
 
-#define LM_DCVS_LIMIT			1
+#define LM_DCVS_LIMIT			2
 /* FW file tages */
+#define GPMU_HEADER_ID			1
 #define GPMU_FIRMWARE_ID		2
 #define GPMU_SEQUENCE_ID		3
 #define GPMU_INST_RAM_SIZE		0xFFF
@@ -208,9 +190,9 @@ void a5xx_hwcg_set(struct adreno_device *adreno_dev, bool on);
 #define MAX_HEADER_SIZE			10
 
 #define LM_SEQUENCE_ID			1
+#define HWCG_SEQUENCE_ID		2
 #define MAX_SEQUENCE_ID			3
 
-#define GPMU_ISENSE_SAVE	(A5XX_GPMU_DATA_RAM_BASE + 200/4)
 /* LM defaults */
 #define LM_DEFAULT_LIMIT		6000
 #define A530_DEFAULT_LEAKAGE		0x004E001A
@@ -220,22 +202,4 @@ static inline bool lm_on(struct adreno_device *adreno_dev)
 	return ADRENO_FEATURE(adreno_dev, ADRENO_LM) &&
 		test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag);
 }
-
-/* Preemption functions */
-void a5xx_preemption_trigger(struct adreno_device *adreno_dev);
-void a5xx_preemption_schedule(struct adreno_device *adreno_dev);
-void a5xx_preemption_start(struct adreno_device *adreno_dev);
-int a5xx_preemption_init(struct adreno_device *adreno_dev);
-int a5xx_preemption_yield_enable(unsigned int *cmds);
-
-unsigned int a5xx_preemption_post_ibsubmit(struct adreno_device *adreno_dev,
-		unsigned int *cmds);
-unsigned int a5xx_preemption_pre_ibsubmit(
-			struct adreno_device *adreno_dev,
-			struct adreno_ringbuffer *rb,
-			unsigned int *cmds, struct kgsl_context *context);
-
-
-void a5xx_preempt_callback(struct adreno_device *adreno_dev, int bit);
-
 #endif
diff --git a/drivers/gpu/msm/adreno_a5xx_preempt.c b/drivers/gpu/msm/adreno_a5xx_preempt.c
deleted file mode 100644
index d5da56261d39..000000000000
--- a/drivers/gpu/msm/adreno_a5xx_preempt.c
+++ /dev/null
@@ -1,630 +0,0 @@
-/* Copyright (c) 2014-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include "adreno.h"
-#include "adreno_a5xx.h"
-#include "a5xx_reg.h"
-#include "adreno_trace.h"
-#include "adreno_pm4types.h"
-
-#define PREEMPT_RECORD(_field) \
-		offsetof(struct a5xx_cp_preemption_record, _field)
-
-#define PREEMPT_SMMU_RECORD(_field) \
-		offsetof(struct a5xx_cp_smmu_info, _field)
-
-static void _update_wptr(struct adreno_device *adreno_dev, bool reset_timer)
-{
-	struct adreno_ringbuffer *rb = adreno_dev->cur_rb;
-	unsigned int wptr;
-	unsigned long flags;
-
-	spin_lock_irqsave(&rb->preempt_lock, flags);
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
-
-	if (wptr != rb->wptr) {
-		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR,
-			rb->wptr);
-		/*
-		 * In case something got submitted while preemption was on
-		 * going, reset the timer.
-		 */
-		reset_timer = 1;
-	}
-
-	if (reset_timer)
-		rb->dispatch_q.expires = jiffies +
-			msecs_to_jiffies(adreno_drawobj_timeout);
-
-	spin_unlock_irqrestore(&rb->preempt_lock, flags);
-}
-
-static inline bool adreno_move_preempt_state(struct adreno_device *adreno_dev,
-	enum adreno_preempt_states old, enum adreno_preempt_states new)
-{
-	return (atomic_cmpxchg(&adreno_dev->preempt.state, old, new) == old);
-}
-
-static void _a5xx_preemption_done(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int status;
-
-	/*
-	 * In the very unlikely case that the power is off, do nothing - the
-	 * state will be reset on power up and everybody will be happy
-	 */
-
-	if (!kgsl_state_is_awake(device))
-		return;
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &status);
-
-	if (status != 0) {
-		KGSL_DRV_ERR(device,
-			"Preemption not complete: status=%X cur=%d R/W=%X/%X next=%d R/W=%X/%X\n",
-			status, adreno_dev->cur_rb->id,
-			adreno_get_rptr(adreno_dev->cur_rb),
-			adreno_dev->cur_rb->wptr, adreno_dev->next_rb->id,
-			adreno_get_rptr(adreno_dev->next_rb),
-			adreno_dev->next_rb->wptr);
-
-		/* Set a fault and restart */
-		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-		adreno_dispatcher_schedule(device);
-
-		return;
-	}
-
-	del_timer_sync(&adreno_dev->preempt.timer);
-
-	trace_adreno_preempt_done(adreno_dev->cur_rb, adreno_dev->next_rb, 0);
-
-	/* Clean up all the bits */
-	adreno_dev->prev_rb = adreno_dev->cur_rb;
-	adreno_dev->cur_rb = adreno_dev->next_rb;
-	adreno_dev->next_rb = NULL;
-
-	/* Update the wptr for the new command queue */
-	_update_wptr(adreno_dev, true);
-
-	/* Update the dispatcher timer for the new command queue */
-	mod_timer(&adreno_dev->dispatcher.timer,
-		adreno_dev->cur_rb->dispatch_q.expires);
-
-	/* Clear the preempt state */
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-}
-
-static void _a5xx_preemption_fault(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int status;
-
-	/*
-	 * If the power is on check the preemption status one more time - if it
-	 * was successful then just transition to the complete state
-	 */
-	if (kgsl_state_is_awake(device)) {
-		adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &status);
-
-		if (status == 0) {
-			adreno_set_preempt_state(adreno_dev,
-				ADRENO_PREEMPT_COMPLETE);
-
-			adreno_dispatcher_schedule(device);
-			return;
-		}
-	}
-
-	KGSL_DRV_ERR(device,
-		"Preemption timed out: cur=%d R/W=%X/%X, next=%d R/W=%X/%X\n",
-		adreno_dev->cur_rb->id,
-		adreno_get_rptr(adreno_dev->cur_rb), adreno_dev->cur_rb->wptr,
-		adreno_dev->next_rb->id,
-		adreno_get_rptr(adreno_dev->next_rb),
-		adreno_dev->next_rb->wptr);
-
-	adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-	adreno_dispatcher_schedule(device);
-}
-
-static void _a5xx_preemption_worker(struct work_struct *work)
-{
-	struct adreno_preemption *preempt = container_of(work,
-		struct adreno_preemption, work);
-	struct adreno_device *adreno_dev = container_of(preempt,
-		struct adreno_device, preempt);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	/* Need to take the mutex to make sure that the power stays on */
-	mutex_lock(&device->mutex);
-
-	if (adreno_in_preempt_state(adreno_dev, ADRENO_PREEMPT_FAULTED))
-		_a5xx_preemption_fault(adreno_dev);
-
-	mutex_unlock(&device->mutex);
-}
-
-static void _a5xx_preemption_timer(unsigned long data)
-{
-	struct adreno_device *adreno_dev = (struct adreno_device *) data;
-
-	/* We should only be here from a triggered state */
-	if (!adreno_move_preempt_state(adreno_dev,
-		ADRENO_PREEMPT_TRIGGERED, ADRENO_PREEMPT_FAULTED))
-		return;
-
-	/* Schedule the worker to take care of the details */
-	queue_work(system_unbound_wq, &adreno_dev->preempt.work);
-}
-
-/* Find the highest priority active ringbuffer */
-static struct adreno_ringbuffer *a5xx_next_ringbuffer(
-		struct adreno_device *adreno_dev)
-{
-	struct adreno_ringbuffer *rb;
-	unsigned long flags;
-	unsigned int i;
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		bool empty;
-
-		spin_lock_irqsave(&rb->preempt_lock, flags);
-		empty = adreno_rb_empty(rb);
-		spin_unlock_irqrestore(&rb->preempt_lock, flags);
-
-		if (empty == false)
-			return rb;
-	}
-
-	return NULL;
-}
-
-void a5xx_preemption_trigger(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-	struct adreno_ringbuffer *next;
-	uint64_t ttbr0;
-	unsigned int contextidr;
-	unsigned long flags;
-
-	/* Put ourselves into a possible trigger state */
-	if (!adreno_move_preempt_state(adreno_dev,
-		ADRENO_PREEMPT_NONE, ADRENO_PREEMPT_START))
-		return;
-
-	/* Get the next ringbuffer to preempt in */
-	next = a5xx_next_ringbuffer(adreno_dev);
-
-	/*
-	 * Nothing to do if every ringbuffer is empty or if the current
-	 * ringbuffer is the only active one
-	 */
-	if (next == NULL || next == adreno_dev->cur_rb) {
-		/*
-		 * Update any critical things that might have been skipped while
-		 * we were looking for a new ringbuffer
-		 */
-
-		if (next != NULL) {
-			_update_wptr(adreno_dev, false);
-
-			mod_timer(&adreno_dev->dispatcher.timer,
-				adreno_dev->cur_rb->dispatch_q.expires);
-		}
-
-		adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-		return;
-	}
-
-	/* Turn off the dispatcher timer */
-	del_timer(&adreno_dev->dispatcher.timer);
-
-	/*
-	 * This is the most critical section - we need to take care not to race
-	 * until we have programmed the CP for the switch
-	 */
-
-	spin_lock_irqsave(&next->preempt_lock, flags);
-
-	/*
-	 * Get the pagetable from the pagetable info.
-	 * The pagetable_desc is allocated and mapped at probe time, and
-	 * preemption_desc at init time, so no need to check if
-	 * sharedmem accesses to these memdescs succeed.
-	 */
-	kgsl_sharedmem_readq(&next->pagetable_desc, &ttbr0,
-		PT_INFO_OFFSET(ttbr0));
-	kgsl_sharedmem_readl(&next->pagetable_desc, &contextidr,
-		PT_INFO_OFFSET(contextidr));
-
-	kgsl_sharedmem_writel(device, &next->preemption_desc,
-		PREEMPT_RECORD(wptr), next->wptr);
-
-	spin_unlock_irqrestore(&next->preempt_lock, flags);
-
-	/* And write it to the smmu info */
-	kgsl_sharedmem_writeq(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(ttbr0), ttbr0);
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(context_idr), contextidr);
-
-	kgsl_regwrite(device, A5XX_CP_CONTEXT_SWITCH_RESTORE_ADDR_LO,
-		lower_32_bits(next->preemption_desc.gpuaddr));
-	kgsl_regwrite(device, A5XX_CP_CONTEXT_SWITCH_RESTORE_ADDR_HI,
-		upper_32_bits(next->preemption_desc.gpuaddr));
-
-	adreno_dev->next_rb = next;
-
-	/* Start the timer to detect a stuck preemption */
-	mod_timer(&adreno_dev->preempt.timer,
-		jiffies + msecs_to_jiffies(ADRENO_PREEMPT_TIMEOUT));
-
-	trace_adreno_preempt_trigger(adreno_dev->cur_rb, adreno_dev->next_rb,
-		1);
-
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_TRIGGERED);
-
-	/* Trigger the preemption */
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_PREEMPT, 1);
-}
-
-void a5xx_preempt_callback(struct adreno_device *adreno_dev, int bit)
-{
-	unsigned int status;
-
-	if (!adreno_move_preempt_state(adreno_dev,
-		ADRENO_PREEMPT_TRIGGERED, ADRENO_PREEMPT_PENDING))
-		return;
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &status);
-
-	if (status != 0) {
-		KGSL_DRV_ERR(KGSL_DEVICE(adreno_dev),
-			"preempt interrupt with non-zero status: %X\n", status);
-
-		/*
-		 * Under the assumption that this is a race between the
-		 * interrupt and the register, schedule the worker to clean up.
-		 * If the status still hasn't resolved itself by the time we get
-		 * there then we have to assume something bad happened
-		 */
-		adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_COMPLETE);
-		adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
-		return;
-	}
-
-	del_timer(&adreno_dev->preempt.timer);
-
-	trace_adreno_preempt_done(adreno_dev->cur_rb, adreno_dev->next_rb, 0);
-
-	adreno_dev->prev_rb = adreno_dev->cur_rb;
-	adreno_dev->cur_rb = adreno_dev->next_rb;
-	adreno_dev->next_rb = NULL;
-
-	/* Update the wptr if it changed while preemption was ongoing */
-	_update_wptr(adreno_dev, true);
-
-	/* Update the dispatcher timer for the new command queue */
-	mod_timer(&adreno_dev->dispatcher.timer,
-		adreno_dev->cur_rb->dispatch_q.expires);
-
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-
-	a5xx_preemption_trigger(adreno_dev);
-}
-
-void a5xx_preemption_schedule(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return;
-
-	mutex_lock(&device->mutex);
-
-	if (adreno_in_preempt_state(adreno_dev, ADRENO_PREEMPT_COMPLETE))
-		_a5xx_preemption_done(adreno_dev);
-
-	a5xx_preemption_trigger(adreno_dev);
-
-	mutex_unlock(&device->mutex);
-}
-
-unsigned int a5xx_preemption_pre_ibsubmit(
-			struct adreno_device *adreno_dev,
-			struct adreno_ringbuffer *rb,
-			unsigned int *cmds, struct kgsl_context *context)
-{
-	unsigned int *cmds_orig = cmds;
-	uint64_t gpuaddr = rb->preemption_desc.gpuaddr;
-	unsigned int preempt_style = 0;
-
-	if (context) {
-		/*
-		 * Preemption from secure to unsecure needs Zap shader to be
-		 * run to clear all secure content. CP does not know during
-		 * preemption if it is switching between secure and unsecure
-		 * contexts so restrict Secure contexts to be preempted at
-		 * ringbuffer level.
-		 */
-		if (context->flags & KGSL_CONTEXT_SECURE)
-			preempt_style = KGSL_CONTEXT_PREEMPT_STYLE_RINGBUFFER;
-		else
-			preempt_style = ADRENO_PREEMPT_STYLE(context->flags);
-	}
-
-	/*
-	 * CP_PREEMPT_ENABLE_GLOBAL(global preemption) can only be set by KMD
-	 * in ringbuffer.
-	 * 1) set global preemption to 0x0 to disable global preemption.
-	 *    Only RB level preemption is allowed in this mode
-	 * 2) Set global preemption to defer(0x2) for finegrain preemption.
-	 *    when global preemption is set to defer(0x2),
-	 *    CP_PREEMPT_ENABLE_LOCAL(local preemption) determines the
-	 *    preemption point. Local preemption
-	 *    can be enabled by both UMD(within IB) and KMD.
-	 */
-	*cmds++ = cp_type7_packet(CP_PREEMPT_ENABLE_GLOBAL, 1);
-	*cmds++ = ((preempt_style == KGSL_CONTEXT_PREEMPT_STYLE_FINEGRAIN)
-				? 2 : 0);
-
-	/* Turn CP protection OFF */
-	*cmds++ = cp_type7_packet(CP_SET_PROTECTED_MODE, 1);
-	*cmds++ = 0;
-
-	/*
-	 * CP during context switch will save context switch info to
-	 * a5xx_cp_preemption_record pointed by CONTEXT_SWITCH_SAVE_ADDR
-	 */
-	*cmds++ = cp_type4_packet(A5XX_CP_CONTEXT_SWITCH_SAVE_ADDR_LO, 1);
-	*cmds++ = lower_32_bits(gpuaddr);
-	*cmds++ = cp_type4_packet(A5XX_CP_CONTEXT_SWITCH_SAVE_ADDR_HI, 1);
-	*cmds++ = upper_32_bits(gpuaddr);
-
-	/* Turn CP protection ON */
-	*cmds++ = cp_type7_packet(CP_SET_PROTECTED_MODE, 1);
-	*cmds++ = 1;
-
-	/*
-	 * Enable local preemption for finegrain preemption in case of
-	 * a misbehaving IB
-	 */
-	if (preempt_style == KGSL_CONTEXT_PREEMPT_STYLE_FINEGRAIN) {
-		*cmds++ = cp_type7_packet(CP_PREEMPT_ENABLE_LOCAL, 1);
-		*cmds++ = 1;
-	} else {
-		*cmds++ = cp_type7_packet(CP_PREEMPT_ENABLE_LOCAL, 1);
-		*cmds++ = 0;
-	}
-
-	/* Enable CP_CONTEXT_SWITCH_YIELD packets in the IB2s */
-	*cmds++ = cp_type7_packet(CP_YIELD_ENABLE, 1);
-	*cmds++ = 2;
-
-	return (unsigned int) (cmds - cmds_orig);
-}
-
-int a5xx_preemption_yield_enable(unsigned int *cmds)
-{
-	/*
-	 * SRM -- set render mode (ex binning, direct render etc)
-	 * SRM is set by UMD usually at start of IB to tell CP the type of
-	 * preemption.
-	 * KMD needs to set SRM to NULL to indicate CP that rendering is
-	 * done by IB.
-	 */
-	*cmds++ = cp_type7_packet(CP_SET_RENDER_MODE, 5);
-	*cmds++ = 0;
-	*cmds++ = 0;
-	*cmds++ = 0;
-	*cmds++ = 0;
-	*cmds++ = 0;
-
-	*cmds++ = cp_type7_packet(CP_YIELD_ENABLE, 1);
-	*cmds++ = 1;
-
-	return 8;
-}
-
-unsigned int a5xx_preemption_post_ibsubmit(struct adreno_device *adreno_dev,
-	unsigned int *cmds)
-{
-	int dwords = 0;
-
-	cmds[dwords++] = cp_type7_packet(CP_CONTEXT_SWITCH_YIELD, 4);
-	/* Write NULL to the address to skip the data write */
-	dwords += cp_gpuaddr(adreno_dev, &cmds[dwords], 0x0);
-	cmds[dwords++] = 1;
-	/* generate interrupt on preemption completion */
-	cmds[dwords++] = 1;
-
-	return dwords;
-}
-
-void a5xx_preemption_start(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-	struct adreno_ringbuffer *rb;
-	unsigned int i;
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return;
-
-	/* Force the state to be clear */
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-
-	/* smmu_info is allocated and mapped in a5xx_preemption_iommu_init */
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(magic), A5XX_CP_SMMU_INFO_MAGIC_REF);
-	kgsl_sharedmem_writeq(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(ttbr0), MMU_DEFAULT_TTBR0(device));
-
-	/* The CP doesn't use the asid record, so poison it */
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(asid), 0xDECAFBAD);
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(context_idr),
-		MMU_DEFAULT_CONTEXTIDR(device));
-
-	adreno_writereg64(adreno_dev,
-			ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_LO,
-			ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_HI,
-			iommu->smmu_info.gpuaddr);
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		/*
-		 * preemption_desc is allocated and mapped at init time,
-		 * so no need to check sharedmem_writel return value
-		 */
-		kgsl_sharedmem_writel(device, &rb->preemption_desc,
-			PREEMPT_RECORD(rptr), 0);
-		kgsl_sharedmem_writel(device, &rb->preemption_desc,
-			PREEMPT_RECORD(wptr), 0);
-
-		adreno_ringbuffer_set_pagetable(rb,
-			device->mmu.defaultpagetable);
-	}
-
-}
-
-static int a5xx_preemption_ringbuffer_init(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb, uint64_t counteraddr)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	int ret;
-
-	ret = kgsl_allocate_global(device, &rb->preemption_desc,
-		A5XX_CP_CTXRECORD_SIZE_IN_BYTES, 0, KGSL_MEMDESC_PRIVILEGED,
-		"preemption_desc");
-	if (ret)
-		return ret;
-
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(magic), A5XX_CP_CTXRECORD_MAGIC_REF);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(info), 0);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(data), 0);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(cntl), A5XX_CP_RB_CNTL_DEFAULT);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(rptr), 0);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(wptr), 0);
-	kgsl_sharedmem_writeq(device, &rb->preemption_desc,
-		PREEMPT_RECORD(rptr_addr), SCRATCH_RPTR_GPU_ADDR(device,
-			rb->id));
-	kgsl_sharedmem_writeq(device, &rb->preemption_desc,
-		PREEMPT_RECORD(rbase), rb->buffer_desc.gpuaddr);
-	kgsl_sharedmem_writeq(device, &rb->preemption_desc,
-		PREEMPT_RECORD(counter), counteraddr);
-
-	return 0;
-}
-
-#ifdef CONFIG_QCOM_KGSL_IOMMU
-static int a5xx_preemption_iommu_init(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-
-	/* Allocate mem for storing preemption smmu record */
-	return kgsl_allocate_global(device, &iommu->smmu_info, PAGE_SIZE,
-		KGSL_MEMFLAGS_GPUREADONLY, KGSL_MEMDESC_PRIVILEGED,
-		"smmu_info");
-}
-
-static void a5xx_preemption_iommu_close(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-
-	kgsl_free_global(device, &iommu->smmu_info);
-}
-
-#else
-static int a5xx_preemption_iommu_init(struct adreno_device *adreno_dev)
-{
-	return -ENODEV;
-}
-
-static void a5xx_preemption_iommu_close(struct adreno_device *adreno_dev)
-{
-}
-#endif
-
-static void a5xx_preemption_close(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-	struct adreno_ringbuffer *rb;
-	unsigned int i;
-
-	del_timer(&preempt->timer);
-	kgsl_free_global(device, &preempt->counters);
-	a5xx_preemption_iommu_close(adreno_dev);
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		kgsl_free_global(device, &rb->preemption_desc);
-	}
-}
-
-int a5xx_preemption_init(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-	struct adreno_ringbuffer *rb;
-	int ret;
-	unsigned int i;
-	uint64_t addr;
-
-	/* We are dependent on IOMMU to make preemption go on the CP side */
-	if (kgsl_mmu_get_mmutype(device) != KGSL_MMU_TYPE_IOMMU)
-		return -ENODEV;
-
-	INIT_WORK(&preempt->work, _a5xx_preemption_worker);
-
-	setup_timer(&preempt->timer, _a5xx_preemption_timer,
-		(unsigned long) adreno_dev);
-
-	/* Allocate mem for storing preemption counters */
-	ret = kgsl_allocate_global(device, &preempt->counters,
-		adreno_dev->num_ringbuffers *
-		A5XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE, 0, 0,
-		"preemption_counters");
-	if (ret)
-		goto err;
-
-	addr = preempt->counters.gpuaddr;
-
-	/* Allocate mem for storing preemption switch record */
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		ret = a5xx_preemption_ringbuffer_init(adreno_dev, rb, addr);
-		if (ret)
-			goto err;
-
-		addr += A5XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE;
-	}
-
-	ret = a5xx_preemption_iommu_init(adreno_dev);
-
-err:
-	if (ret)
-		a5xx_preemption_close(device);
-
-	return ret;
-}
diff --git a/drivers/gpu/msm/adreno_a5xx_snapshot.c b/drivers/gpu/msm/adreno_a5xx_snapshot.c
index 4bde8c6fbbae..3d1f81fb01f7 100644
--- a/drivers/gpu/msm/adreno_a5xx_snapshot.c
+++ b/drivers/gpu/msm/adreno_a5xx_snapshot.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2015-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2015-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -126,10 +126,7 @@ static const struct adreno_debugbus_block a5xx_debugbus_blocks[] = {
 };
 
 #define A5XX_NUM_AXI_ARB_BLOCKS	2
-#define A5XX_NUM_XIN_BLOCKS	4
-
-/* Width of A5XX_CP_DRAW_STATE_ADDR is 8 bits */
-#define A5XX_CP_DRAW_STATE_ADDR_WIDTH 8
+#define A5XX_NUM_XIN_BLOCKS	5
 
 /* a5xx_snapshot_cp_pm4() - Dump PM4 data in snapshot */
 static size_t a5xx_snapshot_cp_pm4(struct kgsl_device *device, u8 *buf,
@@ -138,8 +135,7 @@ static size_t a5xx_snapshot_cp_pm4(struct kgsl_device *device, u8 *buf,
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct kgsl_snapshot_debug *header = (struct kgsl_snapshot_debug *)buf;
 	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_PM4);
-	size_t size = fw->size;
+	size_t size = adreno_dev->pm4_fw_size;
 
 	if (remain < DEBUG_SECTION_SZ(size)) {
 		SNAPSHOT_ERR_NOMEM(device, "CP PM4 RAM DEBUG");
@@ -149,7 +145,7 @@ static size_t a5xx_snapshot_cp_pm4(struct kgsl_device *device, u8 *buf,
 	header->type = SNAPSHOT_DEBUG_CP_PM4_RAM;
 	header->size = size;
 
-	memcpy(data, fw->memdesc.hostptr, size * sizeof(uint32_t));
+	memcpy(data, adreno_dev->pm4.hostptr, size * sizeof(uint32_t));
 
 	return DEBUG_SECTION_SZ(size);
 }
@@ -161,8 +157,7 @@ static size_t a5xx_snapshot_cp_pfp(struct kgsl_device *device, u8 *buf,
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct kgsl_snapshot_debug *header = (struct kgsl_snapshot_debug *)buf;
 	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_PFP);
-	int size = fw->size;
+	int size = adreno_dev->pfp_fw_size;
 
 	if (remain < DEBUG_SECTION_SZ(size)) {
 		SNAPSHOT_ERR_NOMEM(device, "CP PFP RAM DEBUG");
@@ -172,7 +167,7 @@ static size_t a5xx_snapshot_cp_pfp(struct kgsl_device *device, u8 *buf,
 	header->type = SNAPSHOT_DEBUG_CP_PFP_RAM;
 	header->size = size;
 
-	memcpy(data, fw->memdesc.hostptr, size * sizeof(uint32_t));
+	memcpy(data, adreno_dev->pfp.hostptr, size * sizeof(uint32_t));
 
 	return DEBUG_SECTION_SZ(size);
 }
@@ -207,11 +202,11 @@ static size_t a5xx_snapshot_vbif_debugbus(struct kgsl_device *device,
 	/*
 	 * Total number of VBIF data words considering 3 sections:
 	 * 2 arbiter blocks of 16 words
-	 * 4 AXI XIN blocks of 18 dwords each
-	 * 4 core clock side XIN blocks of 12 dwords each
+	 * 5 AXI XIN blocks of 4 dwords each
+	 * 5 core clock side XIN blocks of 5 dwords each
 	 */
 	unsigned int dwords = (16 * A5XX_NUM_AXI_ARB_BLOCKS) +
-			(18 * A5XX_NUM_XIN_BLOCKS) + (12 * A5XX_NUM_XIN_BLOCKS);
+			(4 * A5XX_NUM_XIN_BLOCKS) + (5 * A5XX_NUM_XIN_BLOCKS);
 	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
 	size_t size;
 	unsigned int reg_clk;
@@ -249,7 +244,7 @@ static size_t a5xx_snapshot_vbif_debugbus(struct kgsl_device *device,
 	/* XIN blocks AXI side */
 	for (i = 0; i < A5XX_NUM_XIN_BLOCKS; i++) {
 		kgsl_regwrite(device, A5XX_VBIF_TEST_BUS2_CTRL0, 1 << i);
-		for (j = 0; j < 18; j++) {
+		for (j = 0; j < 4; j++) {
 			kgsl_regwrite(device, A5XX_VBIF_TEST_BUS2_CTRL1,
 				((j & A5XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_MASK)
 				<< A5XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_SHIFT));
@@ -262,7 +257,7 @@ static size_t a5xx_snapshot_vbif_debugbus(struct kgsl_device *device,
 	/* XIN blocks core clock side */
 	for (i = 0; i < A5XX_NUM_XIN_BLOCKS; i++) {
 		kgsl_regwrite(device, A5XX_VBIF_TEST_BUS1_CTRL0, 1 << i);
-		for (j = 0; j < 12; j++) {
+		for (j = 0; j < 5; j++) {
 			kgsl_regwrite(device, A5XX_VBIF_TEST_BUS1_CTRL1,
 				((j & A5XX_VBIF_TEST_BUS1_CTRL1_DATA_SEL_MASK)
 				<< A5XX_VBIF_TEST_BUS1_CTRL1_DATA_SEL_SHIFT));
@@ -318,7 +313,7 @@ static void a5xx_snapshot_debugbus(struct kgsl_device *device,
 		0xf << A5XX_RBBM_CFG_DEBBUS_CTLTM_ENABLE_SHIFT);
 
 	for (i = 0; i < ARRAY_SIZE(a5xx_debugbus_blocks); i++) {
-		if (a5xx_debugbus_blocks[i].block_id == A5XX_RBBM_DBGBUS_VBIF)
+		if (A5XX_RBBM_DBGBUS_VBIF == a5xx_debugbus_blocks[i].block_id)
 			kgsl_snapshot_add_section(device,
 				KGSL_SNAPSHOT_SECTION_DEBUGBUS,
 				snapshot, a5xx_snapshot_vbif_debugbus,
@@ -331,7 +326,8 @@ static void a5xx_snapshot_debugbus(struct kgsl_device *device,
 	}
 }
 
-static const unsigned int a5xx_vbif_ver_20xxxxxx_registers[] = {
+static const unsigned int a5xx_vbif_ver_20040000_registers[] = {
+	/* VBIF version 0x20040000*/
 	0x3000, 0x3007, 0x300C, 0x3014, 0x3018, 0x302C, 0x3030, 0x3030,
 	0x3034, 0x3036, 0x3038, 0x3038, 0x303C, 0x303D, 0x3040, 0x3040,
 	0x3042, 0x3042, 0x3049, 0x3049, 0x3058, 0x3058, 0x305A, 0x3061,
@@ -345,8 +341,10 @@ static const unsigned int a5xx_vbif_ver_20xxxxxx_registers[] = {
 
 static const struct adreno_vbif_snapshot_registers
 a5xx_vbif_snapshot_registers[] = {
-	{ 0x20000000, 0xFF000000, a5xx_vbif_ver_20xxxxxx_registers,
-				ARRAY_SIZE(a5xx_vbif_ver_20xxxxxx_registers)/2},
+	{ 0x20040000, a5xx_vbif_ver_20040000_registers,
+				ARRAY_SIZE(a5xx_vbif_ver_20040000_registers)/2},
+	{ 0x20040001, a5xx_vbif_ver_20040000_registers,
+				ARRAY_SIZE(a5xx_vbif_ver_20040000_registers)/2},
 };
 
 /*
@@ -360,12 +358,10 @@ static const unsigned int a5xx_registers[] = {
 	0x0000, 0x0002, 0x0004, 0x0020, 0x0022, 0x0026, 0x0029, 0x002B,
 	0x002E, 0x0035, 0x0038, 0x0042, 0x0044, 0x0044, 0x0047, 0x0095,
 	0x0097, 0x00BB, 0x03A0, 0x0464, 0x0469, 0x046F, 0x04D2, 0x04D3,
-	0x04E0, 0x04F4, 0X04F8, 0x0529, 0x0531, 0x0533, 0x0540, 0x0555,
-	0xF400, 0xF400, 0xF800, 0xF807,
+	0x04E0, 0x0533, 0x0540, 0x0555, 0xF400, 0xF400, 0xF800, 0xF807,
 	/* CP */
 	0x0800, 0x081A, 0x081F, 0x0841, 0x0860, 0x0860, 0x0880, 0x08A0,
-	0x0B00, 0x0B12, 0x0B15, 0X0B1C, 0X0B1E, 0x0B28, 0x0B78, 0x0B7F,
-	0x0BB0, 0x0BBD,
+	0x0B00, 0x0B12, 0x0B15, 0x0B28, 0x0B78, 0x0B7F, 0x0BB0, 0x0BBD,
 	/* VSC */
 	0x0BC0, 0x0BC6, 0x0BD0, 0x0C53, 0x0C60, 0x0C61,
 	/* GRAS */
@@ -383,7 +379,7 @@ static const unsigned int a5xx_registers[] = {
 	/* VPC */
 	0x0E60, 0x0E7C,
 	/* UCHE */
-	0x0E80, 0x0E8F, 0x0E90, 0x0E96, 0xEA0, 0xEA8, 0xEB0, 0xEB2,
+	0x0E80, 0x0E8E, 0x0E90, 0x0E96, 0xEA0, 0xEA8, 0xEB0, 0xEB2,
 
 	/* RB CTX 0 */
 	0xE140, 0xE147, 0xE150, 0xE187, 0xE1A0, 0xE1A9, 0xE1B0, 0xE1B6,
@@ -412,50 +408,27 @@ static const unsigned int a5xx_registers[] = {
 	0xEC00, 0xEC05, 0xEC08, 0xECE9, 0xECF0, 0xECF0,
 	/* VPC CTX 1 */
 	0xEA80, 0xEA80, 0xEA82, 0xEAA3, 0xEAA5, 0xEAC2,
-};
-
-/*
- * GPMU registers to dump for A5XX on snapshot.
- * Registers in pairs - first value is the start offset, second
- * is the stop offset (inclusive)
- */
-
-static const unsigned int a5xx_gpmu_registers[] = {
 	/* GPMU */
 	0xA800, 0xA8FF, 0xAC60, 0xAC60,
+	/* DPM */
+	0xB000, 0xB97F, 0xB9A0, 0xB9BF,
 };
 
-/*
- * Set of registers to dump for A5XX before actually triggering crash dumper.
- * Registers in pairs - first value is the start offset, second
- * is the stop offset (inclusive)
- */
-static const unsigned int a5xx_pre_crashdumper_registers[] = {
-	/* RBBM: RBBM_STATUS - RBBM_STATUS3 */
-	0x04F5, 0x04F7, 0x0530, 0x0530,
-	/* CP: CP_STATUS_1 */
-	0x0B1D, 0x0B1D,
-};
-
-
 struct a5xx_hlsq_sp_tp_regs {
 	unsigned int statetype;
 	unsigned int ahbaddr;
 	unsigned int size;
-	uint64_t offset;
 };
 
-static struct a5xx_hlsq_sp_tp_regs a5xx_hlsq_sp_tp_registers[] = {
-	/* HSLQ non context. 0xe32 - 0xe3f are holes so don't include them */
-	{ 0x35, 0xE00, 0x32 },
+static const struct a5xx_hlsq_sp_tp_regs a5xx_hlsq_sp_tp_registers[] = {
 	/* HLSQ CTX 0 2D */
 	{ 0x31, 0x2080, 0x1 },
 	/* HLSQ CTX 1 2D */
 	{ 0x33, 0x2480, 0x1 },
-	/* HLSQ CTX 0 3D. 0xe7e2 - 0xe7ff are holes so don't inculde them */
-	{ 0x32, 0xE780, 0x62 },
-	/* HLSQ CTX 1 3D. 0xefe2 - 0xefff are holes so don't include them */
-	{ 0x34, 0xEF80, 0x62 },
+	/* HLSQ CTX 0 3D */
+	{ 0x32, 0xE780, 0x7f },
+	/* HLSQ CTX 1 3D */
+	{ 0x34, 0xEF80, 0x7f },
 
 	/* SP non context */
 	{ 0x3f, 0x0EC0, 0x40 },
@@ -468,18 +441,22 @@ static struct a5xx_hlsq_sp_tp_regs a5xx_hlsq_sp_tp_registers[] = {
 	/* SP CTX 1 3D */
 	{ 0x3c, 0xED80, 0x180 },
 
-	/* TP non context. 0x0f1c - 0x0f3f are holes so don't include them */
-	{ 0x3a, 0x0F00, 0x1c },
-	/* TP CTX 0 2D. 0x200a - 0x200f are holes so don't include them */
-	{ 0x38, 0x2000, 0xa },
-	/* TP CTX 1 2D.   0x240a - 0x240f are holes so don't include them */
-	{ 0x36, 0x2400, 0xa },
+	/* TP non context */
+	{ 0x3a, 0x0F00, 0x40 },
+	/* TP CTX 0 2D */
+	{ 0x38, 0x2000, 0x10 },
+	/* TP CTX 1 2D */
+	{ 0x36, 0x2400, 0x10 },
 	/* TP CTX 0 3D */
-	{ 0x39, 0xE700, 0x80 },
+	{ 0x39, 0xE700, 0x128 },
 	/* TP CTX 1 3D */
-	{ 0x37, 0xEF00, 0x80 },
+	{ 0x37, 0xEF00, 0x128 },
 };
 
+/* HLSQ non context registers - can't be read on A530v1 */
+static const struct a5xx_hlsq_sp_tp_regs a5xx_hlsq_non_ctx_registers = {
+	0x35, 0xE00, 0x1C
+};
 
 #define A5XX_NUM_SHADER_BANKS 4
 #define A5XX_SHADER_STATETYPE_SHIFT 8
@@ -543,16 +520,14 @@ enum a5xx_shader_obj {
 struct a5xx_shader_block {
 	unsigned int statetype;
 	unsigned int sz;
-	uint64_t offset;
 };
 
 struct a5xx_shader_block_info {
-	struct a5xx_shader_block *block;
-	unsigned int bank;
-	uint64_t offset;
+	const struct a5xx_shader_block *shader_block;
+	unsigned int shader_num;
 };
 
-static struct a5xx_shader_block a5xx_shader_blocks[] = {
+static const struct a5xx_shader_block a5xx_shader_blocks[] = {
 	{A5XX_TP_W_MEMOBJ,              0x200},
 	{A5XX_TP_W_MIPMAP_BASE,         0x3C0},
 	{A5XX_TP_W_SAMPLER_TAG,          0x40},
@@ -607,185 +582,181 @@ static struct a5xx_shader_block a5xx_shader_blocks[] = {
 	{A5XX_TP_POWER_RESTORE_RAM,      0x40},
 };
 
-static struct kgsl_memdesc capturescript;
-static struct kgsl_memdesc registers;
-static bool crash_dump_valid;
-
 static size_t a5xx_snapshot_shader_memory(struct kgsl_device *device,
 	u8 *buf, size_t remain, void *priv)
 {
 	struct kgsl_snapshot_shader *header =
-		(struct kgsl_snapshot_shader *) buf;
-	struct a5xx_shader_block_info *info =
-		(struct a5xx_shader_block_info *) priv;
-	struct a5xx_shader_block *block = info->block;
-	unsigned int *data = (unsigned int *) (buf + sizeof(*header));
+				(struct kgsl_snapshot_shader *)buf;
+	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
+	unsigned int i;
+	struct a5xx_shader_block_info *shader_block_info =
+				(struct a5xx_shader_block_info *)priv;
+	unsigned int statetype = shader_block_info->shader_block->statetype;
+	unsigned int size = shader_block_info->shader_block->sz;
+	unsigned int shader_num = shader_block_info->shader_num;
 
-	if (remain < SHADER_SECTION_SZ(block->sz)) {
+
+	if (remain < SHADER_SECTION_SZ(size)) {
 		SNAPSHOT_ERR_NOMEM(device, "SHADER MEMORY");
 		return 0;
 	}
 
-	header->type = block->statetype;
-	header->index = info->bank;
-	header->size = block->sz;
+	kgsl_regwrite(device, A5XX_HLSQ_DBG_READ_SEL,
+		  ((statetype << A5XX_SHADER_STATETYPE_SHIFT) | shader_num));
+
+	header->type = statetype;
+	header->index = shader_num;
+	header->size = size;
 
-	memcpy(data, registers.hostptr + info->offset,
-		block->sz * sizeof(unsigned int));
+	for (i = 0; i < size; i++)
+		kgsl_regread(device, A5XX_HLSQ_DBG_AHB_READ_APERTURE + i,
+				data++);
 
-	return SHADER_SECTION_SZ(block->sz);
+	return SHADER_SECTION_SZ(size);
 }
 
 static void a5xx_snapshot_shader(struct kgsl_device *device,
 			   struct kgsl_snapshot *snapshot)
 {
 	unsigned int i, j;
-	struct a5xx_shader_block_info info;
-
-	/* Shader blocks can only be read by the crash dumper */
-	if (crash_dump_valid == false)
-		return;
+	struct a5xx_shader_block_info blk;
 
 	for (i = 0; i < ARRAY_SIZE(a5xx_shader_blocks); i++) {
 		for (j = 0; j < A5XX_NUM_SHADER_BANKS; j++) {
-			info.block = &a5xx_shader_blocks[i];
-			info.bank = j;
-			info.offset = a5xx_shader_blocks[i].offset +
-				(j * a5xx_shader_blocks[i].sz);
-
+			blk.shader_block = &a5xx_shader_blocks[i];
+			blk.shader_num = j;
 			/* Shader working/shadow memory */
 			kgsl_snapshot_add_section(device,
 				KGSL_SNAPSHOT_SECTION_SHADER,
-				snapshot, a5xx_snapshot_shader_memory, &info);
+				snapshot, a5xx_snapshot_shader_memory, &blk);
 		}
 	}
 }
 
-/* Dump registers which get affected by crash dumper trigger */
-static size_t a5xx_snapshot_pre_crashdump_regs(struct kgsl_device *device,
-		u8 *buf, size_t remain, void *priv)
+static int get_hlsq_registers(struct kgsl_device *device,
+		const struct a5xx_hlsq_sp_tp_regs *regs, unsigned int *data)
 {
-	struct kgsl_snapshot_registers pre_cdregs = {
-			.regs = a5xx_pre_crashdumper_registers,
-			.count = ARRAY_SIZE(a5xx_pre_crashdumper_registers)/2,
-	};
-
-	return kgsl_snapshot_dump_registers(device, buf, remain, &pre_cdregs);
-}
+	int j;
+	unsigned int val;
 
-struct registers {
-	const unsigned int *regs;
-	size_t size;
-};
+	kgsl_regwrite(device, A5XX_HLSQ_DBG_READ_SEL,
+			(regs->statetype << A5XX_SHADER_STATETYPE_SHIFT));
 
-static size_t a5xx_legacy_snapshot_registers(struct kgsl_device *device,
-		u8 *buf, size_t remain, const unsigned int *regs, size_t size)
-{
-	struct kgsl_snapshot_registers snapshot_regs = {
-		.regs = regs,
-		.count = size / 2,
-	};
+	for (j = 0; j < regs->size; j++) {
+		kgsl_regread(device, A5XX_HLSQ_DBG_AHB_READ_APERTURE + j, &val);
+		*data++ = regs->ahbaddr + j;
+		*data++ = val;
+	}
 
-	return kgsl_snapshot_dump_registers(device, buf, remain,
-			&snapshot_regs);
+	return (regs->size * 2);
 }
 
-#define REG_PAIR_COUNT(_a, _i) \
-	(((_a)[(2 * (_i)) + 1] - (_a)[2 * (_i)]) + 1)
-
-static size_t a5xx_snapshot_registers(struct kgsl_device *device, u8 *buf,
-		size_t remain, void *priv)
+static size_t a5xx_snapshot_dump_hlsq_sp_tp_regs(struct kgsl_device *device,
+		u8 *buf, size_t remain, void *priv)
 {
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct kgsl_snapshot_regs *header = (struct kgsl_snapshot_regs *)buf;
 	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	unsigned int *src = (unsigned int *) registers.hostptr;
-	struct registers *regs = (struct registers *)priv;
-	unsigned int j, k;
-	unsigned int count = 0;
+	int count = 0, i;
+
+	/* Figure out how many registers we are going to dump */
+	for (i = 0; i < ARRAY_SIZE(a5xx_hlsq_sp_tp_registers); i++)
+			count += a5xx_hlsq_sp_tp_registers[i].size;
 
-	if (crash_dump_valid == false)
-		return a5xx_legacy_snapshot_registers(device, buf, remain,
-				regs->regs, regs->size);
+	/* the HLSQ non context registers cannot be dumped on A530v1 */
+	if (!adreno_is_a530v1(adreno_dev))
+		count += a5xx_hlsq_non_ctx_registers.size;
 
-	if (remain < sizeof(*header)) {
+	if (remain < (count * 8) + sizeof(*header)) {
 		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
 		return 0;
 	}
 
-	remain -= sizeof(*header);
-
-	for (j = 0; j < regs->size / 2; j++) {
-		unsigned int start = regs->regs[2 * j];
-		unsigned int end = regs->regs[(2 * j) + 1];
-
-		if (remain < ((end - start) + 1) * 8) {
-			SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-			goto out;
-		}
-
-		remain -= ((end - start) + 1) * 8;
+	for (i = 0; i < ARRAY_SIZE(a5xx_hlsq_sp_tp_registers); i++)
+		data += get_hlsq_registers(device,
+			&a5xx_hlsq_sp_tp_registers[i], data);
 
-		for (k = start; k <= end; k++, count++) {
-			*data++ = k;
-			*data++ = *src++;
-		}
-	}
+	if (!adreno_is_a530v1(adreno_dev))
+		data += get_hlsq_registers(device,
+			&a5xx_hlsq_non_ctx_registers, data);
 
-out:
 	header->count = count;
 
 	/* Return the size of the section */
 	return (count * 8) + sizeof(*header);
 }
 
-/* Snapshot a preemption record buffer */
-static size_t snapshot_preemption_record(struct kgsl_device *device, u8 *buf,
-	size_t remain, void *priv)
+static size_t a5xx_legacy_snapshot_registers(struct kgsl_device *device,
+		u8 *buf, size_t remain)
 {
-	struct kgsl_memdesc *memdesc = priv;
+	struct kgsl_snapshot_registers regs = {
+		.regs = a5xx_registers,
+		.count = ARRAY_SIZE(a5xx_registers) / 2,
+	};
 
-	struct kgsl_snapshot_gpu_object_v2 *header =
-		(struct kgsl_snapshot_gpu_object_v2 *)buf;
+	return kgsl_snapshot_dump_registers(device, buf, remain, &regs);
+}
 
-	u8 *ptr = buf + sizeof(*header);
+static struct kgsl_memdesc capturescript;
+static struct kgsl_memdesc registers;
 
-	if (remain < (SZ_64K + sizeof(*header))) {
-		SNAPSHOT_ERR_NOMEM(device, "PREEMPTION RECORD");
-		return 0;
-	}
+#define REG_PAIR_COUNT(_a, _i) \
+	(((_a)[(2 * (_i)) + 1] - (_a)[2 * (_i)]) + 1)
 
-	header->size = SZ_64K >> 2;
-	header->gpuaddr = memdesc->gpuaddr;
-	header->ptbase =
-		kgsl_mmu_pagetable_get_ttbr0(device->mmu.defaultpagetable);
-	header->type = SNAPSHOT_GPU_OBJECT_GLOBAL;
+static inline unsigned int count_registers(void)
+{
+	unsigned int i, count = 0;
 
-	memcpy(ptr, memdesc->hostptr, SZ_64K);
+	for (i = 0; i < ARRAY_SIZE(a5xx_registers) / 2; i++)
+		count += REG_PAIR_COUNT(a5xx_registers, i);
 
-	return SZ_64K + sizeof(*header);
+	return count;
 }
 
+static unsigned int copy_registers(unsigned int *dst)
+{
+	unsigned int *src = (unsigned int *) registers.hostptr;
+	unsigned int i, count = 0;
+
+	for (i = 0; i < ARRAY_SIZE(a5xx_registers) / 2; i++) {
+		unsigned int j;
+		unsigned int start = a5xx_registers[2 * i];
+		unsigned int end = a5xx_registers[(2 * i) + 1];
+
+		for (j = start; j <= end; j++, count++) {
+			*dst++ = j;
+			*dst++ = *src++;
+		}
+	}
+
+	return count;
+}
 
-static void _a5xx_do_crashdump(struct kgsl_device *device)
+static size_t a5xx_snapshot_registers(struct kgsl_device *device, u8 *buf,
+		size_t remain, void *priv)
 {
+	struct kgsl_snapshot_regs *header = (struct kgsl_snapshot_regs *)buf;
+	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
 	unsigned long wait_time;
 	unsigned int reg = 0;
 	unsigned int val;
 
-	crash_dump_valid = false;
-
-	if (!device->snapshot_crashdumper)
-		return;
+	/* Jump to legacy if the crash dump script was not initialized */
 	if (capturescript.gpuaddr == 0 || registers.gpuaddr == 0)
-		return;
+		return a5xx_legacy_snapshot_registers(device, buf, remain);
 
-	/* IF the SMMU is stalled we cannot do a crash dump */
+	/*
+	 * If we got here because we are stalled on fault the crash dumper has
+	 * won't work
+	 */
 	kgsl_regread(device, A5XX_RBBM_STATUS3, &val);
 	if (val & BIT(24))
-		return;
+		return a5xx_legacy_snapshot_registers(device, buf, remain);
 
-	/* Turn on APRIV so we can access the buffers */
-	kgsl_regwrite(device, A5XX_CP_CNTL, 1);
+	if (remain < (count_registers() * 8) + sizeof(*header)) {
+		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
+		return 0;
+	}
 
 	kgsl_regwrite(device, A5XX_CP_CRASH_SCRIPT_BASE_LO,
 			lower_32_bits(capturescript.gpuaddr));
@@ -801,54 +772,15 @@ static void _a5xx_do_crashdump(struct kgsl_device *device)
 		cpu_relax();
 	}
 
-	kgsl_regwrite(device, A5XX_CP_CNTL, 0);
-
 	if (!(reg & 0x4)) {
 		KGSL_CORE_ERR("Crash dump timed out: 0x%X\n", reg);
-		return;
-	}
-
-	crash_dump_valid = true;
-}
-
-static int get_hlsq_registers(struct kgsl_device *device,
-		const struct a5xx_hlsq_sp_tp_regs *regs, unsigned int *data)
-{
-	unsigned int i;
-	unsigned int *src = registers.hostptr + regs->offset;
-
-	for (i = 0; i < regs->size; i++) {
-		*data++ = regs->ahbaddr + i;
-		*data++ = *(src + i);
+		return a5xx_legacy_snapshot_registers(device, buf, remain);
 	}
 
-	return (2 * regs->size);
-}
-
-static size_t a5xx_snapshot_dump_hlsq_sp_tp_regs(struct kgsl_device *device,
-		u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_regs *header = (struct kgsl_snapshot_regs *)buf;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	int count = 0, i;
-
-	/* Figure out how many registers we are going to dump */
-	for (i = 0; i < ARRAY_SIZE(a5xx_hlsq_sp_tp_registers); i++)
-		count += a5xx_hlsq_sp_tp_registers[i].size;
-
-	if (remain < (count * 8) + sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-		return 0;
-	}
-
-	for (i = 0; i < ARRAY_SIZE(a5xx_hlsq_sp_tp_registers); i++)
-		data += get_hlsq_registers(device,
-				&a5xx_hlsq_sp_tp_registers[i], data);
-
-	header->count = count;
+	header->count = copy_registers(data);
 
 	/* Return the size of the section */
-	return (count * 8) + sizeof(*header);
+	return (header->count * 8) + sizeof(*header);
 }
 
 /*
@@ -865,40 +797,18 @@ void a5xx_snapshot(struct adreno_device *adreno_dev,
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	struct adreno_snapshot_data *snap_data = gpudev->snapshot_data;
-	unsigned int reg, i;
-	struct adreno_ringbuffer *rb;
-	struct registers regs;
+	unsigned int reg;
 
 	/* Disable Clock gating temporarily for the debug bus to work */
 	a5xx_hwcg_set(adreno_dev, false);
 
-	/* Dump the registers which get affected by crash dumper trigger */
 	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_REGS,
-		snapshot, a5xx_snapshot_pre_crashdump_regs, NULL);
+		snapshot, a5xx_snapshot_registers, NULL);
 
-	/* Dump vbif registers as well which get affected by crash dumper */
 	adreno_snapshot_vbif_registers(device, snapshot,
 		a5xx_vbif_snapshot_registers,
 		ARRAY_SIZE(a5xx_vbif_snapshot_registers));
 
-	/* Try to run the crash dumper */
-	_a5xx_do_crashdump(device);
-
-	regs.regs = a5xx_registers;
-	regs.size = ARRAY_SIZE(a5xx_registers);
-
-	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_REGS, snapshot,
-			a5xx_snapshot_registers, &regs);
-
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_GPMU)) {
-		regs.regs = a5xx_gpmu_registers;
-		regs.size = ARRAY_SIZE(a5xx_gpmu_registers);
-
-		kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_REGS,
-				snapshot, a5xx_snapshot_registers, &regs);
-	}
-
-
 	/* Dump SP TP HLSQ registers */
 	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_REGS, snapshot,
 		a5xx_snapshot_dump_hlsq_sp_tp_regs, NULL);
@@ -908,33 +818,33 @@ void a5xx_snapshot(struct adreno_device *adreno_dev,
 		A5XX_CP_PFP_STAT_ADDR, A5XX_CP_PFP_STAT_DATA,
 		0, snap_data->sect_sizes->cp_pfp);
 
-	/* CP_ME indexed registers */
-	kgsl_snapshot_indexed_registers(device, snapshot,
+	 /* CP_ME indexed registers */
+	 kgsl_snapshot_indexed_registers(device, snapshot,
 		A5XX_CP_ME_STAT_ADDR, A5XX_CP_ME_STAT_DATA,
 		0, snap_data->sect_sizes->cp_me);
 
-	/* CP_DRAW_STATE */
-	kgsl_snapshot_indexed_registers(device, snapshot,
+	 /* CP_DRAW_STATE */
+	 kgsl_snapshot_indexed_registers(device, snapshot,
 		A5XX_CP_DRAW_STATE_ADDR, A5XX_CP_DRAW_STATE_DATA,
-		0, 1 << A5XX_CP_DRAW_STATE_ADDR_WIDTH);
+		0, 128);
 
-	/*
-	 * CP needs to be halted on a530v1 before reading CP_PFP_UCODE_DBG_DATA
-	 * and CP_PM4_UCODE_DBG_DATA registers
-	 */
-	if (adreno_is_a530v1(adreno_dev)) {
+	 /*
+	  * CP needs to be halted on a530v1 before reading CP_PFP_UCODE_DBG_DATA
+	  * and CP_PM4_UCODE_DBG_DATA registers
+	  */
+	 if (adreno_is_a530v1(adreno_dev)) {
 		adreno_readreg(adreno_dev, ADRENO_REG_CP_ME_CNTL, &reg);
 		reg |= (1 << 27) | (1 << 28);
 		adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_CNTL, reg);
-	}
+	 }
 
-	/* ME_UCODE Cache */
-	kgsl_snapshot_indexed_registers(device, snapshot,
+	 /* ME_UCODE Cache */
+	 kgsl_snapshot_indexed_registers(device, snapshot,
 		A5XX_CP_ME_UCODE_DBG_ADDR, A5XX_CP_ME_UCODE_DBG_DATA,
 		0, 0x53F);
 
-	/* PFP_UCODE Cache */
-	kgsl_snapshot_indexed_registers(device, snapshot,
+	 /* PFP_UCODE Cache */
+	 kgsl_snapshot_indexed_registers(device, snapshot,
 		A5XX_CP_PFP_UCODE_DBG_ADDR, A5XX_CP_PFP_UCODE_DBG_DATA,
 		0, 0x53F);
 
@@ -964,179 +874,53 @@ void a5xx_snapshot(struct adreno_device *adreno_dev,
 
 	/* Debug bus */
 	a5xx_snapshot_debugbus(device, snapshot);
-
-	/* Preemption record */
-	if (adreno_is_preemption_enabled(adreno_dev)) {
-		FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-			kgsl_snapshot_add_section(device,
-				KGSL_SNAPSHOT_SECTION_GPU_OBJECT_V2,
-				snapshot, snapshot_preemption_record,
-				&rb->preemption_desc);
-		}
-	}
-
-}
-
-static int _a5xx_crashdump_init_shader(struct a5xx_shader_block *block,
-		uint64_t *ptr, uint64_t *offset)
-{
-	int qwords = 0;
-	unsigned int j;
-
-	/* Capture each bank in the block */
-	for (j = 0; j < A5XX_NUM_SHADER_BANKS; j++) {
-		/* Program the aperture */
-		ptr[qwords++] =
-			(block->statetype << A5XX_SHADER_STATETYPE_SHIFT) | j;
-		ptr[qwords++] = (((uint64_t) A5XX_HLSQ_DBG_READ_SEL << 44)) |
-			(1 << 21) | 1;
-
-		/* Read all the data in one chunk */
-		ptr[qwords++] = registers.gpuaddr + *offset;
-		ptr[qwords++] =
-			(((uint64_t) A5XX_HLSQ_DBG_AHB_READ_APERTURE << 44)) |
-			block->sz;
-
-		/* Remember the offset of the first bank for easy access */
-		if (j == 0)
-			block->offset = *offset;
-
-		*offset += block->sz * sizeof(unsigned int);
-	}
-
-	return qwords;
-}
-
-static int _a5xx_crashdump_init_hlsq(struct a5xx_hlsq_sp_tp_regs *regs,
-		uint64_t *ptr, uint64_t *offset)
-{
-	int qwords = 0;
-
-	/* Program the aperture */
-	ptr[qwords++] =
-		(regs->statetype << A5XX_SHADER_STATETYPE_SHIFT);
-	ptr[qwords++] = (((uint64_t) A5XX_HLSQ_DBG_READ_SEL << 44)) |
-		(1 << 21) | 1;
-
-	/* Read all the data in one chunk */
-	ptr[qwords++] = registers.gpuaddr + *offset;
-	ptr[qwords++] =
-		(((uint64_t) A5XX_HLSQ_DBG_AHB_READ_APERTURE << 44)) |
-		regs->size;
-
-	/* Remember the offset of the first bank for easy access */
-	regs->offset = *offset;
-
-	*offset += regs->size * sizeof(unsigned int);
-
-	return qwords;
 }
 
 void a5xx_crashdump_init(struct adreno_device *adreno_dev)
 {
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int script_size = 0;
-	unsigned int data_size = 0;
-	unsigned int i, j;
+	unsigned int i, count;
 	uint64_t *ptr;
-	uint64_t offset = 0;
+	uint64_t gpuaddr;
 
 	if (capturescript.gpuaddr != 0 && registers.gpuaddr != 0)
 		return;
 
 	/*
-	 * We need to allocate two buffers:
-	 * 1 - the buffer to hold the draw script
-	 * 2 - the buffer to hold the data
-	 */
-
-	/*
-	 * To save the registers, we need 16 bytes per register pair for the
-	 * script and a dword for each register int the data
+	 * For the capture script two blocks of memory are needed: A block of
+	 * GPU readonly memory for the special capture script and a destination
+	 * block for the register values. The size of the capture script needs
+	 * is 128 bits (4 dwords) per register pair and 4 dwords at the end.
+	 * The destination block needs to be big enough to hold all the
+	 * registers that we will capture.
 	 */
 
-	/* Each pair needs 16 bytes (2 qwords) */
-	script_size += (ARRAY_SIZE(a5xx_registers) / 2) * 16;
-
-	/* Each register needs a dword in the data */
-	for (j = 0; j < ARRAY_SIZE(a5xx_registers) / 2; j++)
-		data_size += REG_PAIR_COUNT(a5xx_registers, j) *
-			sizeof(unsigned int);
-
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_GPMU)) {
-		/* Each pair needs 16 bytes (2 qwords) */
-		script_size += (ARRAY_SIZE(a5xx_gpmu_registers) / 2) * 16;
-
-		/* Each register needs a dword in the data */
-		for (j = 0; j < ARRAY_SIZE(a5xx_gpmu_registers) / 2; j++)
-			data_size += REG_PAIR_COUNT(a5xx_gpmu_registers, j) *
-				sizeof(unsigned int);
-	}
-
-	/*
-	 * To save the shader blocks for each block in each type we need 32
-	 * bytes for the script (16 bytes to program the aperture and 16 to
-	 * read the data) and then a block specific number of bytes to hold
-	 * the data
-	 */
-	for (i = 0; i < ARRAY_SIZE(a5xx_shader_blocks); i++) {
-		script_size += 32 * A5XX_NUM_SHADER_BANKS;
-		data_size += a5xx_shader_blocks[i].sz * sizeof(unsigned int) *
-			A5XX_NUM_SHADER_BANKS;
-	}
-	for (i = 0; i < ARRAY_SIZE(a5xx_hlsq_sp_tp_registers); i++) {
-		script_size += 32;
-		data_size +=
-		a5xx_hlsq_sp_tp_registers[i].size * sizeof(unsigned int);
-	}
-
-	/* Now allocate the script and data buffers */
-
-	/* The script buffers needs 2 extra qwords on the end */
-	if (kgsl_allocate_global(device, &capturescript,
-		script_size + 16, KGSL_MEMFLAGS_GPUREADONLY,
-		KGSL_MEMDESC_PRIVILEGED, "capturescript"))
+	if (kgsl_allocate_global(KGSL_DEVICE(adreno_dev), &capturescript,
+		((ARRAY_SIZE(a5xx_registers) / 2) * 16) + 16,
+		KGSL_MEMFLAGS_GPUREADONLY, 0))
 		return;
 
-	if (kgsl_allocate_global(device, &registers, data_size, 0,
-		KGSL_MEMDESC_PRIVILEGED, "capturescript_regs")) {
+	/* Count the total number of registers to capture */
+	count = count_registers();
+
+	if (kgsl_allocate_global(KGSL_DEVICE(adreno_dev), &registers,
+		count * sizeof(unsigned int), 0, 0)) {
 		kgsl_free_global(KGSL_DEVICE(adreno_dev), &capturescript);
 		return;
 	}
+
 	/* Build the crash script */
 
 	ptr = (uint64_t *) capturescript.hostptr;
+	gpuaddr = registers.gpuaddr;
 
-	/* For the registers, program a read command for each pair */
+	for (i = 0; i < ARRAY_SIZE(a5xx_registers) / 2; i++) {
+		unsigned int regs = REG_PAIR_COUNT(a5xx_registers, i);
+		*ptr++ = gpuaddr;
+		*ptr++ = (((uint64_t) a5xx_registers[2 * i]) << 44) | regs;
 
-	for (j = 0; j < ARRAY_SIZE(a5xx_registers) / 2; j++) {
-		unsigned int r = REG_PAIR_COUNT(a5xx_registers, j);
-		*ptr++ = registers.gpuaddr + offset;
-		*ptr++ = (((uint64_t) a5xx_registers[2 * j]) << 44)
-			| r;
-		offset += r * sizeof(unsigned int);
+		gpuaddr += regs * sizeof(unsigned int);
 	}
 
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_GPMU)) {
-		for (j = 0; j < ARRAY_SIZE(a5xx_gpmu_registers) / 2; j++) {
-			unsigned int r = REG_PAIR_COUNT(a5xx_gpmu_registers, j);
-			*ptr++ = registers.gpuaddr + offset;
-			*ptr++ = (((uint64_t) a5xx_gpmu_registers[2 * j]) << 44)
-				| r;
-			offset += r * sizeof(unsigned int);
-		}
-	}
-
-	/* Program each shader block */
-	for (i = 0; i < ARRAY_SIZE(a5xx_shader_blocks); i++) {
-		ptr += _a5xx_crashdump_init_shader(&a5xx_shader_blocks[i], ptr,
-			&offset);
-	}
-	/* Program the hlsq sp tp register sets */
-	for (i = 0; i < ARRAY_SIZE(a5xx_hlsq_sp_tp_registers); i++)
-		ptr += _a5xx_crashdump_init_hlsq(&a5xx_hlsq_sp_tp_registers[i],
-			ptr, &offset);
-
 	*ptr++ = 0;
 	*ptr++ = 0;
 }
diff --git a/drivers/gpu/msm/adreno_a6xx.c b/drivers/gpu/msm/adreno_a6xx.c
deleted file mode 100644
index 517b813b4dcc..000000000000
--- a/drivers/gpu/msm/adreno_a6xx.c
+++ /dev/null
@@ -1,3952 +0,0 @@
-/* Copyright (c) 2017-2018, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-#include <linux/firmware.h>
-#include <soc/qcom/subsystem_restart.h>
-#include <linux/pm_opp.h>
-#include <linux/jiffies.h>
-
-#include "adreno.h"
-#include "a6xx_reg.h"
-#include "adreno_a6xx.h"
-#include "adreno_cp_parser.h"
-#include "adreno_trace.h"
-#include "adreno_pm4types.h"
-#include "adreno_perfcounter.h"
-#include "adreno_ringbuffer.h"
-#include "adreno_llc.h"
-#include "kgsl_sharedmem.h"
-#include "kgsl_log.h"
-#include "kgsl.h"
-#include "kgsl_gmu.h"
-#include "kgsl_trace.h"
-
-#define MIN_HBB		13
-
-#define A6XX_LLC_NUM_GPU_SCIDS		5
-#define A6XX_GPU_LLC_SCID_NUM_BITS	5
-#define A6XX_GPU_LLC_SCID_MASK \
-	((1 << (A6XX_LLC_NUM_GPU_SCIDS * A6XX_GPU_LLC_SCID_NUM_BITS)) - 1)
-#define A6XX_GPUHTW_LLC_SCID_SHIFT	25
-#define A6XX_GPUHTW_LLC_SCID_MASK \
-	(((1 << A6XX_GPU_LLC_SCID_NUM_BITS) - 1) << A6XX_GPUHTW_LLC_SCID_SHIFT)
-
-#define A6XX_GPU_CX_REG_BASE		0x509E000
-#define A6XX_GPU_CX_REG_SIZE		0x1000
-
-#define GPU_LIMIT_THRESHOLD_ENABLE	BIT(31)
-
-static int _load_gmu_firmware(struct kgsl_device *device);
-
-static const struct adreno_vbif_data a630_vbif[] = {
-	{A6XX_VBIF_GATE_OFF_WRREQ_EN, 0x00000009},
-	{A6XX_RBBM_VBIF_CLIENT_QOS_CNTL, 0x3},
-	{0, 0},
-};
-
-static const struct adreno_vbif_data a615_gbif[] = {
-	{A6XX_RBBM_VBIF_CLIENT_QOS_CNTL, 0x3},
-	{0, 0},
-};
-
-static const struct adreno_vbif_platform a6xx_vbif_platforms[] = {
-	{ adreno_is_a630, a630_vbif },
-	{ adreno_is_a615, a615_gbif },
-	{ adreno_is_a616, a615_gbif },
-};
-
-
-static unsigned long a6xx_oob_state_bitmask;
-
-struct kgsl_hwcg_reg {
-	unsigned int off;
-	unsigned int val;
-};
-static const struct kgsl_hwcg_reg a630_hwcg_regs[] = {
-	{A6XX_RBBM_CLOCK_CNTL_SP0, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL_SP1, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL_SP2, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL_SP3, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL2_SP0, 0x02022220},
-	{A6XX_RBBM_CLOCK_CNTL2_SP1, 0x02022220},
-	{A6XX_RBBM_CLOCK_CNTL2_SP2, 0x02022220},
-	{A6XX_RBBM_CLOCK_CNTL2_SP3, 0x02022220},
-	{A6XX_RBBM_CLOCK_DELAY_SP0, 0x00000080},
-	{A6XX_RBBM_CLOCK_DELAY_SP1, 0x00000080},
-	{A6XX_RBBM_CLOCK_DELAY_SP2, 0x00000080},
-	{A6XX_RBBM_CLOCK_DELAY_SP3, 0x00000080},
-	{A6XX_RBBM_CLOCK_HYST_SP0, 0x0000F3CF},
-	{A6XX_RBBM_CLOCK_HYST_SP1, 0x0000F3CF},
-	{A6XX_RBBM_CLOCK_HYST_SP2, 0x0000F3CF},
-	{A6XX_RBBM_CLOCK_HYST_SP3, 0x0000F3CF},
-	{A6XX_RBBM_CLOCK_CNTL_TP0, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL_TP1, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL_TP2, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL_TP3, 0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL2_TP0, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_TP1, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_TP2, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_TP3, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_TP0, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_TP1, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_TP2, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_TP3, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL4_TP0, 0x00022222},
-	{A6XX_RBBM_CLOCK_CNTL4_TP1, 0x00022222},
-	{A6XX_RBBM_CLOCK_CNTL4_TP2, 0x00022222},
-	{A6XX_RBBM_CLOCK_CNTL4_TP3, 0x00022222},
-	{A6XX_RBBM_CLOCK_HYST_TP0, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST_TP1, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST_TP2, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST_TP3, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST2_TP0, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST2_TP1, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST2_TP2, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST2_TP3, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST3_TP0, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST3_TP1, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST3_TP2, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST3_TP3, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST4_TP0, 0x00077777},
-	{A6XX_RBBM_CLOCK_HYST4_TP1, 0x00077777},
-	{A6XX_RBBM_CLOCK_HYST4_TP2, 0x00077777},
-	{A6XX_RBBM_CLOCK_HYST4_TP3, 0x00077777},
-	{A6XX_RBBM_CLOCK_DELAY_TP0, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY_TP1, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY_TP2, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY_TP3, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY2_TP0, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY2_TP1, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY2_TP2, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY2_TP3, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY3_TP0, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY3_TP1, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY3_TP2, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY3_TP3, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY4_TP0, 0x00011111},
-	{A6XX_RBBM_CLOCK_DELAY4_TP1, 0x00011111},
-	{A6XX_RBBM_CLOCK_DELAY4_TP2, 0x00011111},
-	{A6XX_RBBM_CLOCK_DELAY4_TP3, 0x00011111},
-	{A6XX_RBBM_CLOCK_CNTL_UCHE, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_UCHE, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_UCHE, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL4_UCHE, 0x00222222},
-	{A6XX_RBBM_CLOCK_HYST_UCHE, 0x00000004},
-	{A6XX_RBBM_CLOCK_DELAY_UCHE, 0x00000002},
-	{A6XX_RBBM_CLOCK_CNTL_RB0, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL_RB1, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL_RB2, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL_RB3, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_RB0, 0x00002222},
-	{A6XX_RBBM_CLOCK_CNTL2_RB1, 0x00002222},
-	{A6XX_RBBM_CLOCK_CNTL2_RB2, 0x00002222},
-	{A6XX_RBBM_CLOCK_CNTL2_RB3, 0x00002222},
-	{A6XX_RBBM_CLOCK_CNTL_CCU0, 0x00002220},
-	{A6XX_RBBM_CLOCK_CNTL_CCU1, 0x00002220},
-	{A6XX_RBBM_CLOCK_CNTL_CCU2, 0x00002220},
-	{A6XX_RBBM_CLOCK_CNTL_CCU3, 0x00002220},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU0, 0x00040F00},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU1, 0x00040F00},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU2, 0x00040F00},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU3, 0x00040F00},
-	{A6XX_RBBM_CLOCK_CNTL_RAC, 0x05022022},
-	{A6XX_RBBM_CLOCK_CNTL2_RAC, 0x00005555},
-	{A6XX_RBBM_CLOCK_DELAY_RAC, 0x00000011},
-	{A6XX_RBBM_CLOCK_HYST_RAC, 0x00445044},
-	{A6XX_RBBM_CLOCK_CNTL_TSE_RAS_RBBM, 0x04222222},
-	{A6XX_RBBM_CLOCK_MODE_GPC, 0x00222222},
-	{A6XX_RBBM_CLOCK_MODE_VFD, 0x00002222},
-	{A6XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00000000},
-	{A6XX_RBBM_CLOCK_HYST_GPC, 0x04104004},
-	{A6XX_RBBM_CLOCK_HYST_VFD, 0x00000000},
-	{A6XX_RBBM_CLOCK_DELAY_HLSQ, 0x00000000},
-	{A6XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00004000},
-	{A6XX_RBBM_CLOCK_DELAY_GPC, 0x00000200},
-	{A6XX_RBBM_CLOCK_DELAY_VFD, 0x00002222},
-	{A6XX_RBBM_CLOCK_DELAY_HLSQ_2, 0x00000002},
-	{A6XX_RBBM_CLOCK_MODE_HLSQ, 0x00002222},
-	{A6XX_RBBM_CLOCK_CNTL_GMU_GX, 0x00000222},
-	{A6XX_RBBM_CLOCK_DELAY_GMU_GX, 0x00000111},
-	{A6XX_RBBM_CLOCK_HYST_GMU_GX, 0x00000555}
-};
-
-static const struct kgsl_hwcg_reg a615_hwcg_regs[] = {
-	{A6XX_RBBM_CLOCK_CNTL_SP0,  0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL2_SP0, 0x02222220},
-	{A6XX_RBBM_CLOCK_DELAY_SP0, 0x00000080},
-	{A6XX_RBBM_CLOCK_HYST_SP0,  0x0000F3CF},
-	{A6XX_RBBM_CLOCK_CNTL_TP0,  0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL_TP1,  0x02222222},
-	{A6XX_RBBM_CLOCK_CNTL2_TP0, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_TP1, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_TP0, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_TP1, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL4_TP0, 0x00022222},
-	{A6XX_RBBM_CLOCK_CNTL4_TP1, 0x00022222},
-	{A6XX_RBBM_CLOCK_HYST_TP0,  0x77777777},
-	{A6XX_RBBM_CLOCK_HYST_TP1,  0x77777777},
-	{A6XX_RBBM_CLOCK_HYST2_TP0, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST2_TP1, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST3_TP0, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST3_TP1, 0x77777777},
-	{A6XX_RBBM_CLOCK_HYST4_TP0, 0x00077777},
-	{A6XX_RBBM_CLOCK_HYST4_TP1, 0x00077777},
-	{A6XX_RBBM_CLOCK_DELAY_TP0, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY_TP1, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY2_TP0, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY2_TP1, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY3_TP0, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY3_TP1, 0x11111111},
-	{A6XX_RBBM_CLOCK_DELAY4_TP0, 0x00011111},
-	{A6XX_RBBM_CLOCK_DELAY4_TP1, 0x00011111},
-	{A6XX_RBBM_CLOCK_CNTL_UCHE,  0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_UCHE, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL3_UCHE, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL4_UCHE, 0x00222222},
-	{A6XX_RBBM_CLOCK_HYST_UCHE,  0x00000004},
-	{A6XX_RBBM_CLOCK_DELAY_UCHE, 0x00000002},
-	{A6XX_RBBM_CLOCK_CNTL_RB0, 0x22222222},
-	{A6XX_RBBM_CLOCK_CNTL2_RB0, 0x00002222},
-	{A6XX_RBBM_CLOCK_CNTL_CCU0, 0x00002220},
-	{A6XX_RBBM_CLOCK_CNTL_CCU1, 0x00002220},
-	{A6XX_RBBM_CLOCK_CNTL_CCU2, 0x00002220},
-	{A6XX_RBBM_CLOCK_CNTL_CCU3, 0x00002220},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU0, 0x00040F00},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU1, 0x00040F00},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU2, 0x00040F00},
-	{A6XX_RBBM_CLOCK_HYST_RB_CCU3, 0x00040F00},
-	{A6XX_RBBM_CLOCK_CNTL_RAC, 0x05022022},
-	{A6XX_RBBM_CLOCK_CNTL2_RAC, 0x00005555},
-	{A6XX_RBBM_CLOCK_DELAY_RAC, 0x00000011},
-	{A6XX_RBBM_CLOCK_HYST_RAC, 0x00445044},
-	{A6XX_RBBM_CLOCK_CNTL_TSE_RAS_RBBM, 0x04222222},
-	{A6XX_RBBM_CLOCK_MODE_GPC, 0x00222222},
-	{A6XX_RBBM_CLOCK_MODE_VFD, 0x00002222},
-	{A6XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00000000},
-	{A6XX_RBBM_CLOCK_HYST_GPC, 0x04104004},
-	{A6XX_RBBM_CLOCK_HYST_VFD, 0x00000000},
-	{A6XX_RBBM_CLOCK_DELAY_HLSQ, 0x00000000},
-	{A6XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00004000},
-	{A6XX_RBBM_CLOCK_DELAY_GPC, 0x00000200},
-	{A6XX_RBBM_CLOCK_DELAY_VFD, 0x00002222},
-	{A6XX_RBBM_CLOCK_DELAY_HLSQ_2, 0x00000002},
-	{A6XX_RBBM_CLOCK_MODE_HLSQ, 0x00002222},
-	{A6XX_RBBM_CLOCK_CNTL_GMU_GX, 0x00000222},
-	{A6XX_RBBM_CLOCK_DELAY_GMU_GX, 0x00000111},
-	{A6XX_RBBM_CLOCK_HYST_GMU_GX, 0x00000555}
-};
-
-static const struct {
-	int (*devfunc)(struct adreno_device *adreno_dev);
-	const struct kgsl_hwcg_reg *regs;
-	unsigned int count;
-} a6xx_hwcg_registers[] = {
-	{adreno_is_a630, a630_hwcg_regs, ARRAY_SIZE(a630_hwcg_regs)},
-	{adreno_is_a615, a615_hwcg_regs, ARRAY_SIZE(a615_hwcg_regs)},
-	{adreno_is_a616, a615_hwcg_regs, ARRAY_SIZE(a615_hwcg_regs)},
-};
-
-static struct a6xx_protected_regs {
-	unsigned int base;
-	unsigned int count;
-	int read_protect;
-} a6xx_protected_regs_group[] = {
-	{ 0x600, 0x51, 0 },
-	{ 0xAE50, 0x2, 1 },
-	{ 0x9624, 0x13, 1 },
-	{ 0x8630, 0x8, 1 },
-	{ 0x9E70, 0x1, 1 },
-	{ 0x9E78, 0x187, 1 },
-	{ 0xF000, 0x810, 1 },
-	{ 0xFC00, 0x3, 0 },
-	{ 0x50E, 0x0, 1 },
-	{ 0x50F, 0x0, 0 },
-	{ 0x510, 0x0, 1 },
-	{ 0x0, 0x4F9, 0 },
-	{ 0x501, 0xA, 0 },
-	{ 0x511, 0x44, 0 },
-	{ 0xE00, 0x1, 1 },
-	{ 0xE03, 0xB, 1 },
-	{ 0x8E00, 0x0, 1 },
-	{ 0x8E50, 0xF, 1 },
-	{ 0xBE02, 0x0, 1 },
-	{ 0xBE20, 0x11F3, 1 },
-	{ 0x800, 0x82, 1 },
-	{ 0x8A0, 0x8, 1 },
-	{ 0x8AB, 0x19, 1 },
-	{ 0x900, 0x4D, 1 },
-	{ 0x98D, 0x76, 1 },
-	{ 0x8D0, 0x23, 0 },
-	{ 0x980, 0x4, 0 },
-	{ 0xA630, 0x0, 1 },
-};
-
-/* IFPC & Preemption static powerup restore list */
-static struct reg_list_pair {
-	uint32_t offset;
-	uint32_t val;
-} a6xx_pwrup_reglist[] = {
-	{ A6XX_VSC_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_GRAS_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_RB_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_PC_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_HLSQ_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_VFD_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_VPC_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_UCHE_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_SP_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_TPL1_ADDR_MODE_CNTL, 0x0 },
-	{ A6XX_UCHE_WRITE_RANGE_MAX_LO, 0x0 },
-	{ A6XX_UCHE_WRITE_RANGE_MAX_HI, 0x0 },
-	{ A6XX_UCHE_TRAP_BASE_LO, 0x0 },
-	{ A6XX_UCHE_TRAP_BASE_HI, 0x0 },
-	{ A6XX_UCHE_WRITE_THRU_BASE_LO, 0x0 },
-	{ A6XX_UCHE_WRITE_THRU_BASE_HI, 0x0 },
-	{ A6XX_UCHE_GMEM_RANGE_MIN_LO, 0x0 },
-	{ A6XX_UCHE_GMEM_RANGE_MIN_HI, 0x0 },
-	{ A6XX_UCHE_GMEM_RANGE_MAX_LO, 0x0 },
-	{ A6XX_UCHE_GMEM_RANGE_MAX_HI, 0x0 },
-	{ A6XX_UCHE_FILTER_CNTL, 0x0 },
-	{ A6XX_UCHE_CACHE_WAYS, 0x0 },
-	{ A6XX_UCHE_MODE_CNTL, 0x0 },
-	{ A6XX_RB_NC_MODE_CNTL, 0x0 },
-	{ A6XX_TPL1_NC_MODE_CNTL, 0x0 },
-	{ A6XX_SP_NC_MODE_CNTL, 0x0 },
-	{ A6XX_PC_DBG_ECO_CNTL, 0x0 },
-	{ A6XX_RB_CONTEXT_SWITCH_GMEM_SAVE_RESTORE, 0x0 },
-};
-
-/* IFPC only static powerup restore list */
-static struct reg_list_pair a6xx_ifpc_pwrup_reglist[] = {
-	{ A6XX_RBBM_VBIF_CLIENT_QOS_CNTL, 0x0 },
-	{ A6XX_CP_CHICKEN_DBG, 0x0 },
-	{ A6XX_CP_DBG_ECO_CNTL, 0x0 },
-	{ A6XX_CP_PROTECT_CNTL, 0x0 },
-	{ A6XX_CP_PROTECT_REG, 0x0 },
-	{ A6XX_CP_PROTECT_REG+1, 0x0 },
-	{ A6XX_CP_PROTECT_REG+2, 0x0 },
-	{ A6XX_CP_PROTECT_REG+3, 0x0 },
-	{ A6XX_CP_PROTECT_REG+4, 0x0 },
-	{ A6XX_CP_PROTECT_REG+5, 0x0 },
-	{ A6XX_CP_PROTECT_REG+6, 0x0 },
-	{ A6XX_CP_PROTECT_REG+7, 0x0 },
-	{ A6XX_CP_PROTECT_REG+8, 0x0 },
-	{ A6XX_CP_PROTECT_REG+9, 0x0 },
-	{ A6XX_CP_PROTECT_REG+10, 0x0 },
-	{ A6XX_CP_PROTECT_REG+11, 0x0 },
-	{ A6XX_CP_PROTECT_REG+12, 0x0 },
-	{ A6XX_CP_PROTECT_REG+13, 0x0 },
-	{ A6XX_CP_PROTECT_REG+14, 0x0 },
-	{ A6XX_CP_PROTECT_REG+15, 0x0 },
-	{ A6XX_CP_PROTECT_REG+16, 0x0 },
-	{ A6XX_CP_PROTECT_REG+17, 0x0 },
-	{ A6XX_CP_PROTECT_REG+18, 0x0 },
-	{ A6XX_CP_PROTECT_REG+19, 0x0 },
-	{ A6XX_CP_PROTECT_REG+20, 0x0 },
-	{ A6XX_CP_PROTECT_REG+21, 0x0 },
-	{ A6XX_CP_PROTECT_REG+22, 0x0 },
-	{ A6XX_CP_PROTECT_REG+23, 0x0 },
-	{ A6XX_CP_PROTECT_REG+24, 0x0 },
-	{ A6XX_CP_PROTECT_REG+25, 0x0 },
-	{ A6XX_CP_PROTECT_REG+26, 0x0 },
-	{ A6XX_CP_PROTECT_REG+27, 0x0 },
-	{ A6XX_CP_PROTECT_REG+28, 0x0 },
-	{ A6XX_CP_PROTECT_REG+29, 0x0 },
-	{ A6XX_CP_PROTECT_REG+30, 0x0 },
-	{ A6XX_CP_PROTECT_REG+31, 0x0 },
-	{ A6XX_CP_AHB_CNTL, 0x0 },
-};
-
-static struct reg_list_pair a615_pwrup_reglist[] = {
-	{ A6XX_UCHE_GBIF_GX_CONFIG, 0x0 },
-};
-
-static void _update_always_on_regs(struct adreno_device *adreno_dev)
-{
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	unsigned int *const regs = gpudev->reg_offsets->offsets;
-
-	regs[ADRENO_REG_RBBM_ALWAYSON_COUNTER_LO] =
-		A6XX_CP_ALWAYS_ON_COUNTER_LO;
-	regs[ADRENO_REG_RBBM_ALWAYSON_COUNTER_HI] =
-		A6XX_CP_ALWAYS_ON_COUNTER_HI;
-}
-
-static uint64_t read_AO_counter(struct kgsl_device *device)
-{
-	unsigned int l, h, h1;
-
-	kgsl_gmu_regread(device, A6XX_GMU_CX_GMU_ALWAYS_ON_COUNTER_H, &h);
-	kgsl_gmu_regread(device, A6XX_GMU_CX_GMU_ALWAYS_ON_COUNTER_L, &l);
-	kgsl_gmu_regread(device, A6XX_GMU_CX_GMU_ALWAYS_ON_COUNTER_H, &h1);
-
-	if (h == h1)
-		return (uint64_t) l | ((uint64_t) h << 32);
-
-	kgsl_gmu_regread(device, A6XX_GMU_CX_GMU_ALWAYS_ON_COUNTER_L, &l);
-	return (uint64_t) l | ((uint64_t) h1 << 32);
-}
-
-static void a6xx_pwrup_reglist_init(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (kgsl_allocate_global(device, &adreno_dev->pwrup_reglist,
-		PAGE_SIZE, 0, KGSL_MEMDESC_CONTIG | KGSL_MEMDESC_PRIVILEGED,
-		"powerup_register_list")) {
-		adreno_dev->pwrup_reglist.gpuaddr = 0;
-		return;
-	}
-
-	kgsl_sharedmem_set(device, &adreno_dev->pwrup_reglist, 0, 0,
-		PAGE_SIZE);
-}
-
-static void a6xx_init(struct adreno_device *adreno_dev)
-{
-	a6xx_crashdump_init(adreno_dev);
-
-	/*
-	 * If the GMU is not enabled, rewrite the offset for the always on
-	 * counters to point to the CP always on instead of GMU always on
-	 */
-	if (!kgsl_gmu_isenabled(KGSL_DEVICE(adreno_dev)))
-		_update_always_on_regs(adreno_dev);
-
-	a6xx_pwrup_reglist_init(adreno_dev);
-}
-
-/**
- * a6xx_protect_init() - Initializes register protection on a6xx
- * @device: Pointer to the device structure
- * Performs register writes to enable protected access to sensitive
- * registers
- */
-static void a6xx_protect_init(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_protected_registers *mmu_prot =
-		kgsl_mmu_get_prot_regs(&device->mmu);
-	int i, num_sets;
-	int req_sets = ARRAY_SIZE(a6xx_protected_regs_group);
-	int max_sets = adreno_dev->gpucore->num_protected_regs;
-	unsigned int mmu_base = 0, mmu_range = 0, cur_range;
-
-	/* enable access protection to privileged registers */
-	kgsl_regwrite(device, A6XX_CP_PROTECT_CNTL, 0x00000003);
-
-	if (mmu_prot) {
-		mmu_base = mmu_prot->base;
-		mmu_range = mmu_prot->range;
-		req_sets += DIV_ROUND_UP(mmu_range, 0x2000);
-	}
-
-	if (req_sets > max_sets)
-		WARN(1, "Size exceeds the num of protection regs available\n");
-
-	/* Protect GPU registers */
-	num_sets = min_t(unsigned int,
-		ARRAY_SIZE(a6xx_protected_regs_group), max_sets);
-	for (i = 0; i < num_sets; i++) {
-		struct a6xx_protected_regs *regs =
-					&a6xx_protected_regs_group[i];
-
-		kgsl_regwrite(device, A6XX_CP_PROTECT_REG + i,
-				regs->base | (regs->count << 18) |
-				(regs->read_protect << 31));
-	}
-
-	/* Protect MMU registers */
-	if (mmu_prot) {
-		while ((i < max_sets) && (mmu_range > 0)) {
-			cur_range = min_t(unsigned int, mmu_range,
-						0x2000);
-			kgsl_regwrite(device, A6XX_CP_PROTECT_REG + i,
-				mmu_base | ((cur_range - 1) << 18) | (1 << 31));
-
-			mmu_base += cur_range;
-			mmu_range -= cur_range;
-			i++;
-		}
-	}
-}
-
-static void a6xx_enable_64bit(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	kgsl_regwrite(device, A6XX_CP_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_VSC_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_GRAS_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_RB_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_PC_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_HLSQ_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_VFD_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_VPC_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_UCHE_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_SP_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_TPL1_ADDR_MODE_CNTL, 0x1);
-	kgsl_regwrite(device, A6XX_RBBM_SECVID_TSB_ADDR_MODE_CNTL, 0x1);
-}
-
-static inline unsigned int
-__get_rbbm_clock_cntl_on(struct adreno_device *adreno_dev)
-{
-	if (adreno_is_a615(adreno_dev) || adreno_is_a616(adreno_dev))
-		return 0x8AA8AA82;
-	else
-		return 0x8AA8AA02;
-}
-
-static inline unsigned int
-__get_gmu_ao_cgc_mode_cntl(struct adreno_device *adreno_dev)
-{
-	if (adreno_is_a615(adreno_dev) || adreno_is_a616(adreno_dev))
-		return 0x00000222;
-	else
-		return 0x00020202;
-}
-
-static inline unsigned int
-__get_gmu_ao_cgc_delay_cntl(struct adreno_device *adreno_dev)
-{
-	if (adreno_is_a615(adreno_dev) || adreno_is_a616(adreno_dev))
-		return 0x00000111;
-	else
-		return 0x00010111;
-}
-
-static inline unsigned int
-__get_gmu_ao_cgc_hyst_cntl(struct adreno_device *adreno_dev)
-{
-	if (adreno_is_a615(adreno_dev) || adreno_is_a616(adreno_dev))
-		return 0x00000555;
-	else
-		return 0x00005555;
-}
-
-static void a6xx_hwcg_set(struct adreno_device *adreno_dev, bool on)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	const struct kgsl_hwcg_reg *regs;
-	unsigned int value;
-	int i, j;
-
-	if (!test_bit(ADRENO_HWCG_CTRL, &adreno_dev->pwrctrl_flag))
-		on = false;
-
-	if (kgsl_gmu_isenabled(device)) {
-		kgsl_gmu_regwrite(device, A6XX_GPU_GMU_AO_GMU_CGC_MODE_CNTL,
-			on ? __get_gmu_ao_cgc_mode_cntl(adreno_dev) : 0);
-		kgsl_gmu_regwrite(device, A6XX_GPU_GMU_AO_GMU_CGC_DELAY_CNTL,
-			on ? __get_gmu_ao_cgc_delay_cntl(adreno_dev) : 0);
-		kgsl_gmu_regwrite(device, A6XX_GPU_GMU_AO_GMU_CGC_HYST_CNTL,
-			on ? __get_gmu_ao_cgc_hyst_cntl(adreno_dev) : 0);
-	}
-
-	kgsl_regread(device, A6XX_RBBM_CLOCK_CNTL, &value);
-
-	if (value == __get_rbbm_clock_cntl_on(adreno_dev) && on)
-		return;
-
-	if (value == 0 && !on)
-		return;
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_hwcg_registers); i++) {
-		if (a6xx_hwcg_registers[i].devfunc(adreno_dev))
-			break;
-	}
-
-	if (i == ARRAY_SIZE(a6xx_hwcg_registers))
-		return;
-
-	regs = a6xx_hwcg_registers[i].regs;
-
-	/* Disable SP clock before programming HWCG registers */
-	kgsl_gmu_regrmw(device, A6XX_GPU_GMU_GX_SPTPRAC_CLOCK_CONTROL, 1, 0);
-
-	for (j = 0; j < a6xx_hwcg_registers[i].count; j++)
-		kgsl_regwrite(device, regs[j].off, on ? regs[j].val : 0);
-
-	/* Enable SP clock */
-	kgsl_gmu_regrmw(device, A6XX_GPU_GMU_GX_SPTPRAC_CLOCK_CONTROL, 0, 1);
-
-	/* enable top level HWCG */
-	kgsl_regwrite(device, A6XX_RBBM_CLOCK_CNTL,
-		on ? __get_rbbm_clock_cntl_on(adreno_dev) : 0);
-}
-
-#define LM_DEFAULT_LIMIT	6000
-
-static uint32_t lm_limit(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (adreno_dev->lm_limit)
-		return adreno_dev->lm_limit;
-
-	if (of_property_read_u32(device->pdev->dev.of_node, "qcom,lm-limit",
-		&adreno_dev->lm_limit))
-		adreno_dev->lm_limit = LM_DEFAULT_LIMIT;
-
-	return adreno_dev->lm_limit;
-}
-
-static void a6xx_patch_pwrup_reglist(struct adreno_device *adreno_dev)
-{
-	uint32_t i;
-	struct cpu_gpu_lock *lock;
-	struct reg_list_pair *r;
-
-	/* Set up the register values */
-	for (i = 0; i < ARRAY_SIZE(a6xx_ifpc_pwrup_reglist); i++) {
-		r = &a6xx_ifpc_pwrup_reglist[i];
-		kgsl_regread(KGSL_DEVICE(adreno_dev), r->offset, &r->val);
-	}
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_pwrup_reglist); i++) {
-		r = &a6xx_pwrup_reglist[i];
-		kgsl_regread(KGSL_DEVICE(adreno_dev), r->offset, &r->val);
-	}
-
-	lock = (struct cpu_gpu_lock *) adreno_dev->pwrup_reglist.hostptr;
-	lock->flag_ucode = 0;
-	lock->flag_kmd = 0;
-	lock->turn = 0;
-
-	/*
-	 * The overall register list is composed of
-	 * 1. Static IFPC-only registers
-	 * 2. Static IFPC + preemption registers
-	 * 2. Dynamic IFPC + preemption registers (ex: perfcounter selects)
-	 *
-	 * The CP views the second and third entries as one dynamic list
-	 * starting from list_offset. Thus, list_length should be the sum
-	 * of all three lists above (of which the third list will start off
-	 * empty). And list_offset should be specified as the size in dwords
-	 * of the static IFPC-only register list.
-	 */
-	lock->list_length = (sizeof(a6xx_ifpc_pwrup_reglist) +
-			sizeof(a6xx_pwrup_reglist)) >> 2;
-	lock->list_offset = sizeof(a6xx_ifpc_pwrup_reglist) >> 2;
-
-	memcpy(adreno_dev->pwrup_reglist.hostptr + sizeof(*lock),
-		a6xx_ifpc_pwrup_reglist, sizeof(a6xx_ifpc_pwrup_reglist));
-	memcpy(adreno_dev->pwrup_reglist.hostptr + sizeof(*lock)
-		+ sizeof(a6xx_ifpc_pwrup_reglist), a6xx_pwrup_reglist,
-		sizeof(a6xx_pwrup_reglist));
-
-	if (adreno_is_a615(adreno_dev) || adreno_is_a616(adreno_dev)) {
-		for (i = 0; i < ARRAY_SIZE(a615_pwrup_reglist); i++) {
-			r = &a615_pwrup_reglist[i];
-			kgsl_regread(KGSL_DEVICE(adreno_dev),
-				r->offset, &r->val);
-		}
-
-		memcpy(adreno_dev->pwrup_reglist.hostptr + sizeof(*lock)
-			+ sizeof(a6xx_ifpc_pwrup_reglist)
-			+ sizeof(a6xx_pwrup_reglist), a615_pwrup_reglist,
-			sizeof(a615_pwrup_reglist));
-
-		lock->list_length += sizeof(a615_pwrup_reglist);
-	}
-}
-
-/*
- * a6xx_start() - Device start
- * @adreno_dev: Pointer to adreno device
- *
- * a6xx device start
- */
-static void a6xx_start(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int bit, mal, mode, glbl_inv;
-	unsigned int amsbc = 0;
-	static bool patch_reglist;
-
-	/* runtime adjust callbacks based on feature sets */
-	if (!kgsl_gmu_isenabled(device))
-		/* Legacy idle management if gmu is disabled */
-		ADRENO_GPU_DEVICE(adreno_dev)->hw_isidle = NULL;
-	/* enable hardware clockgating */
-	a6xx_hwcg_set(adreno_dev, true);
-
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_LM))
-		adreno_dev->lm_threshold_count = A6XX_GMU_GENERAL_1;
-
-	adreno_vbif_start(adreno_dev, a6xx_vbif_platforms,
-			ARRAY_SIZE(a6xx_vbif_platforms));
-
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_LIMIT_UCHE_GBIF_RW))
-		kgsl_regwrite(device, A6XX_UCHE_GBIF_GX_CONFIG, 0x10200F9);
-
-	/* Make all blocks contribute to the GPU BUSY perf counter */
-	kgsl_regwrite(device, A6XX_RBBM_PERFCTR_GPU_BUSY_MASKED, 0xFFFFFFFF);
-
-	/*
-	 * Set UCHE_WRITE_THRU_BASE to the UCHE_TRAP_BASE effectively
-	 * disabling L2 bypass
-	 */
-	kgsl_regwrite(device, A6XX_UCHE_WRITE_RANGE_MAX_LO, 0xffffffc0);
-	kgsl_regwrite(device, A6XX_UCHE_WRITE_RANGE_MAX_HI, 0x0001ffff);
-	kgsl_regwrite(device, A6XX_UCHE_TRAP_BASE_LO, 0xfffff000);
-	kgsl_regwrite(device, A6XX_UCHE_TRAP_BASE_HI, 0x0001ffff);
-	kgsl_regwrite(device, A6XX_UCHE_WRITE_THRU_BASE_LO, 0xfffff000);
-	kgsl_regwrite(device, A6XX_UCHE_WRITE_THRU_BASE_HI, 0x0001ffff);
-
-	/* Program the GMEM VA range for the UCHE path */
-	kgsl_regwrite(device, A6XX_UCHE_GMEM_RANGE_MIN_LO,
-				ADRENO_UCHE_GMEM_BASE);
-	kgsl_regwrite(device, A6XX_UCHE_GMEM_RANGE_MIN_HI, 0x0);
-	kgsl_regwrite(device, A6XX_UCHE_GMEM_RANGE_MAX_LO,
-				ADRENO_UCHE_GMEM_BASE +
-				adreno_dev->gmem_size - 1);
-	kgsl_regwrite(device, A6XX_UCHE_GMEM_RANGE_MAX_HI, 0x0);
-
-	kgsl_regwrite(device, A6XX_UCHE_FILTER_CNTL, 0x804);
-	kgsl_regwrite(device, A6XX_UCHE_CACHE_WAYS, 0x4);
-
-	kgsl_regwrite(device, A6XX_CP_ROQ_THRESHOLDS_2, 0x010000C0);
-	kgsl_regwrite(device, A6XX_CP_ROQ_THRESHOLDS_1, 0x8040362C);
-
-	/* Setting the mem pool size */
-	kgsl_regwrite(device, A6XX_CP_MEM_POOL_SIZE, 128);
-
-	/* Setting the primFifo thresholds default values */
-	kgsl_regwrite(device, A6XX_PC_DBG_ECO_CNTL, (0x300 << 11));
-
-	/* Set the AHB default slave response to "ERROR" */
-	kgsl_regwrite(device, A6XX_CP_AHB_CNTL, 0x1);
-
-	/* Turn on performance counters */
-	kgsl_regwrite(device, A6XX_RBBM_PERFCTR_CNTL, 0x1);
-
-	if (of_property_read_u32(device->pdev->dev.of_node,
-		"qcom,highest-bank-bit", &bit))
-		bit = MIN_HBB;
-
-	if (of_property_read_u32(device->pdev->dev.of_node,
-		"qcom,min-access-length", &mal))
-		mal = 32;
-
-	if (of_property_read_u32(device->pdev->dev.of_node,
-		"qcom,ubwc-mode", &mode))
-		mode = 0;
-
-	switch (mode) {
-	case KGSL_UBWC_1_0:
-		mode = 1;
-		break;
-	case KGSL_UBWC_2_0:
-		mode = 0;
-		break;
-	case KGSL_UBWC_3_0:
-		mode = 0;
-		amsbc = 1; /* Only valid for A640 and A680 */
-		break;
-	default:
-		break;
-	}
-
-	if (bit >= 13 && bit <= 16)
-		bit = (bit - 13) & 0x03;
-	else
-		bit = 0;
-
-	mal = (mal == 64) ? 1 : 0;
-
-	/* (1 << 29)globalInvFlushFilterDis bit needs to be set for A630 V1 */
-	glbl_inv = (adreno_is_a630v1(adreno_dev)) ? 1 : 0;
-
-	kgsl_regwrite(device, A6XX_RB_NC_MODE_CNTL, (amsbc << 4) | (mal << 3) |
-							(bit << 1) | mode);
-	kgsl_regwrite(device, A6XX_TPL1_NC_MODE_CNTL, (mal << 3) |
-							(bit << 1) | mode);
-	kgsl_regwrite(device, A6XX_SP_NC_MODE_CNTL, (mal << 3) | (bit << 1) |
-								mode);
-
-	kgsl_regwrite(device, A6XX_UCHE_MODE_CNTL, (glbl_inv << 29) |
-						(mal << 23) | (bit << 21));
-
-	/* Set hang detection threshold to 0x1FFFFF * 16 cycles */
-	kgsl_regwrite(device, A6XX_RBBM_INTERFACE_HANG_INT_CNTL,
-					  (1 << 30) | 0x1fffff);
-
-	kgsl_regwrite(device, A6XX_UCHE_CLIENT_PF, 1);
-
-	/* Set TWOPASSUSEWFI in A6XX_PC_DBG_ECO_CNTL if requested */
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_TWO_PASS_USE_WFI))
-		kgsl_regrmw(device, A6XX_PC_DBG_ECO_CNTL, 0, (1 << 8));
-
-	/* Enable the GMEM save/restore feature for preemption */
-	if (adreno_is_preemption_enabled(adreno_dev))
-		kgsl_regwrite(device, A6XX_RB_CONTEXT_SWITCH_GMEM_SAVE_RESTORE,
-			0x1);
-
-	a6xx_protect_init(adreno_dev);
-
-	if (!patch_reglist && (adreno_dev->pwrup_reglist.gpuaddr != 0)) {
-		a6xx_patch_pwrup_reglist(adreno_dev);
-		patch_reglist = true;
-	}
-
-	a6xx_preemption_start(adreno_dev);
-
-	/*
-	 * We start LM here because we want all the following to be up
-	 * 1. GX HS
-	 * 2. SPTPRAC
-	 * 3. HFI
-	 * At this point, we are guaranteed all.
-	 */
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_LM) &&
-		test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag)) {
-		int result;
-		struct gmu_device *gmu = &device->gmu;
-		struct device *dev = &gmu->pdev->dev;
-
-		kgsl_gmu_regwrite(device, A6XX_GPU_GMU_CX_GMU_PWR_THRESHOLD,
-			GPU_LIMIT_THRESHOLD_ENABLE | lm_limit(adreno_dev));
-		kgsl_gmu_regwrite(device, A6XX_GMU_AO_SPARE_CNTL, 1);
-		kgsl_gmu_regwrite(device, A6XX_GPU_GMU_CX_GMU_ISENSE_CTRL, 0x1);
-
-		gmu->lm_config.lm_type = 1;
-		gmu->lm_config.lm_sensor_type = 1;
-		gmu->lm_config.throttle_config = 1;
-		gmu->lm_config.idle_throttle_en = 0;
-		gmu->lm_config.acd_en = 0;
-		gmu->bcl_config = 0;
-		gmu->lm_dcvs_level = 0;
-
-		result = hfi_send_lmconfig(gmu);
-		if (result)
-			dev_err(dev, "Failure enabling limits management (%d)\n",
-			result);
-	}
-}
-
-/*
- * a6xx_microcode_load() - Load microcode
- * @adreno_dev: Pointer to adreno device
- */
-static int a6xx_microcode_load(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_SQE);
-	uint64_t gpuaddr;
-	void *zap;
-	int ret = 0, zap_retry = 0;
-
-	gpuaddr = fw->memdesc.gpuaddr;
-	kgsl_regwrite(device, A6XX_CP_SQE_INSTR_BASE_LO,
-				lower_32_bits(gpuaddr));
-	kgsl_regwrite(device, A6XX_CP_SQE_INSTR_BASE_HI,
-				upper_32_bits(gpuaddr));
-
-	/*
-	 * Do not invoke to load zap shader if MMU does
-	 * not support secure mode.
-	 */
-	if (!device->mmu.secured)
-		return 0;
-
-	/* Load the zap shader firmware through PIL if its available */
-	if (adreno_dev->gpucore->zap_name && !adreno_dev->zap_loaded) {
-		/*
-		 * subsystem_get() may return -EAGAIN in case system is busy
-		 * and unable to load the firmware. So keep trying since this
-		 * is not a fatal error.
-		 */
-		do {
-			ret = 0;
-			zap = subsystem_get(adreno_dev->gpucore->zap_name);
-
-			/* Return error if the zap shader cannot be loaded */
-			if (IS_ERR_OR_NULL(zap)) {
-				ret = (zap == NULL) ? -ENODEV : PTR_ERR(zap);
-				zap = NULL;
-			} else
-				adreno_dev->zap_loaded = 1;
-		} while ((ret == -EAGAIN) && (zap_retry++ < ZAP_RETRY_MAX));
-	}
-
-	return ret;
-}
-
-
-/*
- * CP_INIT_MAX_CONTEXT bit tells if the multiple hardware contexts can
- * be used at once of if they should be serialized
- */
-#define CP_INIT_MAX_CONTEXT BIT(0)
-
-/* Enables register protection mode */
-#define CP_INIT_ERROR_DETECTION_CONTROL BIT(1)
-
-/* Header dump information */
-#define CP_INIT_HEADER_DUMP BIT(2) /* Reserved */
-
-/* Default Reset states enabled for PFP and ME */
-#define CP_INIT_DEFAULT_RESET_STATE BIT(3)
-
-/* Drawcall filter range */
-#define CP_INIT_DRAWCALL_FILTER_RANGE BIT(4)
-
-/* Ucode workaround masks */
-#define CP_INIT_UCODE_WORKAROUND_MASK BIT(5)
-
-/*
- * Operation mode mask
- *
- * This ordinal provides the option to disable the
- * save/restore of performance counters across preemption.
- */
-#define CP_INIT_OPERATION_MODE_MASK BIT(6)
-
-/* Register initialization list */
-#define CP_INIT_REGISTER_INIT_LIST BIT(7)
-
-/* Register initialization list with spinlock */
-#define CP_INIT_REGISTER_INIT_LIST_WITH_SPINLOCK BIT(8)
-
-#define CP_INIT_MASK (CP_INIT_MAX_CONTEXT | \
-		CP_INIT_ERROR_DETECTION_CONTROL | \
-		CP_INIT_HEADER_DUMP | \
-		CP_INIT_DEFAULT_RESET_STATE | \
-		CP_INIT_UCODE_WORKAROUND_MASK | \
-		CP_INIT_OPERATION_MODE_MASK | \
-		CP_INIT_REGISTER_INIT_LIST_WITH_SPINLOCK)
-
-static void _set_ordinals(struct adreno_device *adreno_dev,
-		unsigned int *cmds, unsigned int count)
-{
-	unsigned int *start = cmds;
-
-	/* Enabled ordinal mask */
-	*cmds++ = CP_INIT_MASK;
-
-	if (CP_INIT_MASK & CP_INIT_MAX_CONTEXT)
-		*cmds++ = 0x00000003;
-
-	if (CP_INIT_MASK & CP_INIT_ERROR_DETECTION_CONTROL)
-		*cmds++ = 0x20000000;
-
-	if (CP_INIT_MASK & CP_INIT_HEADER_DUMP) {
-		/* Header dump address */
-		*cmds++ = 0x00000000;
-		/* Header dump enable and dump size */
-		*cmds++ = 0x00000000;
-	}
-
-	if (CP_INIT_MASK & CP_INIT_DRAWCALL_FILTER_RANGE) {
-		/* Start range */
-		*cmds++ = 0x00000000;
-		/* End range (inclusive) */
-		*cmds++ = 0x00000000;
-	}
-
-	if (CP_INIT_MASK & CP_INIT_UCODE_WORKAROUND_MASK)
-		*cmds++ = 0x00000000;
-
-	if (CP_INIT_MASK & CP_INIT_OPERATION_MODE_MASK)
-		*cmds++ = 0x00000002;
-
-	if (CP_INIT_MASK & CP_INIT_REGISTER_INIT_LIST_WITH_SPINLOCK) {
-		uint64_t gpuaddr = adreno_dev->pwrup_reglist.gpuaddr;
-
-		*cmds++ = lower_32_bits(gpuaddr);
-		*cmds++ = upper_32_bits(gpuaddr);
-		*cmds++ =  0;
-
-	} else if (CP_INIT_MASK & CP_INIT_REGISTER_INIT_LIST) {
-		uint64_t gpuaddr = adreno_dev->pwrup_reglist.gpuaddr;
-
-		*cmds++ = lower_32_bits(gpuaddr);
-		*cmds++ = upper_32_bits(gpuaddr);
-		/* Size is in dwords */
-		*cmds++ = (sizeof(a6xx_ifpc_pwrup_reglist) +
-			sizeof(a6xx_pwrup_reglist)) >> 2;
-	}
-
-	/* Pad rest of the cmds with 0's */
-	while ((unsigned int)(cmds - start) < count)
-		*cmds++ = 0x0;
-}
-
-/*
- * a6xx_send_cp_init() - Initialize ringbuffer
- * @adreno_dev: Pointer to adreno device
- * @rb: Pointer to the ringbuffer of device
- *
- * Submit commands for ME initialization,
- */
-static int a6xx_send_cp_init(struct adreno_device *adreno_dev,
-			 struct adreno_ringbuffer *rb)
-{
-	unsigned int *cmds;
-	int ret;
-
-	cmds = adreno_ringbuffer_allocspace(rb, 12);
-	if (IS_ERR(cmds))
-		return PTR_ERR(cmds);
-
-	*cmds++ = cp_type7_packet(CP_ME_INIT, 11);
-
-	_set_ordinals(adreno_dev, cmds, 11);
-
-	ret = adreno_ringbuffer_submit_spin(rb, NULL, 2000);
-	if (ret)
-		adreno_spin_idle_debug(adreno_dev,
-				"CP initialization failed to idle\n");
-
-	return ret;
-}
-
-/*
- * Follow the ME_INIT sequence with a preemption yield to allow the GPU to move
- * to a different ringbuffer, if desired
- */
-static int _preemption_init(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb, unsigned int *cmds,
-		struct kgsl_context *context)
-{
-	unsigned int *cmds_orig = cmds;
-
-	/* Turn CP protection OFF */
-	*cmds++ = cp_type7_packet(CP_SET_PROTECTED_MODE, 1);
-	*cmds++ = 0;
-
-	*cmds++ = cp_type7_packet(CP_SET_PSEUDO_REGISTER, 6);
-	*cmds++ = 1;
-	cmds += cp_gpuaddr(adreno_dev, cmds,
-			rb->preemption_desc.gpuaddr);
-
-	*cmds++ = 2;
-	cmds += cp_gpuaddr(adreno_dev, cmds,
-			rb->secure_preemption_desc.gpuaddr);
-
-	/* Turn CP protection ON */
-	*cmds++ = cp_type7_packet(CP_SET_PROTECTED_MODE, 1);
-	*cmds++ = 1;
-
-	*cmds++ = cp_type7_packet(CP_CONTEXT_SWITCH_YIELD, 4);
-	cmds += cp_gpuaddr(adreno_dev, cmds, 0x0);
-	*cmds++ = 0;
-	/* generate interrupt on preemption completion */
-	*cmds++ = 0;
-
-	return cmds - cmds_orig;
-}
-
-static int a6xx_post_start(struct adreno_device *adreno_dev)
-{
-	int ret;
-	unsigned int *cmds, *start;
-	struct adreno_ringbuffer *rb = adreno_dev->cur_rb;
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return 0;
-
-	cmds = adreno_ringbuffer_allocspace(rb, 42);
-	if (IS_ERR(cmds)) {
-		KGSL_DRV_ERR(device, "error allocating preemption init cmds");
-		return PTR_ERR(cmds);
-	}
-	start = cmds;
-
-	cmds += _preemption_init(adreno_dev, rb, cmds, NULL);
-
-	rb->_wptr = rb->_wptr - (42 - (cmds - start));
-
-	ret = adreno_ringbuffer_submit_spin(rb, NULL, 2000);
-	if (ret)
-		adreno_spin_idle_debug(adreno_dev,
-			"hw preemption initialization failed to idle\n");
-
-	return ret;
-}
-
-/*
- * a6xx_rb_start() - Start the ringbuffer
- * @adreno_dev: Pointer to adreno device
- * @start_type: Warm or cold start
- */
-static int a6xx_rb_start(struct adreno_device *adreno_dev,
-			 unsigned int start_type)
-{
-	struct adreno_ringbuffer *rb = ADRENO_CURRENT_RINGBUFFER(adreno_dev);
-	struct kgsl_device *device = &adreno_dev->dev;
-	uint64_t addr;
-	int ret;
-
-	addr = SCRATCH_RPTR_GPU_ADDR(device, rb->id);
-
-	adreno_writereg64(adreno_dev, ADRENO_REG_CP_RB_RPTR_ADDR_LO,
-				ADRENO_REG_CP_RB_RPTR_ADDR_HI, addr);
-
-	/*
-	 * The size of the ringbuffer in the hardware is the log2
-	 * representation of the size in quadwords (sizedwords / 2).
-	 */
-	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_CNTL,
-					A6XX_CP_RB_CNTL_DEFAULT);
-
-	adreno_writereg64(adreno_dev, ADRENO_REG_CP_RB_BASE,
-			ADRENO_REG_CP_RB_BASE_HI, rb->buffer_desc.gpuaddr);
-
-	ret = a6xx_microcode_load(adreno_dev);
-	if (ret)
-		return ret;
-
-	/* Clear the SQE_HALT to start the CP engine */
-	kgsl_regwrite(device, A6XX_CP_SQE_CNTL, 1);
-
-	ret = a6xx_send_cp_init(adreno_dev, rb);
-	if (ret)
-		return ret;
-
-	/* GPU comes up in secured mode, make it unsecured by default */
-	ret = adreno_set_unsecured_mode(adreno_dev, rb);
-	if (ret)
-		return ret;
-
-	return a6xx_post_start(adreno_dev);
-}
-
-unsigned int a6xx_set_marker(
-		unsigned int *cmds, enum adreno_cp_marker_type type)
-{
-	unsigned int cmd = 0;
-
-	*cmds++ = cp_type7_packet(CP_SET_MARKER, 1);
-
-	/*
-	 * Indicate the beginning and end of the IB1 list with a SET_MARKER.
-	 * Among other things, this will implicitly enable and disable
-	 * preemption respectively. IFPC can also be disabled and enabled
-	 * with a SET_MARKER. Bit 8 tells the CP the marker is for IFPC.
-	 */
-	switch (type) {
-	case IFPC_DISABLE:
-		cmd = 0x101;
-		break;
-	case IFPC_ENABLE:
-		cmd = 0x100;
-		break;
-	case IB1LIST_START:
-		cmd = 0xD;
-		break;
-	case IB1LIST_END:
-		cmd = 0xE;
-		break;
-	}
-
-	*cmds++ = cmd;
-	return 2;
-}
-
-static int _load_firmware(struct kgsl_device *device, const char *fwfile,
-			  struct adreno_firmware *firmware)
-{
-	const struct firmware *fw = NULL;
-	int ret;
-
-	ret = request_firmware(&fw, fwfile, device->dev);
-
-	if (ret) {
-		KGSL_DRV_ERR(device, "request_firmware(%s) failed: %d\n",
-				fwfile, ret);
-		return ret;
-	}
-
-	ret = kgsl_allocate_global(device, &firmware->memdesc, fw->size - 4,
-				KGSL_MEMFLAGS_GPUREADONLY, 0, "ucode");
-
-	if (!ret) {
-		memcpy(firmware->memdesc.hostptr, &fw->data[4], fw->size - 4);
-		firmware->size = (fw->size - 4) / sizeof(uint32_t);
-		firmware->version = *(unsigned int *)&fw->data[4];
-	}
-
-	release_firmware(fw);
-	return ret;
-}
-
-#define RSC_CMD_OFFSET 2
-#define PDC_CMD_OFFSET 4
-
-static void _regwrite(void __iomem *regbase,
-		unsigned int offsetwords, unsigned int value)
-{
-	void __iomem *reg;
-
-	reg = regbase + (offsetwords << 2);
-	__raw_writel(value, reg);
-}
-
-/*
- * _load_gmu_rpmh_ucode() - Load the ucode into the GPU PDC/RSC blocks
- * PDC and RSC execute GPU power on/off RPMh sequence
- * @device: Pointer to KGSL device
- */
-static void _load_gmu_rpmh_ucode(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-
-	/* Disable SDE clock gating */
-	kgsl_gmu_regwrite(device, A6XX_GPU_RSCC_RSC_STATUS0_DRV0, BIT(24));
-
-	/* Setup RSC PDC handshake for sleep and wakeup */
-	kgsl_gmu_regwrite(device, A6XX_RSCC_PDC_SLAVE_ID_DRV0, 1);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_HIDDEN_TCS_CMD0_DATA, 0);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_HIDDEN_TCS_CMD0_ADDR, 0);
-	kgsl_gmu_regwrite(device,
-			A6XX_RSCC_HIDDEN_TCS_CMD0_DATA + RSC_CMD_OFFSET, 0);
-	kgsl_gmu_regwrite(device,
-			A6XX_RSCC_HIDDEN_TCS_CMD0_ADDR + RSC_CMD_OFFSET, 0);
-	kgsl_gmu_regwrite(device,
-			A6XX_RSCC_HIDDEN_TCS_CMD0_DATA + RSC_CMD_OFFSET * 2,
-			0x80000000);
-	kgsl_gmu_regwrite(device,
-			A6XX_RSCC_HIDDEN_TCS_CMD0_ADDR + RSC_CMD_OFFSET * 2,
-			0);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_OVERRIDE_START_ADDR, 0);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_PDC_SEQ_START_ADDR, 0x4520);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_PDC_MATCH_VALUE_LO, 0x4510);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_PDC_MATCH_VALUE_HI, 0x4514);
-
-	/* Enable timestamp event for v1 only */
-	if (adreno_is_a630v1(adreno_dev))
-		kgsl_gmu_regwrite(device, A6XX_RSCC_TIMESTAMP_UNIT1_EN_DRV0, 1);
-
-	/* Load RSC sequencer uCode for sleep and wakeup */
-	kgsl_gmu_regwrite(device, A6XX_RSCC_SEQ_MEM_0_DRV0, 0xA7A506A0);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_SEQ_MEM_0_DRV0 + 1, 0xA1E6A6E7);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_SEQ_MEM_0_DRV0 + 2, 0xA2E081E1);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_SEQ_MEM_0_DRV0 + 3, 0xE9A982E2);
-	kgsl_gmu_regwrite(device, A6XX_RSCC_SEQ_MEM_0_DRV0 + 4, 0x0020E8A8);
-
-	/* Load PDC sequencer uCode for power up and power down sequence */
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_SEQ_MEM_0, 0xFEBEA1E1);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_SEQ_MEM_0 + 1, 0xA5A4A3A2);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_SEQ_MEM_0 + 2, 0x8382A6E0);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_SEQ_MEM_0 + 3, 0xBCE3E284);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_SEQ_MEM_0 + 4, 0x002081FC);
-
-	/* Set TCS commands used by PDC sequence for low power modes */
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS1_CMD_ENABLE_BANK, 7);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS1_CMD_WAIT_FOR_CMPL_BANK, 0);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS1_CONTROL, 0);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS1_CMD0_MSGID, 0x10108);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS1_CMD0_ADDR, 0x30010);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS1_CMD0_DATA, 1);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS1_CMD0_MSGID + PDC_CMD_OFFSET, 0x10108);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS1_CMD0_ADDR + PDC_CMD_OFFSET, 0x30000);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS1_CMD0_DATA + PDC_CMD_OFFSET, 0x0);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS1_CMD0_MSGID + PDC_CMD_OFFSET * 2, 0x10108);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS1_CMD0_ADDR + PDC_CMD_OFFSET * 2, 0x30080);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS1_CMD0_DATA + PDC_CMD_OFFSET * 2, 0x0);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS3_CMD_ENABLE_BANK, 7);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS3_CMD_WAIT_FOR_CMPL_BANK, 0);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS3_CONTROL, 0);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS3_CMD0_MSGID, 0x10108);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS3_CMD0_ADDR, 0x30010);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_TCS3_CMD0_DATA, 2);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS3_CMD0_MSGID + PDC_CMD_OFFSET, 0x10108);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS3_CMD0_ADDR + PDC_CMD_OFFSET, 0x30000);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS3_CMD0_DATA + PDC_CMD_OFFSET, 0x3);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS3_CMD0_MSGID + PDC_CMD_OFFSET * 2, 0x10108);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS3_CMD0_ADDR + PDC_CMD_OFFSET * 2, 0x30080);
-	_regwrite(gmu->pdc_reg_virt,
-			PDC_GPU_TCS3_CMD0_DATA + PDC_CMD_OFFSET * 2, 0x3);
-
-	/* Setup GPU PDC */
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_SEQ_START_ADDR, 0);
-	_regwrite(gmu->pdc_reg_virt, PDC_GPU_ENABLE_PDC, 0x80000001);
-
-	/* ensure no writes happen before the uCode is fully written */
-	wmb();
-}
-
-#define GMU_START_TIMEOUT	100	/* ms */
-#define GPU_START_TIMEOUT	100	/* ms */
-#define GPU_RESET_TIMEOUT	1	/* ms */
-#define GPU_RESET_TIMEOUT_US	10	/* us */
-
-/*
- * timed_poll_check() - polling *gmu* register at given offset until
- * its value changed to match expected value. The function times
- * out and returns after given duration if register is not updated
- * as expected.
- *
- * @device: Pointer to KGSL device
- * @offset: Register offset
- * @expected_ret: expected register value that stops polling
- * @timout: number of jiffies to abort the polling
- * @mask: bitmask to filter register value to match expected_ret
- */
-static int timed_poll_check(struct kgsl_device *device,
-		unsigned int offset, unsigned int expected_ret,
-		unsigned int timeout, unsigned int mask)
-{
-	unsigned long t;
-	unsigned int value;
-
-	t = jiffies + msecs_to_jiffies(timeout);
-
-	do {
-		kgsl_gmu_regread(device, offset, &value);
-		if ((value & mask) == expected_ret)
-			return 0;
-		/* Wait 100us to reduce unnecessary AHB bus traffic */
-		usleep_range(10, 100);
-	} while (!time_after(jiffies, t));
-
-	/* Double check one last time */
-	kgsl_gmu_regread(device, offset, &value);
-	if ((value & mask) == expected_ret)
-		return 0;
-
-	return -EINVAL;
-}
-
-/*
- * The lowest 16 bits of this value are the number of XO clock cycles
- * for main hysteresis. This is the first hysteresis. Here we set it
- * to 0x1680 cycles, or 300 us. The highest 16 bits of this value are
- * the number of XO clock cycles for short hysteresis. This happens
- * after main hysteresis. Here we set it to 0xA cycles, or 0.5 us.
- */
-#define GMU_PWR_COL_HYST 0x000A1680
-
-/*
- * a6xx_gmu_power_config() - Configure and enable GMU's low power mode
- * setting based on ADRENO feature flags.
- * @device: Pointer to KGSL device
- */
-static void a6xx_gmu_power_config(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-
-	/* Configure registers for idle setting. The setting is cumulative */
-
-	/* Disable GMU WB/RB buffer */
-	kgsl_gmu_regwrite(device, A6XX_GMU_SYS_BUS_CONFIG, 0x1);
-
-	kgsl_gmu_regwrite(device,
-		A6XX_GMU_PWR_COL_INTER_FRAME_CTRL,  0x9C40400);
-
-	switch (gmu->idle_level) {
-	case GPU_HW_MIN_VOLT:
-		kgsl_gmu_regrmw(device, A6XX_GMU_RPMH_CTRL, 0,
-				MIN_BW_ENABLE_MASK);
-		kgsl_gmu_regrmw(device, A6XX_GMU_RPMH_HYST_CTRL, 0,
-				MIN_BW_HYST);
-		/* fall through */
-	case GPU_HW_NAP:
-		kgsl_gmu_regrmw(device, A6XX_GMU_GPU_NAP_CTRL, 0,
-				HW_NAP_ENABLE_MASK);
-		/* fall through */
-	case GPU_HW_IFPC:
-		kgsl_gmu_regwrite(device, A6XX_GMU_PWR_COL_INTER_FRAME_HYST,
-				GMU_PWR_COL_HYST);
-		kgsl_gmu_regrmw(device, A6XX_GMU_PWR_COL_INTER_FRAME_CTRL, 0,
-				IFPC_ENABLE_MASK);
-		/* fall through */
-	case GPU_HW_SPTP_PC:
-		kgsl_gmu_regwrite(device, A6XX_GMU_PWR_COL_SPTPRAC_HYST,
-				GMU_PWR_COL_HYST);
-		kgsl_gmu_regrmw(device, A6XX_GMU_PWR_COL_INTER_FRAME_CTRL, 0,
-				SPTP_ENABLE_MASK);
-		/* fall through */
-	default:
-		break;
-	}
-
-	/* ACD feature enablement */
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_LM) &&
-		test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag))
-		kgsl_gmu_regrmw(device, A6XX_GMU_BOOT_KMD_LM_HANDSHAKE, 0,
-				BIT(10));
-
-	/* Enable RPMh GPU client */
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_RPMH))
-		kgsl_gmu_regrmw(device, A6XX_GMU_RPMH_CTRL, 0,
-				RPMH_ENABLE_MASK);
-}
-
-/*
- * a6xx_gmu_start() - Start GMU and wait until FW boot up.
- * @device: Pointer to KGSL device
- */
-static int a6xx_gmu_start(struct kgsl_device *device)
-{
-	struct gmu_device *gmu = &device->gmu;
-
-	kgsl_regwrite(device, A6XX_GMU_CX_GMU_WFI_CONFIG, 0x0);
-	/* Write 1 first to make sure the GMU is reset */
-	kgsl_gmu_regwrite(device, A6XX_GMU_CM3_SYSRESET, 1);
-
-	/* Make sure putting in reset doesn't happen after clearing */
-	wmb();
-
-	/* Bring GMU out of reset */
-	kgsl_gmu_regwrite(device, A6XX_GMU_CM3_SYSRESET, 0);
-	if (timed_poll_check(device,
-			A6XX_GMU_CM3_FW_INIT_RESULT,
-			0xBABEFACE,
-			GMU_START_TIMEOUT,
-			0xFFFFFFFF)) {
-		dev_err(&gmu->pdev->dev, "GMU doesn't boot\n");
-		return -ETIMEDOUT;
-	}
-
-	return 0;
-}
-
-/*
- * a6xx_gmu_hfi_start() - Write registers and start HFI.
- * @device: Pointer to KGSL device
- */
-static int a6xx_gmu_hfi_start(struct kgsl_device *device)
-{
-	struct gmu_device *gmu = &device->gmu;
-
-	kgsl_gmu_regrmw(device, A6XX_GMU_GMU2HOST_INTR_MASK,
-			HFI_IRQ_MSGQ_MASK, 0);
-	kgsl_gmu_regwrite(device, A6XX_GMU_HFI_CTRL_INIT, 1);
-
-	if (timed_poll_check(device,
-			A6XX_GMU_HFI_CTRL_STATUS,
-			BIT(0),
-			GMU_START_TIMEOUT,
-			BIT(0))) {
-		dev_err(&gmu->pdev->dev, "GMU HFI init failed\n");
-		return -ETIMEDOUT;
-	}
-
-	return 0;
-}
-
-/*
- * a6xx_oob_set() - Set OOB interrupt to GMU.
- * @adreno_dev: Pointer to adreno device
- * @set_mask: set_mask is a bitmask that defines a set of OOB
- *	interrupts to trigger.
- * @check_mask: check_mask is a bitmask that provides a set of
- *	OOB ACK bits. check_mask usually matches set_mask to
- *	ensure OOBs are handled.
- * @clear_mask: After GMU handles a OOB interrupt, GMU driver
- *	clears the interrupt. clear_mask is a bitmask defines
- *	a set of OOB interrupts to clear.
- */
-static int a6xx_oob_set(struct adreno_device *adreno_dev,
-		unsigned int set_mask, unsigned int check_mask,
-		unsigned int clear_mask)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	int ret = 0;
-
-	if (!kgsl_gmu_isenabled(device) || !clear_mask)
-		return 0;
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_HOST2GMU_INTR_SET, set_mask);
-
-	if (timed_poll_check(device,
-			A6XX_GMU_GMU2HOST_INTR_INFO,
-			check_mask,
-			GPU_START_TIMEOUT,
-			check_mask)) {
-		ret = -ETIMEDOUT;
-		WARN(1, "OOB set timed out, mask %x\n", set_mask);
-	}
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_GMU2HOST_INTR_CLR, clear_mask);
-
-	set_bit((fls(clear_mask) - 1), &a6xx_oob_state_bitmask);
-
-	trace_kgsl_gmu_oob_set(set_mask);
-	return ret;
-}
-
-/*
- * a6xx_oob_clear() - Clear a previously set  OOB request.
- * @adreno_dev: Pointer to the adreno device that has the GMU
- * @clear_mask: Bitmask that provides the OOB bits to clear
- */
-static inline void a6xx_oob_clear(struct adreno_device *adreno_dev,
-		unsigned int clear_mask)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (!kgsl_gmu_isenabled(device) || !clear_mask)
-		return;
-
-	if (test_and_clear_bit(fls(clear_mask) - 1,
-				&a6xx_oob_state_bitmask))
-		kgsl_gmu_regwrite(device,
-			A6XX_GMU_HOST2GMU_INTR_SET,
-			clear_mask);
-
-	trace_kgsl_gmu_oob_clear(clear_mask);
-}
-
-/*
- * a6xx_gpu_keepalive() - GMU reg write to request GPU stays on
- * @adreno_dev: Pointer to the adreno device that has the GMU
- * @state: State to set: true is ON, false is OFF
- */
-static inline void a6xx_gpu_keepalive(struct adreno_device *adreno_dev,
-		bool state)
-{
-	adreno_write_gmureg(adreno_dev,
-			ADRENO_REG_GMU_PWR_COL_KEEPALIVE, state);
-}
-
-#define SPTPRAC_POWERON_CTRL_MASK	0x00778000
-#define SPTPRAC_POWEROFF_CTRL_MASK	0x00778001
-#define SPTPRAC_POWEROFF_STATUS_MASK	BIT(2)
-#define SPTPRAC_POWERON_STATUS_MASK	BIT(3)
-#define SPTPRAC_CTRL_TIMEOUT		10 /* ms */
-#define A6XX_RETAIN_FF_ENABLE_ENABLE_MASK BIT(11)
-
-/*
- * a6xx_sptprac_enable() - Power on SPTPRAC
- * @adreno_dev: Pointer to Adreno device
- */
-static int a6xx_sptprac_enable(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-
-	if (!gmu->pdev)
-		return -EINVAL;
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_GX_SPTPRAC_POWER_CONTROL,
-			SPTPRAC_POWERON_CTRL_MASK);
-
-	if (timed_poll_check(device,
-			A6XX_GMU_SPTPRAC_PWR_CLK_STATUS,
-			SPTPRAC_POWERON_STATUS_MASK,
-			SPTPRAC_CTRL_TIMEOUT,
-			SPTPRAC_POWERON_STATUS_MASK)) {
-		dev_err(&gmu->pdev->dev, "power on SPTPRAC fail\n");
-		return -EINVAL;
-	}
-
-	return 0;
-}
-
-/*
- * a6xx_sptprac_disable() - Power of SPTPRAC
- * @adreno_dev: Pointer to Adreno device
- */
-static void a6xx_sptprac_disable(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-
-	if (!gmu->pdev)
-		return;
-
-	/* Ensure that retention is on */
-	kgsl_gmu_regrmw(device, A6XX_GPU_CC_GX_GDSCR, 0,
-			A6XX_RETAIN_FF_ENABLE_ENABLE_MASK);
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_GX_SPTPRAC_POWER_CONTROL,
-			SPTPRAC_POWEROFF_CTRL_MASK);
-
-	if (timed_poll_check(device,
-			A6XX_GMU_SPTPRAC_PWR_CLK_STATUS,
-			SPTPRAC_POWEROFF_STATUS_MASK,
-			SPTPRAC_CTRL_TIMEOUT,
-			SPTPRAC_POWEROFF_STATUS_MASK))
-		dev_err(&gmu->pdev->dev, "power off SPTPRAC fail\n");
-}
-
-#define SPTPRAC_POWER_OFF	BIT(2)
-#define SP_CLK_OFF		BIT(4)
-#define GX_GDSC_POWER_OFF	BIT(6)
-#define GX_CLK_OFF		BIT(7)
-#define is_on(val)		(!(val & (GX_GDSC_POWER_OFF | GX_CLK_OFF)))
-/*
- * a6xx_gx_is_on() - Check if GX is on using pwr status register
- * @adreno_dev - Pointer to adreno_device
- * This check should only be performed if the keepalive bit is set or it
- * can be guaranteed that the power state of the GPU will remain unchanged
- */
-static bool a6xx_gx_is_on(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int val;
-
-	if (!kgsl_gmu_isenabled(device))
-		return true;
-
-	kgsl_gmu_regread(device, A6XX_GMU_SPTPRAC_PWR_CLK_STATUS, &val);
-	return is_on(val);
-}
-
-/*
- * a6xx_sptprac_is_on() - Check if SPTP is on using pwr status register
- * @adreno_dev - Pointer to adreno_device
- * This check should only be performed if the keepalive bit is set or it
- * can be guaranteed that the power state of the GPU will remain unchanged
- */
-static bool a6xx_sptprac_is_on(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int val;
-
-	if (!kgsl_gmu_isenabled(device))
-		return true;
-
-	kgsl_gmu_regread(device, A6XX_GMU_SPTPRAC_PWR_CLK_STATUS, &val);
-	return !(val & (SPTPRAC_POWER_OFF | SP_CLK_OFF));
-}
-
-/*
- * a6xx_gfx_rail_on() - request GMU to power GPU at given OPP.
- * @device: Pointer to KGSL device
- *
- */
-static int a6xx_gfx_rail_on(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	struct gmu_device *gmu = &device->gmu;
-	struct arc_vote_desc *default_opp;
-	unsigned int perf_idx;
-	int ret;
-
-	perf_idx = pwr->num_pwrlevels - pwr->default_pwrlevel - 1;
-	default_opp = &gmu->rpmh_votes.gx_votes[perf_idx];
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_BOOT_SLUMBER_OPTION,
-			OOB_BOOT_OPTION);
-	kgsl_gmu_regwrite(device, A6XX_GMU_GX_VOTE_IDX, default_opp->pri_idx);
-	kgsl_gmu_regwrite(device, A6XX_GMU_MX_VOTE_IDX, default_opp->sec_idx);
-
-	ret = a6xx_oob_set(adreno_dev, OOB_BOOT_SLUMBER_SET_MASK,
-			OOB_BOOT_SLUMBER_CHECK_MASK,
-			OOB_BOOT_SLUMBER_CLEAR_MASK);
-
-	if (ret)
-		dev_err(&gmu->pdev->dev, "Boot OOB timed out\n");
-
-	return ret;
-}
-
-#define GMU_POWER_STATE_SLUMBER 15
-
-/*
- * a6xx_notify_slumber() - initiate request to GMU to prepare to slumber
- * @device: Pointer to KGSL device
- */
-static int a6xx_notify_slumber(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	struct gmu_device *gmu = &device->gmu;
-	int bus_level = pwr->pwrlevels[pwr->default_pwrlevel].bus_freq;
-	int perf_idx = gmu->num_gpupwrlevels - pwr->default_pwrlevel - 1;
-	int ret, state;
-
-	/* Disable the power counter so that the GMU is not busy */
-	kgsl_gmu_regwrite(device, A6XX_GMU_CX_GMU_POWER_COUNTER_ENABLE, 0);
-
-	/* Turn off SPTPRAC if we own it */
-	if (gmu->idle_level < GPU_HW_SPTP_PC)
-		a6xx_sptprac_disable(adreno_dev);
-
-	if (!ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_HFI_USE_REG)) {
-		ret = hfi_notify_slumber(gmu, perf_idx, bus_level);
-		goto out;
-	}
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_BOOT_SLUMBER_OPTION,
-			OOB_SLUMBER_OPTION);
-	kgsl_gmu_regwrite(device, A6XX_GMU_GX_VOTE_IDX, perf_idx);
-	kgsl_gmu_regwrite(device, A6XX_GMU_MX_VOTE_IDX, bus_level);
-
-	ret = a6xx_oob_set(adreno_dev, OOB_BOOT_SLUMBER_SET_MASK,
-			OOB_BOOT_SLUMBER_CHECK_MASK,
-			OOB_BOOT_SLUMBER_CLEAR_MASK);
-	a6xx_oob_clear(adreno_dev, OOB_BOOT_SLUMBER_CLEAR_MASK);
-
-	if (ret)
-		dev_err(&gmu->pdev->dev, "Notify slumber OOB timed out\n");
-	else {
-		kgsl_gmu_regread(device,
-			A6XX_GPU_GMU_CX_GMU_RPMH_POWER_STATE, &state);
-		if (state != GPU_HW_SLUMBER) {
-			dev_err(&gmu->pdev->dev,
-					"Failed to prepare for slumber: 0x%x\n",
-					state);
-			ret = -EINVAL;
-		}
-	}
-
-out:
-	/* Make sure the fence is in ALLOW mode */
-	kgsl_gmu_regwrite(device, A6XX_GMU_AO_AHB_FENCE_CTRL, 0);
-	return ret;
-}
-
-static int a6xx_rpmh_power_on_gpu(struct kgsl_device *device)
-{
-	struct gmu_device *gmu = &device->gmu;
-	struct device *dev = &gmu->pdev->dev;
-	int val;
-
-	/* Only trigger wakeup sequence if sleep sequence was done earlier */
-	if (!test_bit(GMU_RSCC_SLEEP_SEQ_DONE, &gmu->flags))
-		return 0;
-
-	kgsl_gmu_regread(device, A6XX_GPU_CC_GX_DOMAIN_MISC, &val);
-	if (!(val & 0x1))
-		dev_err_ratelimited(&gmu->pdev->dev,
-			"GMEM CLAMP IO not set while GFX rail off\n");
-
-	/* RSC wake sequence */
-	kgsl_gmu_regwrite(device, A6XX_GMU_RSCC_CONTROL_REQ, BIT(1));
-
-	/* Write request before polling */
-	wmb();
-
-	if (timed_poll_check(device,
-			A6XX_GMU_RSCC_CONTROL_ACK,
-			BIT(1),
-			GPU_START_TIMEOUT,
-			BIT(1))) {
-		dev_err(dev, "Failed to do GPU RSC power on\n");
-		return -EINVAL;
-	}
-
-	if (timed_poll_check(device,
-			A6XX_RSCC_SEQ_BUSY_DRV0,
-			0,
-			GPU_START_TIMEOUT,
-			0xFFFFFFFF))
-		goto error_rsc;
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_RSCC_CONTROL_REQ, 0);
-
-	/* Clear sleep sequence flag as wakeup sequence is successful */
-	clear_bit(GMU_RSCC_SLEEP_SEQ_DONE, &gmu->flags);
-
-	/* Enable the power counter because it was disabled before slumber */
-	kgsl_gmu_regwrite(device, A6XX_GMU_CX_GMU_POWER_COUNTER_ENABLE, 1);
-
-	return 0;
-error_rsc:
-	dev_err(dev, "GPU RSC sequence stuck in waking up GPU\n");
-	return -EINVAL;
-}
-
-static int a6xx_rpmh_power_off_gpu(struct kgsl_device *device)
-{
-	struct gmu_device *gmu = &device->gmu;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	int ret;
-
-	if (test_bit(GMU_RSCC_SLEEP_SEQ_DONE, &gmu->flags))
-		return 0;
-
-	/* RSC sleep sequence is different on v1 */
-	if (adreno_is_a630v1(adreno_dev))
-		kgsl_gmu_regwrite(device, A6XX_RSCC_TIMESTAMP_UNIT1_EN_DRV0, 1);
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_RSCC_CONTROL_REQ, 1);
-	wmb();
-
-	if (adreno_is_a630v1(adreno_dev))
-		ret = timed_poll_check(device,
-				A6XX_RSCC_TIMESTAMP_UNIT1_OUTPUT_DRV0,
-				BIT(0),
-				GPU_START_TIMEOUT,
-				BIT(0));
-	else
-		ret = timed_poll_check(device,
-				A6XX_GPU_RSCC_RSC_STATUS0_DRV0,
-				BIT(16),
-				GPU_START_TIMEOUT,
-				BIT(16));
-
-	if (ret) {
-		dev_err(&gmu->pdev->dev, "GPU RSC power off fail\n");
-		return -ETIMEDOUT;
-	}
-
-	/* Read to clear the timestamp valid signal. Don't care what we read. */
-	if (adreno_is_a630v1(adreno_dev)) {
-		kgsl_gmu_regread(device,
-				A6XX_RSCC_TIMESTAMP_UNIT0_TIMESTAMP_L_DRV0,
-				&ret);
-		kgsl_gmu_regread(device,
-				A6XX_RSCC_TIMESTAMP_UNIT0_TIMESTAMP_H_DRV0,
-				&ret);
-	}
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_RSCC_CONTROL_REQ, 0);
-
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_LM) &&
-			test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag))
-		kgsl_gmu_regwrite(device, A6XX_GMU_AO_SPARE_CNTL, 0);
-
-	set_bit(GMU_RSCC_SLEEP_SEQ_DONE, &gmu->flags);
-	return 0;
-}
-
-/*
- * a6xx_gmu_fw_start() - set up GMU and start FW
- * @device: Pointer to KGSL device
- * @boot_state: State of the GMU being started
- */
-static int a6xx_gmu_fw_start(struct kgsl_device *device,
-		unsigned int boot_state)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-	struct gmu_memdesc *mem_addr = gmu->hfi_mem;
-	int ret, i;
-	unsigned int chipid = 0;
-
-	switch (boot_state) {
-	case GMU_COLD_BOOT:
-		/* Turn on TCM retention */
-		kgsl_gmu_regwrite(device, A6XX_GMU_GENERAL_7, 1);
-
-		if (!test_and_set_bit(GMU_BOOT_INIT_DONE, &gmu->flags))
-			_load_gmu_rpmh_ucode(device);
-		else {
-			ret = a6xx_rpmh_power_on_gpu(device);
-			if (ret)
-				return ret;
-		}
-
-		if (gmu->load_mode == TCM_BOOT) {
-			/* Load GMU image via AHB bus */
-			for (i = 0; i < MAX_GMUFW_SIZE; i++)
-				kgsl_gmu_regwrite(device,
-						A6XX_GMU_CM3_ITCM_START + i,
-						*((uint32_t *) gmu->fw_image.
-						hostptr + i));
-
-			/* Prevent leaving reset before the FW is written */
-			wmb();
-		} else {
-			dev_err(&gmu->pdev->dev, "Incorrect GMU load mode %d\n",
-					gmu->load_mode);
-			return -EINVAL;
-		}
-		break;
-	case GMU_WARM_BOOT:
-		ret = a6xx_rpmh_power_on_gpu(device);
-		if (ret)
-			return ret;
-		break;
-	default:
-		break;
-	}
-
-	/* Clear init result to make sure we are getting fresh value */
-	kgsl_gmu_regwrite(device, A6XX_GMU_CM3_FW_INIT_RESULT, 0);
-	kgsl_gmu_regwrite(device, A6XX_GMU_CM3_BOOT_CONFIG, gmu->load_mode);
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_HFI_QTBL_ADDR,
-			mem_addr->gmuaddr);
-	kgsl_gmu_regwrite(device, A6XX_GMU_HFI_QTBL_INFO, 1);
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_AHB_FENCE_RANGE_0,
-			FENCE_RANGE_MASK);
-
-	/* Pass chipid to GMU FW, must happen before starting GMU */
-
-	/* Keep Core and Major bitfields unchanged */
-	chipid = adreno_dev->chipid & 0xFFFF0000;
-
-	/*
-	 * Compress minor and patch version into 8 bits
-	 * Bit 15-12: minor version
-	 * Bit 11-8: patch version
-	 */
-	chipid = chipid | (ADRENO_CHIPID_MINOR(adreno_dev->chipid) << 12)
-			| (ADRENO_CHIPID_PATCH(adreno_dev->chipid) << 8);
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_HFI_SFR_ADDR, chipid);
-
-	/* Configure power control and bring the GMU out of reset */
-	a6xx_gmu_power_config(device);
-	ret = a6xx_gmu_start(device);
-	if (ret)
-		return ret;
-
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_HFI_USE_REG)) {
-		ret = a6xx_gfx_rail_on(device);
-		if (ret) {
-			a6xx_oob_clear(adreno_dev,
-					OOB_BOOT_SLUMBER_CLEAR_MASK);
-			return ret;
-		}
-	}
-
-	if (gmu->idle_level < GPU_HW_SPTP_PC) {
-		ret = a6xx_sptprac_enable(adreno_dev);
-		if (ret)
-			return ret;
-	}
-
-	ret = a6xx_gmu_hfi_start(device);
-	if (ret)
-		return ret;
-
-	/* Make sure the write to start HFI happens before sending a message */
-	wmb();
-	return ret;
-}
-
-/*
- * a6xx_gmu_dcvs_nohfi() - request GMU to do DCVS without using HFI
- * @device: Pointer to KGSL device
- * @perf_idx: Index into GPU performance level table defined in
- *	HFI DCVS table message
- * @bw_idx: Index into GPU b/w table defined in HFI b/w table message
- *
- */
-static int a6xx_gmu_dcvs_nohfi(struct kgsl_device *device,
-		unsigned int perf_idx, unsigned int bw_idx)
-{
-	struct hfi_dcvs_cmd dcvs_cmd = {
-		.ack_type = ACK_NONBLOCK,
-		.freq = {
-			.perf_idx = perf_idx,
-			.clkset_opt = OPTION_AT_LEAST,
-		},
-		.bw = {
-			.bw_idx = bw_idx,
-		},
-	};
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-	union gpu_perf_vote vote;
-	int ret;
-
-	kgsl_gmu_regwrite(device, A6XX_GMU_DCVS_ACK_OPTION, dcvs_cmd.ack_type);
-
-	vote.fvote = dcvs_cmd.freq;
-	kgsl_gmu_regwrite(device, A6XX_GMU_DCVS_PERF_SETTING, vote.raw);
-
-	vote.bvote = dcvs_cmd.bw;
-	kgsl_gmu_regwrite(device, A6XX_GMU_DCVS_BW_SETTING, vote.raw);
-
-	ret = a6xx_oob_set(adreno_dev, OOB_DCVS_SET_MASK, OOB_DCVS_CHECK_MASK,
-		OOB_DCVS_CLEAR_MASK);
-
-	if (ret) {
-		dev_err(&gmu->pdev->dev, "DCVS OOB timed out\n");
-		goto done;
-	}
-
-	kgsl_gmu_regread(device, A6XX_GMU_DCVS_RETURN, &ret);
-	if (ret)
-		dev_err(&gmu->pdev->dev, "OOB DCVS error %d\n", ret);
-
-done:
-	a6xx_oob_clear(adreno_dev, OOB_DCVS_CLEAR_MASK);
-
-	return ret;
-}
-
-static bool a6xx_hw_isidle(struct adreno_device *adreno_dev)
-{
-	unsigned int reg;
-
-	kgsl_gmu_regread(KGSL_DEVICE(adreno_dev),
-		A6XX_GPU_GMU_AO_GPU_CX_BUSY_STATUS, &reg);
-	if (reg & GPUBUSYIGNAHB)
-		return false;
-	return true;
-}
-
-static bool idle_trandition_complete(unsigned int idle_level,
-	unsigned int gmu_power_reg,
-	unsigned int sptprac_clk_reg)
-{
-	if (idle_level != gmu_power_reg)
-		return false;
-
-	switch (idle_level) {
-	case GPU_HW_IFPC:
-		if (is_on(sptprac_clk_reg))
-			return false;
-		break;
-	/* other GMU idle levels can be added here */
-	case GPU_HW_ACTIVE:
-	default:
-		break;
-	}
-	return true;
-}
-
-static int a6xx_wait_for_lowest_idle(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-	unsigned int reg, reg1;
-	unsigned long t;
-	uint64_t ts1, ts2, ts3;
-
-	if (!kgsl_gmu_isenabled(device))
-		return 0;
-
-	ts1 = read_AO_counter(device);
-
-	t = jiffies + msecs_to_jiffies(GMU_IDLE_TIMEOUT);
-	do {
-		kgsl_gmu_regread(device,
-			A6XX_GPU_GMU_CX_GMU_RPMH_POWER_STATE, &reg);
-		kgsl_gmu_regread(device,
-			A6XX_GMU_SPTPRAC_PWR_CLK_STATUS, &reg1);
-
-		if (idle_trandition_complete(gmu->idle_level, reg, reg1))
-			return 0;
-		/* Wait 100us to reduce unnecessary AHB bus traffic */
-		usleep_range(10, 100);
-	} while (!time_after(jiffies, t));
-
-	ts2 = read_AO_counter(device);
-	/* Check one last time */
-
-	kgsl_gmu_regread(device, A6XX_GPU_GMU_CX_GMU_RPMH_POWER_STATE, &reg);
-	kgsl_gmu_regread(device, A6XX_GMU_SPTPRAC_PWR_CLK_STATUS, &reg1);
-
-	if (idle_trandition_complete(gmu->idle_level, reg, reg1))
-		return 0;
-
-	ts3 = read_AO_counter(device);
-	WARN(1, "Timeout waiting for lowest idle: %08x %llx %llx %llx %x\n",
-		reg, ts1, ts2, ts3, reg1);
-
-	return -ETIMEDOUT;
-}
-
-static int a6xx_wait_for_gmu_idle(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-	unsigned int status2;
-	uint64_t ts1;
-
-	ts1 = read_AO_counter(device);
-	if (timed_poll_check(device, A6XX_GPU_GMU_AO_GPU_CX_BUSY_STATUS,
-			0, GMU_START_TIMEOUT, CXGXCPUBUSYIGNAHB)) {
-		kgsl_gmu_regread(device,
-				A6XX_GPU_GMU_AO_GPU_CX_BUSY_STATUS2, &status2);
-		dev_err(&gmu->pdev->dev,
-				"GMU not idling: status2=0x%x %llx %llx\n",
-				status2, ts1, read_AO_counter(device));
-		return -ETIMEDOUT;
-	}
-
-	return 0;
-}
-
-/*
- * _load_gmu_firmware() - Load the ucode into the GPMU RAM & PDC/RSC
- * @device: Pointer to KGSL device
- */
-static int _load_gmu_firmware(struct kgsl_device *device)
-{
-	const struct firmware *fw = NULL;
-	const struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-	const struct adreno_gpu_core *gpucore = adreno_dev->gpucore;
-	int image_size, ret =  -EINVAL;
-
-	/* there is no GMU */
-	if (!kgsl_gmu_isenabled(device))
-		return 0;
-
-	/* GMU fw already saved and verified so do nothing new */
-	if (gmu->fw_image.hostptr != 0)
-		return 0;
-
-	if (gpucore->gpmufw_name == NULL)
-		return -EINVAL;
-
-	ret = request_firmware(&fw, gpucore->gpmufw_name, device->dev);
-	if (ret || fw == NULL) {
-		KGSL_CORE_ERR("request_firmware (%s) failed: %d\n",
-				gpucore->gpmufw_name, ret);
-		return ret;
-	}
-
-	image_size = PAGE_ALIGN(fw->size);
-
-	ret = allocate_gmu_image(gmu, image_size);
-
-	/* load into shared memory with GMU */
-	if (!ret)
-		memcpy(gmu->fw_image.hostptr, fw->data, fw->size);
-
-	release_firmware(fw);
-
-	return ret;
-}
-
-/*
- * a6xx_microcode_read() - Read microcode
- * @adreno_dev: Pointer to adreno device
- */
-static int a6xx_microcode_read(struct adreno_device *adreno_dev)
-{
-	int ret;
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_firmware *sqe_fw = ADRENO_FW(adreno_dev, ADRENO_FW_SQE);
-
-	if (sqe_fw->memdesc.hostptr == NULL) {
-		ret = _load_firmware(device, adreno_dev->gpucore->sqefw_name,
-				sqe_fw);
-		if (ret)
-			return ret;
-	}
-
-	return _load_gmu_firmware(device);
-}
-
-#define GBIF_CX_HALT_MASK BIT(1)
-
-static int a6xx_soft_reset(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int reg;
-	unsigned long time;
-	bool vbif_acked = false;
-
-	/*
-	 * For the soft reset case with GMU enabled this part is done
-	 * by the GMU firmware
-	 */
-	if (kgsl_gmu_isenabled(device) &&
-		!test_bit(ADRENO_DEVICE_HARD_RESET, &adreno_dev->priv))
-		return 0;
-
-
-	adreno_writereg(adreno_dev, ADRENO_REG_RBBM_SW_RESET_CMD, 1);
-	/*
-	 * Do a dummy read to get a brief read cycle delay for the
-	 * reset to take effect
-	 */
-	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_SW_RESET_CMD, &reg);
-	adreno_writereg(adreno_dev, ADRENO_REG_RBBM_SW_RESET_CMD, 0);
-
-	/* Wait for the VBIF reset ack to complete */
-	time = jiffies + msecs_to_jiffies(VBIF_RESET_ACK_TIMEOUT);
-
-	do {
-		kgsl_regread(device, A6XX_RBBM_VBIF_GX_RESET_STATUS, &reg);
-		if ((reg & VBIF_RESET_ACK_MASK) == VBIF_RESET_ACK_MASK) {
-			vbif_acked = true;
-			break;
-		}
-		cpu_relax();
-	} while (!time_after(jiffies, time));
-
-	if (!vbif_acked)
-		return -ETIMEDOUT;
-
-	/*
-	 * GBIF GX halt will be released automatically by sw_reset.
-	 * Release GBIF CX halt after sw_reset
-	 */
-	if (adreno_has_gbif(adreno_dev))
-		kgsl_regrmw(device, A6XX_GBIF_HALT, GBIF_CX_HALT_MASK, 0);
-
-	a6xx_sptprac_enable(adreno_dev);
-
-	return 0;
-}
-
-#define A6XX_STATE_OF_CHILD             (BIT(4) | BIT(5))
-#define A6XX_IDLE_FULL_LLM              BIT(0)
-#define A6XX_WAKEUP_ACK                 BIT(1)
-#define A6XX_IDLE_FULL_ACK              BIT(0)
-#define A6XX_VBIF_XIN_HALT_CTRL1_ACKS   (BIT(0) | BIT(1) | BIT(2) | BIT(3))
-
-static void a6xx_isense_disable(struct kgsl_device *device)
-{
-	unsigned int val;
-	const struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_LM) ||
-		!test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag))
-		return;
-
-	kgsl_gmu_regread(device, A6XX_GPU_CS_ENABLE_REG, &val);
-	if (val) {
-		kgsl_gmu_regwrite(device, A6XX_GPU_CS_ENABLE_REG, 0);
-		kgsl_gmu_regwrite(device, A6XX_GMU_ISENSE_CTRL, 0);
-	}
-}
-
-static int a6xx_llm_glm_handshake(struct kgsl_device *device)
-{
-	unsigned int val;
-	const struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_LM) ||
-		!test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag))
-		return 0;
-
-	kgsl_gmu_regread(device, A6XX_GMU_LLM_GLM_SLEEP_CTRL, &val);
-	if (!(val & A6XX_STATE_OF_CHILD)) {
-		kgsl_gmu_regrmw(device, A6XX_GMU_LLM_GLM_SLEEP_CTRL, 0, BIT(4));
-		kgsl_gmu_regrmw(device, A6XX_GMU_LLM_GLM_SLEEP_CTRL, 0,
-				A6XX_IDLE_FULL_LLM);
-		if (timed_poll_check(device, A6XX_GMU_LLM_GLM_SLEEP_STATUS,
-				A6XX_IDLE_FULL_ACK, GPU_RESET_TIMEOUT,
-				A6XX_IDLE_FULL_ACK)) {
-			dev_err(&gmu->pdev->dev, "LLM-GLM handshake failed\n");
-			return -EINVAL;
-		}
-	}
-
-	return 0;
-}
-
-
-static void a6xx_count_throttles(struct adreno_device *adreno_dev,
-	uint64_t adj)
-{
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_LM) ||
-		!test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag))
-		return;
-
-	kgsl_gmu_regread(KGSL_DEVICE(adreno_dev),
-		adreno_dev->lm_threshold_count,
-		&adreno_dev->lm_threshold_cross);
-}
-
-static int a6xx_complete_rpmh_votes(struct kgsl_device *device)
-{
-	int ret = 0;
-
-	if (!kgsl_gmu_isenabled(device))
-		return ret;
-
-	ret |= timed_poll_check(device, A6XX_RSCC_TCS0_DRV0_STATUS, BIT(0),
-			GPU_RESET_TIMEOUT, BIT(0));
-	ret |= timed_poll_check(device, A6XX_RSCC_TCS1_DRV0_STATUS, BIT(0),
-			GPU_RESET_TIMEOUT, BIT(0));
-	ret |= timed_poll_check(device, A6XX_RSCC_TCS2_DRV0_STATUS, BIT(0),
-			GPU_RESET_TIMEOUT, BIT(0));
-	ret |= timed_poll_check(device, A6XX_RSCC_TCS3_DRV0_STATUS, BIT(0),
-			GPU_RESET_TIMEOUT, BIT(0));
-
-	return ret;
-}
-
-static int a6xx_gmu_suspend(struct kgsl_device *device)
-{
-	/* Max GX clients on A6xx is 2: GMU and KMD */
-	int ret = 0, max_client_num = 2;
-	struct gmu_device *gmu = &device->gmu;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	/* do it only if LM feature is enabled */
-	/* Disable ISENSE if it's on */
-	a6xx_isense_disable(device);
-
-	/* LLM-GLM handshake sequence */
-	a6xx_llm_glm_handshake(device);
-
-	/* If SPTP_RAC is on, turn off SPTP_RAC HS */
-	a6xx_sptprac_disable(adreno_dev);
-
-	/* Disconnect GPU from BUS is not needed if CX GDSC goes off later */
-
-	/* Check no outstanding RPMh voting */
-	a6xx_complete_rpmh_votes(device);
-
-	if (gmu->gx_gdsc) {
-		if (regulator_is_enabled(gmu->gx_gdsc)) {
-			/* Switch gx gdsc control from GMU to CPU
-			 * force non-zero reference count in clk driver
-			 * so next disable call will turn
-			 * off the GDSC
-			 */
-			ret = regulator_enable(gmu->gx_gdsc);
-			if (ret)
-				dev_err(&gmu->pdev->dev,
-					"suspend fail: gx enable\n");
-
-			while ((max_client_num)) {
-				ret = regulator_disable(gmu->gx_gdsc);
-				if (!regulator_is_enabled(gmu->gx_gdsc))
-					break;
-				max_client_num -= 1;
-			}
-
-			if (!max_client_num)
-				dev_err(&gmu->pdev->dev,
-					"suspend fail: cannot disable gx\n");
-		}
-	}
-
-	return ret;
-}
-
-/*
- * a6xx_rpmh_gpu_pwrctrl() - GPU power control via RPMh/GMU interface
- * @adreno_dev: Pointer to adreno device
- * @mode: requested power mode
- * @arg1: first argument for mode control
- * @arg2: second argument for mode control
- */
-static int a6xx_rpmh_gpu_pwrctrl(struct adreno_device *adreno_dev,
-		unsigned int mode, unsigned int arg1, unsigned int arg2)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-	int ret;
-
-	switch (mode) {
-	case GMU_FW_START:
-		ret = a6xx_gmu_fw_start(device, arg1);
-		break;
-	case GMU_SUSPEND:
-		ret = a6xx_gmu_suspend(device);
-		break;
-	case GMU_FW_STOP:
-		if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_HFI_USE_REG))
-			a6xx_oob_clear(adreno_dev,
-					OOB_BOOT_SLUMBER_CLEAR_MASK);
-		ret = a6xx_rpmh_power_off_gpu(device);
-		break;
-	case GMU_DCVS_NOHFI:
-		ret = a6xx_gmu_dcvs_nohfi(device, arg1, arg2);
-		break;
-	case GMU_NOTIFY_SLUMBER:
-		ret = a6xx_notify_slumber(device);
-		break;
-	default:
-		dev_err(&gmu->pdev->dev,
-				"unsupported GMU power ctrl mode:%d\n", mode);
-		ret = -EINVAL;
-		break;
-	}
-
-	return ret;
-}
-
-/**
- * a6xx_reset() - Helper function to reset the GPU
- * @device: Pointer to the KGSL device structure for the GPU
- * @fault: Type of fault. Needed to skip soft reset for MMU fault
- *
- * Try to reset the GPU to recover from a fault.  First, try to do a low latency
- * soft reset.  If the soft reset fails for some reason, then bring out the big
- * guns and toggle the footswitch.
- */
-static int a6xx_reset(struct kgsl_device *device, int fault)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	int ret = -EINVAL;
-	int i = 0;
-
-	/* Use the regular reset sequence for No GMU */
-	if (!kgsl_gmu_isenabled(device))
-		return adreno_reset(device, fault);
-
-	/* Transition from ACTIVE to RESET state */
-	kgsl_pwrctrl_change_state(device, KGSL_STATE_RESET);
-
-	/* Try soft reset first */
-	if (!(fault & ADRENO_IOMMU_PAGE_FAULT)) {
-		int acked;
-
-		/* NMI */
-		kgsl_gmu_regwrite(device, A6XX_GMU_NMI_CONTROL_STATUS, 0);
-		kgsl_gmu_regwrite(device, A6XX_GMU_CM3_CFG, (1 << 9));
-
-		for (i = 0; i < 10; i++) {
-			kgsl_gmu_regread(device,
-					A6XX_GMU_NMI_CONTROL_STATUS, &acked);
-
-			/* NMI FW ACK recevied */
-			if (acked == 0x1)
-				break;
-
-			udelay(100);
-		}
-
-		if (acked) {
-			/* Make sure VBIF/GBIF is cleared before resetting */
-			ret = adreno_vbif_clear_pending_transactions(device);
-
-			if (ret == 0)
-				ret = adreno_soft_reset(device);
-		}
-
-		if (ret)
-			KGSL_DEV_ERR_ONCE(device, "Device soft reset failed\n");
-	}
-	if (ret) {
-		/* If soft reset failed/skipped, then pull the power */
-		set_bit(ADRENO_DEVICE_HARD_RESET, &adreno_dev->priv);
-		/* since device is officially off now clear start bit */
-		clear_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv);
-
-		/* Keep trying to start the device until it works */
-		for (i = 0; i < NUM_TIMES_RESET_RETRY; i++) {
-			ret = adreno_start(device, 0);
-			if (!ret)
-				break;
-
-			msleep(20);
-		}
-	}
-
-	clear_bit(ADRENO_DEVICE_HARD_RESET, &adreno_dev->priv);
-
-	if (ret)
-		return ret;
-
-	if (i != 0)
-		KGSL_DRV_WARN(device, "Device hard reset tried %d tries\n", i);
-
-	/*
-	 * If active_cnt is non-zero then the system was active before
-	 * going into a reset - put it back in that state
-	 */
-
-	if (atomic_read(&device->active_cnt))
-		kgsl_pwrctrl_change_state(device, KGSL_STATE_ACTIVE);
-	else
-		kgsl_pwrctrl_change_state(device, KGSL_STATE_NAP);
-
-	return ret;
-}
-
-static void a6xx_cp_hw_err_callback(struct adreno_device *adreno_dev, int bit)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int status1, status2;
-
-	kgsl_regread(device, A6XX_CP_INTERRUPT_STATUS, &status1);
-
-	if (status1 & BIT(A6XX_CP_OPCODE_ERROR)) {
-		unsigned int opcode;
-
-		kgsl_regwrite(device, A6XX_CP_SQE_STAT_ADDR, 1);
-		kgsl_regread(device, A6XX_CP_SQE_STAT_DATA, &opcode);
-		KGSL_DRV_CRIT_RATELIMIT(device,
-				"CP opcode error interrupt | opcode=0x%8.8x\n",
-				opcode);
-	}
-	if (status1 & BIT(A6XX_CP_UCODE_ERROR))
-		KGSL_DRV_CRIT_RATELIMIT(device, "CP ucode error interrupt\n");
-	if (status1 & BIT(A6XX_CP_HW_FAULT_ERROR)) {
-		kgsl_regread(device, A6XX_CP_HW_FAULT, &status2);
-		KGSL_DRV_CRIT_RATELIMIT(device,
-			"CP | Ringbuffer HW fault | status=%x\n",
-			status2);
-	}
-	if (status1 & BIT(A6XX_CP_REGISTER_PROTECTION_ERROR)) {
-		kgsl_regread(device, A6XX_CP_PROTECT_STATUS, &status2);
-		KGSL_DRV_CRIT_RATELIMIT(device,
-			"CP | Protected mode error | %s | addr=%x | status=%x\n",
-			status2 & (1 << 20) ? "READ" : "WRITE",
-			status2 & 0x3FFFF, status2);
-	}
-	if (status1 & BIT(A6XX_CP_AHB_ERROR))
-		KGSL_DRV_CRIT_RATELIMIT(device,
-			"CP AHB error interrupt\n");
-	if (status1 & BIT(A6XX_CP_VSD_PARITY_ERROR))
-		KGSL_DRV_CRIT_RATELIMIT(device,
-			"CP VSD decoder parity error\n");
-	if (status1 & BIT(A6XX_CP_ILLEGAL_INSTR_ERROR))
-		KGSL_DRV_CRIT_RATELIMIT(device,
-			"CP Illegal instruction error\n");
-
-}
-
-static void a6xx_err_callback(struct adreno_device *adreno_dev, int bit)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	switch (bit) {
-	case A6XX_INT_CP_AHB_ERROR:
-		KGSL_DRV_CRIT_RATELIMIT(device, "CP: AHB bus error\n");
-		break;
-	case A6XX_INT_ATB_ASYNCFIFO_OVERFLOW:
-		KGSL_DRV_CRIT_RATELIMIT(device, "RBBM: ATB ASYNC overflow\n");
-		break;
-	case A6XX_INT_RBBM_ATB_BUS_OVERFLOW:
-		KGSL_DRV_CRIT_RATELIMIT(device, "RBBM: ATB bus overflow\n");
-		break;
-	case A6XX_INT_UCHE_OOB_ACCESS:
-		KGSL_DRV_CRIT_RATELIMIT(device, "UCHE: Out of bounds access\n");
-		break;
-	case A6XX_INT_UCHE_TRAP_INTR:
-		KGSL_DRV_CRIT_RATELIMIT(device, "UCHE: Trap interrupt\n");
-		break;
-	default:
-		KGSL_DRV_CRIT_RATELIMIT(device, "Unknown interrupt %d\n", bit);
-	}
-}
-
-/* GPU System Cache control registers */
-#define A6XX_GPU_CX_MISC_SYSTEM_CACHE_CNTL_0   0x4
-#define A6XX_GPU_CX_MISC_SYSTEM_CACHE_CNTL_1   0x8
-
-static inline void _reg_rmw(void __iomem *regaddr,
-	unsigned int mask, unsigned int bits)
-{
-	unsigned int val = 0;
-
-	val = __raw_readl(regaddr);
-	/* Make sure the above read completes before we proceed  */
-	rmb();
-	val &= ~mask;
-	__raw_writel(val | bits, regaddr);
-	/* Make sure the above write posts before we proceed*/
-	wmb();
-}
-
-/*
- * a6xx_llc_configure_gpu_scid() - Program the sub-cache ID for all GPU blocks
- * @adreno_dev: The adreno device pointer
- */
-static void a6xx_llc_configure_gpu_scid(struct adreno_device *adreno_dev)
-{
-	uint32_t gpu_scid;
-	uint32_t gpu_cntl1_val = 0;
-	int i;
-	void __iomem *gpu_cx_reg;
-
-	gpu_scid = adreno_llc_get_scid(adreno_dev->gpu_llc_slice);
-	for (i = 0; i < A6XX_LLC_NUM_GPU_SCIDS; i++)
-		gpu_cntl1_val = (gpu_cntl1_val << A6XX_GPU_LLC_SCID_NUM_BITS)
-			| gpu_scid;
-
-	gpu_cx_reg = ioremap(A6XX_GPU_CX_REG_BASE, A6XX_GPU_CX_REG_SIZE);
-	_reg_rmw(gpu_cx_reg + A6XX_GPU_CX_MISC_SYSTEM_CACHE_CNTL_1,
-			A6XX_GPU_LLC_SCID_MASK, gpu_cntl1_val);
-	iounmap(gpu_cx_reg);
-}
-
-/*
- * a6xx_llc_configure_gpuhtw_scid() - Program the SCID for GPU pagetables
- * @adreno_dev: The adreno device pointer
- */
-static void a6xx_llc_configure_gpuhtw_scid(struct adreno_device *adreno_dev)
-{
-	uint32_t gpuhtw_scid;
-	void __iomem *gpu_cx_reg;
-
-	gpuhtw_scid = adreno_llc_get_scid(adreno_dev->gpuhtw_llc_slice);
-
-	gpu_cx_reg = ioremap(A6XX_GPU_CX_REG_BASE, A6XX_GPU_CX_REG_SIZE);
-	_reg_rmw(gpu_cx_reg + A6XX_GPU_CX_MISC_SYSTEM_CACHE_CNTL_1,
-			A6XX_GPUHTW_LLC_SCID_MASK,
-			gpuhtw_scid << A6XX_GPUHTW_LLC_SCID_SHIFT);
-	iounmap(gpu_cx_reg);
-}
-
-/*
- * a6xx_llc_enable_overrides() - Override the page attributes
- * @adreno_dev: The adreno device pointer
- */
-static void a6xx_llc_enable_overrides(struct adreno_device *adreno_dev)
-{
-	void __iomem *gpu_cx_reg;
-
-	/*
-	 * 0x3: readnoallocoverrideen=0
-	 *      read-no-alloc=0 - Allocate lines on read miss
-	 *      writenoallocoverrideen=1
-	 *      write-no-alloc=1 - Do not allocates lines on write miss
-	 */
-	gpu_cx_reg = ioremap(A6XX_GPU_CX_REG_BASE, A6XX_GPU_CX_REG_SIZE);
-	__raw_writel(0x3, gpu_cx_reg + A6XX_GPU_CX_MISC_SYSTEM_CACHE_CNTL_0);
-	/* Make sure the above write posts before we proceed*/
-	wmb();
-	iounmap(gpu_cx_reg);
-}
-
-static const char *fault_block[8] = {
-	[0] = "CP",
-	[1] = "UCHE",
-	[2] = "VFD",
-	[3] = "UCHE",
-	[4] = "CCU",
-	[5] = "unknown",
-	[6] = "CDP Prefetch",
-	[7] = "GPMU",
-};
-
-static const char *uche_client[8] = {
-	[0] = "VFD",
-	[1] = "SP",
-	[2] = "VSC",
-	[3] = "VPC",
-	[4] = "HLSQ",
-	[5] = "PC",
-	[6] = "LRZ",
-	[7] = "unknown",
-};
-
-static const char *a6xx_iommu_fault_block(struct adreno_device *adreno_dev,
-						unsigned int fsynr1)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int client_id;
-	unsigned int uche_client_id;
-
-	client_id = fsynr1 & 0xff;
-
-	if (client_id >= ARRAY_SIZE(fault_block))
-		return "unknown";
-	else if (client_id != 3)
-		return fault_block[client_id];
-
-	mutex_lock(&device->mutex);
-	kgsl_regread(device, A6XX_UCHE_CLIENT_PF, &uche_client_id);
-	mutex_unlock(&device->mutex);
-
-	return uche_client[uche_client_id & A6XX_UCHE_CLIENT_PF_CLIENT_ID_MASK];
-}
-
-static void a6xx_cp_callback(struct adreno_device *adreno_dev, int bit)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (adreno_is_preemption_enabled(adreno_dev))
-		a6xx_preemption_trigger(adreno_dev);
-
-	adreno_dispatcher_schedule(device);
-}
-
-/*
- * a6xx_gpc_err_int_callback() - Isr for GPC error interrupts
- * @adreno_dev: Pointer to device
- * @bit: Interrupt bit
- */
-static void a6xx_gpc_err_int_callback(struct adreno_device *adreno_dev, int bit)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	/*
-	 * GPC error is typically the result of mistake SW programming.
-	 * Force GPU fault for this interrupt so that we can debug it
-	 * with help of register dump.
-	 */
-
-	KGSL_DRV_CRIT_RATELIMIT(device, "RBBM: GPC error\n");
-	adreno_irqctrl(adreno_dev, 0);
-
-	/* Trigger a fault in the dispatcher - this will effect a restart */
-	adreno_set_gpu_fault(adreno_dev, ADRENO_SOFT_FAULT);
-	adreno_dispatcher_schedule(device);
-}
-
-#define A6XX_INT_MASK \
-	((1 << A6XX_INT_CP_AHB_ERROR) |			\
-	 (1 << A6XX_INT_ATB_ASYNCFIFO_OVERFLOW) |	\
-	 (1 << A6XX_INT_RBBM_GPC_ERROR) |		\
-	 (1 << A6XX_INT_CP_SW) |			\
-	 (1 << A6XX_INT_CP_HW_ERROR) |			\
-	 (1 << A6XX_INT_CP_IB2) |			\
-	 (1 << A6XX_INT_CP_IB1) |			\
-	 (1 << A6XX_INT_CP_RB) |			\
-	 (1 << A6XX_INT_CP_CACHE_FLUSH_TS) |		\
-	 (1 << A6XX_INT_RBBM_ATB_BUS_OVERFLOW) |	\
-	 (1 << A6XX_INT_RBBM_HANG_DETECT) |		\
-	 (1 << A6XX_INT_UCHE_OOB_ACCESS) |		\
-	 (1 << A6XX_INT_UCHE_TRAP_INTR))
-
-static struct adreno_irq_funcs a6xx_irq_funcs[32] = {
-	ADRENO_IRQ_CALLBACK(NULL),              /* 0 - RBBM_GPU_IDLE */
-	ADRENO_IRQ_CALLBACK(a6xx_err_callback), /* 1 - RBBM_AHB_ERROR */
-	ADRENO_IRQ_CALLBACK(NULL), /* 2 - UNUSED */
-	ADRENO_IRQ_CALLBACK(NULL), /* 3 - UNUSED */
-	ADRENO_IRQ_CALLBACK(NULL), /* 4 - UNUSED */
-	ADRENO_IRQ_CALLBACK(NULL), /* 5 - UNUSED */
-	/* 6 - RBBM_ATB_ASYNC_OVERFLOW */
-	ADRENO_IRQ_CALLBACK(a6xx_err_callback),
-	ADRENO_IRQ_CALLBACK(a6xx_gpc_err_int_callback), /* 7 - GPC_ERR */
-	ADRENO_IRQ_CALLBACK(a6xx_preemption_callback),/* 8 - CP_SW */
-	ADRENO_IRQ_CALLBACK(a6xx_cp_hw_err_callback), /* 9 - CP_HW_ERROR */
-	ADRENO_IRQ_CALLBACK(NULL),  /* 10 - CP_CCU_FLUSH_DEPTH_TS */
-	ADRENO_IRQ_CALLBACK(NULL), /* 11 - CP_CCU_FLUSH_COLOR_TS */
-	ADRENO_IRQ_CALLBACK(NULL), /* 12 - CP_CCU_RESOLVE_TS */
-	ADRENO_IRQ_CALLBACK(adreno_cp_callback), /* 13 - CP_IB2_INT */
-	ADRENO_IRQ_CALLBACK(adreno_cp_callback), /* 14 - CP_IB1_INT */
-	ADRENO_IRQ_CALLBACK(adreno_cp_callback), /* 15 - CP_RB_INT */
-	ADRENO_IRQ_CALLBACK(NULL), /* 16 - UNUSED */
-	ADRENO_IRQ_CALLBACK(NULL), /* 17 - CP_RB_DONE_TS */
-	ADRENO_IRQ_CALLBACK(NULL), /* 18 - CP_WT_DONE_TS */
-	ADRENO_IRQ_CALLBACK(NULL), /* 19 - UNUSED */
-	ADRENO_IRQ_CALLBACK(a6xx_cp_callback), /* 20 - CP_CACHE_FLUSH_TS */
-	ADRENO_IRQ_CALLBACK(NULL), /* 21 - UNUSED */
-	ADRENO_IRQ_CALLBACK(a6xx_err_callback), /* 22 - RBBM_ATB_BUS_OVERFLOW */
-	/* 23 - MISC_HANG_DETECT */
-	ADRENO_IRQ_CALLBACK(adreno_hang_int_callback),
-	ADRENO_IRQ_CALLBACK(a6xx_err_callback), /* 24 - UCHE_OOB_ACCESS */
-	ADRENO_IRQ_CALLBACK(a6xx_err_callback), /* 25 - UCHE_TRAP_INTR */
-	ADRENO_IRQ_CALLBACK(NULL), /* 26 - DEBBUS_INTR_0 */
-	ADRENO_IRQ_CALLBACK(NULL), /* 27 - DEBBUS_INTR_1 */
-	ADRENO_IRQ_CALLBACK(NULL), /* 28 - UNUSED */
-	ADRENO_IRQ_CALLBACK(NULL), /* 29 - UNUSED */
-	ADRENO_IRQ_CALLBACK(NULL), /* 30 - ISDB_CPU_IRQ */
-	ADRENO_IRQ_CALLBACK(NULL), /* 31 - ISDB_UNDER_DEBUG */
-};
-
-static struct adreno_irq a6xx_irq = {
-	.funcs = a6xx_irq_funcs,
-	.mask = A6XX_INT_MASK,
-};
-
-static struct adreno_snapshot_sizes a6xx_snap_sizes = {
-	.cp_pfp = 0x33,
-	.roq = 0x400,
-};
-
-static struct adreno_snapshot_data a6xx_snapshot_data = {
-	.sect_sizes = &a6xx_snap_sizes,
-};
-
-static struct adreno_coresight_register a6xx_coresight_regs[] = {
-	{ A6XX_DBGC_CFG_DBGBUS_SEL_A },
-	{ A6XX_DBGC_CFG_DBGBUS_SEL_B },
-	{ A6XX_DBGC_CFG_DBGBUS_SEL_C },
-	{ A6XX_DBGC_CFG_DBGBUS_SEL_D },
-	{ A6XX_DBGC_CFG_DBGBUS_CNTLT },
-	{ A6XX_DBGC_CFG_DBGBUS_CNTLM },
-	{ A6XX_DBGC_CFG_DBGBUS_OPL },
-	{ A6XX_DBGC_CFG_DBGBUS_OPE },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTL_0 },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTL_1 },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTL_2 },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTL_3 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKL_0 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKL_1 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKL_2 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKL_3 },
-	{ A6XX_DBGC_CFG_DBGBUS_BYTEL_0 },
-	{ A6XX_DBGC_CFG_DBGBUS_BYTEL_1 },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTE_0 },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTE_1 },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTE_2 },
-	{ A6XX_DBGC_CFG_DBGBUS_IVTE_3 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKE_0 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKE_1 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKE_2 },
-	{ A6XX_DBGC_CFG_DBGBUS_MASKE_3 },
-	{ A6XX_DBGC_CFG_DBGBUS_NIBBLEE },
-	{ A6XX_DBGC_CFG_DBGBUS_PTRC0 },
-	{ A6XX_DBGC_CFG_DBGBUS_PTRC1 },
-	{ A6XX_DBGC_CFG_DBGBUS_LOADREG },
-	{ A6XX_DBGC_CFG_DBGBUS_IDX },
-	{ A6XX_DBGC_CFG_DBGBUS_CLRC },
-	{ A6XX_DBGC_CFG_DBGBUS_LOADIVT },
-	{ A6XX_DBGC_VBIF_DBG_CNTL },
-	{ A6XX_DBGC_DBG_LO_HI_GPIO },
-	{ A6XX_DBGC_EXT_TRACE_BUS_CNTL },
-	{ A6XX_DBGC_READ_AHB_THROUGH_DBG },
-	{ A6XX_DBGC_CFG_DBGBUS_TRACE_BUF1 },
-	{ A6XX_DBGC_CFG_DBGBUS_TRACE_BUF2 },
-	{ A6XX_DBGC_EVT_CFG },
-	{ A6XX_DBGC_EVT_INTF_SEL_0 },
-	{ A6XX_DBGC_EVT_INTF_SEL_1 },
-	{ A6XX_DBGC_PERF_ATB_CFG },
-	{ A6XX_DBGC_PERF_ATB_COUNTER_SEL_0 },
-	{ A6XX_DBGC_PERF_ATB_COUNTER_SEL_1 },
-	{ A6XX_DBGC_PERF_ATB_COUNTER_SEL_2 },
-	{ A6XX_DBGC_PERF_ATB_COUNTER_SEL_3 },
-	{ A6XX_DBGC_PERF_ATB_TRIG_INTF_SEL_0 },
-	{ A6XX_DBGC_PERF_ATB_TRIG_INTF_SEL_1 },
-	{ A6XX_DBGC_PERF_ATB_DRAIN_CMD },
-	{ A6XX_DBGC_ECO_CNTL },
-	{ A6XX_DBGC_AHB_DBG_CNTL },
-};
-
-static struct adreno_coresight_register a6xx_coresight_regs_cx[] = {
-	{ A6XX_CX_DBGC_CFG_DBGBUS_SEL_A },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_SEL_B },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_SEL_C },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_SEL_D },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_CNTLT },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_CNTLM },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_OPL },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_OPE },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTL_0 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTL_1 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTL_2 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTL_3 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKL_0 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKL_1 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKL_2 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKL_3 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_0 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_1 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTE_0 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTE_1 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTE_2 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IVTE_3 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKE_0 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKE_1 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKE_2 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_MASKE_3 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_NIBBLEE },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_PTRC0 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_PTRC1 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_LOADREG },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_IDX },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_CLRC },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_LOADIVT },
-	{ A6XX_CX_DBGC_VBIF_DBG_CNTL },
-	{ A6XX_CX_DBGC_DBG_LO_HI_GPIO },
-	{ A6XX_CX_DBGC_EXT_TRACE_BUS_CNTL },
-	{ A6XX_CX_DBGC_READ_AHB_THROUGH_DBG },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF1 },
-	{ A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF2 },
-	{ A6XX_CX_DBGC_EVT_CFG },
-	{ A6XX_CX_DBGC_EVT_INTF_SEL_0 },
-	{ A6XX_CX_DBGC_EVT_INTF_SEL_1 },
-	{ A6XX_CX_DBGC_PERF_ATB_CFG },
-	{ A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_0 },
-	{ A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_1 },
-	{ A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_2 },
-	{ A6XX_CX_DBGC_PERF_ATB_COUNTER_SEL_3 },
-	{ A6XX_CX_DBGC_PERF_ATB_TRIG_INTF_SEL_0 },
-	{ A6XX_CX_DBGC_PERF_ATB_TRIG_INTF_SEL_1 },
-	{ A6XX_CX_DBGC_PERF_ATB_DRAIN_CMD },
-	{ A6XX_CX_DBGC_ECO_CNTL },
-	{ A6XX_CX_DBGC_AHB_DBG_CNTL },
-};
-
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_sel_a, &a6xx_coresight_regs[0]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_sel_b, &a6xx_coresight_regs[1]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_sel_c, &a6xx_coresight_regs[2]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_sel_d, &a6xx_coresight_regs[3]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_cntlt, &a6xx_coresight_regs[4]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_cntlm, &a6xx_coresight_regs[5]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_opl, &a6xx_coresight_regs[6]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ope, &a6xx_coresight_regs[7]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivtl_0, &a6xx_coresight_regs[8]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivtl_1, &a6xx_coresight_regs[9]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivtl_2, &a6xx_coresight_regs[10]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivtl_3, &a6xx_coresight_regs[11]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maskl_0, &a6xx_coresight_regs[12]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maskl_1, &a6xx_coresight_regs[13]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maskl_2, &a6xx_coresight_regs[14]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maskl_3, &a6xx_coresight_regs[15]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_bytel_0, &a6xx_coresight_regs[16]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_bytel_1, &a6xx_coresight_regs[17]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivte_0, &a6xx_coresight_regs[18]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivte_1, &a6xx_coresight_regs[19]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivte_2, &a6xx_coresight_regs[20]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ivte_3, &a6xx_coresight_regs[21]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maske_0, &a6xx_coresight_regs[22]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maske_1, &a6xx_coresight_regs[23]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maske_2, &a6xx_coresight_regs[24]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_maske_3, &a6xx_coresight_regs[25]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_nibblee, &a6xx_coresight_regs[26]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ptrc0, &a6xx_coresight_regs[27]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_ptrc1, &a6xx_coresight_regs[28]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_loadreg, &a6xx_coresight_regs[29]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_idx, &a6xx_coresight_regs[30]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_clrc, &a6xx_coresight_regs[31]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_loadivt, &a6xx_coresight_regs[32]);
-static ADRENO_CORESIGHT_ATTR(vbif_dbg_cntl, &a6xx_coresight_regs[33]);
-static ADRENO_CORESIGHT_ATTR(dbg_lo_hi_gpio, &a6xx_coresight_regs[34]);
-static ADRENO_CORESIGHT_ATTR(ext_trace_bus_cntl, &a6xx_coresight_regs[35]);
-static ADRENO_CORESIGHT_ATTR(read_ahb_through_dbg, &a6xx_coresight_regs[36]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_trace_buf1, &a6xx_coresight_regs[37]);
-static ADRENO_CORESIGHT_ATTR(cfg_dbgbus_trace_buf2, &a6xx_coresight_regs[38]);
-static ADRENO_CORESIGHT_ATTR(evt_cfg, &a6xx_coresight_regs[39]);
-static ADRENO_CORESIGHT_ATTR(evt_intf_sel_0, &a6xx_coresight_regs[40]);
-static ADRENO_CORESIGHT_ATTR(evt_intf_sel_1, &a6xx_coresight_regs[41]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_cfg, &a6xx_coresight_regs[42]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_counter_sel_0, &a6xx_coresight_regs[43]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_counter_sel_1, &a6xx_coresight_regs[44]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_counter_sel_2, &a6xx_coresight_regs[45]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_counter_sel_3, &a6xx_coresight_regs[46]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_trig_intf_sel_0,
-				&a6xx_coresight_regs[47]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_trig_intf_sel_1,
-				&a6xx_coresight_regs[48]);
-static ADRENO_CORESIGHT_ATTR(perf_atb_drain_cmd, &a6xx_coresight_regs[49]);
-static ADRENO_CORESIGHT_ATTR(eco_cntl, &a6xx_coresight_regs[50]);
-static ADRENO_CORESIGHT_ATTR(ahb_dbg_cntl, &a6xx_coresight_regs[51]);
-
-/*CX debug registers*/
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_sel_a,
-				&a6xx_coresight_regs_cx[0]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_sel_b,
-				&a6xx_coresight_regs_cx[1]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_sel_c,
-				&a6xx_coresight_regs_cx[2]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_sel_d,
-				&a6xx_coresight_regs_cx[3]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_cntlt,
-				&a6xx_coresight_regs_cx[4]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_cntlm,
-				&a6xx_coresight_regs_cx[5]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_opl,
-				&a6xx_coresight_regs_cx[6]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ope,
-				&a6xx_coresight_regs_cx[7]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivtl_0,
-				&a6xx_coresight_regs_cx[8]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivtl_1,
-				&a6xx_coresight_regs_cx[9]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivtl_2,
-				&a6xx_coresight_regs_cx[10]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivtl_3,
-				&a6xx_coresight_regs_cx[11]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maskl_0,
-				&a6xx_coresight_regs_cx[12]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maskl_1,
-				&a6xx_coresight_regs_cx[13]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maskl_2,
-				&a6xx_coresight_regs_cx[14]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maskl_3,
-				&a6xx_coresight_regs_cx[15]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_bytel_0,
-				&a6xx_coresight_regs_cx[16]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_bytel_1,
-				&a6xx_coresight_regs_cx[17]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivte_0,
-				&a6xx_coresight_regs_cx[18]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivte_1,
-				&a6xx_coresight_regs_cx[19]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivte_2,
-				&a6xx_coresight_regs_cx[20]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ivte_3,
-				&a6xx_coresight_regs_cx[21]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maske_0,
-				&a6xx_coresight_regs_cx[22]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maske_1,
-				&a6xx_coresight_regs_cx[23]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maske_2,
-				&a6xx_coresight_regs_cx[24]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_maske_3,
-				&a6xx_coresight_regs_cx[25]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_nibblee,
-				&a6xx_coresight_regs_cx[26]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ptrc0,
-				&a6xx_coresight_regs_cx[27]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_ptrc1,
-				&a6xx_coresight_regs_cx[28]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_loadreg,
-				&a6xx_coresight_regs_cx[29]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_idx,
-				&a6xx_coresight_regs_cx[30]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_clrc,
-				&a6xx_coresight_regs_cx[31]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_loadivt,
-				&a6xx_coresight_regs_cx[32]);
-static ADRENO_CORESIGHT_ATTR(cx_vbif_dbg_cntl,
-				&a6xx_coresight_regs_cx[33]);
-static ADRENO_CORESIGHT_ATTR(cx_dbg_lo_hi_gpio,
-				&a6xx_coresight_regs_cx[34]);
-static ADRENO_CORESIGHT_ATTR(cx_ext_trace_bus_cntl,
-				&a6xx_coresight_regs_cx[35]);
-static ADRENO_CORESIGHT_ATTR(cx_read_ahb_through_dbg,
-				&a6xx_coresight_regs_cx[36]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_trace_buf1,
-				&a6xx_coresight_regs_cx[37]);
-static ADRENO_CORESIGHT_ATTR(cx_cfg_dbgbus_trace_buf2,
-				&a6xx_coresight_regs_cx[38]);
-static ADRENO_CORESIGHT_ATTR(cx_evt_cfg,
-				&a6xx_coresight_regs_cx[39]);
-static ADRENO_CORESIGHT_ATTR(cx_evt_intf_sel_0,
-				&a6xx_coresight_regs_cx[40]);
-static ADRENO_CORESIGHT_ATTR(cx_evt_intf_sel_1,
-				&a6xx_coresight_regs_cx[41]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_cfg,
-				&a6xx_coresight_regs_cx[42]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_counter_sel_0,
-				&a6xx_coresight_regs_cx[43]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_counter_sel_1,
-				&a6xx_coresight_regs_cx[44]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_counter_sel_2,
-				&a6xx_coresight_regs_cx[45]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_counter_sel_3,
-				&a6xx_coresight_regs_cx[46]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_trig_intf_sel_0,
-				&a6xx_coresight_regs_cx[47]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_trig_intf_sel_1,
-				&a6xx_coresight_regs_cx[48]);
-static ADRENO_CORESIGHT_ATTR(cx_perf_atb_drain_cmd,
-				&a6xx_coresight_regs_cx[49]);
-static ADRENO_CORESIGHT_ATTR(cx_eco_cntl,
-				&a6xx_coresight_regs_cx[50]);
-static ADRENO_CORESIGHT_ATTR(cx_ahb_dbg_cntl,
-				&a6xx_coresight_regs_cx[51]);
-
-static struct attribute *a6xx_coresight_attrs[] = {
-	&coresight_attr_cfg_dbgbus_sel_a.attr.attr,
-	&coresight_attr_cfg_dbgbus_sel_b.attr.attr,
-	&coresight_attr_cfg_dbgbus_sel_c.attr.attr,
-	&coresight_attr_cfg_dbgbus_sel_d.attr.attr,
-	&coresight_attr_cfg_dbgbus_cntlt.attr.attr,
-	&coresight_attr_cfg_dbgbus_cntlm.attr.attr,
-	&coresight_attr_cfg_dbgbus_opl.attr.attr,
-	&coresight_attr_cfg_dbgbus_ope.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivtl_0.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivtl_1.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivtl_2.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivtl_3.attr.attr,
-	&coresight_attr_cfg_dbgbus_maskl_0.attr.attr,
-	&coresight_attr_cfg_dbgbus_maskl_1.attr.attr,
-	&coresight_attr_cfg_dbgbus_maskl_2.attr.attr,
-	&coresight_attr_cfg_dbgbus_maskl_3.attr.attr,
-	&coresight_attr_cfg_dbgbus_bytel_0.attr.attr,
-	&coresight_attr_cfg_dbgbus_bytel_1.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivte_0.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivte_1.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivte_2.attr.attr,
-	&coresight_attr_cfg_dbgbus_ivte_3.attr.attr,
-	&coresight_attr_cfg_dbgbus_maske_0.attr.attr,
-	&coresight_attr_cfg_dbgbus_maske_1.attr.attr,
-	&coresight_attr_cfg_dbgbus_maske_2.attr.attr,
-	&coresight_attr_cfg_dbgbus_maske_3.attr.attr,
-	&coresight_attr_cfg_dbgbus_nibblee.attr.attr,
-	&coresight_attr_cfg_dbgbus_ptrc0.attr.attr,
-	&coresight_attr_cfg_dbgbus_ptrc1.attr.attr,
-	&coresight_attr_cfg_dbgbus_loadreg.attr.attr,
-	&coresight_attr_cfg_dbgbus_idx.attr.attr,
-	&coresight_attr_cfg_dbgbus_clrc.attr.attr,
-	&coresight_attr_cfg_dbgbus_loadivt.attr.attr,
-	&coresight_attr_vbif_dbg_cntl.attr.attr,
-	&coresight_attr_dbg_lo_hi_gpio.attr.attr,
-	&coresight_attr_ext_trace_bus_cntl.attr.attr,
-	&coresight_attr_read_ahb_through_dbg.attr.attr,
-	&coresight_attr_cfg_dbgbus_trace_buf1.attr.attr,
-	&coresight_attr_cfg_dbgbus_trace_buf2.attr.attr,
-	&coresight_attr_evt_cfg.attr.attr,
-	&coresight_attr_evt_intf_sel_0.attr.attr,
-	&coresight_attr_evt_intf_sel_1.attr.attr,
-	&coresight_attr_perf_atb_cfg.attr.attr,
-	&coresight_attr_perf_atb_counter_sel_0.attr.attr,
-	&coresight_attr_perf_atb_counter_sel_1.attr.attr,
-	&coresight_attr_perf_atb_counter_sel_2.attr.attr,
-	&coresight_attr_perf_atb_counter_sel_3.attr.attr,
-	&coresight_attr_perf_atb_trig_intf_sel_0.attr.attr,
-	&coresight_attr_perf_atb_trig_intf_sel_1.attr.attr,
-	&coresight_attr_perf_atb_drain_cmd.attr.attr,
-	&coresight_attr_eco_cntl.attr.attr,
-	&coresight_attr_ahb_dbg_cntl.attr.attr,
-	NULL,
-};
-
-/*cx*/
-static struct attribute *a6xx_coresight_attrs_cx[] = {
-	&coresight_attr_cx_cfg_dbgbus_sel_a.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_sel_b.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_sel_c.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_sel_d.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_cntlt.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_cntlm.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_opl.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ope.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivtl_0.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivtl_1.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivtl_2.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivtl_3.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maskl_0.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maskl_1.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maskl_2.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maskl_3.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_bytel_0.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_bytel_1.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivte_0.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivte_1.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivte_2.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ivte_3.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maske_0.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maske_1.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maske_2.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_maske_3.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_nibblee.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ptrc0.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_ptrc1.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_loadreg.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_idx.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_clrc.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_loadivt.attr.attr,
-	&coresight_attr_cx_vbif_dbg_cntl.attr.attr,
-	&coresight_attr_cx_dbg_lo_hi_gpio.attr.attr,
-	&coresight_attr_cx_ext_trace_bus_cntl.attr.attr,
-	&coresight_attr_cx_read_ahb_through_dbg.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_trace_buf1.attr.attr,
-	&coresight_attr_cx_cfg_dbgbus_trace_buf2.attr.attr,
-	&coresight_attr_cx_evt_cfg.attr.attr,
-	&coresight_attr_cx_evt_intf_sel_0.attr.attr,
-	&coresight_attr_cx_evt_intf_sel_1.attr.attr,
-	&coresight_attr_cx_perf_atb_cfg.attr.attr,
-	&coresight_attr_cx_perf_atb_counter_sel_0.attr.attr,
-	&coresight_attr_cx_perf_atb_counter_sel_1.attr.attr,
-	&coresight_attr_cx_perf_atb_counter_sel_2.attr.attr,
-	&coresight_attr_cx_perf_atb_counter_sel_3.attr.attr,
-	&coresight_attr_cx_perf_atb_trig_intf_sel_0.attr.attr,
-	&coresight_attr_cx_perf_atb_trig_intf_sel_1.attr.attr,
-	&coresight_attr_cx_perf_atb_drain_cmd.attr.attr,
-	&coresight_attr_cx_eco_cntl.attr.attr,
-	&coresight_attr_cx_ahb_dbg_cntl.attr.attr,
-	NULL,
-};
-
-static const struct attribute_group a6xx_coresight_group = {
-	.attrs = a6xx_coresight_attrs,
-};
-
-static const struct attribute_group *a6xx_coresight_groups[] = {
-	&a6xx_coresight_group,
-	NULL,
-};
-
-static const struct attribute_group a6xx_coresight_group_cx = {
-	.attrs = a6xx_coresight_attrs_cx,
-};
-
-static const struct attribute_group *a6xx_coresight_groups_cx[] = {
-	&a6xx_coresight_group_cx,
-	NULL,
-};
-
-static struct adreno_coresight a6xx_coresight = {
-	.registers = a6xx_coresight_regs,
-	.count = ARRAY_SIZE(a6xx_coresight_regs),
-	.groups = a6xx_coresight_groups,
-};
-
-static struct adreno_coresight a6xx_coresight_cx = {
-	.registers = a6xx_coresight_regs_cx,
-	.count = ARRAY_SIZE(a6xx_coresight_regs_cx),
-	.groups = a6xx_coresight_groups_cx,
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_cp[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_0_LO,
-		A6XX_RBBM_PERFCTR_CP_0_HI, 0, A6XX_CP_PERFCTR_CP_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_1_LO,
-		A6XX_RBBM_PERFCTR_CP_1_HI, 1, A6XX_CP_PERFCTR_CP_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_2_LO,
-		A6XX_RBBM_PERFCTR_CP_2_HI, 2, A6XX_CP_PERFCTR_CP_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_3_LO,
-		A6XX_RBBM_PERFCTR_CP_3_HI, 3, A6XX_CP_PERFCTR_CP_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_4_LO,
-		A6XX_RBBM_PERFCTR_CP_4_HI, 4, A6XX_CP_PERFCTR_CP_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_5_LO,
-		A6XX_RBBM_PERFCTR_CP_5_HI, 5, A6XX_CP_PERFCTR_CP_SEL_5 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_6_LO,
-		A6XX_RBBM_PERFCTR_CP_6_HI, 6, A6XX_CP_PERFCTR_CP_SEL_6 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_7_LO,
-		A6XX_RBBM_PERFCTR_CP_7_HI, 7, A6XX_CP_PERFCTR_CP_SEL_7 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_8_LO,
-		A6XX_RBBM_PERFCTR_CP_8_HI, 8, A6XX_CP_PERFCTR_CP_SEL_8 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_9_LO,
-		A6XX_RBBM_PERFCTR_CP_9_HI, 9, A6XX_CP_PERFCTR_CP_SEL_9 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_10_LO,
-		A6XX_RBBM_PERFCTR_CP_10_HI, 10, A6XX_CP_PERFCTR_CP_SEL_10 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_11_LO,
-		A6XX_RBBM_PERFCTR_CP_11_HI, 11, A6XX_CP_PERFCTR_CP_SEL_11 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_12_LO,
-		A6XX_RBBM_PERFCTR_CP_12_HI, 12, A6XX_CP_PERFCTR_CP_SEL_12 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CP_13_LO,
-		A6XX_RBBM_PERFCTR_CP_13_HI, 13, A6XX_CP_PERFCTR_CP_SEL_13 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_rbbm[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RBBM_0_LO,
-		A6XX_RBBM_PERFCTR_RBBM_0_HI, 15, A6XX_RBBM_PERFCTR_RBBM_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RBBM_1_LO,
-		A6XX_RBBM_PERFCTR_RBBM_1_HI, 15, A6XX_RBBM_PERFCTR_RBBM_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RBBM_2_LO,
-		A6XX_RBBM_PERFCTR_RBBM_2_HI, 16, A6XX_RBBM_PERFCTR_RBBM_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RBBM_3_LO,
-		A6XX_RBBM_PERFCTR_RBBM_3_HI, 17, A6XX_RBBM_PERFCTR_RBBM_SEL_3 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_pc[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_0_LO,
-		A6XX_RBBM_PERFCTR_PC_0_HI, 18, A6XX_PC_PERFCTR_PC_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_1_LO,
-		A6XX_RBBM_PERFCTR_PC_1_HI, 19, A6XX_PC_PERFCTR_PC_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_2_LO,
-		A6XX_RBBM_PERFCTR_PC_2_HI, 20, A6XX_PC_PERFCTR_PC_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_3_LO,
-		A6XX_RBBM_PERFCTR_PC_3_HI, 21, A6XX_PC_PERFCTR_PC_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_4_LO,
-		A6XX_RBBM_PERFCTR_PC_4_HI, 22, A6XX_PC_PERFCTR_PC_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_5_LO,
-		A6XX_RBBM_PERFCTR_PC_5_HI, 23, A6XX_PC_PERFCTR_PC_SEL_5 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_6_LO,
-		A6XX_RBBM_PERFCTR_PC_6_HI, 24, A6XX_PC_PERFCTR_PC_SEL_6 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_PC_7_LO,
-		A6XX_RBBM_PERFCTR_PC_7_HI, 25, A6XX_PC_PERFCTR_PC_SEL_7 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_vfd[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_0_LO,
-		A6XX_RBBM_PERFCTR_VFD_0_HI, 26, A6XX_VFD_PERFCTR_VFD_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_1_LO,
-		A6XX_RBBM_PERFCTR_VFD_1_HI, 27, A6XX_VFD_PERFCTR_VFD_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_2_LO,
-		A6XX_RBBM_PERFCTR_VFD_2_HI, 28, A6XX_VFD_PERFCTR_VFD_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_3_LO,
-		A6XX_RBBM_PERFCTR_VFD_3_HI, 29, A6XX_VFD_PERFCTR_VFD_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_4_LO,
-		A6XX_RBBM_PERFCTR_VFD_4_HI, 30, A6XX_VFD_PERFCTR_VFD_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_5_LO,
-		A6XX_RBBM_PERFCTR_VFD_5_HI, 31, A6XX_VFD_PERFCTR_VFD_SEL_5 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_6_LO,
-		A6XX_RBBM_PERFCTR_VFD_6_HI, 32, A6XX_VFD_PERFCTR_VFD_SEL_6 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VFD_7_LO,
-		A6XX_RBBM_PERFCTR_VFD_7_HI, 33, A6XX_VFD_PERFCTR_VFD_SEL_7 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_hlsq[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_HLSQ_0_LO,
-		A6XX_RBBM_PERFCTR_HLSQ_0_HI, 34, A6XX_HLSQ_PERFCTR_HLSQ_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_HLSQ_1_LO,
-		A6XX_RBBM_PERFCTR_HLSQ_1_HI, 35, A6XX_HLSQ_PERFCTR_HLSQ_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_HLSQ_2_LO,
-		A6XX_RBBM_PERFCTR_HLSQ_2_HI, 36, A6XX_HLSQ_PERFCTR_HLSQ_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_HLSQ_3_LO,
-		A6XX_RBBM_PERFCTR_HLSQ_3_HI, 37, A6XX_HLSQ_PERFCTR_HLSQ_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_HLSQ_4_LO,
-		A6XX_RBBM_PERFCTR_HLSQ_4_HI, 38, A6XX_HLSQ_PERFCTR_HLSQ_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_HLSQ_5_LO,
-		A6XX_RBBM_PERFCTR_HLSQ_5_HI, 39, A6XX_HLSQ_PERFCTR_HLSQ_SEL_5 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_vpc[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VPC_0_LO,
-		A6XX_RBBM_PERFCTR_VPC_0_HI, 40, A6XX_VPC_PERFCTR_VPC_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VPC_1_LO,
-		A6XX_RBBM_PERFCTR_VPC_1_HI, 41, A6XX_VPC_PERFCTR_VPC_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VPC_2_LO,
-		A6XX_RBBM_PERFCTR_VPC_2_HI, 42, A6XX_VPC_PERFCTR_VPC_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VPC_3_LO,
-		A6XX_RBBM_PERFCTR_VPC_3_HI, 43, A6XX_VPC_PERFCTR_VPC_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VPC_4_LO,
-		A6XX_RBBM_PERFCTR_VPC_4_HI, 44, A6XX_VPC_PERFCTR_VPC_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VPC_5_LO,
-		A6XX_RBBM_PERFCTR_VPC_5_HI, 45, A6XX_VPC_PERFCTR_VPC_SEL_5 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_ccu[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CCU_0_LO,
-		A6XX_RBBM_PERFCTR_CCU_0_HI, 46, A6XX_RB_PERFCTR_CCU_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CCU_1_LO,
-		A6XX_RBBM_PERFCTR_CCU_1_HI, 47, A6XX_RB_PERFCTR_CCU_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CCU_2_LO,
-		A6XX_RBBM_PERFCTR_CCU_2_HI, 48, A6XX_RB_PERFCTR_CCU_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CCU_3_LO,
-		A6XX_RBBM_PERFCTR_CCU_3_HI, 49, A6XX_RB_PERFCTR_CCU_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CCU_4_LO,
-		A6XX_RBBM_PERFCTR_CCU_4_HI, 50, A6XX_RB_PERFCTR_CCU_SEL_4 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_tse[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TSE_0_LO,
-		A6XX_RBBM_PERFCTR_TSE_0_HI, 51, A6XX_GRAS_PERFCTR_TSE_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TSE_1_LO,
-		A6XX_RBBM_PERFCTR_TSE_1_HI, 52, A6XX_GRAS_PERFCTR_TSE_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TSE_2_LO,
-		A6XX_RBBM_PERFCTR_TSE_2_HI, 53, A6XX_GRAS_PERFCTR_TSE_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TSE_3_LO,
-		A6XX_RBBM_PERFCTR_TSE_3_HI, 54, A6XX_GRAS_PERFCTR_TSE_SEL_3 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_ras[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RAS_0_LO,
-		A6XX_RBBM_PERFCTR_RAS_0_HI, 55, A6XX_GRAS_PERFCTR_RAS_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RAS_1_LO,
-		A6XX_RBBM_PERFCTR_RAS_1_HI, 56, A6XX_GRAS_PERFCTR_RAS_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RAS_2_LO,
-		A6XX_RBBM_PERFCTR_RAS_2_HI, 57, A6XX_GRAS_PERFCTR_RAS_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RAS_3_LO,
-		A6XX_RBBM_PERFCTR_RAS_3_HI, 58, A6XX_GRAS_PERFCTR_RAS_SEL_3 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_uche[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_0_LO,
-		A6XX_RBBM_PERFCTR_UCHE_0_HI, 59, A6XX_UCHE_PERFCTR_UCHE_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_1_LO,
-		A6XX_RBBM_PERFCTR_UCHE_1_HI, 60, A6XX_UCHE_PERFCTR_UCHE_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_2_LO,
-		A6XX_RBBM_PERFCTR_UCHE_2_HI, 61, A6XX_UCHE_PERFCTR_UCHE_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_3_LO,
-		A6XX_RBBM_PERFCTR_UCHE_3_HI, 62, A6XX_UCHE_PERFCTR_UCHE_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_4_LO,
-		A6XX_RBBM_PERFCTR_UCHE_4_HI, 63, A6XX_UCHE_PERFCTR_UCHE_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_5_LO,
-		A6XX_RBBM_PERFCTR_UCHE_5_HI, 64, A6XX_UCHE_PERFCTR_UCHE_SEL_5 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_6_LO,
-		A6XX_RBBM_PERFCTR_UCHE_6_HI, 65, A6XX_UCHE_PERFCTR_UCHE_SEL_6 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_7_LO,
-		A6XX_RBBM_PERFCTR_UCHE_7_HI, 66, A6XX_UCHE_PERFCTR_UCHE_SEL_7 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_8_LO,
-		A6XX_RBBM_PERFCTR_UCHE_8_HI, 67, A6XX_UCHE_PERFCTR_UCHE_SEL_8 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_9_LO,
-		A6XX_RBBM_PERFCTR_UCHE_9_HI, 68, A6XX_UCHE_PERFCTR_UCHE_SEL_9 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_10_LO,
-		A6XX_RBBM_PERFCTR_UCHE_10_HI, 69,
-					A6XX_UCHE_PERFCTR_UCHE_SEL_10 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_UCHE_11_LO,
-		A6XX_RBBM_PERFCTR_UCHE_11_HI, 70,
-					A6XX_UCHE_PERFCTR_UCHE_SEL_11 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_tp[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_0_LO,
-		A6XX_RBBM_PERFCTR_TP_0_HI, 71, A6XX_TPL1_PERFCTR_TP_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_1_LO,
-		A6XX_RBBM_PERFCTR_TP_1_HI, 72, A6XX_TPL1_PERFCTR_TP_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_2_LO,
-		A6XX_RBBM_PERFCTR_TP_2_HI, 73, A6XX_TPL1_PERFCTR_TP_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_3_LO,
-		A6XX_RBBM_PERFCTR_TP_3_HI, 74, A6XX_TPL1_PERFCTR_TP_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_4_LO,
-		A6XX_RBBM_PERFCTR_TP_4_HI, 75, A6XX_TPL1_PERFCTR_TP_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_5_LO,
-		A6XX_RBBM_PERFCTR_TP_5_HI, 76, A6XX_TPL1_PERFCTR_TP_SEL_5 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_6_LO,
-		A6XX_RBBM_PERFCTR_TP_6_HI, 77, A6XX_TPL1_PERFCTR_TP_SEL_6 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_7_LO,
-		A6XX_RBBM_PERFCTR_TP_7_HI, 78, A6XX_TPL1_PERFCTR_TP_SEL_7 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_8_LO,
-		A6XX_RBBM_PERFCTR_TP_8_HI, 79, A6XX_TPL1_PERFCTR_TP_SEL_8 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_9_LO,
-		A6XX_RBBM_PERFCTR_TP_9_HI, 80, A6XX_TPL1_PERFCTR_TP_SEL_9 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_10_LO,
-		A6XX_RBBM_PERFCTR_TP_10_HI, 81, A6XX_TPL1_PERFCTR_TP_SEL_10 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_TP_11_LO,
-		A6XX_RBBM_PERFCTR_TP_11_HI, 82, A6XX_TPL1_PERFCTR_TP_SEL_11 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_sp[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_0_LO,
-		A6XX_RBBM_PERFCTR_SP_0_HI, 83, A6XX_SP_PERFCTR_SP_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_1_LO,
-		A6XX_RBBM_PERFCTR_SP_1_HI, 84, A6XX_SP_PERFCTR_SP_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_2_LO,
-		A6XX_RBBM_PERFCTR_SP_2_HI, 85, A6XX_SP_PERFCTR_SP_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_3_LO,
-		A6XX_RBBM_PERFCTR_SP_3_HI, 86, A6XX_SP_PERFCTR_SP_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_4_LO,
-		A6XX_RBBM_PERFCTR_SP_4_HI, 87, A6XX_SP_PERFCTR_SP_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_5_LO,
-		A6XX_RBBM_PERFCTR_SP_5_HI, 88, A6XX_SP_PERFCTR_SP_SEL_5 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_6_LO,
-		A6XX_RBBM_PERFCTR_SP_6_HI, 89, A6XX_SP_PERFCTR_SP_SEL_6 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_7_LO,
-		A6XX_RBBM_PERFCTR_SP_7_HI, 90, A6XX_SP_PERFCTR_SP_SEL_7 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_8_LO,
-		A6XX_RBBM_PERFCTR_SP_8_HI, 91, A6XX_SP_PERFCTR_SP_SEL_8 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_9_LO,
-		A6XX_RBBM_PERFCTR_SP_9_HI, 92, A6XX_SP_PERFCTR_SP_SEL_9 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_10_LO,
-		A6XX_RBBM_PERFCTR_SP_10_HI, 93, A6XX_SP_PERFCTR_SP_SEL_10 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_11_LO,
-		A6XX_RBBM_PERFCTR_SP_11_HI, 94, A6XX_SP_PERFCTR_SP_SEL_11 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_12_LO,
-		A6XX_RBBM_PERFCTR_SP_12_HI, 95, A6XX_SP_PERFCTR_SP_SEL_12 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_13_LO,
-		A6XX_RBBM_PERFCTR_SP_13_HI, 96, A6XX_SP_PERFCTR_SP_SEL_13 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_14_LO,
-		A6XX_RBBM_PERFCTR_SP_14_HI, 97, A6XX_SP_PERFCTR_SP_SEL_14 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_15_LO,
-		A6XX_RBBM_PERFCTR_SP_15_HI, 98, A6XX_SP_PERFCTR_SP_SEL_15 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_16_LO,
-		A6XX_RBBM_PERFCTR_SP_16_HI, 99, A6XX_SP_PERFCTR_SP_SEL_16 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_17_LO,
-		A6XX_RBBM_PERFCTR_SP_17_HI, 100, A6XX_SP_PERFCTR_SP_SEL_17 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_18_LO,
-		A6XX_RBBM_PERFCTR_SP_18_HI, 101, A6XX_SP_PERFCTR_SP_SEL_18 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_19_LO,
-		A6XX_RBBM_PERFCTR_SP_19_HI, 102, A6XX_SP_PERFCTR_SP_SEL_19 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_20_LO,
-		A6XX_RBBM_PERFCTR_SP_20_HI, 103, A6XX_SP_PERFCTR_SP_SEL_20 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_21_LO,
-		A6XX_RBBM_PERFCTR_SP_21_HI, 104, A6XX_SP_PERFCTR_SP_SEL_21 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_22_LO,
-		A6XX_RBBM_PERFCTR_SP_22_HI, 105, A6XX_SP_PERFCTR_SP_SEL_22 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_SP_23_LO,
-		A6XX_RBBM_PERFCTR_SP_23_HI, 106, A6XX_SP_PERFCTR_SP_SEL_23 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_rb[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_0_LO,
-		A6XX_RBBM_PERFCTR_RB_0_HI, 107, A6XX_RB_PERFCTR_RB_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_1_LO,
-		A6XX_RBBM_PERFCTR_RB_1_HI, 108, A6XX_RB_PERFCTR_RB_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_2_LO,
-		A6XX_RBBM_PERFCTR_RB_2_HI, 109, A6XX_RB_PERFCTR_RB_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_3_LO,
-		A6XX_RBBM_PERFCTR_RB_3_HI, 110, A6XX_RB_PERFCTR_RB_SEL_3 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_4_LO,
-		A6XX_RBBM_PERFCTR_RB_4_HI, 111, A6XX_RB_PERFCTR_RB_SEL_4 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_5_LO,
-		A6XX_RBBM_PERFCTR_RB_5_HI, 112, A6XX_RB_PERFCTR_RB_SEL_5 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_6_LO,
-		A6XX_RBBM_PERFCTR_RB_6_HI, 113, A6XX_RB_PERFCTR_RB_SEL_6 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_RB_7_LO,
-		A6XX_RBBM_PERFCTR_RB_7_HI, 114, A6XX_RB_PERFCTR_RB_SEL_7 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_vsc[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VSC_0_LO,
-		A6XX_RBBM_PERFCTR_VSC_0_HI, 115, A6XX_VSC_PERFCTR_VSC_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_VSC_1_LO,
-		A6XX_RBBM_PERFCTR_VSC_1_HI, 116, A6XX_VSC_PERFCTR_VSC_SEL_1 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_lrz[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_LRZ_0_LO,
-		A6XX_RBBM_PERFCTR_LRZ_0_HI, 117, A6XX_GRAS_PERFCTR_LRZ_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_LRZ_1_LO,
-		A6XX_RBBM_PERFCTR_LRZ_1_HI, 118, A6XX_GRAS_PERFCTR_LRZ_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_LRZ_2_LO,
-		A6XX_RBBM_PERFCTR_LRZ_2_HI, 119, A6XX_GRAS_PERFCTR_LRZ_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_LRZ_3_LO,
-		A6XX_RBBM_PERFCTR_LRZ_3_HI, 120, A6XX_GRAS_PERFCTR_LRZ_SEL_3 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_cmp[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CMP_0_LO,
-		A6XX_RBBM_PERFCTR_CMP_0_HI, 121, A6XX_RB_PERFCTR_CMP_SEL_0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CMP_1_LO,
-		A6XX_RBBM_PERFCTR_CMP_1_HI, 122, A6XX_RB_PERFCTR_CMP_SEL_1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CMP_2_LO,
-		A6XX_RBBM_PERFCTR_CMP_2_HI, 123, A6XX_RB_PERFCTR_CMP_SEL_2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_RBBM_PERFCTR_CMP_3_LO,
-		A6XX_RBBM_PERFCTR_CMP_3_HI, 124, A6XX_RB_PERFCTR_CMP_SEL_3 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_vbif[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_VBIF_PERF_CNT_LOW0,
-		A6XX_VBIF_PERF_CNT_HIGH0, -1, A6XX_VBIF_PERF_CNT_SEL0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_VBIF_PERF_CNT_LOW1,
-		A6XX_VBIF_PERF_CNT_HIGH1, -1, A6XX_VBIF_PERF_CNT_SEL1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_VBIF_PERF_CNT_LOW2,
-		A6XX_VBIF_PERF_CNT_HIGH2, -1, A6XX_VBIF_PERF_CNT_SEL2 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_VBIF_PERF_CNT_LOW3,
-		A6XX_VBIF_PERF_CNT_HIGH3, -1, A6XX_VBIF_PERF_CNT_SEL3 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_vbif_pwr[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_VBIF_PERF_PWR_CNT_LOW0,
-		A6XX_VBIF_PERF_PWR_CNT_HIGH0, -1, A6XX_VBIF_PERF_PWR_CNT_EN0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_VBIF_PERF_PWR_CNT_LOW1,
-		A6XX_VBIF_PERF_PWR_CNT_HIGH1, -1, A6XX_VBIF_PERF_PWR_CNT_EN1 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_VBIF_PERF_PWR_CNT_LOW2,
-		A6XX_VBIF_PERF_PWR_CNT_HIGH2, -1, A6XX_VBIF_PERF_PWR_CNT_EN2 },
-};
-
-
-static struct adreno_perfcount_register a6xx_perfcounters_gbif[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_GBIF_PERF_CNT_LOW0,
-		A6XX_GBIF_PERF_CNT_HIGH0, -1, A6XX_GBIF_PERF_CNT_SEL },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_GBIF_PERF_CNT_LOW1,
-		A6XX_GBIF_PERF_CNT_HIGH1, -1, A6XX_GBIF_PERF_CNT_SEL },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_GBIF_PERF_CNT_LOW2,
-		A6XX_GBIF_PERF_CNT_HIGH2, -1, A6XX_GBIF_PERF_CNT_SEL },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_GBIF_PERF_CNT_LOW3,
-		A6XX_GBIF_PERF_CNT_HIGH3, -1, A6XX_GBIF_PERF_CNT_SEL },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_gbif_pwr[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_GBIF_PWR_CNT_LOW0,
-		A6XX_GBIF_PWR_CNT_HIGH0, -1, A6XX_GBIF_PERF_PWR_CNT_EN },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_GBIF_PWR_CNT_LOW1,
-		A6XX_GBIF_PWR_CNT_HIGH1, -1, A6XX_GBIF_PERF_PWR_CNT_EN },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_GBIF_PWR_CNT_LOW2,
-		A6XX_GBIF_PWR_CNT_HIGH2, -1, A6XX_GBIF_PERF_PWR_CNT_EN },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_pwr[] = {
-	{ KGSL_PERFCOUNTER_BROKEN, 0, 0, 0, 0, -1, 0 },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_0_L,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_0_H, -1, 0 },
-};
-
-static struct adreno_perfcount_register a6xx_perfcounters_alwayson[] = {
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0, A6XX_CP_ALWAYS_ON_COUNTER_LO,
-		A6XX_CP_ALWAYS_ON_COUNTER_HI, -1 },
-};
-
-static struct adreno_perfcount_register a6xx_pwrcounters_gpmu[] = {
-	/*
-	 * A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_0 is used for the GPU
-	 * busy count (see the PWR group above). Mark it as broken
-	 * so it's not re-used.
-	 */
-	{ KGSL_PERFCOUNTER_BROKEN, 0, 0,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_0_L,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_0_H, -1,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_0, },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_1_L,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_1_H, -1,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_0, },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_2_L,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_2_H, -1,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_0, },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_3_L,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_3_H, -1,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_0, },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_4_L,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_4_H, -1,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_1, },
-	{ KGSL_PERFCOUNTER_NOT_USED, 0, 0,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_5_L,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_XOCLK_5_H, -1,
-		A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_1, },
-};
-
-/*
- * ADRENO_PERFCOUNTER_GROUP_RESTORE flag is enabled by default
- * because most of the perfcounter groups need to be restored
- * as part of preemption and IFPC. Perfcounter groups that are
- * not restored as part of preemption and IFPC should be defined
- * using A6XX_PERFCOUNTER_GROUP_FLAGS macro
- */
-#define A6XX_PERFCOUNTER_GROUP(offset, name) \
-	ADRENO_PERFCOUNTER_GROUP_FLAGS(a6xx, offset, name, \
-	ADRENO_PERFCOUNTER_GROUP_RESTORE)
-
-#define A6XX_PERFCOUNTER_GROUP_FLAGS(offset, name, flags) \
-	ADRENO_PERFCOUNTER_GROUP_FLAGS(a6xx, offset, name, flags)
-
-#define A6XX_POWER_COUNTER_GROUP(offset, name) \
-	ADRENO_POWER_COUNTER_GROUP(a6xx, offset, name)
-
-static struct adreno_perfcount_group a6xx_perfcounter_groups
-				[KGSL_PERFCOUNTER_GROUP_MAX] = {
-	A6XX_PERFCOUNTER_GROUP(CP, cp),
-	A6XX_PERFCOUNTER_GROUP_FLAGS(RBBM, rbbm, 0),
-	A6XX_PERFCOUNTER_GROUP(PC, pc),
-	A6XX_PERFCOUNTER_GROUP(VFD, vfd),
-	A6XX_PERFCOUNTER_GROUP(HLSQ, hlsq),
-	A6XX_PERFCOUNTER_GROUP(VPC, vpc),
-	A6XX_PERFCOUNTER_GROUP(CCU, ccu),
-	A6XX_PERFCOUNTER_GROUP(CMP, cmp),
-	A6XX_PERFCOUNTER_GROUP(TSE, tse),
-	A6XX_PERFCOUNTER_GROUP(RAS, ras),
-	A6XX_PERFCOUNTER_GROUP(LRZ, lrz),
-	A6XX_PERFCOUNTER_GROUP(UCHE, uche),
-	A6XX_PERFCOUNTER_GROUP(TP, tp),
-	A6XX_PERFCOUNTER_GROUP(SP, sp),
-	A6XX_PERFCOUNTER_GROUP(RB, rb),
-	A6XX_PERFCOUNTER_GROUP(VSC, vsc),
-	A6XX_PERFCOUNTER_GROUP_FLAGS(VBIF, vbif, 0),
-	A6XX_PERFCOUNTER_GROUP_FLAGS(VBIF_PWR, vbif_pwr,
-		ADRENO_PERFCOUNTER_GROUP_FIXED),
-	A6XX_PERFCOUNTER_GROUP_FLAGS(PWR, pwr,
-		ADRENO_PERFCOUNTER_GROUP_FIXED),
-	A6XX_PERFCOUNTER_GROUP_FLAGS(ALWAYSON, alwayson,
-		ADRENO_PERFCOUNTER_GROUP_FIXED),
-	A6XX_POWER_COUNTER_GROUP(GPMU, gpmu),
-};
-
-static struct adreno_perfcounters a6xx_perfcounters = {
-	a6xx_perfcounter_groups,
-	ARRAY_SIZE(a6xx_perfcounter_groups),
-};
-
-/* Program the GMU power counter to count GPU busy cycles */
-static int a6xx_enable_pwr_counters(struct adreno_device *adreno_dev,
-		unsigned int counter)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	/*
-	 * We have a limited number of power counters. Since we're not using
-	 * total GPU cycle count, return error if requested.
-	 */
-	if (counter == 0)
-		return -EINVAL;
-
-	if (!device->gmu.pdev)
-		return -ENODEV;
-
-	kgsl_regwrite(device, A6XX_GPU_GMU_AO_GPU_CX_BUSY_MASK, 0xFF000000);
-	kgsl_regrmw(device,
-			A6XX_GMU_CX_GMU_POWER_COUNTER_SELECT_0, 0xFF, 0x20);
-	kgsl_regwrite(device, A6XX_GMU_CX_GMU_POWER_COUNTER_ENABLE, 0x1);
-
-	return 0;
-}
-
-static void a6xx_efuse_speed_bin(struct adreno_device *adreno_dev)
-{
-	unsigned int val;
-	unsigned int speed_bin[3];
-	struct kgsl_device *device = &adreno_dev->dev;
-
-	if (of_property_read_u32_array(device->pdev->dev.of_node,
-		"qcom,gpu-speed-bin", speed_bin, 3))
-		return;
-
-	adreno_efuse_read_u32(adreno_dev, speed_bin[0], &val);
-
-	adreno_dev->speed_bin = (val & speed_bin[1]) >> speed_bin[2];
-}
-
-static const struct {
-	int (*check)(struct adreno_device *adreno_dev);
-	void (*func)(struct adreno_device *adreno_dev);
-} a6xx_efuse_funcs[] = {
-	{ adreno_is_a615, a6xx_efuse_speed_bin },
-	{ adreno_is_a616, a6xx_efuse_speed_bin },
-};
-
-static void a6xx_check_features(struct adreno_device *adreno_dev)
-{
-	unsigned int i;
-
-	if (adreno_efuse_map(adreno_dev))
-		return;
-	for (i = 0; i < ARRAY_SIZE(a6xx_efuse_funcs); i++) {
-		if (a6xx_efuse_funcs[i].check(adreno_dev))
-			a6xx_efuse_funcs[i].func(adreno_dev);
-	}
-
-	adreno_efuse_unmap(adreno_dev);
-}
-static void a6xx_platform_setup(struct adreno_device *adreno_dev)
-{
-	uint64_t addr;
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	/* Calculate SP local and private mem addresses */
-	addr = ALIGN(ADRENO_UCHE_GMEM_BASE + adreno_dev->gmem_size, SZ_64K);
-	adreno_dev->sp_local_gpuaddr = addr;
-	adreno_dev->sp_pvt_gpuaddr = addr + SZ_64K;
-
-	if (adreno_has_gbif(adreno_dev)) {
-		a6xx_perfcounter_groups[KGSL_PERFCOUNTER_GROUP_VBIF].regs =
-				a6xx_perfcounters_gbif;
-		a6xx_perfcounter_groups[KGSL_PERFCOUNTER_GROUP_VBIF].reg_count
-				= ARRAY_SIZE(a6xx_perfcounters_gbif);
-
-		a6xx_perfcounter_groups[KGSL_PERFCOUNTER_GROUP_VBIF_PWR].regs =
-				a6xx_perfcounters_gbif_pwr;
-		a6xx_perfcounter_groups[
-			KGSL_PERFCOUNTER_GROUP_VBIF_PWR].reg_count
-				= ARRAY_SIZE(a6xx_perfcounters_gbif_pwr);
-
-		gpudev->vbif_xin_halt_ctrl0_mask =
-				A6XX_GBIF_HALT_MASK;
-	} else
-		gpudev->vbif_xin_halt_ctrl0_mask =
-				A6XX_VBIF_XIN_HALT_CTRL0_MASK;
-
-	/* Check efuse bits for various capabilties */
-	a6xx_check_features(adreno_dev);
-}
-
-
-static unsigned int a6xx_ccu_invalidate(struct adreno_device *adreno_dev,
-	unsigned int *cmds)
-{
-	/* CCU_INVALIDATE_DEPTH */
-	*cmds++ = cp_packet(adreno_dev, CP_EVENT_WRITE, 1);
-	*cmds++ = 24;
-
-	/* CCU_INVALIDATE_COLOR */
-	*cmds++ = cp_packet(adreno_dev, CP_EVENT_WRITE, 1);
-	*cmds++ = 25;
-
-	return 4;
-}
-
-/* Register offset defines for A6XX, in order of enum adreno_regs */
-static unsigned int a6xx_register_offsets[ADRENO_REG_REGISTER_MAX] = {
-
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_BASE, A6XX_CP_RB_BASE),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_BASE_HI, A6XX_CP_RB_BASE_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR_ADDR_LO,
-				A6XX_CP_RB_RPTR_ADDR_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR_ADDR_HI,
-				A6XX_CP_RB_RPTR_ADDR_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_RPTR, A6XX_CP_RB_RPTR),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_WPTR, A6XX_CP_RB_WPTR),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_RB_CNTL, A6XX_CP_RB_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_ME_CNTL, A6XX_CP_SQE_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_CNTL, A6XX_CP_MISC_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_HW_FAULT, A6XX_CP_HW_FAULT),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_IB1_BASE, A6XX_CP_IB1_BASE),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_IB1_BASE_HI, A6XX_CP_IB1_BASE_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_IB1_BUFSZ, A6XX_CP_IB1_REM_SIZE),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_IB2_BASE, A6XX_CP_IB2_BASE),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_IB2_BASE_HI, A6XX_CP_IB2_BASE_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_IB2_BUFSZ, A6XX_CP_IB2_REM_SIZE),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_ROQ_ADDR, A6XX_CP_ROQ_DBG_ADDR),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_ROQ_DATA, A6XX_CP_ROQ_DBG_DATA),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_PREEMPT, A6XX_CP_CONTEXT_SWITCH_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_LO,
-			A6XX_CP_CONTEXT_SWITCH_SMMU_INFO_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_HI,
-			A6XX_CP_CONTEXT_SWITCH_SMMU_INFO_HI),
-	ADRENO_REG_DEFINE(
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_LO,
-			A6XX_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_LO),
-	ADRENO_REG_DEFINE(
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_HI,
-			A6XX_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_HI),
-	ADRENO_REG_DEFINE(
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_LO,
-			A6XX_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_LO),
-	ADRENO_REG_DEFINE(
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_HI,
-			A6XX_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_LO,
-			A6XX_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_HI,
-			A6XX_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_CP_PREEMPT_LEVEL_STATUS,
-			A6XX_CP_CONTEXT_SWITCH_LEVEL_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_STATUS, A6XX_RBBM_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_STATUS3, A6XX_RBBM_STATUS3),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_CTL, A6XX_RBBM_PERFCTR_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_CMD0,
-					A6XX_RBBM_PERFCTR_LOAD_CMD0),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_CMD1,
-					A6XX_RBBM_PERFCTR_LOAD_CMD1),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_CMD2,
-					A6XX_RBBM_PERFCTR_LOAD_CMD2),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_CMD3,
-					A6XX_RBBM_PERFCTR_LOAD_CMD3),
-
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_INT_0_MASK, A6XX_RBBM_INT_0_MASK),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_INT_0_STATUS, A6XX_RBBM_INT_0_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_CLOCK_CTL, A6XX_RBBM_CLOCK_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_INT_CLEAR_CMD,
-				A6XX_RBBM_INT_CLEAR_CMD),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SW_RESET_CMD, A6XX_RBBM_SW_RESET_CMD),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_BLOCK_SW_RESET_CMD,
-					  A6XX_RBBM_BLOCK_SW_RESET_CMD),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_BLOCK_SW_RESET_CMD2,
-					  A6XX_RBBM_BLOCK_SW_RESET_CMD2),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_LO,
-				A6XX_RBBM_PERFCTR_LOAD_VALUE_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_PERFCTR_LOAD_VALUE_HI,
-				A6XX_RBBM_PERFCTR_LOAD_VALUE_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_VBIF_VERSION, A6XX_VBIF_VERSION),
-	ADRENO_REG_DEFINE(ADRENO_REG_VBIF_XIN_HALT_CTRL0,
-				A6XX_VBIF_XIN_HALT_CTRL0),
-	ADRENO_REG_DEFINE(ADRENO_REG_VBIF_XIN_HALT_CTRL1,
-				A6XX_VBIF_XIN_HALT_CTRL1),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_GPR0_CNTL, A6XX_RBBM_GPR0_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_VBIF_GX_RESET_STATUS,
-				A6XX_RBBM_VBIF_GX_RESET_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_GBIF_HALT, A6XX_GBIF_HALT),
-	ADRENO_REG_DEFINE(ADRENO_REG_GBIF_HALT_ACK, A6XX_GBIF_HALT_ACK),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_ALWAYSON_COUNTER_LO,
-				A6XX_GMU_ALWAYS_ON_COUNTER_L),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_ALWAYSON_COUNTER_HI,
-				A6XX_GMU_ALWAYS_ON_COUNTER_H),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_AO_AHB_FENCE_CTRL,
-				A6XX_GMU_AO_AHB_FENCE_CTRL),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_AO_INTERRUPT_EN,
-				A6XX_GMU_AO_INTERRUPT_EN),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_AO_HOST_INTERRUPT_CLR,
-				A6XX_GMU_AO_HOST_INTERRUPT_CLR),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_AO_HOST_INTERRUPT_STATUS,
-				A6XX_GMU_AO_HOST_INTERRUPT_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_AO_HOST_INTERRUPT_MASK,
-				A6XX_GMU_AO_HOST_INTERRUPT_MASK),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_PWR_COL_KEEPALIVE,
-				A6XX_GMU_GMU_PWR_COL_KEEPALIVE),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_AHB_FENCE_STATUS,
-				A6XX_GMU_AHB_FENCE_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_HFI_CTRL_STATUS,
-				A6XX_GMU_HFI_CTRL_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_HFI_VERSION_INFO,
-				A6XX_GMU_HFI_VERSION_INFO),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_HFI_SFR_ADDR,
-				A6XX_GMU_HFI_SFR_ADDR),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_RPMH_POWER_STATE,
-				A6XX_GPU_GMU_CX_GMU_RPMH_POWER_STATE),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_GMU2HOST_INTR_CLR,
-				A6XX_GMU_GMU2HOST_INTR_CLR),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_GMU2HOST_INTR_INFO,
-				A6XX_GMU_GMU2HOST_INTR_INFO),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_GMU2HOST_INTR_MASK,
-				A6XX_GMU_GMU2HOST_INTR_MASK),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_HOST2GMU_INTR_SET,
-				A6XX_GMU_HOST2GMU_INTR_SET),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_HOST2GMU_INTR_CLR,
-				A6XX_GMU_HOST2GMU_INTR_CLR),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_HOST2GMU_INTR_RAW_INFO,
-				A6XX_GMU_HOST2GMU_INTR_RAW_INFO),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_NMI_CONTROL_STATUS,
-				A6XX_GMU_NMI_CONTROL_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_CM3_CFG,
-				A6XX_GMU_CM3_CFG),
-	ADRENO_REG_DEFINE(ADRENO_REG_GMU_RBBM_INT_UNMASKED_STATUS,
-				A6XX_GMU_RBBM_INT_UNMASKED_STATUS),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SECVID_TRUST_CONTROL,
-				A6XX_RBBM_SECVID_TRUST_CNTL),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE,
-				A6XX_RBBM_SECVID_TSB_TRUSTED_BASE_LO),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_BASE_HI,
-				A6XX_RBBM_SECVID_TSB_TRUSTED_BASE_HI),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SECVID_TSB_TRUSTED_SIZE,
-				A6XX_RBBM_SECVID_TSB_TRUSTED_SIZE),
-	ADRENO_REG_DEFINE(ADRENO_REG_RBBM_SECVID_TSB_CONTROL,
-				A6XX_RBBM_SECVID_TSB_CNTL),
-};
-
-static const struct adreno_reg_offsets a6xx_reg_offsets = {
-	.offsets = a6xx_register_offsets,
-	.offset_0 = ADRENO_REG_REGISTER_MAX,
-};
-
-static int a6xx_perfcounter_update(struct adreno_device *adreno_dev,
-	struct adreno_perfcount_register *reg, bool update_reg)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct cpu_gpu_lock *lock = adreno_dev->pwrup_reglist.hostptr;
-	struct reg_list_pair *reg_pair = (struct reg_list_pair *)(lock + 1);
-	unsigned int i;
-	unsigned long timeout = jiffies + msecs_to_jiffies(1000);
-	int ret = 0;
-
-	lock->flag_kmd = 1;
-	/* Write flag_kmd before turn */
-	wmb();
-	lock->turn = 0;
-	/* Write these fields before looping */
-	mb();
-
-	/*
-	 * Spin here while GPU ucode holds the lock, lock->flag_ucode will
-	 * be set to 0 after GPU ucode releases the lock. Minimum wait time
-	 * is 1 second and this should be enough for GPU to release the lock
-	 */
-	while (lock->flag_ucode == 1 && lock->turn == 0) {
-		cpu_relax();
-		/* Get the latest updates from GPU */
-		rmb();
-		/*
-		 * Make sure we wait at least 1sec for the lock,
-		 * if we did not get it after 1sec return an error.
-		 */
-		if (time_after(jiffies, timeout) &&
-			(lock->flag_ucode == 1 && lock->turn == 0)) {
-			ret = -EBUSY;
-			goto unlock;
-		}
-	}
-
-	/* Read flag_ucode and turn before list_length */
-	rmb();
-	/*
-	 * If the perfcounter select register is already present in reglist
-	 * update it, otherwise append the <select register, value> pair to
-	 * the end of the list.
-	 */
-	for (i = 0; i < lock->list_length >> 1; i++)
-		if (reg_pair[i].offset == reg->select)
-			break;
-
-	reg_pair[i].offset = reg->select;
-	reg_pair[i].val = reg->countable;
-	if (i == lock->list_length >> 1)
-		lock->list_length += 2;
-
-	if (update_reg)
-		kgsl_regwrite(device, reg->select, reg->countable);
-
-unlock:
-	/* All writes done before releasing the lock */
-	wmb();
-	lock->flag_kmd = 0;
-	return ret;
-}
-
-struct adreno_gpudev adreno_a6xx_gpudev = {
-	.reg_offsets = &a6xx_reg_offsets,
-	.start = a6xx_start,
-	.snapshot = a6xx_snapshot,
-	.snapshot_gmu = a6xx_snapshot_gmu,
-	.irq = &a6xx_irq,
-	.snapshot_data = &a6xx_snapshot_data,
-	.irq_trace = trace_kgsl_a5xx_irq_status,
-	.num_prio_levels = KGSL_PRIORITY_MAX_RB_LEVELS,
-	.platform_setup = a6xx_platform_setup,
-	.init = a6xx_init,
-	.rb_start = a6xx_rb_start,
-	.regulator_enable = a6xx_sptprac_enable,
-	.regulator_disable = a6xx_sptprac_disable,
-	.perfcounters = &a6xx_perfcounters,
-	.enable_pwr_counters = a6xx_enable_pwr_counters,
-	.count_throttles = a6xx_count_throttles,
-	.microcode_read = a6xx_microcode_read,
-	.enable_64bit = a6xx_enable_64bit,
-	.llc_configure_gpu_scid = a6xx_llc_configure_gpu_scid,
-	.llc_configure_gpuhtw_scid = a6xx_llc_configure_gpuhtw_scid,
-	.llc_enable_overrides = a6xx_llc_enable_overrides,
-	.oob_set = a6xx_oob_set,
-	.oob_clear = a6xx_oob_clear,
-	.gpu_keepalive = a6xx_gpu_keepalive,
-	.rpmh_gpu_pwrctrl = a6xx_rpmh_gpu_pwrctrl,
-	.hw_isidle = a6xx_hw_isidle, /* Replaced by NULL if GMU is disabled */
-	.wait_for_lowest_idle = a6xx_wait_for_lowest_idle,
-	.wait_for_gmu_idle = a6xx_wait_for_gmu_idle,
-	.iommu_fault_block = a6xx_iommu_fault_block,
-	.reset = a6xx_reset,
-	.soft_reset = a6xx_soft_reset,
-	.preemption_pre_ibsubmit = a6xx_preemption_pre_ibsubmit,
-	.preemption_post_ibsubmit = a6xx_preemption_post_ibsubmit,
-	.preemption_init = a6xx_preemption_init,
-	.preemption_schedule = a6xx_preemption_schedule,
-	.set_marker = a6xx_set_marker,
-	.preemption_context_init = a6xx_preemption_context_init,
-	.preemption_context_destroy = a6xx_preemption_context_destroy,
-	.gx_is_on = a6xx_gx_is_on,
-	.sptprac_is_on = a6xx_sptprac_is_on,
-	.ccu_invalidate = a6xx_ccu_invalidate,
-	.perfcounter_update = a6xx_perfcounter_update,
-	.coresight = {&a6xx_coresight, &a6xx_coresight_cx},
-};
diff --git a/drivers/gpu/msm/adreno_a6xx.h b/drivers/gpu/msm/adreno_a6xx.h
deleted file mode 100644
index bf1111c6204c..000000000000
--- a/drivers/gpu/msm/adreno_a6xx.h
+++ /dev/null
@@ -1,136 +0,0 @@
-/* Copyright (c) 2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#ifndef _ADRENO_A6XX_H_
-#define _ADRENO_A6XX_H_
-
-#include "a6xx_reg.h"
-
-#define CP_CLUSTER_FE		0x0
-#define CP_CLUSTER_SP_VS	0x1
-#define CP_CLUSTER_PC_VS	0x2
-#define CP_CLUSTER_GRAS		0x3
-#define CP_CLUSTER_SP_PS	0x4
-#define CP_CLUSTER_PS		0x5
-
-/**
- * struct a6xx_cp_preemption_record - CP context record for
- * preemption.
- * @magic: (00) Value at this offset must be equal to
- * A6XX_CP_CTXRECORD_MAGIC_REF.
- * @info: (04) Type of record. Written non-zero (usually) by CP.
- * we must set to zero for all ringbuffers.
- * @errno: (08) Error code. Initialize this to A6XX_CP_CTXRECORD_ERROR_NONE.
- * CP will update to another value if a preemption error occurs.
- * @data: (12) DATA field in YIELD and SET_MARKER packets.
- * Written by CP when switching out. Not used on switch-in. Initialized to 0.
- * @cntl: (16) RB_CNTL, saved and restored by CP. We must initialize this.
- * @rptr: (20) RB_RPTR, saved and restored by CP. We must initialize this.
- * @wptr: (24) RB_WPTR, saved and restored by CP. We must initialize this.
- * @_pad28: (28) Reserved/padding.
- * @rptr_addr: (32) RB_RPTR_ADDR_LO|HI saved and restored. We must initialize.
- * rbase: (40) RB_BASE_LO|HI saved and restored.
- * counter: (48) Pointer to preemption counter.
- */
-struct a6xx_cp_preemption_record {
-	uint32_t  magic;
-	uint32_t  info;
-	uint32_t  errno;
-	uint32_t  data;
-	uint32_t  cntl;
-	uint32_t  rptr;
-	uint32_t  wptr;
-	uint32_t  _pad28;
-	uint64_t  rptr_addr;
-	uint64_t  rbase;
-	uint64_t  counter;
-};
-
-/**
- * struct a6xx_cp_smmu_info - CP preemption SMMU info.
- * @magic: (00) The value at this offset must be equal to
- * A6XX_CP_SMMU_INFO_MAGIC_REF.
- * @_pad4: (04) Reserved/padding
- * @ttbr0: (08) Base address of the page table for the
- * incoming context.
- * @context_idr: (16) Context Identification Register value.
- */
-struct a6xx_cp_smmu_info {
-	uint32_t  magic;
-	uint32_t  _pad4;
-	uint64_t  ttbr0;
-	uint32_t  asid;
-	uint32_t  context_idr;
-};
-
-#define A6XX_CP_SMMU_INFO_MAGIC_REF     0x241350D5UL
-
-/**
- * struct cpu_gpu_spinlock - CP spinlock structure for power up list
- * @flag_ucode: flag value set by CP
- * @flag_kmd: flag value set by KMD
- * @turn: turn variable set by both CP and KMD
- * @list_length: this tells CP the last dword in the list:
- * 16 + (4 * (List_Length - 1))
- * @list_offset: this tells CP the start of preemption only list:
- * 16 + (4 * List_Offset)
- */
-struct cpu_gpu_lock {
-	uint32_t flag_ucode;
-	uint32_t flag_kmd;
-	uint32_t turn;
-	uint16_t list_length;
-	uint16_t list_offset;
-};
-
-#define A6XX_CP_CTXRECORD_MAGIC_REF     0xAE399D6EUL
-/* Size of each CP preemption record */
-#define A6XX_CP_CTXRECORD_SIZE_IN_BYTES     (2112 * 1024)
-/* Size of the preemption counter block (in bytes) */
-#define A6XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE   (16 * 4)
-/* Size of the user context record block (in bytes) */
-#define A6XX_CP_CTXRECORD_USER_RESTORE_SIZE (192 * 1024)
-/* Size of the performance counter save/restore block (in bytes) */
-#define A6XX_CP_PERFCOUNTER_SAVE_RESTORE_SIZE   (4 * 1024)
-
-#define A6XX_CP_RB_CNTL_DEFAULT (((ilog2(4) << 8) & 0x1F00) | \
-		(ilog2(KGSL_RB_DWORDS >> 1) & 0x3F))
-
-/* Preemption functions */
-void a6xx_preemption_trigger(struct adreno_device *adreno_dev);
-void a6xx_preemption_schedule(struct adreno_device *adreno_dev);
-void a6xx_preemption_start(struct adreno_device *adreno_dev);
-int a6xx_preemption_init(struct adreno_device *adreno_dev);
-
-unsigned int a6xx_preemption_post_ibsubmit(struct adreno_device *adreno_dev,
-		unsigned int *cmds);
-unsigned int a6xx_preemption_pre_ibsubmit(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb,
-		unsigned int *cmds, struct kgsl_context *context);
-
-unsigned int a6xx_set_marker(unsigned int *cmds,
-		enum adreno_cp_marker_type type);
-
-void a6xx_preemption_callback(struct adreno_device *adreno_dev, int bit);
-
-int a6xx_preemption_context_init(struct kgsl_context *context);
-
-void a6xx_preemption_context_destroy(struct kgsl_context *context);
-
-void a6xx_snapshot(struct adreno_device *adreno_dev,
-		struct kgsl_snapshot *snapshot);
-void a6xx_snapshot_gmu(struct adreno_device *adreno_dev,
-		struct kgsl_snapshot *snapshot);
-
-void a6xx_crashdump_init(struct adreno_device *adreno_dev);
-#endif
diff --git a/drivers/gpu/msm/adreno_a6xx_preempt.c b/drivers/gpu/msm/adreno_a6xx_preempt.c
deleted file mode 100644
index 89965f2c9b06..000000000000
--- a/drivers/gpu/msm/adreno_a6xx_preempt.c
+++ /dev/null
@@ -1,823 +0,0 @@
-/* Copyright (c) 2017-2018, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#include "adreno.h"
-#include "adreno_a6xx.h"
-#include "a6xx_reg.h"
-#include "adreno_trace.h"
-#include "adreno_pm4types.h"
-
-#define PREEMPT_RECORD(_field) \
-		offsetof(struct a6xx_cp_preemption_record, _field)
-
-#define PREEMPT_SMMU_RECORD(_field) \
-		offsetof(struct a6xx_cp_smmu_info, _field)
-
-enum {
-	SET_PSEUDO_REGISTER_SAVE_REGISTER_SMMU_INFO = 0,
-	SET_PSEUDO_REGISTER_SAVE_REGISTER_PRIV_NON_SECURE_SAVE_ADDR,
-	SET_PSEUDO_REGISTER_SAVE_REGISTER_PRIV_SECURE_SAVE_ADDR,
-	SET_PSEUDO_REGISTER_SAVE_REGISTER_NON_PRIV_SAVE_ADDR,
-	SET_PSEUDO_REGISTER_SAVE_REGISTER_COUNTER,
-};
-
-static void _update_wptr(struct adreno_device *adreno_dev, bool reset_timer)
-{
-	struct adreno_ringbuffer *rb = adreno_dev->cur_rb;
-	unsigned int wptr;
-	unsigned long flags;
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	/*
-	 * Need to make sure GPU is up before we read the
-	 * WPTR as fence doesn't wake GPU on read operation.
-	 */
-	if (in_interrupt() == 0) {
-		int status;
-
-		if (gpudev->oob_set) {
-			status = gpudev->oob_set(adreno_dev,
-				OOB_PREEMPTION_SET_MASK,
-				OOB_PREEMPTION_CHECK_MASK,
-				OOB_PREEMPTION_CLEAR_MASK);
-			if (status) {
-				adreno_set_gpu_fault(adreno_dev,
-					ADRENO_GMU_FAULT);
-				adreno_dispatcher_schedule(
-					KGSL_DEVICE(adreno_dev));
-				return;
-			}
-		}
-	}
-
-
-	spin_lock_irqsave(&rb->preempt_lock, flags);
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
-
-	if (wptr != rb->wptr) {
-		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR,
-			rb->wptr);
-		/*
-		 * In case something got submitted while preemption was on
-		 * going, reset the timer.
-		 */
-		reset_timer = true;
-	}
-
-	if (reset_timer)
-		rb->dispatch_q.expires = jiffies +
-			msecs_to_jiffies(adreno_drawobj_timeout);
-
-	spin_unlock_irqrestore(&rb->preempt_lock, flags);
-
-	if (in_interrupt() == 0) {
-		if (gpudev->oob_clear)
-			gpudev->oob_clear(adreno_dev,
-				OOB_PREEMPTION_CLEAR_MASK);
-	}
-}
-
-static inline bool adreno_move_preempt_state(struct adreno_device *adreno_dev,
-	enum adreno_preempt_states old, enum adreno_preempt_states new)
-{
-	return (atomic_cmpxchg(&adreno_dev->preempt.state, old, new) == old);
-}
-
-static void _a6xx_preemption_done(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int status;
-
-	/*
-	 * In the very unlikely case that the power is off, do nothing - the
-	 * state will be reset on power up and everybody will be happy
-	 */
-
-	if (!kgsl_state_is_awake(device))
-		return;
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &status);
-
-	if (status & 0x1) {
-		KGSL_DRV_ERR(device,
-			"Preemption not complete: status=%X cur=%d R/W=%X/%X next=%d R/W=%X/%X\n",
-			status, adreno_dev->cur_rb->id,
-			adreno_get_rptr(adreno_dev->cur_rb),
-			adreno_dev->cur_rb->wptr, adreno_dev->next_rb->id,
-			adreno_get_rptr(adreno_dev->next_rb),
-			adreno_dev->next_rb->wptr);
-
-		/* Set a fault and restart */
-		adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-		adreno_dispatcher_schedule(device);
-
-		return;
-	}
-
-	del_timer_sync(&adreno_dev->preempt.timer);
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT_LEVEL_STATUS, &status);
-
-	trace_adreno_preempt_done(adreno_dev->cur_rb, adreno_dev->next_rb,
-		status);
-
-	/* Clean up all the bits */
-	adreno_dev->prev_rb = adreno_dev->cur_rb;
-	adreno_dev->cur_rb = adreno_dev->next_rb;
-	adreno_dev->next_rb = NULL;
-
-	/* Update the wptr for the new command queue */
-	_update_wptr(adreno_dev, true);
-
-	/* Update the dispatcher timer for the new command queue */
-	mod_timer(&adreno_dev->dispatcher.timer,
-		adreno_dev->cur_rb->dispatch_q.expires);
-
-	/* Clear the preempt state */
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-}
-
-static void _a6xx_preemption_fault(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int status;
-
-	/*
-	 * If the power is on check the preemption status one more time - if it
-	 * was successful then just transition to the complete state
-	 */
-	if (kgsl_state_is_awake(device)) {
-		adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &status);
-
-		if (status == 0) {
-			adreno_set_preempt_state(adreno_dev,
-				ADRENO_PREEMPT_COMPLETE);
-
-			adreno_dispatcher_schedule(device);
-			return;
-		}
-	}
-
-	KGSL_DRV_ERR(device,
-		"Preemption timed out: cur=%d R/W=%X/%X, next=%d R/W=%X/%X\n",
-		adreno_dev->cur_rb->id,
-		adreno_get_rptr(adreno_dev->cur_rb), adreno_dev->cur_rb->wptr,
-		adreno_dev->next_rb->id,
-		adreno_get_rptr(adreno_dev->next_rb),
-		adreno_dev->next_rb->wptr);
-
-	adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
-	adreno_dispatcher_schedule(device);
-}
-
-static void _a6xx_preemption_worker(struct work_struct *work)
-{
-	struct adreno_preemption *preempt = container_of(work,
-		struct adreno_preemption, work);
-	struct adreno_device *adreno_dev = container_of(preempt,
-		struct adreno_device, preempt);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	/* Need to take the mutex to make sure that the power stays on */
-	mutex_lock(&device->mutex);
-
-	if (adreno_in_preempt_state(adreno_dev, ADRENO_PREEMPT_FAULTED))
-		_a6xx_preemption_fault(adreno_dev);
-
-	mutex_unlock(&device->mutex);
-}
-
-static void _a6xx_preemption_timer(unsigned long data)
-{
-	struct adreno_device *adreno_dev = (struct adreno_device *) data;
-
-	/* We should only be here from a triggered state */
-	if (!adreno_move_preempt_state(adreno_dev,
-		ADRENO_PREEMPT_TRIGGERED, ADRENO_PREEMPT_FAULTED))
-		return;
-
-	/* Schedule the worker to take care of the details */
-	queue_work(system_unbound_wq, &adreno_dev->preempt.work);
-}
-
-/* Find the highest priority active ringbuffer */
-static struct adreno_ringbuffer *a6xx_next_ringbuffer(
-		struct adreno_device *adreno_dev)
-{
-	struct adreno_ringbuffer *rb;
-	unsigned long flags;
-	unsigned int i;
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		bool empty;
-
-		spin_lock_irqsave(&rb->preempt_lock, flags);
-		empty = adreno_rb_empty(rb);
-		spin_unlock_irqrestore(&rb->preempt_lock, flags);
-
-		if (empty == false)
-			return rb;
-	}
-
-	return NULL;
-}
-
-#define GMU_ACTIVE_STATE_RETRY_MAX 100
-
-static int adreno_gmu_wait_for_active(struct adreno_device *adreno_dev)
-{
-	unsigned int reg, num_retries = 0;
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (!kgsl_gmu_isenabled(device))
-		return 0;
-
-	kgsl_gmu_regread(device,
-		A6XX_GPU_GMU_CX_GMU_RPMH_POWER_STATE, &reg);
-
-	while (reg != GPU_HW_ACTIVE) {
-		/* Wait for small time before trying again */
-		udelay(5);
-		kgsl_gmu_regread(device,
-			A6XX_GPU_GMU_CX_GMU_RPMH_POWER_STATE, &reg);
-
-		if (num_retries == GMU_ACTIVE_STATE_RETRY_MAX &&
-			reg != GPU_HW_ACTIVE) {
-			dev_err(adreno_dev->dev.dev,
-				"GMU failed to move to ACTIVE state: 0x%x\n",
-				reg);
-			return -ETIMEDOUT;
-		}
-		num_retries++;
-	}
-
-	return 0;
-}
-
-void a6xx_preemption_trigger(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-	struct adreno_ringbuffer *next;
-	uint64_t ttbr0, gpuaddr;
-	unsigned int contextidr, cntl;
-	unsigned long flags;
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	cntl = (((preempt->preempt_level << 6) & 0xC0) |
-		((preempt->skipsaverestore << 9) & 0x200) |
-		((preempt->usesgmem << 8) & 0x100) | 0x1);
-
-	/* Put ourselves into a possible trigger state */
-	if (!adreno_move_preempt_state(adreno_dev,
-		ADRENO_PREEMPT_NONE, ADRENO_PREEMPT_START))
-		return;
-
-	/* Get the next ringbuffer to preempt in */
-	next = a6xx_next_ringbuffer(adreno_dev);
-
-	/*
-	 * Nothing to do if every ringbuffer is empty or if the current
-	 * ringbuffer is the only active one
-	 */
-	if (next == NULL || next == adreno_dev->cur_rb) {
-		/*
-		 * Update any critical things that might have been skipped while
-		 * we were looking for a new ringbuffer
-		 */
-
-		if (next != NULL) {
-			_update_wptr(adreno_dev, false);
-
-			mod_timer(&adreno_dev->dispatcher.timer,
-				adreno_dev->cur_rb->dispatch_q.expires);
-		}
-
-		adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-		return;
-	}
-
-	/* Turn off the dispatcher timer */
-	del_timer(&adreno_dev->dispatcher.timer);
-
-	/*
-	 * This is the most critical section - we need to take care not to race
-	 * until we have programmed the CP for the switch
-	 */
-
-	spin_lock_irqsave(&next->preempt_lock, flags);
-
-	/*
-	 * Get the pagetable from the pagetable info.
-	 * The pagetable_desc is allocated and mapped at probe time, and
-	 * preemption_desc at init time, so no need to check if
-	 * sharedmem accesses to these memdescs succeed.
-	 */
-	kgsl_sharedmem_readq(&next->pagetable_desc, &ttbr0,
-		PT_INFO_OFFSET(ttbr0));
-	kgsl_sharedmem_readl(&next->pagetable_desc, &contextidr,
-		PT_INFO_OFFSET(contextidr));
-
-	kgsl_sharedmem_writel(device, &next->preemption_desc,
-		PREEMPT_RECORD(wptr), next->wptr);
-
-	preempt->count++;
-
-	spin_unlock_irqrestore(&next->preempt_lock, flags);
-
-	/* And write it to the smmu info */
-	kgsl_sharedmem_writeq(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(ttbr0), ttbr0);
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(context_idr), contextidr);
-
-	kgsl_sharedmem_readq(&device->scratch, &gpuaddr,
-		SCRATCH_PREEMPTION_CTXT_RESTORE_ADDR_OFFSET(next->id));
-
-	/*
-	 * Set a keepalive bit before the first preemption register write.
-	 * This is required since while each individual write to the context
-	 * switch registers will wake the GPU from collapse, it will not in
-	 * itself cause GPU activity. Thus, the GPU could technically be
-	 * re-collapsed between subsequent register writes leading to a
-	 * prolonged preemption sequence. The keepalive bit prevents any
-	 * further power collapse while it is set.
-	 * It is more efficient to use a keepalive+wake-on-fence approach here
-	 * rather than an OOB. Both keepalive and the fence are effectively
-	 * free when the GPU is already powered on, whereas an OOB requires an
-	 * unconditional handshake with the GMU.
-	 */
-	kgsl_gmu_regrmw(device, A6XX_GMU_AO_SPARE_CNTL, 0x0, 0x2);
-
-	/*
-	 * Fenced writes on this path will make sure the GPU is woken up
-	 * in case it was power collapsed by the GMU.
-	 */
-	adreno_gmu_fenced_write(adreno_dev,
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_LO,
-		lower_32_bits(next->preemption_desc.gpuaddr),
-		FENCE_STATUS_WRITEDROPPED1_MASK);
-
-	adreno_gmu_fenced_write(adreno_dev,
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_NON_SECURE_RESTORE_ADDR_HI,
-		upper_32_bits(next->preemption_desc.gpuaddr),
-		FENCE_STATUS_WRITEDROPPED1_MASK);
-
-	adreno_gmu_fenced_write(adreno_dev,
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_LO,
-		lower_32_bits(next->secure_preemption_desc.gpuaddr),
-		FENCE_STATUS_WRITEDROPPED1_MASK);
-
-	adreno_gmu_fenced_write(adreno_dev,
-		ADRENO_REG_CP_CONTEXT_SWITCH_PRIV_SECURE_RESTORE_ADDR_HI,
-		upper_32_bits(next->secure_preemption_desc.gpuaddr),
-		FENCE_STATUS_WRITEDROPPED1_MASK);
-
-	adreno_gmu_fenced_write(adreno_dev,
-		ADRENO_REG_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_LO,
-		lower_32_bits(gpuaddr),
-		FENCE_STATUS_WRITEDROPPED1_MASK);
-
-	adreno_gmu_fenced_write(adreno_dev,
-		ADRENO_REG_CP_CONTEXT_SWITCH_NON_PRIV_RESTORE_ADDR_HI,
-		upper_32_bits(gpuaddr),
-		FENCE_STATUS_WRITEDROPPED1_MASK);
-
-	/*
-	 * Above fence writes will make sure GMU comes out of
-	 * IFPC state if its was in IFPC state but it doesn't
-	 * guarantee that GMU FW actually moved to ACTIVE state
-	 * i.e. wake-up from IFPC is complete.
-	 * Wait for GMU to move to ACTIVE state before triggering
-	 * preemption. This is require to make sure CP doesn't
-	 * interrupt GMU during wake-up from IFPC.
-	 */
-	if (adreno_gmu_wait_for_active(adreno_dev)) {
-		adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-
-		adreno_set_gpu_fault(adreno_dev, ADRENO_GMU_FAULT);
-		adreno_dispatcher_schedule(device);
-		return;
-	}
-
-	adreno_dev->next_rb = next;
-
-	/* Start the timer to detect a stuck preemption */
-	mod_timer(&adreno_dev->preempt.timer,
-		jiffies + msecs_to_jiffies(ADRENO_PREEMPT_TIMEOUT));
-
-	trace_adreno_preempt_trigger(adreno_dev->cur_rb, adreno_dev->next_rb,
-		cntl);
-
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_TRIGGERED);
-
-	/* Trigger the preemption */
-	adreno_gmu_fenced_write(adreno_dev, ADRENO_REG_CP_PREEMPT, cntl,
-		FENCE_STATUS_WRITEDROPPED1_MASK);
-}
-
-void a6xx_preemption_callback(struct adreno_device *adreno_dev, int bit)
-{
-	unsigned int status;
-
-	if (!adreno_move_preempt_state(adreno_dev,
-		ADRENO_PREEMPT_TRIGGERED, ADRENO_PREEMPT_PENDING))
-		return;
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT, &status);
-
-	if (status & 0x1) {
-		KGSL_DRV_ERR(KGSL_DEVICE(adreno_dev),
-			"preempt interrupt with non-zero status: %X\n", status);
-
-		/*
-		 * Under the assumption that this is a race between the
-		 * interrupt and the register, schedule the worker to clean up.
-		 * If the status still hasn't resolved itself by the time we get
-		 * there then we have to assume something bad happened
-		 */
-		adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_COMPLETE);
-		adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
-		return;
-	}
-
-	/*
-	 * We can now safely clear the preemption keepalive bit, allowing
-	 * power collapse to resume its regular activity.
-	 */
-	kgsl_gmu_regrmw(KGSL_DEVICE(adreno_dev), A6XX_GMU_AO_SPARE_CNTL, 0x2,
-			0x0);
-
-	del_timer(&adreno_dev->preempt.timer);
-
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_PREEMPT_LEVEL_STATUS, &status);
-
-	trace_adreno_preempt_done(adreno_dev->cur_rb, adreno_dev->next_rb,
-		status);
-
-	adreno_dev->prev_rb = adreno_dev->cur_rb;
-	adreno_dev->cur_rb = adreno_dev->next_rb;
-	adreno_dev->next_rb = NULL;
-
-	/* Update the wptr if it changed while preemption was ongoing */
-	_update_wptr(adreno_dev, true);
-
-	/* Update the dispatcher timer for the new command queue */
-	mod_timer(&adreno_dev->dispatcher.timer,
-		adreno_dev->cur_rb->dispatch_q.expires);
-
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-
-	a6xx_preemption_trigger(adreno_dev);
-}
-
-void a6xx_preemption_schedule(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return;
-
-	mutex_lock(&device->mutex);
-
-	if (adreno_in_preempt_state(adreno_dev, ADRENO_PREEMPT_COMPLETE))
-		_a6xx_preemption_done(adreno_dev);
-
-	a6xx_preemption_trigger(adreno_dev);
-
-	mutex_unlock(&device->mutex);
-}
-
-unsigned int a6xx_preemption_pre_ibsubmit(
-		struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb,
-		unsigned int *cmds, struct kgsl_context *context)
-{
-	unsigned int *cmds_orig = cmds;
-	uint64_t gpuaddr = 0;
-
-	if (context) {
-		gpuaddr = context->user_ctxt_record->memdesc.gpuaddr;
-		*cmds++ = cp_type7_packet(CP_SET_PSEUDO_REGISTER, 15);
-	} else {
-		*cmds++ = cp_type7_packet(CP_SET_PSEUDO_REGISTER, 12);
-	}
-
-	/* NULL SMMU_INFO buffer - we track in KMD */
-	*cmds++ = SET_PSEUDO_REGISTER_SAVE_REGISTER_SMMU_INFO;
-	cmds += cp_gpuaddr(adreno_dev, cmds, 0x0);
-
-	*cmds++ = SET_PSEUDO_REGISTER_SAVE_REGISTER_PRIV_NON_SECURE_SAVE_ADDR;
-	cmds += cp_gpuaddr(adreno_dev, cmds, rb->preemption_desc.gpuaddr);
-
-	*cmds++ = SET_PSEUDO_REGISTER_SAVE_REGISTER_PRIV_SECURE_SAVE_ADDR;
-	cmds += cp_gpuaddr(adreno_dev, cmds,
-			rb->secure_preemption_desc.gpuaddr);
-
-	if (context) {
-
-		*cmds++ = SET_PSEUDO_REGISTER_SAVE_REGISTER_NON_PRIV_SAVE_ADDR;
-		cmds += cp_gpuaddr(adreno_dev, cmds, gpuaddr);
-	}
-
-	/*
-	 * There is no need to specify this address when we are about to
-	 * trigger preemption. This is because CP internally stores this
-	 * address specified here in the CP_SET_PSEUDO_REGISTER payload to
-	 * the context record and thus knows from where to restore
-	 * the saved perfcounters for the new ringbuffer.
-	 */
-	*cmds++ = SET_PSEUDO_REGISTER_SAVE_REGISTER_COUNTER;
-	cmds += cp_gpuaddr(adreno_dev, cmds,
-			rb->perfcounter_save_restore_desc.gpuaddr);
-
-	if (context) {
-		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-		struct adreno_context *drawctxt = ADRENO_CONTEXT(context);
-		struct adreno_ringbuffer *rb = drawctxt->rb;
-		uint64_t dest =
-			SCRATCH_PREEMPTION_CTXT_RESTORE_GPU_ADDR(device,
-			rb->id);
-
-		*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 2);
-		cmds += cp_gpuaddr(adreno_dev, cmds, dest);
-		*cmds++ = lower_32_bits(gpuaddr);
-		*cmds++ = upper_32_bits(gpuaddr);
-	}
-
-	return (unsigned int) (cmds - cmds_orig);
-}
-
-unsigned int a6xx_preemption_post_ibsubmit(struct adreno_device *adreno_dev,
-	unsigned int *cmds)
-{
-	unsigned int *cmds_orig = cmds;
-	struct adreno_ringbuffer *rb = adreno_dev->cur_rb;
-
-	if (rb) {
-		struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-		uint64_t dest = SCRATCH_PREEMPTION_CTXT_RESTORE_GPU_ADDR(device,
-			rb->id);
-
-		*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 2);
-		cmds += cp_gpuaddr(adreno_dev, cmds, dest);
-		*cmds++ = 0;
-		*cmds++ = 0;
-	}
-
-	*cmds++ = cp_type7_packet(CP_CONTEXT_SWITCH_YIELD, 4);
-	cmds += cp_gpuaddr(adreno_dev, cmds, 0x0);
-	*cmds++ = 1;
-	*cmds++ = 0;
-
-	return (unsigned int) (cmds - cmds_orig);
-}
-
-void a6xx_preemption_start(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-	struct adreno_ringbuffer *rb;
-	unsigned int i;
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return;
-
-	/* Force the state to be clear */
-	adreno_set_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE);
-
-	/* smmu_info is allocated and mapped in a6xx_preemption_iommu_init */
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(magic), A6XX_CP_SMMU_INFO_MAGIC_REF);
-	kgsl_sharedmem_writeq(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(ttbr0), MMU_DEFAULT_TTBR0(device));
-
-	/* The CP doesn't use the asid record, so poison it */
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(asid), 0xDECAFBAD);
-	kgsl_sharedmem_writel(device, &iommu->smmu_info,
-		PREEMPT_SMMU_RECORD(context_idr),
-		MMU_DEFAULT_CONTEXTIDR(device));
-
-	adreno_writereg64(adreno_dev,
-		ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_LO,
-		ADRENO_REG_CP_CONTEXT_SWITCH_SMMU_INFO_HI,
-		iommu->smmu_info.gpuaddr);
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		/*
-		 * preemption_desc is allocated and mapped at init time,
-		 * so no need to check sharedmem_writel return value
-		 */
-		kgsl_sharedmem_writel(device, &rb->preemption_desc,
-			PREEMPT_RECORD(rptr), 0);
-		kgsl_sharedmem_writel(device, &rb->preemption_desc,
-			PREEMPT_RECORD(wptr), 0);
-
-		adreno_ringbuffer_set_pagetable(rb,
-			device->mmu.defaultpagetable);
-	}
-}
-
-static int a6xx_preemption_ringbuffer_init(struct adreno_device *adreno_dev,
-	struct adreno_ringbuffer *rb, uint64_t counteraddr)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	int ret;
-
-	ret = kgsl_allocate_global(device, &rb->preemption_desc,
-		A6XX_CP_CTXRECORD_SIZE_IN_BYTES, 0, KGSL_MEMDESC_PRIVILEGED,
-		"preemption_desc");
-	if (ret)
-		return ret;
-
-	ret = kgsl_allocate_user(device, &rb->secure_preemption_desc,
-		A6XX_CP_CTXRECORD_SIZE_IN_BYTES,
-		KGSL_MEMFLAGS_SECURE | KGSL_MEMDESC_PRIVILEGED);
-	if (ret)
-		return ret;
-
-	ret = kgsl_iommu_map_global_secure_pt_entry(device,
-				&rb->secure_preemption_desc);
-	if (ret)
-		return ret;
-
-	ret = kgsl_allocate_global(device, &rb->perfcounter_save_restore_desc,
-		A6XX_CP_PERFCOUNTER_SAVE_RESTORE_SIZE, 0,
-		KGSL_MEMDESC_PRIVILEGED, "perfcounter_save_restore_desc");
-	if (ret)
-		return ret;
-
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(magic), A6XX_CP_CTXRECORD_MAGIC_REF);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(info), 0);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(data), 0);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(cntl), A6XX_CP_RB_CNTL_DEFAULT);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(rptr), 0);
-	kgsl_sharedmem_writel(device, &rb->preemption_desc,
-		PREEMPT_RECORD(wptr), 0);
-	kgsl_sharedmem_writeq(device, &rb->preemption_desc,
-		PREEMPT_RECORD(rptr_addr), SCRATCH_RPTR_GPU_ADDR(device,
-		rb->id));
-	kgsl_sharedmem_writeq(device, &rb->preemption_desc,
-		PREEMPT_RECORD(rbase), rb->buffer_desc.gpuaddr);
-	kgsl_sharedmem_writeq(device, &rb->preemption_desc,
-		PREEMPT_RECORD(counter), counteraddr);
-
-	return 0;
-}
-
-#ifdef CONFIG_QCOM_KGSL_IOMMU
-static int a6xx_preemption_iommu_init(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-
-	/* Allocate mem for storing preemption smmu record */
-	return kgsl_allocate_global(device, &iommu->smmu_info, PAGE_SIZE,
-		KGSL_MEMFLAGS_GPUREADONLY, KGSL_MEMDESC_PRIVILEGED,
-		"smmu_info");
-}
-
-static void a6xx_preemption_iommu_close(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-
-	kgsl_free_global(device, &iommu->smmu_info);
-}
-#else
-static int a6xx_preemption_iommu_init(struct adreno_device *adreno_dev)
-{
-	return -ENODEV;
-}
-
-static void a6xx_preemption_iommu_close(struct adreno_device *adreno_dev)
-{
-}
-#endif
-
-static void a6xx_preemption_close(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-	struct adreno_ringbuffer *rb;
-	unsigned int i;
-
-	del_timer(&preempt->timer);
-	kgsl_free_global(device, &preempt->counters);
-	a6xx_preemption_iommu_close(adreno_dev);
-
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		kgsl_free_global(device, &rb->preemption_desc);
-		kgsl_free_global(device, &rb->perfcounter_save_restore_desc);
-		kgsl_iommu_unmap_global_secure_pt_entry(device,
-				&rb->secure_preemption_desc);
-		kgsl_sharedmem_free(&rb->secure_preemption_desc);
-	}
-}
-
-int a6xx_preemption_init(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-	struct adreno_ringbuffer *rb;
-	int ret;
-	unsigned int i;
-	uint64_t addr;
-
-	/* We are dependent on IOMMU to make preemption go on the CP side */
-	if (kgsl_mmu_get_mmutype(device) != KGSL_MMU_TYPE_IOMMU)
-		return -ENODEV;
-
-	INIT_WORK(&preempt->work, _a6xx_preemption_worker);
-
-	setup_timer(&preempt->timer, _a6xx_preemption_timer,
-		(unsigned long) adreno_dev);
-
-	/* Allocate mem for storing preemption counters */
-	ret = kgsl_allocate_global(device, &preempt->counters,
-		adreno_dev->num_ringbuffers *
-		A6XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE, 0, 0,
-		"preemption_counters");
-	if (ret)
-		goto err;
-
-	addr = preempt->counters.gpuaddr;
-
-	/* Allocate mem for storing preemption switch record */
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		ret = a6xx_preemption_ringbuffer_init(adreno_dev, rb, addr);
-		if (ret)
-			goto err;
-
-		addr += A6XX_CP_CTXRECORD_PREEMPTION_COUNTER_SIZE;
-	}
-
-	ret = a6xx_preemption_iommu_init(adreno_dev);
-
-err:
-	if (ret)
-		a6xx_preemption_close(device);
-
-	return ret;
-}
-
-void a6xx_preemption_context_destroy(struct kgsl_context *context)
-{
-	struct kgsl_device *device = context->device;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return;
-
-	gpumem_free_entry(context->user_ctxt_record);
-
-	/* Put the extra ref from gpumem_alloc_entry() */
-	kgsl_mem_entry_put(context->user_ctxt_record);
-}
-
-int a6xx_preemption_context_init(struct kgsl_context *context)
-{
-	struct kgsl_device *device = context->device;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	uint64_t flags = 0;
-
-	if (!adreno_is_preemption_enabled(adreno_dev))
-		return 0;
-
-	if (context->flags & KGSL_CONTEXT_SECURE)
-		flags |= KGSL_MEMFLAGS_SECURE;
-
-	if (kgsl_is_compat_task())
-		flags |= KGSL_MEMFLAGS_FORCE_32BIT;
-
-	/*
-	 * gpumem_alloc_entry takes an extra refcount. Put it only when
-	 * destroying the context to keep the context record valid
-	 */
-	context->user_ctxt_record = gpumem_alloc_entry(context->dev_priv,
-			A6XX_CP_CTXRECORD_USER_RESTORE_SIZE, flags);
-	if (IS_ERR(context->user_ctxt_record)) {
-		int ret = PTR_ERR(context->user_ctxt_record);
-
-		context->user_ctxt_record = NULL;
-		return ret;
-	}
-
-	return 0;
-}
diff --git a/drivers/gpu/msm/adreno_a6xx_snapshot.c b/drivers/gpu/msm/adreno_a6xx_snapshot.c
deleted file mode 100644
index 7376a388e6ea..000000000000
--- a/drivers/gpu/msm/adreno_a6xx_snapshot.c
+++ /dev/null
@@ -1,1955 +0,0 @@
-/* Copyright (c) 2017-2018, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#include <linux/io.h>
-#include "kgsl.h"
-#include "adreno.h"
-#include "kgsl_snapshot.h"
-#include "adreno_snapshot.h"
-#include "a6xx_reg.h"
-#include "adreno_a6xx.h"
-#include "kgsl_gmu.h"
-
-#define A6XX_NUM_CTXTS 2
-#define A6XX_NUM_AXI_ARB_BLOCKS 2
-#define A6XX_NUM_XIN_AXI_BLOCKS 5
-#define A6XX_NUM_XIN_CORE_BLOCKS 4
-
-static const unsigned int a6xx_gras_cluster[] = {
-	0x8000, 0x8006, 0x8010, 0x8092, 0x8094, 0x809D, 0x80A0, 0x80A6,
-	0x80AF, 0x80F1, 0x8100, 0x8107, 0x8109, 0x8109, 0x8110, 0x8110,
-	0x8400, 0x840B,
-};
-
-static const unsigned int a6xx_ps_cluster_rac[] = {
-	0x8800, 0x8806, 0x8809, 0x8811, 0x8818, 0x881E, 0x8820, 0x8865,
-	0x8870, 0x8879, 0x8880, 0x8889, 0x8890, 0x8891, 0x8898, 0x8898,
-	0x88C0, 0x88C1, 0x88D0, 0x88E3, 0x8900, 0x890C, 0x890F, 0x891A,
-	0x8C00, 0x8C01, 0x8C08, 0x8C10, 0x8C17, 0x8C1F, 0x8C26, 0x8C33,
-};
-
-static const unsigned int a6xx_ps_cluster_rbp[] = {
-	0x88F0, 0x88F3, 0x890D, 0x890E, 0x8927, 0x8928, 0x8BF0, 0x8BF1,
-	0x8C02, 0x8C07, 0x8C11, 0x8C16, 0x8C20, 0x8C25,
-};
-
-static const unsigned int a6xx_ps_cluster[] = {
-	0x9200, 0x9216, 0x9218, 0x9236, 0x9300, 0x9306,
-};
-
-static const unsigned int a6xx_fe_cluster[] = {
-	0x9300, 0x9306, 0x9800, 0x9806, 0x9B00, 0x9B07, 0xA000, 0xA009,
-	0xA00E, 0xA0EF, 0xA0F8, 0xA0F8,
-};
-
-static const unsigned int a6xx_pc_vs_cluster[] = {
-	0x9100, 0x9108, 0x9300, 0x9306, 0x9980, 0x9981, 0x9B00, 0x9B07,
-};
-
-static const struct sel_reg {
-	unsigned int host_reg;
-	unsigned int cd_reg;
-	unsigned int val;
-} _a6xx_rb_rac_aperture = {
-	.host_reg = A6XX_RB_RB_SUB_BLOCK_SEL_CNTL_HOST,
-	.cd_reg = A6XX_RB_RB_SUB_BLOCK_SEL_CNTL_CD,
-	.val = 0x0,
-},
-_a6xx_rb_rbp_aperture = {
-	.host_reg = A6XX_RB_RB_SUB_BLOCK_SEL_CNTL_HOST,
-	.cd_reg = A6XX_RB_RB_SUB_BLOCK_SEL_CNTL_CD,
-	.val = 0x9,
-};
-
-static struct a6xx_cluster_registers {
-	unsigned int id;
-	const unsigned int *regs;
-	unsigned int num_sets;
-	const struct sel_reg *sel;
-	unsigned int offset0;
-	unsigned int offset1;
-} a6xx_clusters[] = {
-	{ CP_CLUSTER_GRAS, a6xx_gras_cluster, ARRAY_SIZE(a6xx_gras_cluster)/2,
-		NULL },
-	{ CP_CLUSTER_PS, a6xx_ps_cluster_rac, ARRAY_SIZE(a6xx_ps_cluster_rac)/2,
-		&_a6xx_rb_rac_aperture },
-	{ CP_CLUSTER_PS, a6xx_ps_cluster_rbp, ARRAY_SIZE(a6xx_ps_cluster_rbp)/2,
-		&_a6xx_rb_rbp_aperture },
-	{ CP_CLUSTER_PS, a6xx_ps_cluster, ARRAY_SIZE(a6xx_ps_cluster)/2,
-		NULL },
-	{ CP_CLUSTER_FE, a6xx_fe_cluster, ARRAY_SIZE(a6xx_fe_cluster)/2,
-		NULL },
-	{ CP_CLUSTER_PC_VS, a6xx_pc_vs_cluster,
-		ARRAY_SIZE(a6xx_pc_vs_cluster)/2, NULL },
-};
-
-struct a6xx_cluster_regs_info {
-	struct a6xx_cluster_registers *cluster;
-	unsigned int ctxt_id;
-};
-
-static const unsigned int a6xx_sp_vs_hlsq_cluster[] = {
-	0xB800, 0xB803, 0xB820, 0xB822,
-};
-
-static const unsigned int a6xx_sp_vs_sp_cluster[] = {
-	0xA800, 0xA824, 0xA830, 0xA83C, 0xA840, 0xA864, 0xA870, 0xA895,
-	0xA8A0, 0xA8AF, 0xA8C0, 0xA8C3,
-};
-
-static const unsigned int a6xx_hlsq_duplicate_cluster[] = {
-	0xBB10, 0xBB11, 0xBB20, 0xBB29,
-};
-
-static const unsigned int a6xx_hlsq_2d_duplicate_cluster[] = {
-	0xBD80, 0xBD80,
-};
-
-static const unsigned int a6xx_sp_duplicate_cluster[] = {
-	0xAB00, 0xAB00, 0xAB04, 0xAB05, 0xAB10, 0xAB1B, 0xAB20, 0xAB20,
-};
-
-static const unsigned int a6xx_tp_duplicate_cluster[] = {
-	0xB300, 0xB307, 0xB309, 0xB309, 0xB380, 0xB382,
-};
-
-static const unsigned int a6xx_sp_ps_hlsq_cluster[] = {
-	0xB980, 0xB980, 0xB982, 0xB987, 0xB990, 0xB99B, 0xB9A0, 0xB9A2,
-	0xB9C0, 0xB9C9,
-};
-
-static const unsigned int a6xx_sp_ps_hlsq_2d_cluster[] = {
-	0xBD80, 0xBD80,
-};
-
-static const unsigned int a6xx_sp_ps_sp_cluster[] = {
-	0xA980, 0xA9A8, 0xA9B0, 0xA9BC, 0xA9D0, 0xA9D3, 0xA9E0, 0xA9F3,
-	0xAA00, 0xAA00, 0xAA30, 0xAA31,
-};
-
-static const unsigned int a6xx_sp_ps_sp_2d_cluster[] = {
-	0xACC0, 0xACC0,
-};
-
-static const unsigned int a6xx_sp_ps_tp_cluster[] = {
-	0xB180, 0xB183, 0xB190, 0xB191,
-};
-
-static const unsigned int a6xx_sp_ps_tp_2d_cluster[] = {
-	0xB4C0, 0xB4D1,
-};
-
-static struct a6xx_cluster_dbgahb_registers {
-	unsigned int id;
-	unsigned int regbase;
-	unsigned int statetype;
-	const unsigned int *regs;
-	unsigned int num_sets;
-	unsigned int offset0;
-	unsigned int offset1;
-} a6xx_dbgahb_ctx_clusters[] = {
-	{ CP_CLUSTER_SP_VS, 0x0002E000, 0x41, a6xx_sp_vs_hlsq_cluster,
-		ARRAY_SIZE(a6xx_sp_vs_hlsq_cluster) / 2 },
-	{ CP_CLUSTER_SP_VS, 0x0002A000, 0x21, a6xx_sp_vs_sp_cluster,
-		ARRAY_SIZE(a6xx_sp_vs_sp_cluster) / 2 },
-	{ CP_CLUSTER_SP_VS, 0x0002E000, 0x41, a6xx_hlsq_duplicate_cluster,
-		ARRAY_SIZE(a6xx_hlsq_duplicate_cluster) / 2 },
-	{ CP_CLUSTER_SP_VS, 0x0002F000, 0x45, a6xx_hlsq_2d_duplicate_cluster,
-		ARRAY_SIZE(a6xx_hlsq_2d_duplicate_cluster) / 2 },
-	{ CP_CLUSTER_SP_VS, 0x0002A000, 0x21, a6xx_sp_duplicate_cluster,
-		ARRAY_SIZE(a6xx_sp_duplicate_cluster) / 2 },
-	{ CP_CLUSTER_SP_VS, 0x0002C000, 0x1, a6xx_tp_duplicate_cluster,
-		ARRAY_SIZE(a6xx_tp_duplicate_cluster) / 2 },
-	{ CP_CLUSTER_SP_PS, 0x0002E000, 0x42, a6xx_sp_ps_hlsq_cluster,
-		ARRAY_SIZE(a6xx_sp_ps_hlsq_cluster) / 2 },
-	{ CP_CLUSTER_SP_PS, 0x0002F000, 0x46, a6xx_sp_ps_hlsq_2d_cluster,
-		ARRAY_SIZE(a6xx_sp_ps_hlsq_2d_cluster) / 2 },
-	{ CP_CLUSTER_SP_PS, 0x0002A000, 0x22, a6xx_sp_ps_sp_cluster,
-		ARRAY_SIZE(a6xx_sp_ps_sp_cluster) / 2 },
-	{ CP_CLUSTER_SP_PS, 0x0002B000, 0x26, a6xx_sp_ps_sp_2d_cluster,
-		ARRAY_SIZE(a6xx_sp_ps_sp_2d_cluster) / 2 },
-	{ CP_CLUSTER_SP_PS, 0x0002C000, 0x2, a6xx_sp_ps_tp_cluster,
-		ARRAY_SIZE(a6xx_sp_ps_tp_cluster) / 2 },
-	{ CP_CLUSTER_SP_PS, 0x0002D000, 0x6, a6xx_sp_ps_tp_2d_cluster,
-		ARRAY_SIZE(a6xx_sp_ps_tp_2d_cluster) / 2 },
-	{ CP_CLUSTER_SP_PS, 0x0002E000, 0x42, a6xx_hlsq_duplicate_cluster,
-		ARRAY_SIZE(a6xx_hlsq_duplicate_cluster) / 2 },
-	{ CP_CLUSTER_SP_VS, 0x0002A000, 0x22, a6xx_sp_duplicate_cluster,
-		ARRAY_SIZE(a6xx_sp_duplicate_cluster) / 2 },
-	{ CP_CLUSTER_SP_VS, 0x0002C000, 0x2, a6xx_tp_duplicate_cluster,
-		ARRAY_SIZE(a6xx_tp_duplicate_cluster) / 2 },
-};
-
-struct a6xx_cluster_dbgahb_regs_info {
-	struct a6xx_cluster_dbgahb_registers *cluster;
-	unsigned int ctxt_id;
-};
-
-static const unsigned int a6xx_hlsq_non_ctx_registers[] = {
-	0xBE00, 0xBE01, 0xBE04, 0xBE05, 0xBE08, 0xBE09, 0xBE10, 0xBE15,
-	0xBE20, 0xBE23,
-};
-
-static const unsigned int a6xx_sp_non_ctx_registers[] = {
-	0xAE00, 0xAE04, 0xAE0C, 0xAE0C, 0xAE0F, 0xAE2B, 0xAE30, 0xAE32,
-	0xAE35, 0xAE35, 0xAE3A, 0xAE3F, 0xAE50, 0xAE52,
-};
-
-static const unsigned int a6xx_tp_non_ctx_registers[] = {
-	0xB600, 0xB601, 0xB604, 0xB605, 0xB610, 0xB61B, 0xB620, 0xB623,
-};
-
-static struct a6xx_non_ctx_dbgahb_registers {
-	unsigned int regbase;
-	unsigned int statetype;
-	const unsigned int *regs;
-	unsigned int num_sets;
-	unsigned int offset;
-} a6xx_non_ctx_dbgahb[] = {
-	{ 0x0002F800, 0x40, a6xx_hlsq_non_ctx_registers,
-		ARRAY_SIZE(a6xx_hlsq_non_ctx_registers) / 2 },
-	{ 0x0002B800, 0x20, a6xx_sp_non_ctx_registers,
-		ARRAY_SIZE(a6xx_sp_non_ctx_registers) / 2 },
-	{ 0x0002D800, 0x0, a6xx_tp_non_ctx_registers,
-		ARRAY_SIZE(a6xx_tp_non_ctx_registers) / 2 },
-};
-
-static const unsigned int a6xx_vbif_ver_20xxxxxx_registers[] = {
-	/* VBIF */
-	0x3000, 0x3007, 0x300C, 0x3014, 0x3018, 0x302D, 0x3030, 0x3031,
-	0x3034, 0x3036, 0x303C, 0x303D, 0x3040, 0x3040, 0x3042, 0x3042,
-	0x3049, 0x3049, 0x3058, 0x3058, 0x305A, 0x3061, 0x3064, 0x3068,
-	0x306C, 0x306D, 0x3080, 0x3088, 0x308B, 0x308C, 0x3090, 0x3094,
-	0x3098, 0x3098, 0x309C, 0x309C, 0x30C0, 0x30C0, 0x30C8, 0x30C8,
-	0x30D0, 0x30D0, 0x30D8, 0x30D8, 0x30E0, 0x30E0, 0x3100, 0x3100,
-	0x3108, 0x3108, 0x3110, 0x3110, 0x3118, 0x3118, 0x3120, 0x3120,
-	0x3124, 0x3125, 0x3129, 0x3129, 0x3131, 0x3131, 0x3154, 0x3154,
-	0x3156, 0x3156, 0x3158, 0x3158, 0x315A, 0x315A, 0x315C, 0x315C,
-	0x315E, 0x315E, 0x3160, 0x3160, 0x3162, 0x3162, 0x340C, 0x340C,
-	0x3410, 0x3410, 0x3800, 0x3801,
-};
-
-static const unsigned int a6xx_gbif_registers[] = {
-	/* GBIF */
-	0x3C00, 0X3C0B, 0X3C40, 0X3C47, 0X3CC0, 0X3CD1, 0xE3A, 0xE3A,
-};
-
-static const unsigned int a6xx_gmu_gx_registers[] = {
-	/* GMU GX */
-	0x1A800, 0x1A800, 0x1A810, 0x1A813, 0x1A816, 0x1A816, 0x1A818, 0x1A81B,
-	0x1A81E, 0x1A81E, 0x1A820, 0x1A823, 0x1A826, 0x1A826, 0x1A828, 0x1A82B,
-	0x1A82E, 0x1A82E, 0x1A830, 0x1A833, 0x1A836, 0x1A836, 0x1A838, 0x1A83B,
-	0x1A83E, 0x1A83E, 0x1A840, 0x1A843, 0x1A846, 0x1A846, 0x1A880, 0x1A884,
-	0x1A900, 0x1A92B, 0x1A940, 0x1A940,
-};
-
-static const unsigned int a6xx_gmu_registers[] = {
-	/* GMU TCM */
-	0x1B400, 0x1C3FF, 0x1C400, 0x1D3FF,
-	/* GMU CX */
-	0x1F400, 0x1F407, 0x1F410, 0x1F412, 0x1F500, 0x1F500, 0x1F507, 0x1F50A,
-	0x1F800, 0x1F804, 0x1F807, 0x1F808, 0x1F80B, 0x1F80C, 0x1F80F, 0x1F81C,
-	0x1F824, 0x1F82A, 0x1F82D, 0x1F830, 0x1F840, 0x1F853, 0x1F887, 0x1F889,
-	0x1F8A0, 0x1F8A2, 0x1F8A4, 0x1F8AF, 0x1F8C0, 0x1F8C3, 0x1F8D0, 0x1F8D0,
-	0x1F8E4, 0x1F8E4, 0x1F8E8, 0x1F8EC, 0x1F900, 0x1F903, 0x1F940, 0x1F940,
-	0x1F942, 0x1F944, 0x1F94C, 0x1F94D, 0x1F94F, 0x1F951, 0x1F954, 0x1F954,
-	0x1F957, 0x1F958, 0x1F95D, 0x1F95D, 0x1F962, 0x1F962, 0x1F964, 0x1F965,
-	0x1F980, 0x1F986, 0x1F990, 0x1F99E, 0x1F9C0, 0x1F9C0, 0x1F9C5, 0x1F9CC,
-	0x1F9E0, 0x1F9E2, 0x1F9F0, 0x1F9F0, 0x1FA00, 0x1FA01,
-	/* GPU RSCC */
-	0x2348C, 0x2348C, 0x23501, 0x23502, 0x23740, 0x23742, 0x23744, 0x23747,
-	0x2374C, 0x23787, 0x237EC, 0x237EF, 0x237F4, 0x2382F, 0x23894, 0x23897,
-	0x2389C, 0x238D7, 0x2393C, 0x2393F, 0x23944, 0x2397F,
-	/* GMU AO */
-	0x23B00, 0x23B16, 0x23C00, 0x23C00,
-	/* GPU CC */
-	0x24000, 0x24012, 0x24040, 0x24052, 0x24400, 0x24404, 0x24407, 0x2440B,
-	0x24415, 0x2441C, 0x2441E, 0x2442D, 0x2443C, 0x2443D, 0x2443F, 0x24440,
-	0x24442, 0x24449, 0x24458, 0x2445A, 0x24540, 0x2455E, 0x24800, 0x24802,
-	0x24C00, 0x24C02, 0x25400, 0x25402, 0x25800, 0x25802, 0x25C00, 0x25C02,
-	0x26000, 0x26002,
-	/* GPU CC ACD */
-	0x26400, 0x26416, 0x26420, 0x26427,
-};
-
-static const unsigned int a6xx_rb_rac_registers[] = {
-	0x8E04, 0x8E05, 0x8E07, 0x8E08, 0x8E10, 0x8E1C, 0x8E20, 0x8E25,
-	0x8E28, 0x8E28, 0x8E2C, 0x8E2F, 0x8E50, 0x8E52,
-};
-
-static const unsigned int a6xx_rb_rbp_registers[] = {
-	0x8E01, 0x8E01, 0x8E0C, 0x8E0C, 0x8E3B, 0x8E3E, 0x8E40, 0x8E43,
-	0x8E53, 0x8E5F, 0x8E70, 0x8E77,
-};
-
-static const struct adreno_vbif_snapshot_registers
-a6xx_vbif_snapshot_registers[] = {
-	{ 0x20040000, 0xFF000000, a6xx_vbif_ver_20xxxxxx_registers,
-				ARRAY_SIZE(a6xx_vbif_ver_20xxxxxx_registers)/2},
-};
-
-/*
- * Set of registers to dump for A6XX on snapshot.
- * Registers in pairs - first value is the start offset, second
- * is the stop offset (inclusive)
- */
-
-static const unsigned int a6xx_registers[] = {
-	/* RBBM */
-	0x0000, 0x0002, 0x0010, 0x0010, 0x0012, 0x0012, 0x0018, 0x001B,
-	0x001e, 0x0032, 0x0038, 0x003C, 0x0042, 0x0042, 0x0044, 0x0044,
-	0x0047, 0x0047, 0x0056, 0x0056, 0x00AD, 0x00AE, 0x00B0, 0x00FB,
-	0x0100, 0x011D, 0x0200, 0x020D, 0x0218, 0x023D, 0x0400, 0x04F9,
-	0x0500, 0x0500, 0x0505, 0x050B, 0x050E, 0x0511, 0x0533, 0x0533,
-	0x0540, 0x0555,
-	/* CP */
-	0x0800, 0x0808, 0x0810, 0x0813, 0x0820, 0x0821, 0x0823, 0x0824,
-	0x0826, 0x0827, 0x0830, 0x0833, 0x0840, 0x0843, 0x084F, 0x086F,
-	0x0880, 0x088A, 0x08A0, 0x08AB, 0x08C0, 0x08C4, 0x08D0, 0x08DD,
-	0x08F0, 0x08F3, 0x0900, 0x0903, 0x0908, 0x0911, 0x0928, 0x093E,
-	0x0942, 0x094D, 0x0980, 0x0984, 0x098D, 0x0996, 0x0998, 0x099E,
-	0x09A0, 0x09A6, 0x09A8, 0x09AE, 0x09B0, 0x09B1, 0x09C2, 0x09C8,
-	0x0A00, 0x0A03,
-	/* VSC */
-	0x0C00, 0x0C04, 0x0C06, 0x0C06, 0x0C10, 0x0CD9, 0x0E00, 0x0E0E,
-	/* UCHE */
-	0x0E10, 0x0E13, 0x0E17, 0x0E19, 0x0E1C, 0x0E2B, 0x0E30, 0x0E32,
-	0x0E38, 0x0E39,
-	/* GRAS */
-	0x8600, 0x8601, 0x8610, 0x861B, 0x8620, 0x8620, 0x8628, 0x862B,
-	0x8630, 0x8637,
-	/* VPC */
-	0x9600, 0x9604, 0x9624, 0x9637,
-	/* PC */
-	0x9E00, 0x9E01, 0x9E03, 0x9E0E, 0x9E11, 0x9E16, 0x9E19, 0x9E19,
-	0x9E1C, 0x9E1C, 0x9E20, 0x9E23, 0x9E30, 0x9E31, 0x9E34, 0x9E34,
-	0x9E70, 0x9E72, 0x9E78, 0x9E79, 0x9E80, 0x9FFF,
-	/* VFD */
-	0xA600, 0xA601, 0xA603, 0xA603, 0xA60A, 0xA60A, 0xA610, 0xA617,
-	0xA630, 0xA630,
-};
-
-/*
- * Set of registers to dump for A6XX before actually triggering crash dumper.
- * Registers in pairs - first value is the start offset, second
- * is the stop offset (inclusive)
- */
-static const unsigned int a6xx_pre_crashdumper_registers[] = {
-	/* RBBM: RBBM_STATUS - RBBM_STATUS3 */
-	0x210, 0x213,
-	/* CP: CP_STATUS_1 */
-	0x825, 0x825,
-};
-
-enum a6xx_debugbus_id {
-	A6XX_DBGBUS_CP           = 0x1,
-	A6XX_DBGBUS_RBBM         = 0x2,
-	A6XX_DBGBUS_VBIF         = 0x3,
-	A6XX_DBGBUS_HLSQ         = 0x4,
-	A6XX_DBGBUS_UCHE         = 0x5,
-	A6XX_DBGBUS_DPM          = 0x6,
-	A6XX_DBGBUS_TESS         = 0x7,
-	A6XX_DBGBUS_PC           = 0x8,
-	A6XX_DBGBUS_VFDP         = 0x9,
-	A6XX_DBGBUS_VPC          = 0xa,
-	A6XX_DBGBUS_TSE          = 0xb,
-	A6XX_DBGBUS_RAS          = 0xc,
-	A6XX_DBGBUS_VSC          = 0xd,
-	A6XX_DBGBUS_COM          = 0xe,
-	A6XX_DBGBUS_LRZ          = 0x10,
-	A6XX_DBGBUS_A2D          = 0x11,
-	A6XX_DBGBUS_CCUFCHE      = 0x12,
-	A6XX_DBGBUS_GMU_CX       = 0x13,
-	A6XX_DBGBUS_RBP          = 0x14,
-	A6XX_DBGBUS_DCS          = 0x15,
-	A6XX_DBGBUS_RBBM_CFG     = 0x16,
-	A6XX_DBGBUS_CX           = 0x17,
-	A6XX_DBGBUS_GMU_GX       = 0x18,
-	A6XX_DBGBUS_TPFCHE       = 0x19,
-	A6XX_DBGBUS_GBIF_GX      = 0x1a,
-	A6XX_DBGBUS_GPC          = 0x1d,
-	A6XX_DBGBUS_LARC         = 0x1e,
-	A6XX_DBGBUS_HLSQ_SPTP    = 0x1f,
-	A6XX_DBGBUS_RB_0         = 0x20,
-	A6XX_DBGBUS_RB_1         = 0x21,
-	A6XX_DBGBUS_UCHE_WRAPPER = 0x24,
-	A6XX_DBGBUS_CCU_0        = 0x28,
-	A6XX_DBGBUS_CCU_1        = 0x29,
-	A6XX_DBGBUS_VFD_0        = 0x38,
-	A6XX_DBGBUS_VFD_1        = 0x39,
-	A6XX_DBGBUS_VFD_2        = 0x3a,
-	A6XX_DBGBUS_VFD_3        = 0x3b,
-	A6XX_DBGBUS_SP_0         = 0x40,
-	A6XX_DBGBUS_SP_1         = 0x41,
-	A6XX_DBGBUS_TPL1_0       = 0x48,
-	A6XX_DBGBUS_TPL1_1       = 0x49,
-	A6XX_DBGBUS_TPL1_2       = 0x4a,
-	A6XX_DBGBUS_TPL1_3       = 0x4b,
-};
-
-static const struct adreno_debugbus_block a6xx_dbgc_debugbus_blocks[] = {
-	{ A6XX_DBGBUS_CP, 0x100, },
-	{ A6XX_DBGBUS_RBBM, 0x100, },
-	{ A6XX_DBGBUS_HLSQ, 0x100, },
-	{ A6XX_DBGBUS_UCHE, 0x100, },
-	{ A6XX_DBGBUS_DPM, 0x100, },
-	{ A6XX_DBGBUS_TESS, 0x100, },
-	{ A6XX_DBGBUS_PC, 0x100, },
-	{ A6XX_DBGBUS_VFDP, 0x100, },
-	{ A6XX_DBGBUS_VPC, 0x100, },
-	{ A6XX_DBGBUS_TSE, 0x100, },
-	{ A6XX_DBGBUS_RAS, 0x100, },
-	{ A6XX_DBGBUS_VSC, 0x100, },
-	{ A6XX_DBGBUS_COM, 0x100, },
-	{ A6XX_DBGBUS_LRZ, 0x100, },
-	{ A6XX_DBGBUS_A2D, 0x100, },
-	{ A6XX_DBGBUS_CCUFCHE, 0x100, },
-	{ A6XX_DBGBUS_RBP, 0x100, },
-	{ A6XX_DBGBUS_DCS, 0x100, },
-	{ A6XX_DBGBUS_RBBM_CFG, 0x100, },
-	{ A6XX_DBGBUS_GMU_GX, 0x100, },
-	{ A6XX_DBGBUS_TPFCHE, 0x100, },
-	{ A6XX_DBGBUS_GPC, 0x100, },
-	{ A6XX_DBGBUS_LARC, 0x100, },
-	{ A6XX_DBGBUS_HLSQ_SPTP, 0x100, },
-	{ A6XX_DBGBUS_RB_0, 0x100, },
-	{ A6XX_DBGBUS_RB_1, 0x100, },
-	{ A6XX_DBGBUS_UCHE_WRAPPER, 0x100, },
-	{ A6XX_DBGBUS_CCU_0, 0x100, },
-	{ A6XX_DBGBUS_CCU_1, 0x100, },
-	{ A6XX_DBGBUS_VFD_0, 0x100, },
-	{ A6XX_DBGBUS_VFD_1, 0x100, },
-	{ A6XX_DBGBUS_VFD_2, 0x100, },
-	{ A6XX_DBGBUS_VFD_3, 0x100, },
-	{ A6XX_DBGBUS_SP_0, 0x100, },
-	{ A6XX_DBGBUS_SP_1, 0x100, },
-	{ A6XX_DBGBUS_TPL1_0, 0x100, },
-	{ A6XX_DBGBUS_TPL1_1, 0x100, },
-	{ A6XX_DBGBUS_TPL1_2, 0x100, },
-	{ A6XX_DBGBUS_TPL1_3, 0x100, },
-};
-
-static const struct adreno_debugbus_block a6xx_vbif_debugbus_blocks = {
-	A6XX_DBGBUS_VBIF, 0x100,
-};
-
-static const struct adreno_debugbus_block a6xx_cx_dbgc_debugbus_blocks[] = {
-	{ A6XX_DBGBUS_GMU_CX, 0x100, },
-	{ A6XX_DBGBUS_CX, 0x100, },
-};
-
-#define A6XX_NUM_SHADER_BANKS 3
-#define A6XX_SHADER_STATETYPE_SHIFT 8
-
-enum a6xx_shader_obj {
-	A6XX_TP0_TMO_DATA               = 0x9,
-	A6XX_TP0_SMO_DATA               = 0xa,
-	A6XX_TP0_MIPMAP_BASE_DATA       = 0xb,
-	A6XX_TP1_TMO_DATA               = 0x19,
-	A6XX_TP1_SMO_DATA               = 0x1a,
-	A6XX_TP1_MIPMAP_BASE_DATA       = 0x1b,
-	A6XX_SP_INST_DATA               = 0x29,
-	A6XX_SP_LB_0_DATA               = 0x2a,
-	A6XX_SP_LB_1_DATA               = 0x2b,
-	A6XX_SP_LB_2_DATA               = 0x2c,
-	A6XX_SP_LB_3_DATA               = 0x2d,
-	A6XX_SP_LB_4_DATA               = 0x2e,
-	A6XX_SP_LB_5_DATA               = 0x2f,
-	A6XX_SP_CB_BINDLESS_DATA        = 0x30,
-	A6XX_SP_CB_LEGACY_DATA          = 0x31,
-	A6XX_SP_UAV_DATA                = 0x32,
-	A6XX_SP_INST_TAG                = 0x33,
-	A6XX_SP_CB_BINDLESS_TAG         = 0x34,
-	A6XX_SP_TMO_UMO_TAG             = 0x35,
-	A6XX_SP_SMO_TAG                 = 0x36,
-	A6XX_SP_STATE_DATA              = 0x37,
-	A6XX_HLSQ_CHUNK_CVS_RAM         = 0x49,
-	A6XX_HLSQ_CHUNK_CPS_RAM         = 0x4a,
-	A6XX_HLSQ_CHUNK_CVS_RAM_TAG     = 0x4b,
-	A6XX_HLSQ_CHUNK_CPS_RAM_TAG     = 0x4c,
-	A6XX_HLSQ_ICB_CVS_CB_BASE_TAG   = 0x4d,
-	A6XX_HLSQ_ICB_CPS_CB_BASE_TAG   = 0x4e,
-	A6XX_HLSQ_CVS_MISC_RAM          = 0x50,
-	A6XX_HLSQ_CPS_MISC_RAM          = 0x51,
-	A6XX_HLSQ_INST_RAM              = 0x52,
-	A6XX_HLSQ_GFX_CVS_CONST_RAM     = 0x53,
-	A6XX_HLSQ_GFX_CPS_CONST_RAM     = 0x54,
-	A6XX_HLSQ_CVS_MISC_RAM_TAG      = 0x55,
-	A6XX_HLSQ_CPS_MISC_RAM_TAG      = 0x56,
-	A6XX_HLSQ_INST_RAM_TAG          = 0x57,
-	A6XX_HLSQ_GFX_CVS_CONST_RAM_TAG = 0x58,
-	A6XX_HLSQ_GFX_CPS_CONST_RAM_TAG = 0x59,
-	A6XX_HLSQ_PWR_REST_RAM          = 0x5a,
-	A6XX_HLSQ_PWR_REST_TAG          = 0x5b,
-	A6XX_HLSQ_DATAPATH_META         = 0x60,
-	A6XX_HLSQ_FRONTEND_META         = 0x61,
-	A6XX_HLSQ_INDIRECT_META         = 0x62,
-	A6XX_HLSQ_BACKEND_META          = 0x63
-};
-
-struct a6xx_shader_block {
-	unsigned int statetype;
-	unsigned int sz;
-	uint64_t offset;
-};
-
-struct a6xx_shader_block_info {
-	struct a6xx_shader_block *block;
-	unsigned int bank;
-	uint64_t offset;
-};
-
-static struct a6xx_shader_block a6xx_shader_blocks[] = {
-	{A6XX_TP0_TMO_DATA,               0x200},
-	{A6XX_TP0_SMO_DATA,               0x80,},
-	{A6XX_TP0_MIPMAP_BASE_DATA,       0x3C0},
-	{A6XX_TP1_TMO_DATA,               0x200},
-	{A6XX_TP1_SMO_DATA,               0x80,},
-	{A6XX_TP1_MIPMAP_BASE_DATA,       0x3C0},
-	{A6XX_SP_INST_DATA,               0x800},
-	{A6XX_SP_LB_0_DATA,               0x800},
-	{A6XX_SP_LB_1_DATA,               0x800},
-	{A6XX_SP_LB_2_DATA,               0x800},
-	{A6XX_SP_LB_3_DATA,               0x800},
-	{A6XX_SP_LB_4_DATA,               0x800},
-	{A6XX_SP_LB_5_DATA,               0x200},
-	{A6XX_SP_CB_BINDLESS_DATA,        0x2000},
-	{A6XX_SP_CB_LEGACY_DATA,          0x280,},
-	{A6XX_SP_UAV_DATA,                0x80,},
-	{A6XX_SP_INST_TAG,                0x80,},
-	{A6XX_SP_CB_BINDLESS_TAG,         0x80,},
-	{A6XX_SP_TMO_UMO_TAG,             0x80,},
-	{A6XX_SP_SMO_TAG,                 0x80},
-	{A6XX_SP_STATE_DATA,              0x3F},
-	{A6XX_HLSQ_CHUNK_CVS_RAM,         0x1C0},
-	{A6XX_HLSQ_CHUNK_CPS_RAM,         0x280},
-	{A6XX_HLSQ_CHUNK_CVS_RAM_TAG,     0x40,},
-	{A6XX_HLSQ_CHUNK_CPS_RAM_TAG,     0x40,},
-	{A6XX_HLSQ_ICB_CVS_CB_BASE_TAG,   0x4,},
-	{A6XX_HLSQ_ICB_CPS_CB_BASE_TAG,   0x4,},
-	{A6XX_HLSQ_CVS_MISC_RAM,          0x1C0},
-	{A6XX_HLSQ_CPS_MISC_RAM,          0x580},
-	{A6XX_HLSQ_INST_RAM,              0x800},
-	{A6XX_HLSQ_GFX_CVS_CONST_RAM,     0x800},
-	{A6XX_HLSQ_GFX_CPS_CONST_RAM,     0x800},
-	{A6XX_HLSQ_CVS_MISC_RAM_TAG,      0x8,},
-	{A6XX_HLSQ_CPS_MISC_RAM_TAG,      0x4,},
-	{A6XX_HLSQ_INST_RAM_TAG,          0x80,},
-	{A6XX_HLSQ_GFX_CVS_CONST_RAM_TAG, 0xC,},
-	{A6XX_HLSQ_GFX_CPS_CONST_RAM_TAG, 0x10},
-	{A6XX_HLSQ_PWR_REST_RAM,          0x28},
-	{A6XX_HLSQ_PWR_REST_TAG,          0x14},
-	{A6XX_HLSQ_DATAPATH_META,         0x40,},
-	{A6XX_HLSQ_FRONTEND_META,         0x40},
-	{A6XX_HLSQ_INDIRECT_META,         0x40,}
-};
-
-static struct kgsl_memdesc a6xx_capturescript;
-static struct kgsl_memdesc a6xx_crashdump_registers;
-static bool crash_dump_valid;
-
-static struct reg_list {
-	const unsigned int *regs;
-	unsigned int count;
-	const struct sel_reg *sel;
-	uint64_t offset;
-} a6xx_reg_list[] = {
-	{ a6xx_registers, ARRAY_SIZE(a6xx_registers) / 2, NULL },
-	{ a6xx_rb_rac_registers, ARRAY_SIZE(a6xx_rb_rac_registers) / 2,
-		&_a6xx_rb_rac_aperture },
-	{ a6xx_rb_rbp_registers, ARRAY_SIZE(a6xx_rb_rbp_registers) / 2,
-		&_a6xx_rb_rbp_aperture },
-};
-
-#define REG_PAIR_COUNT(_a, _i) \
-	(((_a)[(2 * (_i)) + 1] - (_a)[2 * (_i)]) + 1)
-
-static size_t a6xx_legacy_snapshot_registers(struct kgsl_device *device,
-		u8 *buf, size_t remain, struct reg_list *regs)
-{
-	struct kgsl_snapshot_registers snapshot_regs = {
-		.regs = regs->regs,
-		.count = regs->count,
-	};
-
-	if (regs->sel)
-		kgsl_regwrite(device, regs->sel->host_reg, regs->sel->val);
-
-	return kgsl_snapshot_dump_registers(device, buf, remain,
-		&snapshot_regs);
-}
-
-static size_t a6xx_snapshot_registers(struct kgsl_device *device, u8 *buf,
-		size_t remain, void *priv)
-{
-	struct kgsl_snapshot_regs *header = (struct kgsl_snapshot_regs *)buf;
-	struct reg_list *regs = (struct reg_list *)priv;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	unsigned int *src;
-	unsigned int j, k;
-	unsigned int count = 0;
-
-	if (crash_dump_valid == false)
-		return a6xx_legacy_snapshot_registers(device, buf, remain,
-			regs);
-
-	if (remain < sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-		return 0;
-	}
-
-	src = (unsigned int *)(a6xx_crashdump_registers.hostptr + regs->offset);
-	remain -= sizeof(*header);
-
-	for (j = 0; j < regs->count; j++) {
-		unsigned int start = regs->regs[2 * j];
-		unsigned int end = regs->regs[(2 * j) + 1];
-
-		if (remain < ((end - start) + 1) * 8) {
-			SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-			goto out;
-		}
-
-		remain -= ((end - start) + 1) * 8;
-
-		for (k = start; k <= end; k++, count++) {
-			*data++ = k;
-			*data++ = *src++;
-		}
-	}
-
-out:
-	header->count = count;
-
-	/* Return the size of the section */
-	return (count * 8) + sizeof(*header);
-}
-
-static size_t a6xx_snapshot_pre_crashdump_regs(struct kgsl_device *device,
-		u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_registers pre_cdregs = {
-			.regs = a6xx_pre_crashdumper_registers,
-			.count = ARRAY_SIZE(a6xx_pre_crashdumper_registers)/2,
-	};
-
-	return kgsl_snapshot_dump_registers(device, buf, remain, &pre_cdregs);
-}
-
-static size_t a6xx_snapshot_shader_memory(struct kgsl_device *device,
-		u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_shader *header =
-		(struct kgsl_snapshot_shader *) buf;
-	struct a6xx_shader_block_info *info =
-		(struct a6xx_shader_block_info *) priv;
-	struct a6xx_shader_block *block = info->block;
-	unsigned int *data = (unsigned int *) (buf + sizeof(*header));
-
-	if (remain < SHADER_SECTION_SZ(block->sz)) {
-		SNAPSHOT_ERR_NOMEM(device, "SHADER MEMORY");
-		return 0;
-	}
-
-	header->type = block->statetype;
-	header->index = info->bank;
-	header->size = block->sz;
-
-	memcpy(data, a6xx_crashdump_registers.hostptr + info->offset,
-		block->sz * sizeof(unsigned int));
-
-	return SHADER_SECTION_SZ(block->sz);
-}
-
-static void a6xx_snapshot_shader(struct kgsl_device *device,
-				struct kgsl_snapshot *snapshot)
-{
-	unsigned int i, j;
-	struct a6xx_shader_block_info info;
-
-	/* Shader blocks can only be read by the crash dumper */
-	if (crash_dump_valid == false)
-		return;
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_shader_blocks); i++) {
-		for (j = 0; j < A6XX_NUM_SHADER_BANKS; j++) {
-			info.block = &a6xx_shader_blocks[i];
-			info.bank = j;
-			info.offset = a6xx_shader_blocks[i].offset +
-				(j * a6xx_shader_blocks[i].sz);
-
-			/* Shader working/shadow memory */
-			kgsl_snapshot_add_section(device,
-				KGSL_SNAPSHOT_SECTION_SHADER,
-				snapshot, a6xx_snapshot_shader_memory, &info);
-		}
-	}
-}
-
-static void a6xx_snapshot_mempool(struct kgsl_device *device,
-				struct kgsl_snapshot *snapshot)
-{
-	unsigned int pool_size;
-	u8 *buf = snapshot->ptr;
-
-	/* Set the mempool size to 0 to stabilize it while dumping */
-	kgsl_regread(device, A6XX_CP_MEM_POOL_SIZE, &pool_size);
-	kgsl_regwrite(device, A6XX_CP_MEM_POOL_SIZE, 0);
-
-	kgsl_snapshot_indexed_registers(device, snapshot,
-		A6XX_CP_MEM_POOL_DBG_ADDR, A6XX_CP_MEM_POOL_DBG_DATA,
-		0, 0x2060);
-
-	/*
-	 * Data at offset 0x2000 in the mempool section is the mempool size.
-	 * Since we set it to 0, patch in the original size so that the data
-	 * is consistent.
-	 */
-	if (buf < snapshot->ptr) {
-		unsigned int *data;
-
-		/* Skip over the headers */
-		buf += sizeof(struct kgsl_snapshot_section_header) +
-				sizeof(struct kgsl_snapshot_indexed_regs);
-
-		data = (unsigned int *)buf + 0x2000;
-		*data = pool_size;
-	}
-
-	/* Restore the saved mempool size */
-	kgsl_regwrite(device, A6XX_CP_MEM_POOL_SIZE, pool_size);
-}
-
-static inline unsigned int a6xx_read_dbgahb(struct kgsl_device *device,
-				unsigned int regbase, unsigned int reg)
-{
-	unsigned int read_reg = A6XX_HLSQ_DBG_AHB_READ_APERTURE +
-				reg - regbase / 4;
-	unsigned int val;
-
-	kgsl_regread(device, read_reg, &val);
-	return val;
-}
-
-static size_t a6xx_legacy_snapshot_cluster_dbgahb(struct kgsl_device *device,
-				u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_mvc_regs *header =
-				(struct kgsl_snapshot_mvc_regs *)buf;
-	struct a6xx_cluster_dbgahb_regs_info *info =
-				(struct a6xx_cluster_dbgahb_regs_info *)priv;
-	struct a6xx_cluster_dbgahb_registers *cur_cluster = info->cluster;
-	unsigned int read_sel;
-	unsigned int data_size = 0;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	int i, j;
-
-	if (!device->snapshot_legacy)
-		return 0;
-
-	if (remain < sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-		return 0;
-	}
-
-	remain -= sizeof(*header);
-
-	header->ctxt_id = info->ctxt_id;
-	header->cluster_id = cur_cluster->id;
-
-	read_sel = ((cur_cluster->statetype + info->ctxt_id * 2) & 0xff) << 8;
-	kgsl_regwrite(device, A6XX_HLSQ_DBG_READ_SEL, read_sel);
-
-	for (i = 0; i < cur_cluster->num_sets; i++) {
-		unsigned int start = cur_cluster->regs[2 * i];
-		unsigned int end = cur_cluster->regs[2 * i + 1];
-
-		if (remain < (end - start + 3) * 4) {
-			SNAPSHOT_ERR_NOMEM(device, "MVC REGISTERS");
-			goto out;
-		}
-
-		remain -= (end - start + 3) * 4;
-		data_size += (end - start + 3) * 4;
-
-		*data++ = start | (1 << 31);
-		*data++ = end;
-
-		for (j = start; j <= end; j++) {
-			unsigned int val;
-
-			val = a6xx_read_dbgahb(device, cur_cluster->regbase, j);
-			*data++ = val;
-
-		}
-	}
-
-out:
-	return data_size + sizeof(*header);
-}
-
-static size_t a6xx_snapshot_cluster_dbgahb(struct kgsl_device *device, u8 *buf,
-				size_t remain, void *priv)
-{
-	struct kgsl_snapshot_mvc_regs *header =
-				(struct kgsl_snapshot_mvc_regs *)buf;
-	struct a6xx_cluster_dbgahb_regs_info *info =
-				(struct a6xx_cluster_dbgahb_regs_info *)priv;
-	struct a6xx_cluster_dbgahb_registers *cluster = info->cluster;
-	unsigned int data_size = 0;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	int i, j;
-	unsigned int *src;
-
-
-	if (crash_dump_valid == false)
-		return a6xx_legacy_snapshot_cluster_dbgahb(device, buf, remain,
-				info);
-
-	if (remain < sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-		return 0;
-	}
-
-	remain -= sizeof(*header);
-
-	header->ctxt_id = info->ctxt_id;
-	header->cluster_id = cluster->id;
-
-	src = (unsigned int *)(a6xx_crashdump_registers.hostptr +
-		(header->ctxt_id ? cluster->offset1 : cluster->offset0));
-
-	for (i = 0; i < cluster->num_sets; i++) {
-		unsigned int start;
-		unsigned int end;
-
-		start = cluster->regs[2 * i];
-		end = cluster->regs[2 * i + 1];
-
-		if (remain < (end - start + 3) * 4) {
-			SNAPSHOT_ERR_NOMEM(device, "MVC REGISTERS");
-			goto out;
-		}
-
-		remain -= (end - start + 3) * 4;
-		data_size += (end - start + 3) * 4;
-
-		*data++ = start | (1 << 31);
-		*data++ = end;
-		for (j = start; j <= end; j++)
-			*data++ = *src++;
-	}
-out:
-	return data_size + sizeof(*header);
-}
-
-static size_t a6xx_legacy_snapshot_non_ctx_dbgahb(struct kgsl_device *device,
-				u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_regs *header =
-				(struct kgsl_snapshot_regs *)buf;
-	struct a6xx_non_ctx_dbgahb_registers *regs =
-				(struct a6xx_non_ctx_dbgahb_registers *)priv;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	int count = 0;
-	unsigned int read_sel;
-	int i, j;
-
-	if (!device->snapshot_legacy)
-		return 0;
-
-	/* Figure out how many registers we are going to dump */
-	for (i = 0; i < regs->num_sets; i++) {
-		int start = regs->regs[i * 2];
-		int end = regs->regs[i * 2 + 1];
-
-		count += (end - start + 1);
-	}
-
-	if (remain < (count * 8) + sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-		return 0;
-	}
-
-	header->count = count;
-
-	read_sel = (regs->statetype & 0xff) << 8;
-	kgsl_regwrite(device, A6XX_HLSQ_DBG_READ_SEL, read_sel);
-
-	for (i = 0; i < regs->num_sets; i++) {
-		unsigned int start = regs->regs[2 * i];
-		unsigned int end = regs->regs[2 * i + 1];
-
-		for (j = start; j <= end; j++) {
-			unsigned int val;
-
-			val = a6xx_read_dbgahb(device, regs->regbase, j);
-			*data++ = j;
-			*data++ = val;
-
-		}
-	}
-	return (count * 8) + sizeof(*header);
-}
-
-static size_t a6xx_snapshot_non_ctx_dbgahb(struct kgsl_device *device, u8 *buf,
-				size_t remain, void *priv)
-{
-	struct kgsl_snapshot_regs *header =
-				(struct kgsl_snapshot_regs *)buf;
-	struct a6xx_non_ctx_dbgahb_registers *regs =
-				(struct a6xx_non_ctx_dbgahb_registers *)priv;
-	unsigned int count = 0;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	unsigned int i, k;
-	unsigned int *src;
-
-	if (crash_dump_valid == false)
-		return a6xx_legacy_snapshot_non_ctx_dbgahb(device, buf, remain,
-				regs);
-
-	if (remain < sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-		return 0;
-	}
-
-	remain -= sizeof(*header);
-
-	src = (unsigned int *)(a6xx_crashdump_registers.hostptr + regs->offset);
-
-	for (i = 0; i < regs->num_sets; i++) {
-		unsigned int start;
-		unsigned int end;
-
-		start = regs->regs[2 * i];
-		end = regs->regs[(2 * i) + 1];
-
-		if (remain < (end - start + 1) * 8) {
-			SNAPSHOT_ERR_NOMEM(device, "REGISTERS");
-			goto out;
-		}
-
-		remain -= ((end - start) + 1) * 8;
-
-		for (k = start; k <= end; k++, count++) {
-			*data++ = k;
-			*data++ = *src++;
-		}
-	}
-out:
-	header->count = count;
-
-	/* Return the size of the section */
-	return (count * 8) + sizeof(*header);
-}
-
-static void a6xx_snapshot_dbgahb_regs(struct kgsl_device *device,
-				struct kgsl_snapshot *snapshot)
-{
-	int i, j;
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_dbgahb_ctx_clusters); i++) {
-		struct a6xx_cluster_dbgahb_registers *cluster =
-				&a6xx_dbgahb_ctx_clusters[i];
-		struct a6xx_cluster_dbgahb_regs_info info;
-
-		info.cluster = cluster;
-		for (j = 0; j < A6XX_NUM_CTXTS; j++) {
-			info.ctxt_id = j;
-
-			kgsl_snapshot_add_section(device,
-				KGSL_SNAPSHOT_SECTION_MVC, snapshot,
-				a6xx_snapshot_cluster_dbgahb, &info);
-		}
-	}
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_non_ctx_dbgahb); i++) {
-		kgsl_snapshot_add_section(device,
-			KGSL_SNAPSHOT_SECTION_REGS, snapshot,
-			a6xx_snapshot_non_ctx_dbgahb, &a6xx_non_ctx_dbgahb[i]);
-	}
-}
-
-static size_t a6xx_legacy_snapshot_mvc(struct kgsl_device *device, u8 *buf,
-				size_t remain, void *priv)
-{
-	struct kgsl_snapshot_mvc_regs *header =
-					(struct kgsl_snapshot_mvc_regs *)buf;
-	struct a6xx_cluster_regs_info *info =
-					(struct a6xx_cluster_regs_info *)priv;
-	struct a6xx_cluster_registers *cur_cluster = info->cluster;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	unsigned int ctxt = info->ctxt_id;
-	unsigned int start, end, i, j, aperture_cntl = 0;
-	unsigned int data_size = 0;
-
-	if (remain < sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "MVC REGISTERS");
-		return 0;
-	}
-
-	remain -= sizeof(*header);
-
-	header->ctxt_id = info->ctxt_id;
-	header->cluster_id = cur_cluster->id;
-
-	/*
-	 * Set the AHB control for the Host to read from the
-	 * cluster/context for this iteration.
-	 */
-	aperture_cntl = ((cur_cluster->id & 0x7) << 8) | (ctxt << 4) | ctxt;
-	kgsl_regwrite(device, A6XX_CP_APERTURE_CNTL_HOST, aperture_cntl);
-
-	if (cur_cluster->sel)
-		kgsl_regwrite(device, cur_cluster->sel->host_reg,
-			cur_cluster->sel->val);
-
-	for (i = 0; i < cur_cluster->num_sets; i++) {
-		start = cur_cluster->regs[2 * i];
-		end = cur_cluster->regs[2 * i + 1];
-
-		if (remain < (end - start + 3) * 4) {
-			SNAPSHOT_ERR_NOMEM(device, "MVC REGISTERS");
-			goto out;
-		}
-
-		remain -= (end - start + 3) * 4;
-		data_size += (end - start + 3) * 4;
-
-		*data++ = start | (1 << 31);
-		*data++ = end;
-		for (j = start; j <= end; j++) {
-			unsigned int val;
-
-			kgsl_regread(device, j, &val);
-			*data++ = val;
-		}
-	}
-out:
-	return data_size + sizeof(*header);
-}
-
-static size_t a6xx_snapshot_mvc(struct kgsl_device *device, u8 *buf,
-				size_t remain, void *priv)
-{
-	struct kgsl_snapshot_mvc_regs *header =
-				(struct kgsl_snapshot_mvc_regs *)buf;
-	struct a6xx_cluster_regs_info *info =
-				(struct a6xx_cluster_regs_info *)priv;
-	struct a6xx_cluster_registers *cluster = info->cluster;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	unsigned int *src;
-	int i, j;
-	unsigned int start, end;
-	size_t data_size = 0;
-
-	if (crash_dump_valid == false)
-		return a6xx_legacy_snapshot_mvc(device, buf, remain, info);
-
-	if (remain < sizeof(*header)) {
-		SNAPSHOT_ERR_NOMEM(device, "MVC REGISTERS");
-		return 0;
-	}
-
-	remain -= sizeof(*header);
-
-	header->ctxt_id = info->ctxt_id;
-	header->cluster_id = cluster->id;
-
-	src = (unsigned int *)(a6xx_crashdump_registers.hostptr +
-		(header->ctxt_id ? cluster->offset1 : cluster->offset0));
-
-	for (i = 0; i < cluster->num_sets; i++) {
-		start = cluster->regs[2 * i];
-		end = cluster->regs[2 * i + 1];
-
-		if (remain < (end - start + 3) * 4) {
-			SNAPSHOT_ERR_NOMEM(device, "MVC REGISTERS");
-			goto out;
-		}
-
-		remain -= (end - start + 3) * 4;
-		data_size += (end - start + 3) * 4;
-
-		*data++ = start | (1 << 31);
-		*data++ = end;
-		for (j = start; j <= end; j++)
-			*data++ = *src++;
-	}
-
-out:
-	return data_size + sizeof(*header);
-
-}
-
-static void a6xx_snapshot_mvc_regs(struct kgsl_device *device,
-				struct kgsl_snapshot *snapshot)
-{
-	int i, j;
-	struct a6xx_cluster_regs_info info;
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_clusters); i++) {
-		struct a6xx_cluster_registers *cluster = &a6xx_clusters[i];
-
-		info.cluster = cluster;
-		for (j = 0; j < A6XX_NUM_CTXTS; j++) {
-			info.ctxt_id = j;
-
-			kgsl_snapshot_add_section(device,
-				KGSL_SNAPSHOT_SECTION_MVC, snapshot,
-				a6xx_snapshot_mvc, &info);
-		}
-	}
-}
-
-/* a6xx_dbgc_debug_bus_read() - Read data from trace bus */
-static void a6xx_dbgc_debug_bus_read(struct kgsl_device *device,
-	unsigned int block_id, unsigned int index, unsigned int *val)
-{
-	unsigned int reg;
-
-	reg = (block_id << A6XX_DBGC_CFG_DBGBUS_SEL_PING_BLK_SEL_SHIFT) |
-			(index << A6XX_DBGC_CFG_DBGBUS_SEL_PING_INDEX_SHIFT);
-
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_SEL_A, reg);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_SEL_B, reg);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_SEL_C, reg);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_SEL_D, reg);
-
-	/*
-	 * There needs to be a delay of 1 us to ensure enough time for correct
-	 * data is funneled into the trace buffer
-	 */
-	udelay(1);
-
-	kgsl_regread(device, A6XX_DBGC_CFG_DBGBUS_TRACE_BUF2, val);
-	val++;
-	kgsl_regread(device, A6XX_DBGC_CFG_DBGBUS_TRACE_BUF1, val);
-}
-
-/* a6xx_snapshot_dbgc_debugbus_block() - Capture debug data for a gpu block */
-static size_t a6xx_snapshot_dbgc_debugbus_block(struct kgsl_device *device,
-	u8 *buf, size_t remain, void *priv)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct kgsl_snapshot_debugbus *header =
-		(struct kgsl_snapshot_debugbus *)buf;
-	struct adreno_debugbus_block *block = priv;
-	int i;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	unsigned int dwords;
-	unsigned int block_id;
-	size_t size;
-
-	dwords = block->dwords;
-
-	/* For a6xx each debug bus data unit is 2 DWORDS */
-	size = (dwords * sizeof(unsigned int) * 2) + sizeof(*header);
-
-	if (remain < size) {
-		SNAPSHOT_ERR_NOMEM(device, "DEBUGBUS");
-		return 0;
-	}
-
-	header->id = block->block_id;
-	if ((block->block_id == A6XX_DBGBUS_VBIF) &&
-		adreno_has_gbif(adreno_dev))
-		header->id = A6XX_DBGBUS_GBIF_GX;
-	header->count = dwords * 2;
-
-	block_id = block->block_id;
-	/* GMU_GX data is read using the GMU_CX block id on A630 */
-	if ((adreno_is_a630(adreno_dev) || adreno_is_a615(adreno_dev) ||
-		adreno_is_a616(adreno_dev)) &&
-		(block_id == A6XX_DBGBUS_GMU_GX))
-		block_id = A6XX_DBGBUS_GMU_CX;
-
-	for (i = 0; i < dwords; i++)
-		a6xx_dbgc_debug_bus_read(device, block_id, i, &data[i*2]);
-
-	return size;
-}
-
-/* a6xx_snapshot_vbif_debugbus_block() - Capture debug data for VBIF block */
-static size_t a6xx_snapshot_vbif_debugbus_block(struct kgsl_device *device,
-			u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_debugbus *header =
-		(struct kgsl_snapshot_debugbus *)buf;
-	struct adreno_debugbus_block *block = priv;
-	int i, j;
-	/*
-	 * Total number of VBIF data words considering 3 sections:
-	 * 2 arbiter blocks of 16 words
-	 * 5 AXI XIN blocks of 18 dwords each
-	 * 4 core clock side XIN blocks of 12 dwords each
-	 */
-	unsigned int dwords = (16 * A6XX_NUM_AXI_ARB_BLOCKS) +
-			(18 * A6XX_NUM_XIN_AXI_BLOCKS) +
-			(12 * A6XX_NUM_XIN_CORE_BLOCKS);
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	size_t size;
-	unsigned int reg_clk;
-
-	size = (dwords * sizeof(unsigned int)) + sizeof(*header);
-
-	if (remain < size) {
-		SNAPSHOT_ERR_NOMEM(device, "DEBUGBUS");
-		return 0;
-	}
-	header->id = block->block_id;
-	header->count = dwords;
-
-	kgsl_regread(device, A6XX_VBIF_CLKON, &reg_clk);
-	kgsl_regwrite(device, A6XX_VBIF_CLKON, reg_clk |
-			(A6XX_VBIF_CLKON_FORCE_ON_TESTBUS_MASK <<
-			A6XX_VBIF_CLKON_FORCE_ON_TESTBUS_SHIFT));
-	kgsl_regwrite(device, A6XX_VBIF_TEST_BUS1_CTRL0, 0);
-	kgsl_regwrite(device, A6XX_VBIF_TEST_BUS_OUT_CTRL,
-			(A6XX_VBIF_TEST_BUS_OUT_CTRL_EN_MASK <<
-			A6XX_VBIF_TEST_BUS_OUT_CTRL_EN_SHIFT));
-
-	for (i = 0; i < A6XX_NUM_AXI_ARB_BLOCKS; i++) {
-		kgsl_regwrite(device, A6XX_VBIF_TEST_BUS2_CTRL0,
-			(1 << (i + 16)));
-		for (j = 0; j < 16; j++) {
-			kgsl_regwrite(device, A6XX_VBIF_TEST_BUS2_CTRL1,
-				((j & A6XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_MASK)
-				<< A6XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_SHIFT));
-			kgsl_regread(device, A6XX_VBIF_TEST_BUS_OUT,
-					data);
-			data++;
-		}
-	}
-
-	/* XIN blocks AXI side */
-	for (i = 0; i < A6XX_NUM_XIN_AXI_BLOCKS; i++) {
-		kgsl_regwrite(device, A6XX_VBIF_TEST_BUS2_CTRL0, 1 << i);
-		for (j = 0; j < 18; j++) {
-			kgsl_regwrite(device, A6XX_VBIF_TEST_BUS2_CTRL1,
-				((j & A6XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_MASK)
-				<< A6XX_VBIF_TEST_BUS2_CTRL1_DATA_SEL_SHIFT));
-			kgsl_regread(device, A6XX_VBIF_TEST_BUS_OUT,
-				data);
-			data++;
-		}
-	}
-	kgsl_regwrite(device, A6XX_VBIF_TEST_BUS2_CTRL0, 0);
-
-	/* XIN blocks core clock side */
-	for (i = 0; i < A6XX_NUM_XIN_CORE_BLOCKS; i++) {
-		kgsl_regwrite(device, A6XX_VBIF_TEST_BUS1_CTRL0, 1 << i);
-		for (j = 0; j < 12; j++) {
-			kgsl_regwrite(device, A6XX_VBIF_TEST_BUS1_CTRL1,
-				((j & A6XX_VBIF_TEST_BUS1_CTRL1_DATA_SEL_MASK)
-				<< A6XX_VBIF_TEST_BUS1_CTRL1_DATA_SEL_SHIFT));
-			kgsl_regread(device, A6XX_VBIF_TEST_BUS_OUT,
-				data);
-			data++;
-		}
-	}
-	/* restore the clock of VBIF */
-	kgsl_regwrite(device, A6XX_VBIF_CLKON, reg_clk);
-	return size;
-}
-
-/* a6xx_cx_dbgc_debug_bus_read() - Read data from trace bus */
-static void a6xx_cx_debug_bus_read(struct kgsl_device *device,
-	unsigned int block_id, unsigned int index, unsigned int *val)
-{
-	unsigned int reg;
-
-	reg = (block_id << A6XX_CX_DBGC_CFG_DBGBUS_SEL_PING_BLK_SEL_SHIFT) |
-			(index << A6XX_CX_DBGC_CFG_DBGBUS_SEL_PING_INDEX_SHIFT);
-
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_SEL_A, reg);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_SEL_B, reg);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_SEL_C, reg);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_SEL_D, reg);
-
-	/*
-	 * There needs to be a delay of 1 us to ensure enough time for correct
-	 * data is funneled into the trace buffer
-	 */
-	udelay(1);
-
-	adreno_cx_dbgc_regread(device, A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF2, val);
-	val++;
-	adreno_cx_dbgc_regread(device, A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF1, val);
-}
-
-/*
- * a6xx_snapshot_cx_dbgc_debugbus_block() - Capture debug data for a gpu
- * block from the CX DBGC block
- */
-static size_t a6xx_snapshot_cx_dbgc_debugbus_block(struct kgsl_device *device,
-	u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_debugbus *header =
-		(struct kgsl_snapshot_debugbus *)buf;
-	struct adreno_debugbus_block *block = priv;
-	int i;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	unsigned int dwords;
-	size_t size;
-
-	dwords = block->dwords;
-
-	/* For a6xx each debug bus data unit is 2 DWRODS */
-	size = (dwords * sizeof(unsigned int) * 2) + sizeof(*header);
-
-	if (remain < size) {
-		SNAPSHOT_ERR_NOMEM(device, "DEBUGBUS");
-		return 0;
-	}
-
-	header->id = block->block_id;
-	header->count = dwords * 2;
-
-	for (i = 0; i < dwords; i++)
-		a6xx_cx_debug_bus_read(device, block->block_id, i,
-					&data[i*2]);
-
-	return size;
-}
-
-/* a6xx_snapshot_debugbus() - Capture debug bus data */
-static void a6xx_snapshot_debugbus(struct kgsl_device *device,
-		struct kgsl_snapshot *snapshot)
-{
-	int i;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_CNTLT,
-		(0xf << A6XX_DBGC_CFG_DBGBUS_CNTLT_SEGT_SHIFT) |
-		(0x0 << A6XX_DBGC_CFG_DBGBUS_CNTLT_GRANU_SHIFT) |
-		(0x0 << A6XX_DBGC_CFG_DBGBUS_CNTLT_TRACEEN_SHIFT));
-
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_CNTLM,
-		0xf << A6XX_DBGC_CFG_DBGBUS_CTLTM_ENABLE_SHIFT);
-
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_IVTL_0, 0);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_IVTL_1, 0);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_IVTL_2, 0);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_IVTL_3, 0);
-
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_BYTEL_0,
-		(0 << A6XX_DBGC_CFG_DBGBUS_BYTEL0_SHIFT) |
-		(1 << A6XX_DBGC_CFG_DBGBUS_BYTEL1_SHIFT) |
-		(2 << A6XX_DBGC_CFG_DBGBUS_BYTEL2_SHIFT) |
-		(3 << A6XX_DBGC_CFG_DBGBUS_BYTEL3_SHIFT) |
-		(4 << A6XX_DBGC_CFG_DBGBUS_BYTEL4_SHIFT) |
-		(5 << A6XX_DBGC_CFG_DBGBUS_BYTEL5_SHIFT) |
-		(6 << A6XX_DBGC_CFG_DBGBUS_BYTEL6_SHIFT) |
-		(7 << A6XX_DBGC_CFG_DBGBUS_BYTEL7_SHIFT));
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_BYTEL_1,
-		(8 << A6XX_DBGC_CFG_DBGBUS_BYTEL8_SHIFT) |
-		(9 << A6XX_DBGC_CFG_DBGBUS_BYTEL9_SHIFT) |
-		(10 << A6XX_DBGC_CFG_DBGBUS_BYTEL10_SHIFT) |
-		(11 << A6XX_DBGC_CFG_DBGBUS_BYTEL11_SHIFT) |
-		(12 << A6XX_DBGC_CFG_DBGBUS_BYTEL12_SHIFT) |
-		(13 << A6XX_DBGC_CFG_DBGBUS_BYTEL13_SHIFT) |
-		(14 << A6XX_DBGC_CFG_DBGBUS_BYTEL14_SHIFT) |
-		(15 << A6XX_DBGC_CFG_DBGBUS_BYTEL15_SHIFT));
-
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_MASKL_0, 0);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_MASKL_1, 0);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_MASKL_2, 0);
-	kgsl_regwrite(device, A6XX_DBGC_CFG_DBGBUS_MASKL_3, 0);
-
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_CNTLT,
-		(0xf << A6XX_DBGC_CFG_DBGBUS_CNTLT_SEGT_SHIFT) |
-		(0x0 << A6XX_DBGC_CFG_DBGBUS_CNTLT_GRANU_SHIFT) |
-		(0x0 << A6XX_DBGC_CFG_DBGBUS_CNTLT_TRACEEN_SHIFT));
-
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_CNTLM,
-		0xf << A6XX_CX_DBGC_CFG_DBGBUS_CNTLM_ENABLE_SHIFT);
-
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_IVTL_0, 0);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_IVTL_1, 0);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_IVTL_2, 0);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_IVTL_3, 0);
-
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_0,
-		(0 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL0_SHIFT) |
-		(1 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL1_SHIFT) |
-		(2 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL2_SHIFT) |
-		(3 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL3_SHIFT) |
-		(4 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL4_SHIFT) |
-		(5 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL5_SHIFT) |
-		(6 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL6_SHIFT) |
-		(7 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL7_SHIFT));
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_1,
-		(8 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL8_SHIFT) |
-		(9 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL9_SHIFT) |
-		(10 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL10_SHIFT) |
-		(11 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL11_SHIFT) |
-		(12 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL12_SHIFT) |
-		(13 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL13_SHIFT) |
-		(14 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL14_SHIFT) |
-		(15 << A6XX_CX_DBGC_CFG_DBGBUS_BYTEL15_SHIFT));
-
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_MASKL_0, 0);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_MASKL_1, 0);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_MASKL_2, 0);
-	adreno_cx_dbgc_regwrite(device, A6XX_CX_DBGC_CFG_DBGBUS_MASKL_3, 0);
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_dbgc_debugbus_blocks); i++) {
-		kgsl_snapshot_add_section(device,
-			KGSL_SNAPSHOT_SECTION_DEBUGBUS,
-			snapshot, a6xx_snapshot_dbgc_debugbus_block,
-			(void *) &a6xx_dbgc_debugbus_blocks[i]);
-	}
-	/*
-	 * GBIF has same debugbus as of other GPU blocks hence fall back to
-	 * default path if GPU uses GBIF.
-	 * GBIF uses exactly same ID as of VBIF so use it as it is.
-	 */
-	if (adreno_has_gbif(adreno_dev))
-		kgsl_snapshot_add_section(device,
-			KGSL_SNAPSHOT_SECTION_DEBUGBUS,
-			snapshot, a6xx_snapshot_dbgc_debugbus_block,
-			(void *) &a6xx_vbif_debugbus_blocks);
-	else
-		kgsl_snapshot_add_section(device,
-			KGSL_SNAPSHOT_SECTION_DEBUGBUS,
-			snapshot, a6xx_snapshot_vbif_debugbus_block,
-			(void *) &a6xx_vbif_debugbus_blocks);
-
-	/* Dump the CX debugbus data if the block exists */
-	if (adreno_is_cx_dbgc_register(device, A6XX_CX_DBGC_CFG_DBGBUS_SEL_A)) {
-		for (i = 0; i < ARRAY_SIZE(a6xx_cx_dbgc_debugbus_blocks); i++) {
-			kgsl_snapshot_add_section(device,
-				KGSL_SNAPSHOT_SECTION_DEBUGBUS,
-				snapshot, a6xx_snapshot_cx_dbgc_debugbus_block,
-				(void *) &a6xx_cx_dbgc_debugbus_blocks[i]);
-		}
-		/*
-		 * Get debugbus for GBIF CX part if GPU has GBIF block
-		 * GBIF uses exactly same ID as of VBIF so use
-		 * it as it is.
-		 */
-		if (adreno_has_gbif(adreno_dev))
-			kgsl_snapshot_add_section(device,
-				KGSL_SNAPSHOT_SECTION_DEBUGBUS,
-				snapshot,
-				a6xx_snapshot_cx_dbgc_debugbus_block,
-				(void *) &a6xx_vbif_debugbus_blocks);
-	}
-}
-
-/*
- * a6xx_snapshot_gmu() - A6XX GMU snapshot function
- * @adreno_dev: Device being snapshotted
- * @snapshot: Pointer to the snapshot instance
- *
- * This is where all of the A6XX GMU specific bits and pieces are grabbed
- * into the snapshot memory
- */
-void a6xx_snapshot_gmu(struct adreno_device *adreno_dev,
-		struct kgsl_snapshot *snapshot)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	unsigned int val;
-
-	if (!kgsl_gmu_isenabled(device))
-		return;
-
-	adreno_snapshot_registers(device, snapshot, a6xx_gmu_registers,
-					ARRAY_SIZE(a6xx_gmu_registers) / 2);
-
-	if (gpudev->gx_is_on(adreno_dev)) {
-		/* Set fence to ALLOW mode so registers can be read */
-		kgsl_regwrite(device, A6XX_GMU_AO_AHB_FENCE_CTRL, 0);
-		kgsl_regread(device, A6XX_GMU_AO_AHB_FENCE_CTRL, &val);
-
-		KGSL_DRV_ERR(device, "set FENCE to ALLOW mode:%x\n", val);
-		adreno_snapshot_registers(device, snapshot,
-				a6xx_gmu_gx_registers,
-				ARRAY_SIZE(a6xx_gmu_gx_registers) / 2);
-	}
-
-	a6xx_snapshot_debugbus(device, snapshot);
-}
-
-/* a6xx_snapshot_sqe() - Dump SQE data in snapshot */
-static size_t a6xx_snapshot_sqe(struct kgsl_device *device, u8 *buf,
-		size_t remain, void *priv)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct kgsl_snapshot_debug *header = (struct kgsl_snapshot_debug *)buf;
-	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_SQE);
-
-	if (remain < DEBUG_SECTION_SZ(1)) {
-		SNAPSHOT_ERR_NOMEM(device, "SQE VERSION DEBUG");
-		return 0;
-	}
-
-	/* Dump the SQE firmware version */
-	header->type = SNAPSHOT_DEBUG_SQE_VERSION;
-	header->size = 1;
-	*data = fw->version;
-
-	return DEBUG_SECTION_SZ(1);
-}
-
-static void _a6xx_do_crashdump(struct kgsl_device *device)
-{
-	unsigned long wait_time;
-	unsigned int reg = 0;
-	unsigned int val;
-
-	crash_dump_valid = false;
-
-	if (!device->snapshot_crashdumper)
-		return;
-	if (a6xx_capturescript.gpuaddr == 0 ||
-		a6xx_crashdump_registers.gpuaddr == 0)
-		return;
-
-	/* IF the SMMU is stalled we cannot do a crash dump */
-	kgsl_regread(device, A6XX_RBBM_STATUS3, &val);
-	if (val & BIT(24))
-		return;
-
-	/* Turn on APRIV so we can access the buffers */
-	kgsl_regwrite(device, A6XX_CP_MISC_CNTL, 1);
-
-	kgsl_regwrite(device, A6XX_CP_CRASH_SCRIPT_BASE_LO,
-			lower_32_bits(a6xx_capturescript.gpuaddr));
-	kgsl_regwrite(device, A6XX_CP_CRASH_SCRIPT_BASE_HI,
-			upper_32_bits(a6xx_capturescript.gpuaddr));
-	kgsl_regwrite(device, A6XX_CP_CRASH_DUMP_CNTL, 1);
-
-	wait_time = jiffies + msecs_to_jiffies(CP_CRASH_DUMPER_TIMEOUT);
-	while (!time_after(jiffies, wait_time)) {
-		kgsl_regread(device, A6XX_CP_CRASH_DUMP_STATUS, &reg);
-		if (reg & 0x2)
-			break;
-		cpu_relax();
-	}
-
-	kgsl_regwrite(device, A6XX_CP_MISC_CNTL, 0);
-
-	if (!(reg & 0x2)) {
-		KGSL_CORE_ERR("Crash dump timed out: 0x%X\n", reg);
-		return;
-	}
-
-	crash_dump_valid = true;
-}
-
-/*
- * a6xx_snapshot() - A6XX GPU snapshot function
- * @adreno_dev: Device being snapshotted
- * @snapshot: Pointer to the snapshot instance
- *
- * This is where all of the A6XX specific bits and pieces are grabbed
- * into the snapshot memory
- */
-void a6xx_snapshot(struct adreno_device *adreno_dev,
-		struct kgsl_snapshot *snapshot)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct adreno_snapshot_data *snap_data = gpudev->snapshot_data;
-	bool sptprac_on;
-	unsigned int i;
-
-	/* GMU TCM data dumped through AHB */
-	a6xx_snapshot_gmu(adreno_dev, snapshot);
-
-	sptprac_on = gpudev->sptprac_is_on(adreno_dev);
-
-	/* Return if the GX is off */
-	if (!gpudev->gx_is_on(adreno_dev))
-		return;
-
-	/* Dump the registers which get affected by crash dumper trigger */
-	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_REGS,
-		snapshot, a6xx_snapshot_pre_crashdump_regs, NULL);
-
-	/* Dump vbif registers as well which get affected by crash dumper */
-	if (!adreno_has_gbif(adreno_dev))
-		adreno_snapshot_vbif_registers(device, snapshot,
-			a6xx_vbif_snapshot_registers,
-			ARRAY_SIZE(a6xx_vbif_snapshot_registers));
-	else
-		adreno_snapshot_registers(device, snapshot,
-			a6xx_gbif_registers,
-			ARRAY_SIZE(a6xx_gbif_registers) / 2);
-
-	/* Try to run the crash dumper */
-	if (sptprac_on)
-		_a6xx_do_crashdump(device);
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_reg_list); i++) {
-		kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_REGS,
-			snapshot, a6xx_snapshot_registers, &a6xx_reg_list[i]);
-	}
-
-	/* CP_SQE indexed registers */
-	kgsl_snapshot_indexed_registers(device, snapshot,
-		A6XX_CP_SQE_STAT_ADDR, A6XX_CP_SQE_STAT_DATA,
-		0, snap_data->sect_sizes->cp_pfp);
-
-	/* CP_DRAW_STATE */
-	kgsl_snapshot_indexed_registers(device, snapshot,
-		A6XX_CP_DRAW_STATE_ADDR, A6XX_CP_DRAW_STATE_DATA,
-		0, 0x100);
-
-	 /* SQE_UCODE Cache */
-	kgsl_snapshot_indexed_registers(device, snapshot,
-		A6XX_CP_SQE_UCODE_DBG_ADDR, A6XX_CP_SQE_UCODE_DBG_DATA,
-		0, 0x6000);
-
-	/* CP ROQ */
-	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_DEBUG,
-		snapshot, adreno_snapshot_cp_roq,
-		&snap_data->sect_sizes->roq);
-
-	/* SQE Firmware */
-	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_DEBUG,
-		snapshot, a6xx_snapshot_sqe, NULL);
-
-	/* Mempool debug data */
-	a6xx_snapshot_mempool(device, snapshot);
-
-	if (sptprac_on) {
-		/* Shader memory */
-		a6xx_snapshot_shader(device, snapshot);
-
-		/* MVC register section */
-		a6xx_snapshot_mvc_regs(device, snapshot);
-
-		/* registers dumped through DBG AHB */
-		a6xx_snapshot_dbgahb_regs(device, snapshot);
-	}
-
-}
-
-static int _a6xx_crashdump_init_mvc(uint64_t *ptr, uint64_t *offset)
-{
-	int qwords = 0;
-	unsigned int i, j, k;
-	unsigned int count;
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_clusters); i++) {
-		struct a6xx_cluster_registers *cluster = &a6xx_clusters[i];
-
-		if (cluster->sel) {
-			ptr[qwords++] = cluster->sel->val;
-			ptr[qwords++] = ((uint64_t)cluster->sel->cd_reg << 44) |
-				(1 << 21) | 1;
-		}
-
-		cluster->offset0 = *offset;
-		for (j = 0; j < A6XX_NUM_CTXTS; j++) {
-
-			if (j == 1)
-				cluster->offset1 = *offset;
-
-			ptr[qwords++] = (cluster->id << 8) | (j << 4) | j;
-			ptr[qwords++] =
-				((uint64_t)A6XX_CP_APERTURE_CNTL_CD << 44) |
-				(1 << 21) | 1;
-
-			for (k = 0; k < cluster->num_sets; k++) {
-				count = REG_PAIR_COUNT(cluster->regs, k);
-				ptr[qwords++] =
-				a6xx_crashdump_registers.gpuaddr + *offset;
-				ptr[qwords++] =
-				(((uint64_t)cluster->regs[2 * k]) << 44) |
-						count;
-
-				*offset += count * sizeof(unsigned int);
-			}
-		}
-	}
-
-	return qwords;
-}
-
-static int _a6xx_crashdump_init_shader(struct a6xx_shader_block *block,
-		uint64_t *ptr, uint64_t *offset)
-{
-	int qwords = 0;
-	unsigned int j;
-
-	/* Capture each bank in the block */
-	for (j = 0; j < A6XX_NUM_SHADER_BANKS; j++) {
-		/* Program the aperture */
-		ptr[qwords++] =
-			(block->statetype << A6XX_SHADER_STATETYPE_SHIFT) | j;
-		ptr[qwords++] = (((uint64_t) A6XX_HLSQ_DBG_READ_SEL << 44)) |
-			(1 << 21) | 1;
-
-		/* Read all the data in one chunk */
-		ptr[qwords++] = a6xx_crashdump_registers.gpuaddr + *offset;
-		ptr[qwords++] =
-			(((uint64_t) A6XX_HLSQ_DBG_AHB_READ_APERTURE << 44)) |
-			block->sz;
-
-		/* Remember the offset of the first bank for easy access */
-		if (j == 0)
-			block->offset = *offset;
-
-		*offset += block->sz * sizeof(unsigned int);
-	}
-
-	return qwords;
-}
-
-static int _a6xx_crashdump_init_ctx_dbgahb(uint64_t *ptr, uint64_t *offset)
-{
-	int qwords = 0;
-	unsigned int i, j, k;
-	unsigned int count;
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_dbgahb_ctx_clusters); i++) {
-		struct a6xx_cluster_dbgahb_registers *cluster =
-				&a6xx_dbgahb_ctx_clusters[i];
-
-		cluster->offset0 = *offset;
-
-		for (j = 0; j < A6XX_NUM_CTXTS; j++) {
-			if (j == 1)
-				cluster->offset1 = *offset;
-
-			/* Program the aperture */
-			ptr[qwords++] =
-				((cluster->statetype + j * 2) & 0xff) << 8;
-			ptr[qwords++] =
-				(((uint64_t)A6XX_HLSQ_DBG_READ_SEL << 44)) |
-					(1 << 21) | 1;
-
-			for (k = 0; k < cluster->num_sets; k++) {
-				unsigned int start = cluster->regs[2 * k];
-
-				count = REG_PAIR_COUNT(cluster->regs, k);
-				ptr[qwords++] =
-				a6xx_crashdump_registers.gpuaddr + *offset;
-				ptr[qwords++] =
-				(((uint64_t)(A6XX_HLSQ_DBG_AHB_READ_APERTURE +
-					start - cluster->regbase / 4) << 44)) |
-							count;
-
-				*offset += count * sizeof(unsigned int);
-			}
-		}
-	}
-	return qwords;
-}
-
-static int _a6xx_crashdump_init_non_ctx_dbgahb(uint64_t *ptr, uint64_t *offset)
-{
-	int qwords = 0;
-	unsigned int i, k;
-	unsigned int count;
-
-	for (i = 0; i < ARRAY_SIZE(a6xx_non_ctx_dbgahb); i++) {
-		struct a6xx_non_ctx_dbgahb_registers *regs =
-				&a6xx_non_ctx_dbgahb[i];
-
-		regs->offset = *offset;
-
-		/* Program the aperture */
-		ptr[qwords++] = (regs->statetype & 0xff) << 8;
-		ptr[qwords++] =	(((uint64_t)A6XX_HLSQ_DBG_READ_SEL << 44)) |
-					(1 << 21) | 1;
-
-		for (k = 0; k < regs->num_sets; k++) {
-			unsigned int start = regs->regs[2 * k];
-
-			count = REG_PAIR_COUNT(regs->regs, k);
-			ptr[qwords++] =
-				a6xx_crashdump_registers.gpuaddr + *offset;
-			ptr[qwords++] =
-				(((uint64_t)(A6XX_HLSQ_DBG_AHB_READ_APERTURE +
-					start - regs->regbase / 4) << 44)) |
-							count;
-
-			*offset += count * sizeof(unsigned int);
-		}
-	}
-	return qwords;
-}
-
-void a6xx_crashdump_init(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	unsigned int script_size = 0;
-	unsigned int data_size = 0;
-	unsigned int i, j, k;
-	uint64_t *ptr;
-	uint64_t offset = 0;
-
-	if (a6xx_capturescript.gpuaddr != 0 &&
-		a6xx_crashdump_registers.gpuaddr != 0)
-		return;
-
-	/*
-	 * We need to allocate two buffers:
-	 * 1 - the buffer to hold the draw script
-	 * 2 - the buffer to hold the data
-	 */
-
-	/*
-	 * To save the registers, we need 16 bytes per register pair for the
-	 * script and a dword for each register in the data
-	 */
-	for (i = 0; i < ARRAY_SIZE(a6xx_reg_list); i++) {
-		struct reg_list *regs = &a6xx_reg_list[i];
-
-		/* 16 bytes for programming the aperture */
-		if (regs->sel)
-			script_size += 16;
-
-		/* Each pair needs 16 bytes (2 qwords) */
-		script_size += regs->count * 16;
-
-		/* Each register needs a dword in the data */
-		for (j = 0; j < regs->count; j++)
-			data_size += REG_PAIR_COUNT(regs->regs, j) *
-				sizeof(unsigned int);
-
-	}
-
-	/*
-	 * To save the shader blocks for each block in each type we need 32
-	 * bytes for the script (16 bytes to program the aperture and 16 to
-	 * read the data) and then a block specific number of bytes to hold
-	 * the data
-	 */
-	for (i = 0; i < ARRAY_SIZE(a6xx_shader_blocks); i++) {
-		script_size += 32 * A6XX_NUM_SHADER_BANKS;
-		data_size += a6xx_shader_blocks[i].sz * sizeof(unsigned int) *
-			A6XX_NUM_SHADER_BANKS;
-	}
-
-	/* Calculate the script and data size for MVC registers */
-	for (i = 0; i < ARRAY_SIZE(a6xx_clusters); i++) {
-		struct a6xx_cluster_registers *cluster = &a6xx_clusters[i];
-
-		for (j = 0; j < A6XX_NUM_CTXTS; j++) {
-
-			/* 16 bytes for programming the aperture */
-			script_size += 16;
-
-			/* Reading each pair of registers takes 16 bytes */
-			script_size += 16 * cluster->num_sets;
-
-			/* A dword per register read from the cluster list */
-			for (k = 0; k < cluster->num_sets; k++)
-				data_size += REG_PAIR_COUNT(cluster->regs, k) *
-						sizeof(unsigned int);
-		}
-	}
-
-	/* Calculate the script and data size for debug AHB registers */
-	for (i = 0; i < ARRAY_SIZE(a6xx_dbgahb_ctx_clusters); i++) {
-		struct a6xx_cluster_dbgahb_registers *cluster =
-				&a6xx_dbgahb_ctx_clusters[i];
-
-		for (j = 0; j < A6XX_NUM_CTXTS; j++) {
-
-			/* 16 bytes for programming the aperture */
-			script_size += 16;
-
-			/* Reading each pair of registers takes 16 bytes */
-			script_size += 16 * cluster->num_sets;
-
-			/* A dword per register read from the cluster list */
-			for (k = 0; k < cluster->num_sets; k++)
-				data_size += REG_PAIR_COUNT(cluster->regs, k) *
-						sizeof(unsigned int);
-		}
-	}
-
-	/*
-	 * Calculate the script and data size for non context debug
-	 * AHB registers
-	 */
-	for (i = 0; i < ARRAY_SIZE(a6xx_non_ctx_dbgahb); i++) {
-		struct a6xx_non_ctx_dbgahb_registers *regs =
-				&a6xx_non_ctx_dbgahb[i];
-
-		/* 16 bytes for programming the aperture */
-		script_size += 16;
-
-		/* Reading each pair of registers takes 16 bytes */
-		script_size += 16 * regs->num_sets;
-
-		/* A dword per register read from the cluster list */
-		for (k = 0; k < regs->num_sets; k++)
-			data_size += REG_PAIR_COUNT(regs->regs, k) *
-				sizeof(unsigned int);
-	}
-
-	/* Now allocate the script and data buffers */
-
-	/* The script buffers needs 2 extra qwords on the end */
-	if (kgsl_allocate_global(device, &a6xx_capturescript,
-		script_size + 16, KGSL_MEMFLAGS_GPUREADONLY,
-		KGSL_MEMDESC_PRIVILEGED, "capturescript"))
-		return;
-
-	if (kgsl_allocate_global(device, &a6xx_crashdump_registers, data_size,
-		0, KGSL_MEMDESC_PRIVILEGED, "capturescript_regs")) {
-		kgsl_free_global(KGSL_DEVICE(adreno_dev), &a6xx_capturescript);
-		return;
-	}
-
-	/* Build the crash script */
-
-	ptr = (uint64_t *)a6xx_capturescript.hostptr;
-
-	/* For the registers, program a read command for each pair */
-	for (i = 0; i < ARRAY_SIZE(a6xx_reg_list); i++) {
-		struct reg_list *regs = &a6xx_reg_list[i];
-
-		regs->offset = offset;
-
-		/* Program the SEL_CNTL_CD register appropriately */
-		if (regs->sel) {
-			*ptr++ = regs->sel->val;
-			*ptr++ = (((uint64_t)regs->sel->cd_reg << 44)) |
-					(1 << 21) | 1;
-		}
-
-		for (j = 0; j < regs->count; j++) {
-			unsigned int r = REG_PAIR_COUNT(regs->regs, j);
-			*ptr++ = a6xx_crashdump_registers.gpuaddr + offset;
-			*ptr++ = (((uint64_t) regs->regs[2 * j]) << 44) | r;
-			offset += r * sizeof(unsigned int);
-		}
-	}
-
-	/* Program each shader block */
-	for (i = 0; i < ARRAY_SIZE(a6xx_shader_blocks); i++) {
-		ptr += _a6xx_crashdump_init_shader(&a6xx_shader_blocks[i], ptr,
-							&offset);
-	}
-
-	/* Program the capturescript for the MVC regsiters */
-	ptr += _a6xx_crashdump_init_mvc(ptr, &offset);
-
-	ptr += _a6xx_crashdump_init_ctx_dbgahb(ptr, &offset);
-
-	ptr += _a6xx_crashdump_init_non_ctx_dbgahb(ptr, &offset);
-
-	*ptr++ = 0;
-	*ptr++ = 0;
-}
diff --git a/drivers/gpu/msm/adreno_compat.c b/drivers/gpu/msm/adreno_compat.c
index 1f806dabea25..582cbfb61e78 100644
--- a/drivers/gpu/msm/adreno_compat.c
+++ b/drivers/gpu/msm/adreno_compat.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -89,54 +89,6 @@ int adreno_getproperty_compat(struct kgsl_device *device,
 			status = 0;
 		}
 		break;
-	case KGSL_PROP_DEVICE_QDSS_STM:
-		{
-			struct kgsl_qdss_stm_prop qdssprop = {0};
-			struct kgsl_memdesc *qdss_desc =
-				kgsl_mmu_get_qdss_global_entry(device);
-
-			if (sizebytes != sizeof(qdssprop)) {
-				status = -EINVAL;
-				break;
-			}
-
-			if (qdss_desc) {
-				qdssprop.gpuaddr = qdss_desc->gpuaddr;
-				qdssprop.size = qdss_desc->size;
-			}
-
-			if (copy_to_user(value, &qdssprop,
-						sizeof(qdssprop))) {
-				status = -EFAULT;
-				break;
-			}
-			status = 0;
-		}
-		break;
-	case KGSL_PROP_DEVICE_QTIMER:
-		{
-			struct kgsl_qtimer_prop qtimerprop = {0};
-			struct kgsl_memdesc *qtimer_desc =
-				kgsl_mmu_get_qtimer_global_entry(device);
-
-			if (sizebytes != sizeof(qtimerprop)) {
-				status = -EINVAL;
-				break;
-			}
-
-			if (qtimer_desc) {
-				qtimerprop.gpuaddr = qtimer_desc->gpuaddr;
-				qtimerprop.size = qtimer_desc->size;
-			}
-
-			if (copy_to_user(value, &qtimerprop,
-						sizeof(qtimerprop))) {
-				status = -EFAULT;
-				break;
-			}
-			status = 0;
-		}
-		break;
 	default:
 		/*
 		 * Call the adreno_getproperty to check if the property type
@@ -187,37 +139,6 @@ int adreno_setproperty_compat(struct kgsl_device_private *dev_priv,
 			kgsl_context_put(context);
 		}
 		break;
-	case KGSL_PROP_L3_PWR_CONSTRAINT: {
-			struct kgsl_device_constraint_compat constraint32;
-			struct kgsl_device_constraint constraint;
-			struct kgsl_context *context;
-
-			if (sizebytes != sizeof(constraint32))
-				break;
-
-			if (copy_from_user(&constraint32, value,
-				sizeof(constraint32))) {
-				status = -EFAULT;
-				break;
-			}
-
-			constraint.type = constraint32.type;
-			constraint.context_id = constraint32.context_id;
-			constraint.data = compat_ptr(constraint32.data);
-			constraint.size = (size_t)constraint32.size;
-
-			context = kgsl_context_get_owner(dev_priv,
-							constraint.context_id);
-
-			if (context == NULL)
-				break;
-
-			status = adreno_set_constraint(device, context,
-							&constraint);
-			kgsl_context_put(context);
-
-		}
-		break;
 	default:
 		/*
 		 * Call adreno_setproperty in case the property type was
diff --git a/drivers/gpu/msm/adreno_compat.h b/drivers/gpu/msm/adreno_compat.h
index 55de74cfae98..4fba17bc8b13 100644
--- a/drivers/gpu/msm/adreno_compat.h
+++ b/drivers/gpu/msm/adreno_compat.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2015, 2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -37,20 +37,20 @@ static inline int adreno_getproperty_compat(struct kgsl_device *device,
 				unsigned int type,
 				void __user *value, size_t sizebytes)
 {
-	return -EINVAL;
+	BUG();
 }
 
 static inline int adreno_setproperty_compat(struct kgsl_device_private
 				*dev_priv, unsigned int type,
 				void __user *value, unsigned int sizebytes)
 {
-	return -EINVAL;
+	BUG();
 }
 
 static inline long adreno_compat_ioctl(struct kgsl_device_private *dev_priv,
 				unsigned int cmd, unsigned long arg)
 {
-	return -EINVAL;
+	BUG();
 }
 
 #endif /* CONFIG_COMPAT */
diff --git a/drivers/gpu/msm/adreno_coresight.c b/drivers/gpu/msm/adreno_coresight.c
index ef482a26f2a5..0fdcc2fff032 100644
--- a/drivers/gpu/msm/adreno_coresight.c
+++ b/drivers/gpu/msm/adreno_coresight.c
@@ -31,51 +31,51 @@ ssize_t adreno_coresight_show_register(struct device *dev,
 		struct device_attribute *attr, char *buf)
 {
 	unsigned int val = 0;
-	struct kgsl_device *device = dev_get_drvdata(dev->parent);
-	struct adreno_device *adreno_dev;
-	struct adreno_coresight_attr *cattr = TO_ADRENO_CORESIGHT_ATTR(attr);
-	bool is_cx;
-
-	if (device == NULL)
-		return -EINVAL;
-
-	adreno_dev = ADRENO_DEVICE(device);
-
-	if (cattr->reg == NULL)
-		return -EINVAL;
-
-	is_cx = adreno_is_cx_dbgc_register(device, cattr->reg->offset);
-	/*
-	 * Return the current value of the register if coresight is enabled,
-	 * otherwise report 0
-	 */
-
-	mutex_lock(&device->mutex);
-	if ((is_cx && test_bit(ADRENO_DEVICE_CORESIGHT_CX, &adreno_dev->priv))
-		|| (!is_cx && test_bit(ADRENO_DEVICE_CORESIGHT,
-			&adreno_dev->priv))) {
-		/*
-		 * If the device isn't power collapsed read the actual value
-		 * from the hardware - otherwise return the cached value
-		 */
-
-		if (device->state == KGSL_STATE_ACTIVE ||
-			device->state == KGSL_STATE_NAP) {
-			if (!kgsl_active_count_get(device)) {
-				if (!is_cx)
-					kgsl_regread(device, cattr->reg->offset,
-						&cattr->reg->value);
-				else
-					adreno_cx_dbgc_regread(device,
-						cattr->reg->offset,
-						&cattr->reg->value);
-				kgsl_active_count_put(device);
-			}
-		}
-
-		val = cattr->reg->value;
-	}
-	mutex_unlock(&device->mutex);
+	// struct kgsl_device *device = dev_get_drvdata(dev->parent);
+	// struct adreno_device *adreno_dev;
+	// struct adreno_coresight_attr *cattr = TO_ADRENO_CORESIGHT_ATTR(attr);
+	// bool is_cx;
+
+	// if (device == NULL)
+	// 	return -EINVAL;
+
+	// adreno_dev = ADRENO_DEVICE(device);
+
+	// if (cattr->reg == NULL)
+	// 	return -EINVAL;
+
+	// is_cx = adreno_is_cx_dbgc_register(device, cattr->reg->offset);
+	// /*
+	//  * Return the current value of the register if coresight is enabled,
+	//  * otherwise report 0
+	//  */
+
+	// mutex_lock(&device->mutex);
+	// if ((is_cx && test_bit(ADRENO_DEVICE_CORESIGHT_CX, &adreno_dev->priv))
+	// 	|| (!is_cx && test_bit(ADRENO_DEVICE_CORESIGHT,
+	// 		&adreno_dev->priv))) {
+	// 	/*
+	// 	 * If the device isn't power collapsed read the actual value
+	// 	 * from the hardware - otherwise return the cached value
+	// 	 */
+
+	// 	if (device->state == KGSL_STATE_ACTIVE ||
+	// 		device->state == KGSL_STATE_NAP) {
+	// 		if (!kgsl_active_count_get(device)) {
+	// 			if (!is_cx)
+	// 				kgsl_regread(device, cattr->reg->offset,
+	// 					&cattr->reg->value);
+	// 			else
+	// 				adreno_cx_dbgc_regread(device,
+	// 					cattr->reg->offset,
+	// 					&cattr->reg->value);
+	// 			kgsl_active_count_put(device);
+	// 		}
+	// 	}
+
+	// 	val = cattr->reg->value;
+	// }
+	// mutex_unlock(&device->mutex);
 
 	return snprintf(buf, PAGE_SIZE, "0x%X\n", val);
 }
@@ -83,54 +83,54 @@ ssize_t adreno_coresight_show_register(struct device *dev,
 ssize_t adreno_coresight_store_register(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t size)
 {
-	struct kgsl_device *device = dev_get_drvdata(dev->parent);
-	struct adreno_device *adreno_dev;
-	struct adreno_coresight_attr *cattr = TO_ADRENO_CORESIGHT_ATTR(attr);
-	unsigned long val;
-	int ret, is_cx;
-
-	if (device == NULL)
-		return -EINVAL;
-
-	adreno_dev = ADRENO_DEVICE(device);
-
-	if (cattr->reg == NULL)
-		return -EINVAL;
-
-	is_cx = adreno_is_cx_dbgc_register(device, cattr->reg->offset);
-
-	ret = kstrtoul(buf, 0, &val);
-	if (ret)
-		return ret;
-
-	mutex_lock(&device->mutex);
-
-	/* Ignore writes while coresight is off */
-	if (!((is_cx && test_bit(ADRENO_DEVICE_CORESIGHT_CX, &adreno_dev->priv))
-		|| (!is_cx && test_bit(ADRENO_DEVICE_CORESIGHT,
-		&adreno_dev->priv))))
-		goto out;
-
-	cattr->reg->value = val;
-
-	/* Program the hardware if it is not power collapsed */
-	if (device->state == KGSL_STATE_ACTIVE ||
-		device->state == KGSL_STATE_NAP) {
-		if (!kgsl_active_count_get(device)) {
-			if (!is_cx)
-				kgsl_regwrite(device, cattr->reg->offset,
-					cattr->reg->value);
-			else
-				adreno_cx_dbgc_regwrite(device,
-					cattr->reg->offset,
-					cattr->reg->value);
-
-			kgsl_active_count_put(device);
-		}
-	}
-
-out:
-	mutex_unlock(&device->mutex);
+// 	struct kgsl_device *device = dev_get_drvdata(dev->parent);
+// 	struct adreno_device *adreno_dev;
+// 	struct adreno_coresight_attr *cattr = TO_ADRENO_CORESIGHT_ATTR(attr);
+// 	unsigned long val;
+// 	int ret, is_cx;
+
+// 	if (device == NULL)
+// 		return -EINVAL;
+
+// 	adreno_dev = ADRENO_DEVICE(device);
+
+// 	if (cattr->reg == NULL)
+// 		return -EINVAL;
+
+// 	is_cx = adreno_is_cx_dbgc_register(device, cattr->reg->offset);
+
+// 	ret = kstrtoul(buf, 0, &val);
+// 	if (ret)
+// 		return ret;
+
+// 	mutex_lock(&device->mutex);
+
+// 	/* Ignore writes while coresight is off */
+// 	if (!((is_cx && test_bit(ADRENO_DEVICE_CORESIGHT_CX, &adreno_dev->priv))
+// 		|| (!is_cx && test_bit(ADRENO_DEVICE_CORESIGHT,
+// 		&adreno_dev->priv))))
+// 		goto out;
+
+// 	cattr->reg->value = val;
+
+// 	/* Program the hardware if it is not power collapsed */
+// 	if (device->state == KGSL_STATE_ACTIVE ||
+// 		device->state == KGSL_STATE_NAP) {
+// 		if (!kgsl_active_count_get(device)) {
+// 			if (!is_cx)
+// 				kgsl_regwrite(device, cattr->reg->offset,
+// 					cattr->reg->value);
+// 			else
+// 				adreno_cx_dbgc_regwrite(device,
+// 					cattr->reg->offset,
+// 					cattr->reg->value);
+
+// 			kgsl_active_count_put(device);
+// 		}
+// 	}
+
+// out:
+// 	mutex_unlock(&device->mutex);
 	return size;
 }
 
@@ -150,46 +150,46 @@ ssize_t adreno_coresight_store_register(struct device *dev,
 static void adreno_coresight_disable(struct coresight_device *csdev,
 					struct perf_event *event)
 {
-	struct kgsl_device *device = dev_get_drvdata(csdev->dev.parent);
-	struct adreno_device *adreno_dev;
-	struct adreno_gpudev *gpudev;
-	struct adreno_coresight *coresight;
-	int i, cs_id;
+	// struct kgsl_device *device = dev_get_drvdata(csdev->dev.parent);
+	// struct adreno_device *adreno_dev;
+	// struct adreno_gpudev *gpudev;
+	// struct adreno_coresight *coresight;
+	// int i, cs_id;
 
-	if (device == NULL)
-		return;
+	// if (device == NULL)
+	// 	return;
 
-	adreno_dev = ADRENO_DEVICE(device);
-	gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	// adreno_dev = ADRENO_DEVICE(device);
+	// gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 
-	cs_id = adreno_coresight_identify(dev_name(&csdev->dev));
+	// cs_id = adreno_coresight_identify(dev_name(&csdev->dev));
 
-	if (cs_id < 0)
-		return;
+	// if (cs_id < 0)
+	// 	return;
 
-	coresight = gpudev->coresight[cs_id];
+	// coresight = gpudev->coresight[cs_id];
 
-	if (coresight == NULL)
-		return;
+	// if (coresight == NULL)
+	// 	return;
 
-	mutex_lock(&device->mutex);
+	// mutex_lock(&device->mutex);
 
-	if (!kgsl_active_count_get(device)) {
-		if (cs_id == GPU_CORESIGHT_GX)
-			for (i = 0; i < coresight->count; i++)
-				kgsl_regwrite(device,
-					coresight->registers[i].offset, 0);
-		else if (cs_id == GPU_CORESIGHT_CX)
-			for (i = 0; i < coresight->count; i++)
-				adreno_cx_dbgc_regwrite(device,
-					coresight->registers[i].offset, 0);
+	// if (!kgsl_active_count_get(device)) {
+	// 	if (cs_id == GPU_CORESIGHT_GX)
+	// 		for (i = 0; i < coresight->count; i++)
+	// 			kgsl_regwrite(device,
+	// 				coresight->registers[i].offset, 0);
+	// 	else if (cs_id == GPU_CORESIGHT_CX)
+	// 		for (i = 0; i < coresight->count; i++)
+	// 			adreno_cx_dbgc_regwrite(device,
+	// 				coresight->registers[i].offset, 0);
 
-		kgsl_active_count_put(device);
-	}
+	// 	kgsl_active_count_put(device);
+	// }
 
-	clear_bit(ADRENO_DEVICE_CORESIGHT, &adreno_dev->priv);
+	// clear_bit(ADRENO_DEVICE_CORESIGHT, &adreno_dev->priv);
 
-	mutex_unlock(&device->mutex);
+	// mutex_unlock(&device->mutex);
 }
 
 /**
@@ -201,59 +201,59 @@ static void adreno_coresight_disable(struct coresight_device *csdev,
 static int _adreno_coresight_get_and_clear(struct adreno_device *adreno_dev,
 						int cs_id)
 {
-	int i;
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_coresight *coresight = gpudev->coresight[cs_id];
-
-	if (coresight == NULL)
-		return -ENODEV;
-
-	kgsl_pre_hwaccess(device);
-	/*
-	 * Save the current value of each coresight register
-	 * and then clear each register
-	 */
-	if (cs_id == GPU_CORESIGHT_GX) {
-		for (i = 0; i < coresight->count; i++) {
-			kgsl_regread(device, coresight->registers[i].offset,
-				&coresight->registers[i].value);
-			kgsl_regwrite(device, coresight->registers[i].offset,
-				0);
-		}
-	} else if (cs_id == GPU_CORESIGHT_CX) {
-		for (i = 0; i < coresight->count; i++) {
-			adreno_cx_dbgc_regread(device,
-				coresight->registers[i].offset,
-				&coresight->registers[i].value);
-			adreno_cx_dbgc_regwrite(device,
-				coresight->registers[i].offset, 0);
-		}
-	}
+	// int i;
+	// struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	// struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	// struct adreno_coresight *coresight = gpudev->coresight[cs_id];
+
+	// if (coresight == NULL)
+	// 	return -ENODEV;
+
+	// kgsl_pre_hwaccess(device);
+	// /*
+	//  * Save the current value of each coresight register
+	//  * and then clear each register
+	//  */
+	// if (cs_id == GPU_CORESIGHT_GX) {
+	// 	for (i = 0; i < coresight->count; i++) {
+	// 		kgsl_regread(device, coresight->registers[i].offset,
+	// 			&coresight->registers[i].value);
+	// 		kgsl_regwrite(device, coresight->registers[i].offset,
+	// 			0);
+	// 	}
+	// } else if (cs_id == GPU_CORESIGHT_CX) {
+	// 	for (i = 0; i < coresight->count; i++) {
+	// 		adreno_cx_dbgc_regread(device,
+	// 			coresight->registers[i].offset,
+	// 			&coresight->registers[i].value);
+	// 		adreno_cx_dbgc_regwrite(device,
+	// 			coresight->registers[i].offset, 0);
+	// 	}
+	// }
 
 	return 0;
 }
 
 static int _adreno_coresight_set(struct adreno_device *adreno_dev, int cs_id)
 {
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_coresight *coresight = gpudev->coresight[cs_id];
-	int i;
-
-	if (coresight == NULL)
-		return -ENODEV;
-
-	if (cs_id == GPU_CORESIGHT_GX) {
-		for (i = 0; i < coresight->count; i++)
-			kgsl_regwrite(device, coresight->registers[i].offset,
-				coresight->registers[i].value);
-	} else if (cs_id == GPU_CORESIGHT_CX) {
-		for (i = 0; i < coresight->count; i++)
-			adreno_cx_dbgc_regwrite(device,
-				coresight->registers[i].offset,
-				coresight->registers[i].value);
-	}
+	// struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	// struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	// struct adreno_coresight *coresight = gpudev->coresight[cs_id];
+	// int i;
+
+	// if (coresight == NULL)
+	// 	return -ENODEV;
+
+	// if (cs_id == GPU_CORESIGHT_GX) {
+	// 	for (i = 0; i < coresight->count; i++)
+	// 		kgsl_regwrite(device, coresight->registers[i].offset,
+	// 			coresight->registers[i].value);
+	// } else if (cs_id == GPU_CORESIGHT_CX) {
+	// 	for (i = 0; i < coresight->count; i++)
+	// 		adreno_cx_dbgc_regwrite(device,
+	// 			coresight->registers[i].offset,
+	// 			coresight->registers[i].value);
+	// }
 	return 0;
 }
 /**
@@ -276,49 +276,49 @@ static int adreno_coresight_enable(struct coresight_device *csdev,
 	struct adreno_coresight *coresight;
 	int ret = 0, adreno_dev_flag = -EINVAL, cs_id;
 
-	if (device == NULL)
-		return -ENODEV;
+	// if (device == NULL)
+	// 	return -ENODEV;
 
-	adreno_dev = ADRENO_DEVICE(device);
-	gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	// adreno_dev = ADRENO_DEVICE(device);
+	// gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 
-	cs_id = adreno_coresight_identify(dev_name(&csdev->dev));
+	// cs_id = adreno_coresight_identify(dev_name(&csdev->dev));
 
-	if (cs_id < 0)
-		return -ENODEV;
+	// if (cs_id < 0)
+	// 	return -ENODEV;
 
-	coresight = gpudev->coresight[cs_id];
+	// coresight = gpudev->coresight[cs_id];
 
-	if (coresight == NULL)
-		return -ENODEV;
+	// if (coresight == NULL)
+	// 	return -ENODEV;
 
-	if (cs_id == GPU_CORESIGHT_GX)
-		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
-	else if (cs_id == GPU_CORESIGHT_CX)
-		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
-	else
-		return -ENODEV;
+	// if (cs_id == GPU_CORESIGHT_GX)
+	// 	adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
+	// else if (cs_id == GPU_CORESIGHT_CX)
+	// 	adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
+	// else
+	// 	return -ENODEV;
 
-	mutex_lock(&device->mutex);
-	if (!test_and_set_bit(adreno_dev_flag, &adreno_dev->priv)) {
-		int i;
+	// mutex_lock(&device->mutex);
+	// if (!test_and_set_bit(adreno_dev_flag, &adreno_dev->priv)) {
+	// 	int i;
 
-		/* Reset all the debug registers to their default values */
+	// 	/* Reset all the debug registers to their default values */
 
-		for (i = 0; i < coresight->count; i++)
-			coresight->registers[i].value =
-				coresight->registers[i].initial;
+	// 	for (i = 0; i < coresight->count; i++)
+	// 		coresight->registers[i].value =
+	// 			coresight->registers[i].initial;
 
-		if (kgsl_state_is_awake(device)) {
-			ret = kgsl_active_count_get(device);
-			if (!ret) {
-				ret = _adreno_coresight_set(adreno_dev, cs_id);
-				kgsl_active_count_put(device);
-			}
-		}
-	}
+	// 	if (kgsl_state_is_awake(device)) {
+	// 		ret = kgsl_active_count_get(device);
+	// 		if (!ret) {
+	// 			ret = _adreno_coresight_set(adreno_dev, cs_id);
+	// 			kgsl_active_count_put(device);
+	// 		}
+	// 	}
+	// }
 
-	mutex_unlock(&device->mutex);
+	// mutex_unlock(&device->mutex);
 
 	return ret;
 }
@@ -332,19 +332,19 @@ static int adreno_coresight_enable(struct coresight_device *csdev,
  */
 void adreno_coresight_stop(struct adreno_device *adreno_dev)
 {
-	int i, adreno_dev_flag = -EINVAL;
-
-	for (i = 0; i < GPU_CORESIGHT_MAX; ++i) {
-		if (i == GPU_CORESIGHT_GX)
-			adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
-		else if (i == GPU_CORESIGHT_CX)
-			adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
-		else
-			return;
-
-		if (test_bit(adreno_dev_flag, &adreno_dev->priv))
-			_adreno_coresight_get_and_clear(adreno_dev, i);
-	}
+	// int i, adreno_dev_flag = -EINVAL;
+
+	// for (i = 0; i < GPU_CORESIGHT_MAX; ++i) {
+	// 	if (i == GPU_CORESIGHT_GX)
+	// 		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
+	// 	else if (i == GPU_CORESIGHT_CX)
+	// 		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
+	// 	else
+	// 		return;
+
+	// 	if (test_bit(adreno_dev_flag, &adreno_dev->priv))
+	// 		_adreno_coresight_get_and_clear(adreno_dev, i);
+	// }
 }
 
 /**
@@ -355,33 +355,34 @@ void adreno_coresight_stop(struct adreno_device *adreno_dev)
  */
 void adreno_coresight_start(struct adreno_device *adreno_dev)
 {
-	int i, adreno_dev_flag = -EINVAL;
-
-	for (i = 0; i < GPU_CORESIGHT_MAX; ++i) {
-		if (i == GPU_CORESIGHT_GX)
-			adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
-		else if (i == GPU_CORESIGHT_CX)
-			adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
-		else
-			return;
-
-		if (test_bit(adreno_dev_flag, &adreno_dev->priv))
-			_adreno_coresight_set(adreno_dev, i);
-	}
+	// int i, adreno_dev_flag = -EINVAL;
+
+	// for (i = 0; i < GPU_CORESIGHT_MAX; ++i) {
+	// 	if (i == GPU_CORESIGHT_GX)
+	// 		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
+	// 	else if (i == GPU_CORESIGHT_CX)
+	// 		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
+	// 	else
+	// 		return;
+
+	// 	if (test_bit(adreno_dev_flag, &adreno_dev->priv))
+	// 		_adreno_coresight_set(adreno_dev, i);
+	// }
 }
 
 static int adreno_coresight_trace_id(struct coresight_device *csdev)
 {
-	struct kgsl_device *device = dev_get_drvdata(csdev->dev.parent);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(ADRENO_DEVICE(device));
-	int cs_id;
+	// struct kgsl_device *device = dev_get_drvdata(csdev->dev.parent);
+	// struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(ADRENO_DEVICE(device));
+	// int cs_id;
 
-	cs_id = adreno_coresight_identify(dev_name(&csdev->dev));
+	// cs_id = adreno_coresight_identify(dev_name(&csdev->dev));
 
-	if (cs_id < 0)
-		return -ENODEV;
+	// if (cs_id < 0)
+	// 	return -ENODEV;
 
-	return gpudev->coresight[cs_id]->atid;
+	// return gpudev->coresight[cs_id]->atid;
+	return -ENODEV;
 }
 
 static const struct coresight_ops_source adreno_coresight_source_ops = {
@@ -396,60 +397,60 @@ static const struct coresight_ops adreno_coresight_ops = {
 
 void adreno_coresight_remove(struct adreno_device *adreno_dev)
 {
-	int i, adreno_dev_flag = -EINVAL;
-
-	for (i = 0; i < GPU_CORESIGHT_MAX; ++i) {
-		if (i == GPU_CORESIGHT_GX)
-			adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
-		else if (i == GPU_CORESIGHT_CX)
-			adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
-		else
-			return;
-
-		if (test_bit(adreno_dev_flag, &adreno_dev->priv)) {
-			coresight_unregister(adreno_dev->csdev[i]);
-			adreno_dev->csdev[i] = NULL;
-		}
-	}
+	// int i, adreno_dev_flag = -EINVAL;
+
+	// for (i = 0; i < GPU_CORESIGHT_MAX; ++i) {
+	// 	if (i == GPU_CORESIGHT_GX)
+	// 		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT;
+	// 	else if (i == GPU_CORESIGHT_CX)
+	// 		adreno_dev_flag = ADRENO_DEVICE_CORESIGHT_CX;
+	// 	else
+	// 		return;
+
+	// 	if (test_bit(adreno_dev_flag, &adreno_dev->priv)) {
+	// 		coresight_unregister(adreno_dev->csdev[i]);
+	// 		adreno_dev->csdev[i] = NULL;
+	// 	}
+	// }
 }
 
 int adreno_coresight_init(struct adreno_device *adreno_dev)
 {
 	int ret = 0;
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct coresight_desc desc;
-	int i = 0;
-	struct device_node *node, *child;
-
-	node = of_find_compatible_node(device->pdev->dev.of_node,
-					NULL, "qcom,gpu-coresight");
-
-	for_each_child_of_node(node, child) {
-		memset(&desc, 0, sizeof(desc));
-		desc.pdata = of_get_coresight_platform_data(&device->pdev->dev,
-				child);
-		if (IS_ERR_OR_NULL(desc.pdata))
-			return (desc.pdata == NULL) ? -ENODEV :
-				PTR_ERR(desc.pdata);
-		if (gpudev->coresight[i] == NULL)
-			return -ENODEV;
-
-		desc.type = CORESIGHT_DEV_TYPE_SOURCE;
-		desc.subtype.source_subtype =
-			CORESIGHT_DEV_SUBTYPE_SOURCE_SOFTWARE;
-		desc.ops = &adreno_coresight_ops;
-		desc.dev = &device->pdev->dev;
-		desc.groups = gpudev->coresight[i]->groups;
-
-		adreno_dev->csdev[i] = coresight_register(&desc);
-		if (IS_ERR(adreno_dev->csdev[i]))
-			ret = PTR_ERR(adreno_dev->csdev[i]);
-		if (of_property_read_u32(child, "coresight-atid",
-			&gpudev->coresight[i]->atid))
-			return -EINVAL;
-		i++;
-	}
+	// struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	// struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	// struct coresight_desc desc;
+	// int i = 0;
+	// struct device_node *node, *child;
+
+	// node = of_find_compatible_node(device->pdev->dev.of_node,
+	// 				NULL, "qcom,gpu-coresight");
+
+	// for_each_child_of_node(node, child) {
+	// 	memset(&desc, 0, sizeof(desc));
+	// 	desc.pdata = of_get_coresight_platform_data(&device->pdev->dev,
+	// 			child);
+	// 	if (IS_ERR_OR_NULL(desc.pdata))
+	// 		return (desc.pdata == NULL) ? -ENODEV :
+	// 			PTR_ERR(desc.pdata);
+	// 	if (gpudev->coresight[i] == NULL)
+	// 		return -ENODEV;
+
+	// 	desc.type = CORESIGHT_DEV_TYPE_SOURCE;
+	// 	desc.subtype.source_subtype =
+	// 		CORESIGHT_DEV_SUBTYPE_SOURCE_SOFTWARE;
+	// 	desc.ops = &adreno_coresight_ops;
+	// 	desc.dev = &device->pdev->dev;
+	// 	desc.groups = gpudev->coresight[i]->groups;
+
+	// 	adreno_dev->csdev[i] = coresight_register(&desc);
+	// 	if (IS_ERR(adreno_dev->csdev[i]))
+	// 		ret = PTR_ERR(adreno_dev->csdev[i]);
+	// 	if (of_property_read_u32(child, "coresight-atid",
+	// 		&gpudev->coresight[i]->atid))
+	// 		return -EINVAL;
+	// 	i++;
+	// }
 
 	return ret;
 }
diff --git a/drivers/gpu/msm/adreno_cp_parser.c b/drivers/gpu/msm/adreno_cp_parser.c
index efc0868de394..2ccb8139e6b8 100644
--- a/drivers/gpu/msm/adreno_cp_parser.c
+++ b/drivers/gpu/msm/adreno_cp_parser.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -53,7 +53,7 @@ static int load_state_unit_sizes[7][2] = {
 static int adreno_ib_find_objs(struct kgsl_device *device,
 				struct kgsl_process_private *process,
 				uint64_t gpuaddr, uint64_t dwords,
-				uint64_t ib2base, int obj_type,
+				int obj_type,
 				struct adreno_ib_object_list *ib_obj_list,
 				int ib_level);
 
@@ -80,7 +80,6 @@ static void adreno_ib_merge_range(struct adreno_ib_object *ib_obj,
 {
 	uint64_t addr_end1 = ib_obj->gpuaddr + ib_obj->size;
 	uint64_t addr_end2 = gpuaddr + size;
-
 	if (gpuaddr < ib_obj->gpuaddr)
 		ib_obj->gpuaddr = gpuaddr;
 	if (addr_end2 > addr_end1)
@@ -138,7 +137,7 @@ static int adreno_ib_add(struct kgsl_process_private *process,
 	struct adreno_ib_object *ib_obj;
 	struct kgsl_mem_entry *entry;
 
-	if (ib_obj_list->num_objs >= MAX_IB_OBJS)
+	if (MAX_IB_OBJS <= ib_obj_list->num_objs)
 		return -E2BIG;
 
 	entry = kgsl_sharedmem_find(process, gpuaddr);
@@ -484,7 +483,7 @@ static int ib_parse_draw_indx(struct kgsl_device *device, unsigned int *pkt,
 		ret = adreno_ib_find_objs(device, process,
 			ib_parse_vars->set_draw_groups[i].cmd_stream_addr,
 			ib_parse_vars->set_draw_groups[i].cmd_stream_dwords,
-			0, SNAPSHOT_GPU_OBJECT_DRAW,
+			SNAPSHOT_GPU_OBJECT_DRAW,
 			ib_obj_list, 2);
 		if (ret)
 			break;
@@ -638,12 +637,12 @@ static int ib_parse_type0(struct kgsl_device *device, unsigned int *ptr,
 					ADRENO_CP_UCHE_INVALIDATE0)) ||
 				(offset == adreno_cp_parser_getreg(adreno_dev,
 					ADRENO_CP_UCHE_INVALIDATE1))) {
-				ret = adreno_ib_add(process,
-					ptr[i + 1] & 0xFFFFFFC0,
-					SNAPSHOT_GPU_OBJECT_GENERIC,
-					ib_obj_list);
-				if (ret)
-					break;
+					ret = adreno_ib_add(process,
+						ptr[i + 1] & 0xFFFFFFC0,
+						SNAPSHOT_GPU_OBJECT_GENERIC,
+						ib_obj_list);
+					if (ret)
+						break;
 			}
 		}
 	}
@@ -687,8 +686,8 @@ static int ib_parse_type7_set_draw_state(struct kgsl_device *device,
 			if (cmd_stream_dwords)
 				ret = adreno_ib_find_objs(device, process,
 					cmd_stream_addr, cmd_stream_dwords,
-					0, SNAPSHOT_GPU_OBJECT_DRAW,
-					ib_obj_list, 2);
+					SNAPSHOT_GPU_OBJECT_DRAW, ib_obj_list,
+					2);
 			if (ret)
 				break;
 			continue;
@@ -696,11 +695,10 @@ static int ib_parse_type7_set_draw_state(struct kgsl_device *device,
 		/* load immediate */
 		if (flags & 0x8) {
 			uint64_t gpuaddr = ptr[i + 2];
-
 			gpuaddr = gpuaddr << 32 | ptr[i + 1];
 			ret = adreno_ib_find_objs(device, process,
 				gpuaddr, (ptr[i] & 0x0000FFFF),
-				0, SNAPSHOT_GPU_OBJECT_IB,
+				SNAPSHOT_GPU_OBJECT_IB,
 				ib_obj_list, 2);
 			if (ret)
 				break;
@@ -735,7 +733,6 @@ static int ib_parse_set_draw_state(struct kgsl_device *device,
 		/* Disable all groups */
 		if (flags & 0x4) {
 			int j;
-
 			for (j = 0; j < NUM_SET_DRAW_GROUPS; j++)
 				ib_parse_vars->set_draw_groups[j].
 					cmd_stream_dwords = 0;
@@ -763,7 +760,7 @@ static int ib_parse_set_draw_state(struct kgsl_device *device,
 		if (flags & 0x8) {
 			ret = adreno_ib_find_objs(device, process,
 				ptr[i + 1], (ptr[i] & 0x0000FFFF),
-				0, SNAPSHOT_GPU_OBJECT_IB,
+				SNAPSHOT_GPU_OBJECT_IB,
 				ib_obj_list, 2);
 			if (ret)
 				break;
@@ -778,7 +775,6 @@ static int ib_parse_set_draw_state(struct kgsl_device *device,
  * @process: Process in which the IB is allocated
  * @gpuaddr: IB2 gpuaddr
  * @dwords: IB2 size in dwords
- * @ib2base: Base address of active IB2
  * @ib_obj_list: List of objects found in IB
  * @ib_level: The level from which function is called, either from IB1 or IB2
  *
@@ -787,37 +783,31 @@ static int ib_parse_set_draw_state(struct kgsl_device *device,
  */
 static int adreno_cp_parse_ib2(struct kgsl_device *device,
 			struct kgsl_process_private *process,
-			uint64_t gpuaddr, uint64_t dwords, uint64_t ib2base,
+			uint64_t gpuaddr, uint64_t dwords,
 			struct adreno_ib_object_list *ib_obj_list,
 			int ib_level)
 {
 	int i;
-
 	/*
 	 * We can only expect an IB2 in IB1, if we are
 	 * already processing an IB2 then return error
 	 */
-	if (ib_level == 2)
+	if (2 == ib_level)
 		return -EINVAL;
-
-	/* Save current IB2 statically */
-	if (ib2base == gpuaddr)
-		kgsl_snapshot_push_object(process, gpuaddr, dwords);
 	/*
 	 * only try to find sub objects iff this IB has
 	 * not been processed already
 	 */
 	for (i = 0; i < ib_obj_list->num_objs; i++) {
 		struct adreno_ib_object *ib_obj = &(ib_obj_list->obj_list[i]);
-
-		if ((ib_obj->snapshot_obj_type == SNAPSHOT_GPU_OBJECT_IB) &&
+		if ((SNAPSHOT_GPU_OBJECT_IB == ib_obj->snapshot_obj_type) &&
 			(gpuaddr >= ib_obj->gpuaddr) &&
 			(gpuaddr + dwords * sizeof(unsigned int) <=
 			ib_obj->gpuaddr + ib_obj->size))
 			return 0;
 	}
 
-	return adreno_ib_find_objs(device, process, gpuaddr, dwords, ib2base,
+	return adreno_ib_find_objs(device, process, gpuaddr, dwords,
 		SNAPSHOT_GPU_OBJECT_IB, ib_obj_list, 2);
 }
 
@@ -826,7 +816,6 @@ static int adreno_cp_parse_ib2(struct kgsl_device *device,
  * @device: The device pointer on which the IB executes
  * @process: The process in which the IB and all contained objects are mapped.
  * @gpuaddr: The gpu address of the IB
- * @ib2base: IB2 base address
  * @dwords: Size of ib in dwords
  * @obj_type: The object type can be either an IB or a draw state sequence
  * @ib_obj_list: The list in which the IB and the objects in it are added.
@@ -839,7 +828,7 @@ static int adreno_cp_parse_ib2(struct kgsl_device *device,
 static int adreno_ib_find_objs(struct kgsl_device *device,
 				struct kgsl_process_private *process,
 				uint64_t gpuaddr, uint64_t dwords,
-				uint64_t ib2base, int obj_type,
+				int obj_type,
 				struct adreno_ib_object_list *ib_obj_list,
 				int ib_level)
 {
@@ -915,7 +904,7 @@ static int adreno_ib_find_objs(struct kgsl_device *device,
 				uint64_t size = src[i + 2];
 
 				ret = adreno_cp_parse_ib2(device, process,
-						gpuaddrib2, size, ib2base,
+						gpuaddrib2, size,
 						ib_obj_list, ib_level);
 				if (ret)
 					goto done;
@@ -938,11 +927,10 @@ static int adreno_ib_find_objs(struct kgsl_device *device,
 			if (adreno_cmd_is_ib(adreno_dev, src[i])) {
 				uint64_t size = src[i + 3];
 				uint64_t gpuaddrib2 = src[i + 2];
-
 				gpuaddrib2 = gpuaddrib2 << 32 | src[i + 1];
 
 				ret = adreno_cp_parse_ib2(device, process,
-						gpuaddrib2, size, ib2base,
+						gpuaddrib2, size,
 						ib_obj_list, ib_level);
 				if (ret)
 					goto done;
@@ -994,7 +982,6 @@ static int adreno_ib_find_objs(struct kgsl_device *device,
  * @process: The process in which the IB and all contained objects are mapped
  * @gpuaddr: The gpu address of the IB
  * @dwords: Size of ib in dwords
- * @ib2base: Base address of active IB2
  * @ib_obj_list: The list in which the IB and the objects in it are added.
  *
  * Find all the memory objects that an IB needs for execution and place
@@ -1006,7 +993,7 @@ static int adreno_ib_find_objs(struct kgsl_device *device,
  */
 int adreno_ib_create_object_list(struct kgsl_device *device,
 		struct kgsl_process_private *process,
-		uint64_t gpuaddr, uint64_t dwords, uint64_t ib2base,
+		uint64_t gpuaddr, uint64_t dwords,
 		struct adreno_ib_object_list **out_ib_obj_list)
 {
 	int ret = 0;
@@ -1029,7 +1016,7 @@ int adreno_ib_create_object_list(struct kgsl_device *device,
 		return -ENOMEM;
 	}
 
-	ret = adreno_ib_find_objs(device, process, gpuaddr, dwords, ib2base,
+	ret = adreno_ib_find_objs(device, process, gpuaddr, dwords,
 		SNAPSHOT_GPU_OBJECT_IB, ib_obj_list, 1);
 
 	/* Even if there was an error return the remaining objects found */
diff --git a/drivers/gpu/msm/adreno_cp_parser.h b/drivers/gpu/msm/adreno_cp_parser.h
index 1fa46c147c3c..0248de2d600a 100644
--- a/drivers/gpu/msm/adreno_cp_parser.h
+++ b/drivers/gpu/msm/adreno_cp_parser.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2014, 2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -162,7 +162,6 @@ static inline int adreno_cp_parser_regindex(struct adreno_device *adreno_dev,
 {
 	int i;
 	const unsigned int *regs;
-
 	if (adreno_is_a4xx(adreno_dev))
 		regs = a4xx_cp_addr_regs;
 	else if (adreno_is_a3xx(adreno_dev))
@@ -179,7 +178,7 @@ static inline int adreno_cp_parser_regindex(struct adreno_device *adreno_dev,
 int adreno_ib_create_object_list(
 		struct kgsl_device *device,
 		struct kgsl_process_private *process,
-		uint64_t gpuaddr, uint64_t dwords, uint64_t ib2base,
+		uint64_t gpuaddr, uint64_t dwords,
 		struct adreno_ib_object_list **out_ib_obj_list);
 
 void adreno_ib_destroy_obj_list(struct adreno_ib_object_list *ib_obj_list);
diff --git a/drivers/gpu/msm/adreno_debugfs.c b/drivers/gpu/msm/adreno_debugfs.c
index 2a1d352a2f51..24159871c0ce 100644
--- a/drivers/gpu/msm/adreno_debugfs.c
+++ b/drivers/gpu/msm/adreno_debugfs.c
@@ -19,6 +19,7 @@
 
 #include "kgsl.h"
 #include "adreno.h"
+#include "kgsl_cffdump.h"
 #include "kgsl_sync.h"
 
 static int _isdb_set(void *data, u64 val)
@@ -128,19 +129,25 @@ typedef void (*reg_read_fill_t)(struct kgsl_device *device, int i,
 
 
 static void sync_event_print(struct seq_file *s,
-		struct kgsl_drawobj_sync_event *sync_event)
+		struct kgsl_cmdbatch_sync_event *sync_event)
 {
+	unsigned long flags;
+
 	switch (sync_event->type) {
 	case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP: {
 		seq_printf(s, "sync: ctx: %d ts: %d",
 				sync_event->context->id, sync_event->timestamp);
 		break;
 	}
-	case KGSL_CMD_SYNCPOINT_TYPE_FENCE: {
-		seq_printf(s, "sync: [%pK] %s", sync_event->handle,
-				sync_event->fence_name);
+	case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
+		spin_lock_irqsave(&sync_event->handle_lock, flags);
+
+		seq_printf(s, "sync: [%pk] %s", sync_event->handle,
+		(sync_event->handle && sync_event->handle->fence)
+				? sync_event->handle->fence->name : "NULL");
+
+		spin_unlock_irqrestore(&sync_event->handle_lock, flags);
 		break;
-	}
 	default:
 		seq_printf(s, "sync: type: %d", sync_event->type);
 		break;
@@ -152,12 +159,12 @@ struct flag_entry {
 	const char *str;
 };
 
-static const struct flag_entry drawobj_flags[] = {KGSL_DRAWOBJ_FLAGS};
+static const struct flag_entry cmdbatch_flags[] = {KGSL_CMDBATCH_FLAGS};
 
-static const struct flag_entry cmdobj_priv[] = {
-	{ CMDOBJ_SKIP, "skip"},
-	{ CMDOBJ_FORCE_PREAMBLE, "force_preamble"},
-	{ CMDOBJ_WFI, "wait_for_idle" },
+static const struct flag_entry cmdbatch_priv[] = {
+	{ CMDBATCH_FLAG_SKIP, "skip"},
+	{ CMDBATCH_FLAG_FORCE_PREAMBLE, "force_preamble"},
+	{ CMDBATCH_FLAG_WFI, "wait_for_idle" },
 };
 
 static const struct flag_entry context_flags[] = {KGSL_CONTEXT_FLAGS};
@@ -167,7 +174,6 @@ static const struct flag_entry context_flags[] = {KGSL_CONTEXT_FLAGS};
  * KGSL_CONTEXT_PRIV_DEVICE_SPECIFIC so it is ok to cross the streams here.
  */
 static const struct flag_entry context_priv[] = {
-	{ KGSL_CONTEXT_PRIV_SUBMITTED, "submitted"},
 	{ KGSL_CONTEXT_PRIV_DETACHED, "detached"},
 	{ KGSL_CONTEXT_PRIV_INVALID, "invalid"},
 	{ KGSL_CONTEXT_PRIV_PAGEFAULT, "pagefault"},
@@ -199,59 +205,44 @@ static void print_flags(struct seq_file *s, const struct flag_entry *table,
 		seq_puts(s, "None");
 }
 
-static void syncobj_print(struct seq_file *s,
-			struct kgsl_drawobj_sync *syncobj)
+static void cmdbatch_print(struct seq_file *s, struct kgsl_cmdbatch *cmdbatch)
 {
-	struct kgsl_drawobj_sync_event *event;
+	struct kgsl_cmdbatch_sync_event *event;
 	unsigned int i;
 
-	seq_puts(s, " syncobj ");
+	/* print fences first, since they block this cmdbatch */
 
-	for (i = 0; i < syncobj->numsyncs; i++) {
-		event = &syncobj->synclist[i];
+	for (i = 0; i < cmdbatch->numsyncs; i++) {
+		event = &cmdbatch->synclist[i];
 
-		if (!kgsl_drawobj_event_pending(syncobj, i))
+		if (!kgsl_cmdbatch_event_pending(cmdbatch, i))
 			continue;
 
+		/*
+		 * Timestamp is 0 for KGSL_CONTEXT_SYNC, but print it anyways
+		 * so that it is clear if the fence was a separate submit
+		 * or part of an IB submit.
+		 */
+		seq_printf(s, "\t%d ", cmdbatch->timestamp);
 		sync_event_print(s, event);
 		seq_puts(s, "\n");
 	}
-}
-
-static void cmdobj_print(struct seq_file *s,
-			struct kgsl_drawobj_cmd *cmdobj)
-{
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-
-	if (drawobj->type == CMDOBJ_TYPE)
-		seq_puts(s, " cmdobj ");
-	else
-		seq_puts(s, " markerobj ");
 
-	seq_printf(s, "\t %d ", drawobj->timestamp);
-
-	seq_puts(s, " priv: ");
-	print_flags(s, cmdobj_priv, ARRAY_SIZE(cmdobj_priv),
-				cmdobj->priv);
-}
-
-static void drawobj_print(struct seq_file *s,
-			struct kgsl_drawobj *drawobj)
-{
-	if (!kref_get_unless_zero(&drawobj->refcount))
+	/* if this flag is set, there won't be an IB */
+	if (cmdbatch->flags & KGSL_CONTEXT_SYNC)
 		return;
 
-	if (drawobj->type == SYNCOBJ_TYPE)
-		syncobj_print(s, SYNCOBJ(drawobj));
-	else if ((drawobj->type == CMDOBJ_TYPE) ||
-			(drawobj->type == MARKEROBJ_TYPE))
-		cmdobj_print(s, CMDOBJ(drawobj));
+	seq_printf(s, "\t%d: ib: expires: %lu",
+		cmdbatch->timestamp, cmdbatch->expires);
 
 	seq_puts(s, " flags: ");
-	print_flags(s, drawobj_flags, ARRAY_SIZE(drawobj_flags),
-		    drawobj->flags);
+	print_flags(s, cmdbatch_flags, ARRAY_SIZE(cmdbatch_flags),
+		    cmdbatch->flags);
+
+	seq_puts(s, " priv: ");
+	print_flags(s, cmdbatch_priv, ARRAY_SIZE(cmdbatch_priv),
+		    cmdbatch->priv);
 
-	kgsl_drawobj_put(drawobj);
 	seq_puts(s, "\n");
 }
 
@@ -301,13 +292,13 @@ static int ctx_print(struct seq_file *s, void *unused)
 		   queued, consumed, retired,
 		   drawctxt->internal_timestamp);
 
-	seq_puts(s, "drawqueue:\n");
+	seq_puts(s, "cmdqueue:\n");
 
 	spin_lock(&drawctxt->lock);
-	for (i = drawctxt->drawqueue_head;
-		i != drawctxt->drawqueue_tail;
-		i = DRAWQUEUE_NEXT(i, ADRENO_CONTEXT_DRAWQUEUE_SIZE))
-		drawobj_print(s, drawctxt->drawqueue[i]);
+	for (i = drawctxt->cmdqueue_head;
+		i != drawctxt->cmdqueue_tail;
+		i = CMDQUEUE_NEXT(i, ADRENO_CONTEXT_CMDQUEUE_SIZE))
+		cmdbatch_print(s, drawctxt->cmdqueue[i]);
 	spin_unlock(&drawctxt->lock);
 
 	seq_puts(s, "events:\n");
@@ -375,6 +366,8 @@ void adreno_debugfs_init(struct adreno_device *adreno_dev)
 	if (!device->d_debugfs || IS_ERR(device->d_debugfs))
 		return;
 
+	kgsl_cffdump_debugfs_create(device);
+
 	debugfs_create_file("active_cnt", 0444, device->d_debugfs, device,
 			    &_active_count_fops);
 	adreno_dev->ctx_d_debugfs = debugfs_create_dir("ctx",
diff --git a/drivers/gpu/msm/adreno_dispatch.c b/drivers/gpu/msm/adreno_dispatch.c
index c26ab8f412dd..3215ace84a7d 100644
--- a/drivers/gpu/msm/adreno_dispatch.c
+++ b/drivers/gpu/msm/adreno_dispatch.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2019, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -18,19 +18,20 @@
 #include <linux/err.h>
 
 #include "kgsl.h"
+#include "kgsl_cffdump.h"
 #include "kgsl_sharedmem.h"
 #include "adreno.h"
 #include "adreno_ringbuffer.h"
 #include "adreno_trace.h"
 #include "kgsl_sharedmem.h"
 
-#define DRAWQUEUE_NEXT(_i, _s) (((_i) + 1) % (_s))
+#define CMDQUEUE_NEXT(_i, _s) (((_i) + 1) % (_s))
 
 /* Time in ms after which the dispatcher tries to schedule an unscheduled RB */
-unsigned int adreno_dispatch_starvation_time = 2000;
+static unsigned int _dispatch_starvation_time = 2000;
 
 /* Amount of time in ms that a starved RB is permitted to execute for */
-unsigned int adreno_dispatch_time_slice = 25;
+static unsigned int _dispatch_time_slice = 25;
 
 /*
  * If set then dispatcher tries to schedule lower priority RB's after if they
@@ -42,13 +43,13 @@ unsigned int adreno_dispatch_time_slice = 25;
 unsigned int adreno_disp_preempt_fair_sched;
 
 /* Number of commands that can be queued in a context before it sleeps */
-static unsigned int _context_drawqueue_size = 50;
+static unsigned int _context_cmdqueue_size = 50;
 
 /* Number of milliseconds to wait for the context queue to clear */
 static unsigned int _context_queue_wait = 10000;
 
-/* Number of drawobjs sent at a time from a single context */
-static unsigned int _context_drawobj_burst = 5;
+/* Number of command batches sent at a time from a single context */
+static unsigned int _context_cmdbatch_burst = 5;
 
 /*
  * GFT throttle parameters. If GFT recovered more than
@@ -72,94 +73,64 @@ static unsigned int _dispatcher_q_inflight_hi = 15;
 static unsigned int _dispatcher_q_inflight_lo = 4;
 
 /* Command batch timeout (in milliseconds) */
-unsigned int adreno_drawobj_timeout = 2000;
+unsigned int adreno_cmdbatch_timeout = 2000;
 
 /* Interval for reading and comparing fault detection registers */
 static unsigned int _fault_timer_interval = 200;
 
-#define DRAWQUEUE_RB(_drawqueue) \
-	((struct adreno_ringbuffer *) \
-		container_of((_drawqueue),\
-		struct adreno_ringbuffer, dispatch_q))
-
-#define DRAWQUEUE(_ringbuffer) (&(_ringbuffer)->dispatch_q)
-
-static int adreno_dispatch_retire_drawqueue(struct adreno_device *adreno_dev,
-		struct adreno_dispatcher_drawqueue *drawqueue);
-
-static inline bool drawqueue_is_current(
-		struct adreno_dispatcher_drawqueue *drawqueue)
-{
-	struct adreno_ringbuffer *rb = DRAWQUEUE_RB(drawqueue);
-	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
-
-	return (adreno_dev->cur_rb == rb);
-}
-
-static void _add_context(struct adreno_device *adreno_dev,
-		struct adreno_context *drawctxt)
-{
-	/* Remove it from the list */
-	list_del_init(&drawctxt->active_node);
-
-	/* And push it to the front */
-	drawctxt->active_time = jiffies;
-	list_add(&drawctxt->active_node, &adreno_dev->active_list);
-}
-
-static int __count_context(struct adreno_context *drawctxt, void *data)
-{
-	unsigned long expires = drawctxt->active_time + msecs_to_jiffies(100);
-
-	return time_after(jiffies, expires) ? 0 : 1;
-}
-
-static int __count_drawqueue_context(struct adreno_context *drawctxt,
-				void *data)
-{
-	unsigned long expires = drawctxt->active_time + msecs_to_jiffies(100);
+/**
+ * _track_context - Add a context ID to the list of recently seen contexts
+ * for the command queue
+ * @cmdqueue: cmdqueue to add the context to
+ * @id: ID of the context to add
+ *
+ * This function is called when a new item is added to a context - this tracks
+ * the number of active contexts seen in the last 100ms for the command queue
+ */
+static void _track_context(struct adreno_dispatcher_cmdqueue *cmdqueue,
+		unsigned int id)
+{
+	struct adreno_context_list *list = cmdqueue->active_contexts;
+	int oldest = -1, empty = -1;
+	unsigned long age = 0;
+	int i, count = 0;
+	bool updated = false;
+
+	for (i = 0; i < ACTIVE_CONTEXT_LIST_MAX; i++) {
+
+		/* If the new ID matches the slot update the expire time */
+		if (list[i].id == id) {
+			list[i].jiffies = jiffies + msecs_to_jiffies(100);
+			updated = true;
+			count++;
+			continue;
+		}
 
-	if (time_after(jiffies, expires))
-		return 0;
+		/* Remember and skip empty slots */
+		if ((list[i].id == 0) ||
+			time_after(jiffies, list[i].jiffies)) {
+			empty = i;
+			continue;
+		}
 
-	return (&drawctxt->rb->dispatch_q ==
-			(struct adreno_dispatcher_drawqueue *) data) ? 1 : 0;
-}
+		count++;
 
-static int _adreno_count_active_contexts(struct adreno_device *adreno_dev,
-		int (*func)(struct adreno_context *, void *), void *data)
-{
-	struct adreno_context *ctxt;
-	int count = 0;
+		/* Remember the oldest active entry */
+		if (oldest == -1 || time_before(list[i].jiffies, age)) {
+			age = list[i].jiffies;
+			oldest = i;
+		}
+	}
 
-	list_for_each_entry(ctxt, &adreno_dev->active_list, active_node) {
-		if (func(ctxt, data) == 0)
-			return count;
+	if (updated == false) {
+		int pos = (empty != -1) ? empty : oldest;
 
+		list[pos].jiffies = jiffies + msecs_to_jiffies(100);
+		list[pos].id = id;
 		count++;
 	}
 
-	return count;
-}
-
-static void _track_context(struct adreno_device *adreno_dev,
-		struct adreno_dispatcher_drawqueue *drawqueue,
-		struct adreno_context *drawctxt)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	spin_lock(&adreno_dev->active_list_lock);
-
-	_add_context(adreno_dev, drawctxt);
-
-	device->active_context_count =
-			_adreno_count_active_contexts(adreno_dev,
-					__count_context, NULL);
-	drawqueue->active_context_count =
-			_adreno_count_active_contexts(adreno_dev,
-					__count_drawqueue_context, drawqueue);
-
-	spin_unlock(&adreno_dev->active_list_lock);
+	cmdqueue->active_context_count = count;
 }
 
 /*
@@ -170,9 +141,9 @@ static void _track_context(struct adreno_device *adreno_dev,
  */
 
 static inline int
-_drawqueue_inflight(struct adreno_dispatcher_drawqueue *drawqueue)
+_cmdqueue_inflight(struct adreno_dispatcher_cmdqueue *cmdqueue)
 {
-	return (drawqueue->active_context_count > 1)
+	return (cmdqueue->active_context_count > 1)
 		? _dispatcher_q_inflight_lo : _dispatcher_q_inflight_hi;
 }
 
@@ -185,7 +156,6 @@ static void fault_detect_read(struct adreno_device *adreno_dev)
 
 	for (i = 0; i < adreno_dev->num_ringbuffers; i++) {
 		struct adreno_ringbuffer *rb = &(adreno_dev->ringbuffers[i]);
-
 		adreno_rb_readtimestamp(adreno_dev, rb,
 			KGSL_TIMESTAMP_RETIRED, &(rb->fault_detect_ts));
 	}
@@ -208,9 +178,6 @@ static inline bool _isidle(struct adreno_device *adreno_dev)
 	if (!kgsl_state_is_awake(KGSL_DEVICE(adreno_dev)))
 		goto ret;
 
-	if (!adreno_rb_empty(adreno_dev->cur_rb))
-		return false;
-
 	/* only check rbbm status to determine if GPU is idle */
 	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_STATUS, &reg_rbbm_status);
 
@@ -240,9 +207,6 @@ static int fault_detect_read_compare(struct adreno_device *adreno_dev)
 	int i, ret = 0;
 	unsigned int ts;
 
-	if (!test_bit(ADRENO_DEVICE_SOFT_FAULT_DETECT, &adreno_dev->priv))
-		return 1;
-
 	/* Check to see if the device is idle - if so report no hang */
 	if (_isidle(adreno_dev) == true)
 		ret = 1;
@@ -279,19 +243,18 @@ static void start_fault_timer(struct adreno_device *adreno_dev)
 }
 
 /**
- * _retire_timestamp() - Retire object without sending it
- * to the hardware
- * @drawobj: Pointer to the object to retire
+ * _retire_marker() - Retire a marker command batch without sending it to the
+ * hardware
+ * @cmdbatch: Pointer to the cmdbatch to retire
  *
- * In some cases ibs can be retired by the software
- * without going to the GPU.  In those cases, update the
- * memstore from the CPU, kick off the event engine to handle
- * expired events and destroy the ib.
+ * In some cases marker commands can be retired by the software without going to
+ * the GPU.  In those cases, update the memstore from the CPU, kick off the
+ * event engine to handle expired events and destroy the command batch.
  */
-static void _retire_timestamp(struct kgsl_drawobj *drawobj)
+static void _retire_marker(struct kgsl_cmdbatch *cmdbatch)
 {
-	struct kgsl_context *context = drawobj->context;
-	struct adreno_context *drawctxt = ADRENO_CONTEXT(context);
+	struct kgsl_context *context = cmdbatch->context;
+	struct adreno_context *drawctxt = ADRENO_CONTEXT(cmdbatch->context);
 	struct kgsl_device *device = context->device;
 
 	/*
@@ -300,28 +263,18 @@ static void _retire_timestamp(struct kgsl_drawobj *drawobj)
 	 */
 	kgsl_sharedmem_writel(device, &device->memstore,
 		KGSL_MEMSTORE_OFFSET(context->id, soptimestamp),
-		drawobj->timestamp);
+		cmdbatch->timestamp);
 
 	kgsl_sharedmem_writel(device, &device->memstore,
 		KGSL_MEMSTORE_OFFSET(context->id, eoptimestamp),
-		drawobj->timestamp);
+		cmdbatch->timestamp);
 
 
 	/* Retire pending GPU events for the object */
 	kgsl_process_event_group(device, &context->events);
 
-	/*
-	 * For A3xx we still get the rptr from the CP_RB_RPTR instead of
-	 * rptr scratch out address. At this point GPU clocks turned off.
-	 * So avoid reading GPU register directly for A3xx.
-	 */
-	if (adreno_is_a3xx(ADRENO_DEVICE(device)))
-		trace_adreno_cmdbatch_retired(drawobj, -1, 0, 0, drawctxt->rb,
-				0, 0);
-	else
-		trace_adreno_cmdbatch_retired(drawobj, -1, 0, 0, drawctxt->rb,
-			adreno_get_rptr(drawctxt->rb), 0);
-	kgsl_drawobj_destroy(drawobj);
+	trace_adreno_cmdbatch_retired(cmdbatch, -1, 0, 0, drawctxt->rb);
+	kgsl_cmdbatch_destroy(cmdbatch);
 }
 
 static int _check_context_queue(struct adreno_context *drawctxt)
@@ -338,7 +291,7 @@ static int _check_context_queue(struct adreno_context *drawctxt)
 	if (kgsl_context_invalid(&drawctxt->base))
 		ret = 1;
 	else
-		ret = drawctxt->queued < _context_drawqueue_size ? 1 : 0;
+		ret = drawctxt->queued < _context_cmdqueue_size ? 1 : 0;
 
 	spin_unlock(&drawctxt->lock);
 
@@ -349,161 +302,176 @@ static int _check_context_queue(struct adreno_context *drawctxt)
  * return true if this is a marker command and the dependent timestamp has
  * retired
  */
-static bool _marker_expired(struct kgsl_drawobj_cmd *markerobj)
+static bool _marker_expired(struct kgsl_cmdbatch *cmdbatch)
 {
-	struct kgsl_drawobj *drawobj = DRAWOBJ(markerobj);
-
-	return (drawobj->flags & KGSL_DRAWOBJ_MARKER) &&
-		kgsl_check_timestamp(drawobj->device, drawobj->context,
-			markerobj->marker_timestamp);
+	return (cmdbatch->flags & KGSL_CMDBATCH_MARKER) &&
+		kgsl_check_timestamp(cmdbatch->device, cmdbatch->context,
+			cmdbatch->marker_timestamp);
 }
 
-static inline void _pop_drawobj(struct adreno_context *drawctxt)
+static inline void _pop_cmdbatch(struct adreno_context *drawctxt)
 {
-	drawctxt->drawqueue_head = DRAWQUEUE_NEXT(drawctxt->drawqueue_head,
-		ADRENO_CONTEXT_DRAWQUEUE_SIZE);
+	drawctxt->cmdqueue_head = CMDQUEUE_NEXT(drawctxt->cmdqueue_head,
+		ADRENO_CONTEXT_CMDQUEUE_SIZE);
 	drawctxt->queued--;
 }
+/**
+ * Removes all expired marker and sync cmdbatches from
+ * the context queue when marker command and dependent
+ * timestamp are retired. This function is recursive.
+ * returns cmdbatch if context has command, NULL otherwise.
+ */
+static struct kgsl_cmdbatch *_expire_markers(struct adreno_context *drawctxt)
+{
+	struct kgsl_cmdbatch *cmdbatch;
 
-static void _retire_sparseobj(struct kgsl_drawobj_sparse *sparseobj,
-				struct adreno_context *drawctxt)
+	if (drawctxt->cmdqueue_head == drawctxt->cmdqueue_tail)
+		return NULL;
+
+	cmdbatch = drawctxt->cmdqueue[drawctxt->cmdqueue_head];
+
+	if (cmdbatch == NULL)
+		return NULL;
+
+	/* Check to see if this is a marker we can skip over */
+	if ((cmdbatch->flags & KGSL_CMDBATCH_MARKER) &&
+			_marker_expired(cmdbatch)) {
+		_pop_cmdbatch(drawctxt);
+		_retire_marker(cmdbatch);
+		return _expire_markers(drawctxt);
+	}
+
+	if (cmdbatch->flags & KGSL_CMDBATCH_SYNC) {
+		if (!kgsl_cmdbatch_events_pending(cmdbatch)) {
+			_pop_cmdbatch(drawctxt);
+			kgsl_cmdbatch_destroy(cmdbatch);
+			return _expire_markers(drawctxt);
+		}
+	}
+
+	return cmdbatch;
+}
+
+static void expire_markers(struct adreno_context *drawctxt)
 {
-	kgsl_sparse_bind(drawctxt->base.proc_priv, sparseobj);
-	_retire_timestamp(DRAWOBJ(sparseobj));
+	spin_lock(&drawctxt->lock);
+	_expire_markers(drawctxt);
+	spin_unlock(&drawctxt->lock);
 }
 
-static int _retire_markerobj(struct kgsl_drawobj_cmd *cmdobj,
-				struct adreno_context *drawctxt)
+static struct kgsl_cmdbatch *_get_cmdbatch(struct adreno_context *drawctxt)
 {
-	if (_marker_expired(cmdobj)) {
-		_pop_drawobj(drawctxt);
-		_retire_timestamp(DRAWOBJ(cmdobj));
-		return 0;
-	}
+	struct kgsl_cmdbatch *cmdbatch;
+	bool pending = false;
+
+	cmdbatch = _expire_markers(drawctxt);
+
+	if (cmdbatch == NULL)
+		return NULL;
 
 	/*
-	 * If the marker isn't expired but the SKIP bit
-	 * is set then there are real commands following
-	 * this one in the queue. This means that we
-	 * need to dispatch the command so that we can
-	 * keep the timestamp accounting correct. If
-	 * skip isn't set then we block this queue
+	 * If the marker isn't expired but the SKIP bit is set
+	 * then there are real commands following this one in
+	 * the queue.  This means that we need to dispatch the
+	 * command so that we can keep the timestamp accounting
+	 * correct.  If skip isn't set then we block this queue
 	 * until the dependent timestamp expires
 	 */
-	return test_bit(CMDOBJ_SKIP, &cmdobj->priv) ? 1 : -EAGAIN;
-}
+	if ((cmdbatch->flags & KGSL_CMDBATCH_MARKER) &&
+			(!test_bit(CMDBATCH_FLAG_SKIP, &cmdbatch->priv)))
+		pending = true;
 
-static int _retire_syncobj(struct kgsl_drawobj_sync *syncobj,
-				struct adreno_context *drawctxt)
-{
-	if (!kgsl_drawobj_events_pending(syncobj)) {
-		_pop_drawobj(drawctxt);
-		kgsl_drawobj_destroy(DRAWOBJ(syncobj));
-		return 0;
-	}
+	if (kgsl_cmdbatch_events_pending(cmdbatch))
+		pending = true;
 
 	/*
-	 * If we got here, there are pending events for sync object.
-	 * Start the canary timer if it hasnt been started already.
+	 * If changes are pending and the canary timer hasn't been
+	 * started yet, start it
 	 */
-	if (!syncobj->timeout_jiffies) {
-		syncobj->timeout_jiffies = jiffies + msecs_to_jiffies(5000);
-			mod_timer(&syncobj->timer, syncobj->timeout_jiffies);
+	if (pending) {
+		/*
+		 * If syncpoints are pending start the canary timer if
+		 * it hasn't already been started
+		 */
+		if (!cmdbatch->timeout_jiffies) {
+			cmdbatch->timeout_jiffies =
+				jiffies + msecs_to_jiffies(5000);
+			mod_timer(&cmdbatch->timer, cmdbatch->timeout_jiffies);
+		}
+
+		return ERR_PTR(-EAGAIN);
 	}
 
-	return -EAGAIN;
+	_pop_cmdbatch(drawctxt);
+	return cmdbatch;
 }
 
-/*
- * Retires all expired marker and sync objs from the context
- * queue and returns one of the below
- * a) next drawobj that needs to be sent to ringbuffer
- * b) -EAGAIN for syncobj with syncpoints pending.
- * c) -EAGAIN for markerobj whose marker timestamp has not expired yet.
- * c) NULL for no commands remaining in drawqueue.
+/**
+ * adreno_dispatcher_get_cmdbatch() - Get a new command from a context queue
+ * @drawctxt: Pointer to the adreno draw context
+ *
+ * Dequeue a new command batch from the context list
  */
-static struct kgsl_drawobj *_process_drawqueue_get_next_drawobj(
-				struct adreno_context *drawctxt)
+static struct kgsl_cmdbatch *adreno_dispatcher_get_cmdbatch(
+		struct adreno_context *drawctxt)
 {
-	struct kgsl_drawobj *drawobj;
-	unsigned int i = drawctxt->drawqueue_head;
-	int ret = 0;
-
-	if (drawctxt->drawqueue_head == drawctxt->drawqueue_tail)
-		return NULL;
-
-	for (i = drawctxt->drawqueue_head; i != drawctxt->drawqueue_tail;
-			i = DRAWQUEUE_NEXT(i, ADRENO_CONTEXT_DRAWQUEUE_SIZE)) {
-
-		drawobj = drawctxt->drawqueue[i];
-
-		if (drawobj == NULL)
-			return NULL;
-
-		if (drawobj->type == CMDOBJ_TYPE)
-			return drawobj;
-		else if (drawobj->type == MARKEROBJ_TYPE) {
-			ret = _retire_markerobj(CMDOBJ(drawobj), drawctxt);
-			/* Special case where marker needs to be sent to GPU */
-			if (ret == 1)
-				return drawobj;
-		} else if (drawobj->type == SYNCOBJ_TYPE)
-			ret = _retire_syncobj(SYNCOBJ(drawobj), drawctxt);
-		else
-			return ERR_PTR(-EINVAL);
+	struct kgsl_cmdbatch *cmdbatch;
 
-		if (ret == -EAGAIN)
-			return ERR_PTR(-EAGAIN);
+	spin_lock(&drawctxt->lock);
+	cmdbatch = _get_cmdbatch(drawctxt);
+	spin_unlock(&drawctxt->lock);
 
-		continue;
-	}
+	/*
+	 * Delete the timer and wait for timer handler to finish executing
+	 * on another core before queueing the buffer. We must do this
+	 * without holding any spin lock that the timer handler might be using
+	 */
+	if (!IS_ERR_OR_NULL(cmdbatch))
+		del_timer_sync(&cmdbatch->timer);
 
-	return NULL;
+	return cmdbatch;
 }
 
 /**
- * adreno_dispatcher_requeue_cmdobj() - Put a command back on the context
+ * adreno_dispatcher_requeue_cmdbatch() - Put a command back on the context
  * queue
  * @drawctxt: Pointer to the adreno draw context
- * @cmdobj: Pointer to the KGSL command object to requeue
+ * @cmdbatch: Pointer to the KGSL cmdbatch to requeue
  *
  * Failure to submit a command to the ringbuffer isn't the fault of the command
  * being submitted so if a failure happens, push it back on the head of the the
  * context queue to be reconsidered again unless the context got detached.
  */
-static inline int adreno_dispatcher_requeue_cmdobj(
-		struct adreno_context *drawctxt,
-		struct kgsl_drawobj_cmd *cmdobj)
+static inline int adreno_dispatcher_requeue_cmdbatch(
+		struct adreno_context *drawctxt, struct kgsl_cmdbatch *cmdbatch)
 {
 	unsigned int prev;
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-
 	spin_lock(&drawctxt->lock);
 
 	if (kgsl_context_detached(&drawctxt->base) ||
 		kgsl_context_invalid(&drawctxt->base)) {
 		spin_unlock(&drawctxt->lock);
-		/* get rid of this drawobj since the context is bad */
-		kgsl_drawobj_destroy(drawobj);
+		/* get rid of this cmdbatch since the context is bad */
+		kgsl_cmdbatch_destroy(cmdbatch);
 		return -ENOENT;
 	}
 
-	prev = drawctxt->drawqueue_head == 0 ?
-		(ADRENO_CONTEXT_DRAWQUEUE_SIZE - 1) :
-		(drawctxt->drawqueue_head - 1);
+	prev = drawctxt->cmdqueue_head == 0 ?
+		(ADRENO_CONTEXT_CMDQUEUE_SIZE - 1) :
+		(drawctxt->cmdqueue_head - 1);
 
 	/*
 	 * The maximum queue size always needs to be one less then the size of
-	 * the ringbuffer queue so there is "room" to put the drawobj back in
+	 * the ringbuffer queue so there is "room" to put the cmdbatch back in
 	 */
 
-	WARN_ON(prev == drawctxt->drawqueue_tail);
+	BUG_ON(prev == drawctxt->cmdqueue_tail);
 
-	drawctxt->drawqueue[prev] = drawobj;
+	drawctxt->cmdqueue[prev] = cmdbatch;
 	drawctxt->queued++;
 
 	/* Reset the command queue head to reflect the newly requeued change */
-	drawctxt->drawqueue_head = prev;
+	drawctxt->cmdqueue_head = prev;
 	spin_unlock(&drawctxt->lock);
 	return 0;
 }
@@ -538,22 +506,21 @@ static void  dispatcher_queue_context(struct adreno_device *adreno_dev,
 }
 
 /**
- * sendcmd() - Send a drawobj to the GPU hardware
+ * sendcmd() - Send a command batch to the GPU hardware
  * @dispatcher: Pointer to the adreno dispatcher struct
- * @drawobj: Pointer to the KGSL drawobj being sent
+ * @cmdbatch: Pointer to the KGSL cmdbatch being sent
  *
- * Send a KGSL drawobj to the GPU hardware
+ * Send a KGSL command batch to the GPU hardware
  */
 static int sendcmd(struct adreno_device *adreno_dev,
-	struct kgsl_drawobj_cmd *cmdobj)
+	struct kgsl_cmdbatch *cmdbatch)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
-	struct adreno_context *drawctxt = ADRENO_CONTEXT(drawobj->context);
-	struct adreno_dispatcher_drawqueue *dispatch_q =
-				ADRENO_DRAWOBJ_DISPATCH_DRAWQUEUE(drawobj);
+	struct adreno_context *drawctxt = ADRENO_CONTEXT(cmdbatch->context);
+	struct adreno_dispatcher_cmdqueue *dispatch_q =
+				ADRENO_CMDBATCH_DISPATCH_CMDQUEUE(cmdbatch);
 	struct adreno_submit_time time;
 	uint64_t secs = 0;
 	unsigned long nsecs = 0;
@@ -565,8 +532,6 @@ static int sendcmd(struct adreno_device *adreno_dev,
 		return -EBUSY;
 	}
 
-	memset(&time, 0x0, sizeof(time));
-
 	dispatcher->inflight++;
 	dispatch_q->inflight++;
 
@@ -584,15 +549,15 @@ static int sendcmd(struct adreno_device *adreno_dev,
 		set_bit(ADRENO_DISPATCHER_POWER, &dispatcher->priv);
 	}
 
-	if (test_bit(ADRENO_DEVICE_DRAWOBJ_PROFILE, &adreno_dev->priv)) {
-		set_bit(CMDOBJ_PROFILE, &cmdobj->priv);
-		cmdobj->profile_index = adreno_dev->profile_index;
-		adreno_dev->profile_index =
-			(adreno_dev->profile_index + 1) %
-			ADRENO_DRAWOBJ_PROFILE_COUNT;
+	if (test_bit(ADRENO_DEVICE_CMDBATCH_PROFILE, &adreno_dev->priv)) {
+		set_bit(CMDBATCH_FLAG_PROFILE, &cmdbatch->priv);
+		cmdbatch->profile_index = adreno_dev->cmdbatch_profile_index;
+		adreno_dev->cmdbatch_profile_index =
+			(adreno_dev->cmdbatch_profile_index + 1) %
+			ADRENO_CMDBATCH_PROFILE_COUNT;
 	}
 
-	ret = adreno_ringbuffer_submitcmd(adreno_dev, cmdobj, &time);
+	ret = adreno_ringbuffer_submitcmd(adreno_dev, cmdbatch, &time);
 
 	/*
 	 * On the first command, if the submission was successful, then read the
@@ -601,51 +566,29 @@ static int sendcmd(struct adreno_device *adreno_dev,
 
 	if (dispatcher->inflight == 1) {
 		if (ret == 0) {
-
-			/* Stop fault timer before reading fault registers */
-			del_timer_sync(&dispatcher->fault_timer);
-
 			fault_detect_read(adreno_dev);
 
-			/* Start the fault timer on first submission */
-			start_fault_timer(adreno_dev);
-
 			if (!test_and_set_bit(ADRENO_DISPATCHER_ACTIVE,
 				&dispatcher->priv))
 				reinit_completion(&dispatcher->idle_gate);
-
-			/*
-			 * We update power stats generally at the expire of
-			 * cmdbatch. In cases where the cmdbatch takes a long
-			 * time to finish, it will delay power stats update,
-			 * in effect it will delay DCVS decision. Start a
-			 * timer to update power state on expire of this timer.
-			 */
-			kgsl_pwrscale_midframe_timer_restart(device);
-
 		} else {
 			kgsl_active_count_put(device);
 			clear_bit(ADRENO_DISPATCHER_POWER, &dispatcher->priv);
 		}
 	}
 
+	mutex_unlock(&device->mutex);
 
 	if (ret) {
 		dispatcher->inflight--;
 		dispatch_q->inflight--;
 
-		mutex_unlock(&device->mutex);
-
 		/*
-		 * Don't log a message in case of:
 		 * -ENOENT means that the context was detached before the
-		 * command was submitted
-		 * -ENOSPC means that there temporarily isn't any room in the
-		 *  ringbuffer
-		 *  -PROTO means that a fault is currently being worked
+		 *  command was submitted - don't log a message in that case
 		 */
 
-		if (ret != -ENOENT && ret != -ENOSPC && ret != -EPROTO)
+		if (ret != -ENOENT)
 			KGSL_DRV_ERR(device,
 				"Unable to submit command to the ringbuffer %d\n",
 				ret);
@@ -655,39 +598,38 @@ static int sendcmd(struct adreno_device *adreno_dev,
 	secs = time.ktime;
 	nsecs = do_div(secs, 1000000000);
 
-	trace_adreno_cmdbatch_submitted(drawobj, (int) dispatcher->inflight,
-		time.ticks, (unsigned long) secs, nsecs / 1000, drawctxt->rb,
-		adreno_get_rptr(drawctxt->rb));
-
-	mutex_unlock(&device->mutex);
+	trace_adreno_cmdbatch_submitted(cmdbatch, (int) dispatcher->inflight,
+		time.ticks, (unsigned long) secs, nsecs / 1000, drawctxt->rb);
 
-	cmdobj->submit_ticks = time.ticks;
+	cmdbatch->submit_ticks = time.ticks;
 
-	dispatch_q->cmd_q[dispatch_q->tail] = cmdobj;
+	dispatch_q->cmd_q[dispatch_q->tail] = cmdbatch;
 	dispatch_q->tail = (dispatch_q->tail + 1) %
-		ADRENO_DISPATCH_DRAWQUEUE_SIZE;
-
-	/*
-	 * For the first submission in any given command queue update the
-	 * expected expire time - this won't actually be used / updated until
-	 * the command queue in question goes current, but universally setting
-	 * it here avoids the possibilty of some race conditions with preempt
-	 */
-
-	if (dispatch_q->inflight == 1)
-		dispatch_q->expires = jiffies +
-			msecs_to_jiffies(adreno_drawobj_timeout);
+		ADRENO_DISPATCH_CMDQUEUE_SIZE;
 
 	/*
-	 * If we believe ourselves to be current and preemption isn't a thing,
-	 * then set up the timer.  If this misses, then preemption is indeed a
-	 * thing and the timer will be set up in due time
+	 * If this is the first command in the pipe then the GPU will
+	 * immediately start executing it so we can start the expiry timeout on
+	 * the command batch here.  Subsequent command batches will have their
+	 * timer started when the previous command batch is retired.
+	 * Set the timer if the cmdbatch was submitted to current
+	 * active RB else this timer will need to be set when the
+	 * RB becomes active, also if dispatcher is not is CLEAR
+	 * state then the cmdbatch it is currently executing is
+	 * unclear so do not set timer in that case either.
 	 */
-	if (adreno_in_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE)) {
-		if (drawqueue_is_current(dispatch_q))
-			mod_timer(&dispatcher->timer, dispatch_q->expires);
+	if (1 == dispatch_q->inflight &&
+		(&(adreno_dev->cur_rb->dispatch_q)) == dispatch_q &&
+		adreno_preempt_state(adreno_dev,
+			ADRENO_DISPATCHER_PREEMPT_CLEAR)) {
+		cmdbatch->expires = jiffies +
+			msecs_to_jiffies(adreno_cmdbatch_timeout);
+		mod_timer(&dispatcher->timer, cmdbatch->expires);
 	}
 
+	/* Start the fault detection timer on the first submission */
+	if (dispatcher->inflight == 1)
+		start_fault_timer(adreno_dev);
 
 	/*
 	 * we just submitted something, readjust ringbuffer
@@ -698,76 +640,6 @@ static int sendcmd(struct adreno_device *adreno_dev,
 	return 0;
 }
 
-
-/*
- * Retires all sync objs from the sparse context
- * queue and returns one of the below
- * a) next sparseobj
- * b) -EAGAIN for syncobj with syncpoints pending
- * c) -EINVAL for unexpected drawobj
- * d) NULL for no sparseobj
- */
-static struct kgsl_drawobj_sparse *_get_next_sparseobj(
-				struct adreno_context *drawctxt)
-{
-	struct kgsl_drawobj *drawobj;
-	unsigned int i = drawctxt->drawqueue_head;
-	int ret = 0;
-
-	if (drawctxt->drawqueue_head == drawctxt->drawqueue_tail)
-		return NULL;
-
-	for (i = drawctxt->drawqueue_head; i != drawctxt->drawqueue_tail;
-			i = DRAWQUEUE_NEXT(i, ADRENO_CONTEXT_DRAWQUEUE_SIZE)) {
-
-		drawobj = drawctxt->drawqueue[i];
-
-		if (drawobj == NULL)
-			return NULL;
-
-		if (drawobj->type == SYNCOBJ_TYPE)
-			ret = _retire_syncobj(SYNCOBJ(drawobj), drawctxt);
-		else if (drawobj->type == SPARSEOBJ_TYPE)
-			return SPARSEOBJ(drawobj);
-		else
-			return ERR_PTR(-EINVAL);
-
-		if (ret == -EAGAIN)
-			return ERR_PTR(-EAGAIN);
-
-		continue;
-	}
-
-	return NULL;
-}
-
-static int _process_drawqueue_sparse(
-		struct adreno_context *drawctxt)
-{
-	struct kgsl_drawobj_sparse *sparseobj;
-	int ret = 0;
-	unsigned int i;
-
-	for (i = 0; i < ADRENO_CONTEXT_DRAWQUEUE_SIZE; i++) {
-
-		spin_lock(&drawctxt->lock);
-		sparseobj = _get_next_sparseobj(drawctxt);
-		if (IS_ERR_OR_NULL(sparseobj)) {
-			if (IS_ERR(sparseobj))
-				ret = PTR_ERR(sparseobj);
-			spin_unlock(&drawctxt->lock);
-			return ret;
-		}
-
-		_pop_drawobj(drawctxt);
-		spin_unlock(&drawctxt->lock);
-
-		_retire_sparseobj(sparseobj, drawctxt);
-	}
-
-	return 0;
-}
-
 /**
  * dispatcher_context_sendcmds() - Send commands from a context to the GPU
  * @adreno_dev: Pointer to the adreno device struct
@@ -780,73 +652,75 @@ static int _process_drawqueue_sparse(
 static int dispatcher_context_sendcmds(struct adreno_device *adreno_dev,
 		struct adreno_context *drawctxt)
 {
-	struct adreno_dispatcher_drawqueue *dispatch_q =
+	struct adreno_dispatcher_cmdqueue *dispatch_q =
 					&(drawctxt->rb->dispatch_q);
 	int count = 0;
 	int ret = 0;
-	int inflight = _drawqueue_inflight(dispatch_q);
+	int inflight = _cmdqueue_inflight(dispatch_q);
 	unsigned int timestamp;
 
-	if (drawctxt->base.flags & KGSL_CONTEXT_SPARSE)
-		return _process_drawqueue_sparse(drawctxt);
-
 	if (dispatch_q->inflight >= inflight) {
-		spin_lock(&drawctxt->lock);
-		_process_drawqueue_get_next_drawobj(drawctxt);
-		spin_unlock(&drawctxt->lock);
+		expire_markers(drawctxt);
 		return -EBUSY;
 	}
 
 	/*
-	 * Each context can send a specific number of drawobjs per cycle
+	 * Each context can send a specific number of command batches per cycle
 	 */
-	while ((count < _context_drawobj_burst) &&
+	while ((count < _context_cmdbatch_burst) &&
 		(dispatch_q->inflight < inflight)) {
-		struct kgsl_drawobj *drawobj;
-		struct kgsl_drawobj_cmd *cmdobj;
+		struct kgsl_cmdbatch *cmdbatch;
 
 		if (adreno_gpu_fault(adreno_dev) != 0)
 			break;
 
-		spin_lock(&drawctxt->lock);
-		drawobj = _process_drawqueue_get_next_drawobj(drawctxt);
+		cmdbatch = adreno_dispatcher_get_cmdbatch(drawctxt);
 
 		/*
-		 * adreno_context_get_drawobj returns -EAGAIN if the current
-		 * drawobj has pending sync points so no more to do here.
+		 * adreno_context_get_cmdbatch returns -EAGAIN if the current
+		 * cmdbatch has pending sync points so no more to do here.
 		 * When the sync points are satisfied then the context will get
 		 * reqeueued
 		 */
 
-		if (IS_ERR_OR_NULL(drawobj)) {
-			if (IS_ERR(drawobj))
-				ret = PTR_ERR(drawobj);
-			spin_unlock(&drawctxt->lock);
+		if (IS_ERR_OR_NULL(cmdbatch)) {
+			if (IS_ERR(cmdbatch))
+				ret = PTR_ERR(cmdbatch);
 			break;
 		}
-		_pop_drawobj(drawctxt);
-		spin_unlock(&drawctxt->lock);
 
-		timestamp = drawobj->timestamp;
-		cmdobj = CMDOBJ(drawobj);
-		ret = sendcmd(adreno_dev, cmdobj);
+		/*
+		 * If this is a synchronization submission then there are no
+		 * commands to submit.  Discard it and get the next item from
+		 * the queue.  Decrement count so this packet doesn't count
+		 * against the burst for the context
+		 */
+
+		if (cmdbatch->flags & KGSL_CMDBATCH_SYNC) {
+			kgsl_cmdbatch_destroy(cmdbatch);
+			continue;
+		}
+
+		timestamp = cmdbatch->timestamp;
+
+		ret = sendcmd(adreno_dev, cmdbatch);
 
 		/*
-		 * On error from sendcmd() try to requeue the cmdobj
+		 * On error from sendcmd() try to requeue the command batch
 		 * unless we got back -ENOENT which means that the context has
 		 * been detached and there will be no more deliveries from here
 		 */
 		if (ret != 0) {
-			/* Destroy the cmdobj on -ENOENT */
+			/* Destroy the cmdbatch on -ENOENT */
 			if (ret == -ENOENT)
-				kgsl_drawobj_destroy(drawobj);
+				kgsl_cmdbatch_destroy(cmdbatch);
 			else {
 				/*
 				 * If the requeue returns an error, return that
 				 * instead of whatever sendcmd() sent us
 				 */
-				int r = adreno_dispatcher_requeue_cmdobj(
-					drawctxt, cmdobj);
+				int r = adreno_dispatcher_requeue_cmdbatch(
+					drawctxt, cmdbatch);
 				if (r)
 					ret = r;
 			}
@@ -902,7 +776,7 @@ static void _adreno_dispatcher_issuecmds(struct adreno_device *adreno_dev)
 		if (adreno_gpu_fault(adreno_dev) != 0)
 			break;
 
-		if (adreno_gpu_halt(adreno_dev) != 0)
+		if (0 != adreno_gpu_halt(adreno_dev))
 			break;
 
 		spin_lock(&dispatcher->plist_lock);
@@ -985,13 +859,6 @@ static void _adreno_dispatcher_issuecmds(struct adreno_device *adreno_dev)
 	spin_unlock(&dispatcher->plist_lock);
 }
 
-static inline void _decrement_submit_now(struct kgsl_device *device)
-{
-	spin_lock(&device->submit_lock);
-	device->submit_now--;
-	spin_unlock(&device->submit_lock);
-}
-
 /**
  * adreno_dispatcher_issuecmds() - Issue commmands from pending contexts
  * @adreno_dev: Pointer to the adreno device struct
@@ -1001,122 +868,194 @@ static inline void _decrement_submit_now(struct kgsl_device *device)
 static void adreno_dispatcher_issuecmds(struct adreno_device *adreno_dev)
 {
 	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-
-	spin_lock(&device->submit_lock);
-	/* If state transition to SLUMBER, schedule the work for later */
-	if (device->slumber == true) {
-		spin_unlock(&device->submit_lock);
-		goto done;
-	}
-	device->submit_now++;
-	spin_unlock(&device->submit_lock);
 
 	/* If the dispatcher is busy then schedule the work for later */
 	if (!mutex_trylock(&dispatcher->mutex)) {
-		_decrement_submit_now(device);
-		goto done;
+		adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
+		return;
 	}
 
 	_adreno_dispatcher_issuecmds(adreno_dev);
 	mutex_unlock(&dispatcher->mutex);
-	_decrement_submit_now(device);
-	return;
-done:
-	adreno_dispatcher_schedule(device);
 }
 
 /**
  * get_timestamp() - Return the next timestamp for the context
  * @drawctxt - Pointer to an adreno draw context struct
- * @drawobj - Pointer to a drawobj
+ * @cmdbatch - Pointer to a command batch
  * @timestamp - Pointer to a timestamp value possibly passed from the user
- * @user_ts - user generated timestamp
  *
  * Assign a timestamp based on the settings of the draw context and the command
  * batch.
  */
 static int get_timestamp(struct adreno_context *drawctxt,
-		struct kgsl_drawobj *drawobj, unsigned int *timestamp,
-		unsigned int user_ts)
+		struct kgsl_cmdbatch *cmdbatch, unsigned int *timestamp)
 {
+	/* Synchronization commands don't get a timestamp */
+	if (cmdbatch->flags & KGSL_CMDBATCH_SYNC) {
+		*timestamp = 0;
+		return 0;
+	}
 
 	if (drawctxt->base.flags & KGSL_CONTEXT_USER_GENERATED_TS) {
 		/*
 		 * User specified timestamps need to be greater than the last
 		 * issued timestamp in the context
 		 */
-		if (timestamp_cmp(drawctxt->timestamp, user_ts) >= 0)
+		if (timestamp_cmp(drawctxt->timestamp, *timestamp) >= 0)
 			return -ERANGE;
 
-		drawctxt->timestamp = user_ts;
+		drawctxt->timestamp = *timestamp;
 	} else
 		drawctxt->timestamp++;
 
 	*timestamp = drawctxt->timestamp;
-	drawobj->timestamp = *timestamp;
 	return 0;
 }
 
-static void _set_ft_policy(struct adreno_device *adreno_dev,
-		struct adreno_context *drawctxt,
-		struct kgsl_drawobj_cmd *cmdobj)
+/**
+ * adreno_dispatcher_preempt_timer() - Timer that triggers when preemption has
+ * not completed
+ * @data: Pointer to adreno device that did not preempt in timely manner
+ */
+static void adreno_dispatcher_preempt_timer(unsigned long data)
 {
-	/*
-	 * Set the fault tolerance policy for the command batch - assuming the
-	 * context hasn't disabled FT use the current device policy
-	 */
-	if (drawctxt->base.flags & KGSL_CONTEXT_NO_FAULT_TOLERANCE)
-		set_bit(KGSL_FT_DISABLE, &cmdobj->fault_policy);
-	/*
-	 *  Set the fault tolerance policy to FT_REPLAY - As context wants
-	 *  to invalidate it after a replay attempt fails. This doesn't
-	 *  require to execute the default FT policy.
-	 */
-	else if (drawctxt->base.flags & KGSL_CONTEXT_INVALIDATE_ON_FAULT)
-		set_bit(KGSL_FT_REPLAY, &cmdobj->fault_policy);
-	else
-		cmdobj->fault_policy = adreno_dev->ft_policy;
+	struct adreno_device *adreno_dev = (struct adreno_device *) data;
+	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+
+	KGSL_DRV_ERR(KGSL_DEVICE(adreno_dev),
+	"Preemption timed out. cur_rb rptr/wptr %x/%x id %d, next_rb rptr/wptr %x/%x id %d, disp_state: %d\n",
+	adreno_dev->cur_rb->rptr, adreno_dev->cur_rb->wptr,
+	adreno_dev->cur_rb->id, adreno_dev->next_rb->rptr,
+	adreno_dev->next_rb->wptr, adreno_dev->next_rb->id,
+	atomic_read(&dispatcher->preemption_state));
+	adreno_set_gpu_fault(adreno_dev, ADRENO_PREEMPT_FAULT);
+	adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
 }
 
-static void _cmdobj_set_flags(struct adreno_context *drawctxt,
-			struct kgsl_drawobj_cmd *cmdobj)
+/**
+ * adreno_dispatcher_get_highest_busy_rb() - Returns the highest priority RB
+ * which is busy
+ * @adreno_dev: Device whose RB is returned
+ */
+struct adreno_ringbuffer *adreno_dispatcher_get_highest_busy_rb(
+					struct adreno_device *adreno_dev)
 {
+	struct adreno_ringbuffer *rb, *highest_busy_rb = NULL;
+	int i;
+
+	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
+		if (rb->rptr != rb->wptr && !highest_busy_rb) {
+			highest_busy_rb = rb;
+			goto done;
+		}
+
+		if (!adreno_disp_preempt_fair_sched)
+			continue;
+
+		switch (rb->starve_timer_state) {
+		case ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT:
+			if (rb->rptr != rb->wptr &&
+				adreno_dev->cur_rb != rb) {
+				rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_INIT;
+				rb->sched_timer = jiffies;
+			}
+			break;
+		case ADRENO_DISPATCHER_RB_STARVE_TIMER_INIT:
+			if (time_after(jiffies, rb->sched_timer +
+				msecs_to_jiffies(_dispatch_starvation_time))) {
+				rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_ELAPSED;
+				/* halt dispatcher to remove starvation */
+				adreno_get_gpu_halt(adreno_dev);
+			}
+			break;
+		case ADRENO_DISPATCHER_RB_STARVE_TIMER_SCHEDULED:
+			BUG_ON(adreno_dev->cur_rb != rb);
+			/*
+			 * If the RB has not been running for the minimum
+			 * time slice then allow it to run
+			 */
+			if ((rb->rptr != rb->wptr) && time_before(jiffies,
+				adreno_dev->cur_rb->sched_timer +
+				msecs_to_jiffies(_dispatch_time_slice)))
+				highest_busy_rb = rb;
+			else
+				rb->starve_timer_state =
+				ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT;
+			break;
+		case ADRENO_DISPATCHER_RB_STARVE_TIMER_ELAPSED:
+		default:
+			break;
+		}
+	}
+done:
+	return highest_busy_rb;
+}
+
+/**
+ * adreno_dispactcher_queue_cmd() - Queue a new command in the context
+ * @adreno_dev: Pointer to the adreno device struct
+ * @drawctxt: Pointer to the adreno draw context
+ * @cmdbatch: Pointer to the command batch being submitted
+ * @timestamp: Pointer to the requested timestamp
+ *
+ * Queue a command in the context - if there isn't any room in the queue, then
+ * block until there is
+ */
+int adreno_dispatcher_queue_cmd(struct adreno_device *adreno_dev,
+		struct adreno_context *drawctxt, struct kgsl_cmdbatch *cmdbatch,
+		uint32_t *timestamp)
+{
+	struct adreno_dispatcher_cmdqueue *dispatch_q =
+				ADRENO_CMDBATCH_DISPATCH_CMDQUEUE(cmdbatch);
+	int ret;
+
+	spin_lock(&drawctxt->lock);
+
+	if (kgsl_context_detached(&drawctxt->base)) {
+		spin_unlock(&drawctxt->lock);
+		return -ENOENT;
+	}
+
 	/*
 	 * Force the preamble for this submission only - this is usually
 	 * requested by the dispatcher as part of fault recovery
 	 */
+
 	if (test_and_clear_bit(ADRENO_CONTEXT_FORCE_PREAMBLE,
 				&drawctxt->base.priv))
-		set_bit(CMDOBJ_FORCE_PREAMBLE, &cmdobj->priv);
+		set_bit(CMDBATCH_FLAG_FORCE_PREAMBLE, &cmdbatch->priv);
 
 	/*
-	 * Force the premable if set from userspace in the context or
-	 * command obj flags
+	 * Force the premable if set from userspace in the context or cmdbatch
+	 * flags
 	 */
+
 	if ((drawctxt->base.flags & KGSL_CONTEXT_CTX_SWITCH) ||
-		(cmdobj->base.flags & KGSL_DRAWOBJ_CTX_SWITCH))
-		set_bit(CMDOBJ_FORCE_PREAMBLE, &cmdobj->priv);
+		(cmdbatch->flags & KGSL_CMDBATCH_CTX_SWITCH))
+		set_bit(CMDBATCH_FLAG_FORCE_PREAMBLE, &cmdbatch->priv);
 
-	/* Skip this ib if IFH_NOP is enabled */
+	/* Skip this cmdbatch commands if IFH_NOP is enabled */
 	if (drawctxt->base.flags & KGSL_CONTEXT_IFH_NOP)
-		set_bit(CMDOBJ_SKIP, &cmdobj->priv);
+		set_bit(CMDBATCH_FLAG_SKIP, &cmdbatch->priv);
 
 	/*
 	 * If we are waiting for the end of frame and it hasn't appeared yet,
-	 * then mark the command obj as skipped.  It will still progress
+	 * then mark the command batch as skipped.  It will still progress
 	 * through the pipeline but it won't actually send any commands
 	 */
 
 	if (test_bit(ADRENO_CONTEXT_SKIP_EOF, &drawctxt->base.priv)) {
-		set_bit(CMDOBJ_SKIP, &cmdobj->priv);
+		set_bit(CMDBATCH_FLAG_SKIP, &cmdbatch->priv);
 
 		/*
-		 * If this command obj represents the EOF then clear the way
+		 * If this command batch represents the EOF then clear the way
 		 * for the dispatcher to continue submitting
 		 */
 
-		if (cmdobj->base.flags & KGSL_DRAWOBJ_END_OF_FRAME) {
+		if (cmdbatch->flags & KGSL_CMDBATCH_END_OF_FRAME) {
 			clear_bit(ADRENO_CONTEXT_SKIP_EOF,
 				  &drawctxt->base.priv);
 
@@ -1127,364 +1066,119 @@ static void _cmdobj_set_flags(struct adreno_context *drawctxt,
 			set_bit(ADRENO_CONTEXT_FORCE_PREAMBLE,
 				&drawctxt->base.priv);
 		}
-	}
-}
-
-static inline int _check_context_state(struct kgsl_context *context)
-{
-	if (kgsl_context_invalid(context))
-		return -EDEADLK;
-
-	if (kgsl_context_detached(context))
-		return -ENOENT;
-
-	return 0;
-}
-
-static inline bool _verify_ib(struct kgsl_device_private *dev_priv,
-		struct kgsl_context *context, struct kgsl_memobj_node *ib)
-{
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_process_private *private = dev_priv->process_priv;
-
-	/* The maximum allowable size for an IB in the CP is 0xFFFFF dwords */
-	if (ib->size == 0 || ((ib->size >> 2) > 0xFFFFF)) {
-		pr_context(device, context, "ctxt %d invalid ib size %lld\n",
-			context->id, ib->size);
-		return false;
-	}
-
-	/* Make sure that the address is mapped */
-	if (!kgsl_mmu_gpuaddr_in_range(private->pagetable, ib->gpuaddr)) {
-		pr_context(device, context, "ctxt %d invalid ib gpuaddr %llX\n",
-			context->id, ib->gpuaddr);
-		return false;
-	}
-
-	return true;
-}
-
-static inline int _verify_cmdobj(struct kgsl_device_private *dev_priv,
-		struct kgsl_context *context, struct kgsl_drawobj *drawobj[],
-		uint32_t count)
-{
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_memobj_node *ib;
-	unsigned int i;
-
-	for (i = 0; i < count; i++) {
-		/* Verify the IBs before they get queued */
-		if (drawobj[i]->type == CMDOBJ_TYPE) {
-			struct kgsl_drawobj_cmd *cmdobj = CMDOBJ(drawobj[i]);
-
-			list_for_each_entry(ib, &cmdobj->cmdlist, node)
-				if (_verify_ib(dev_priv,
-					&ADRENO_CONTEXT(context)->base, ib)
-					== false)
-					return -EINVAL;
-			/*
-			 * Clear the wake on touch bit to indicate an IB has
-			 * been submitted since the last time we set it.
-			 * But only clear it when we have rendering commands.
-			 */
-			device->flags &= ~KGSL_FLAG_WAKE_ON_TOUCH;
-		}
-
-		/* A3XX does not have support for drawobj profiling */
-		if (adreno_is_a3xx(ADRENO_DEVICE(device)) &&
-			(drawobj[i]->flags & KGSL_DRAWOBJ_PROFILING))
-			return -EOPNOTSUPP;
-	}
-
-	return 0;
-}
-
-static inline int _wait_for_room_in_context_queue(
-	struct adreno_context *drawctxt)
-{
-	int ret = 0;
-
-	/* Wait for room in the context queue */
-	while (drawctxt->queued >= _context_drawqueue_size) {
-		trace_adreno_drawctxt_sleep(drawctxt);
-		spin_unlock(&drawctxt->lock);
-
-		ret = wait_event_interruptible_timeout(drawctxt->wq,
-			_check_context_queue(drawctxt),
-			msecs_to_jiffies(_context_queue_wait));
-
-		spin_lock(&drawctxt->lock);
-		trace_adreno_drawctxt_wake(drawctxt);
-
-		if (ret <= 0)
-			return (ret == 0) ? -ETIMEDOUT : (int) ret;
-	}
-
-	return 0;
-}
-
-static unsigned int _check_context_state_to_queue_cmds(
-	struct adreno_context *drawctxt)
-{
-	int ret = _check_context_state(&drawctxt->base);
-
-	if (ret)
-		return ret;
-
-	ret = _wait_for_room_in_context_queue(drawctxt);
-	if (ret)
-		return ret;
-
-	/*
-	 * Account for the possiblity that the context got invalidated
-	 * while we were sleeping
-	 */
-	return _check_context_state(&drawctxt->base);
-}
-
-static void _queue_drawobj(struct adreno_context *drawctxt,
-	struct kgsl_drawobj *drawobj)
-{
-	/* Put the command into the queue */
-	drawctxt->drawqueue[drawctxt->drawqueue_tail] = drawobj;
-	drawctxt->drawqueue_tail = (drawctxt->drawqueue_tail + 1) %
-			ADRENO_CONTEXT_DRAWQUEUE_SIZE;
-	drawctxt->queued++;
-	trace_adreno_cmdbatch_queued(drawobj, drawctxt->queued);
-}
-
-static int _queue_sparseobj(struct adreno_device *adreno_dev,
-	struct adreno_context *drawctxt, struct kgsl_drawobj_sparse *sparseobj,
-	uint32_t *timestamp, unsigned int user_ts)
-{
-	struct kgsl_drawobj *drawobj = DRAWOBJ(sparseobj);
-	int ret;
-
-	ret = get_timestamp(drawctxt, drawobj, timestamp, user_ts);
-	if (ret)
-		return ret;
-
-	/*
-	 * See if we can fastpath this thing - if nothing is
-	 * queued bind/unbind without queueing the context
-	 */
-	if (!drawctxt->queued)
-		return 1;
-
-	drawctxt->queued_timestamp = *timestamp;
-	_queue_drawobj(drawctxt, drawobj);
-
-	return 0;
-}
-
-
-static int _queue_markerobj(struct adreno_device *adreno_dev,
-	struct adreno_context *drawctxt, struct kgsl_drawobj_cmd *markerobj,
-	uint32_t *timestamp, unsigned int user_ts)
-{
-	struct kgsl_drawobj *drawobj = DRAWOBJ(markerobj);
-	int ret;
-
-	ret = get_timestamp(drawctxt, drawobj, timestamp, user_ts);
-	if (ret)
-		return ret;
-
-	/*
-	 * See if we can fastpath this thing - if nothing is queued
-	 * and nothing is inflight retire without bothering the GPU
-	 */
-	if (!drawctxt->queued && kgsl_check_timestamp(drawobj->device,
-			drawobj->context, drawctxt->queued_timestamp)) {
-		_retire_timestamp(drawobj);
-		return 1;
-	}
-
-	/*
-	 * Remember the last queued timestamp - the marker will block
-	 * until that timestamp is expired (unless another command
-	 * comes along and forces the marker to execute)
-	 */
-
-	markerobj->marker_timestamp = drawctxt->queued_timestamp;
-	drawctxt->queued_timestamp = *timestamp;
-	_set_ft_policy(adreno_dev, drawctxt, markerobj);
-	_cmdobj_set_flags(drawctxt, markerobj);
-
-	_queue_drawobj(drawctxt, drawobj);
-
-	return 0;
-}
-
-static int _queue_cmdobj(struct adreno_device *adreno_dev,
-	struct adreno_context *drawctxt, struct kgsl_drawobj_cmd *cmdobj,
-	uint32_t *timestamp, unsigned int user_ts)
-{
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-	unsigned int j;
-	int ret;
-
-	ret = get_timestamp(drawctxt, drawobj, timestamp, user_ts);
-	if (ret)
-		return ret;
-
-	/*
-	 * If this is a real command then we need to force any markers
-	 * queued before it to dispatch to keep time linear - set the
-	 * skip bit so the commands get NOPed.
-	 */
-	j = drawctxt->drawqueue_head;
-
-	while (j != drawctxt->drawqueue_tail) {
-		if (drawctxt->drawqueue[j]->type == MARKEROBJ_TYPE) {
-			struct kgsl_drawobj_cmd *markerobj =
-				CMDOBJ(drawctxt->drawqueue[j]);
-				set_bit(CMDOBJ_SKIP, &markerobj->priv);
-		}
-
-		j = DRAWQUEUE_NEXT(j, ADRENO_CONTEXT_DRAWQUEUE_SIZE);
-	}
-
-	drawctxt->queued_timestamp = *timestamp;
-	_set_ft_policy(adreno_dev, drawctxt, cmdobj);
-	_cmdobj_set_flags(drawctxt, cmdobj);
-
-	_queue_drawobj(drawctxt, drawobj);
-
-	return 0;
-}
-
-static void _queue_syncobj(struct adreno_context *drawctxt,
-	struct kgsl_drawobj_sync *syncobj, uint32_t *timestamp)
-{
-	struct kgsl_drawobj *drawobj = DRAWOBJ(syncobj);
-
-	*timestamp = 0;
-	drawobj->timestamp = 0;
-
-	_queue_drawobj(drawctxt, drawobj);
-}
-
-/**
- * adreno_dispactcher_queue_cmds() - Queue a new draw object in the context
- * @dev_priv: Pointer to the device private struct
- * @context: Pointer to the kgsl draw context
- * @drawobj: Pointer to the array of drawobj's being submitted
- * @count: Number of drawobj's being submitted
- * @timestamp: Pointer to the requested timestamp
- *
- * Queue a command in the context - if there isn't any room in the queue, then
- * block until there is
- */
-int adreno_dispatcher_queue_cmds(struct kgsl_device_private *dev_priv,
-		struct kgsl_context *context, struct kgsl_drawobj *drawobj[],
-		uint32_t count, uint32_t *timestamp)
+	}
 
-{
-	struct kgsl_device *device = dev_priv->device;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_context *drawctxt = ADRENO_CONTEXT(context);
-	struct adreno_dispatcher_drawqueue *dispatch_q;
-	int ret;
-	unsigned int i, user_ts;
+	/* Wait for room in the context queue */
 
-	if (!count)
-		return -EINVAL;
+	while (drawctxt->queued >= _context_cmdqueue_size) {
+		trace_adreno_drawctxt_sleep(drawctxt);
+		spin_unlock(&drawctxt->lock);
 
-	ret = _check_context_state(&drawctxt->base);
-	if (ret)
-		return ret;
+		ret = wait_event_interruptible_timeout(drawctxt->wq,
+			_check_context_queue(drawctxt),
+			msecs_to_jiffies(_context_queue_wait));
 
-	ret = _verify_cmdobj(dev_priv, context, drawobj, count);
-	if (ret)
-		return ret;
+		spin_lock(&drawctxt->lock);
+		trace_adreno_drawctxt_wake(drawctxt);
 
-	/* wait for the suspend gate */
-	wait_for_completion(&device->halt_gate);
+		if (ret <= 0) {
+			spin_unlock(&drawctxt->lock);
+			return (ret == 0) ? -ETIMEDOUT : (int) ret;
+		}
+	}
+	/*
+	 * Account for the possiblity that the context got invalidated
+	 * while we were sleeping
+	 */
 
-	spin_lock(&drawctxt->lock);
+	if (kgsl_context_invalid(&drawctxt->base)) {
+		spin_unlock(&drawctxt->lock);
+		return -EDEADLK;
+	}
+	if (kgsl_context_detached(&drawctxt->base)) {
+		spin_unlock(&drawctxt->lock);
+		return -ENOENT;
+	}
 
-	ret = _check_context_state_to_queue_cmds(drawctxt);
+	ret = get_timestamp(drawctxt, cmdbatch, timestamp);
 	if (ret) {
 		spin_unlock(&drawctxt->lock);
 		return ret;
 	}
 
-	user_ts = *timestamp;
+	cmdbatch->timestamp = *timestamp;
+
+	if (cmdbatch->flags & KGSL_CMDBATCH_MARKER) {
 
-	/*
-	 * If there is only one drawobj in the array and it is of
-	 * type SYNCOBJ_TYPE, skip comparing user_ts as it can be 0
-	 */
-	if (!(count == 1 && drawobj[0]->type == SYNCOBJ_TYPE) &&
-		(drawctxt->base.flags & KGSL_CONTEXT_USER_GENERATED_TS)) {
 		/*
-		 * User specified timestamps need to be greater than the last
-		 * issued timestamp in the context
+		 * See if we can fastpath this thing - if nothing is queued
+		 * and nothing is inflight retire without bothering the GPU
 		 */
-		if (timestamp_cmp(drawctxt->timestamp, user_ts) >= 0) {
+
+		if (!drawctxt->queued && kgsl_check_timestamp(cmdbatch->device,
+			cmdbatch->context, drawctxt->queued_timestamp)) {
+			trace_adreno_cmdbatch_queued(cmdbatch,
+				drawctxt->queued);
+
+			_retire_marker(cmdbatch);
 			spin_unlock(&drawctxt->lock);
-			return -ERANGE;
+			return 0;
 		}
+
+		/*
+		 * Remember the last queued timestamp - the marker will block
+		 * until that timestamp is expired (unless another command
+		 * comes along and forces the marker to execute)
+		 */
+
+		cmdbatch->marker_timestamp = drawctxt->queued_timestamp;
 	}
 
-	for (i = 0; i < count; i++) {
+	/* SYNC commands have timestamp 0 and will get optimized out anyway */
+	if (!(cmdbatch->flags & KGSL_CONTEXT_SYNC))
+		drawctxt->queued_timestamp = *timestamp;
 
-		switch (drawobj[i]->type) {
-		case MARKEROBJ_TYPE:
-			ret = _queue_markerobj(adreno_dev, drawctxt,
-					CMDOBJ(drawobj[i]),
-					timestamp, user_ts);
-			if (ret == 1) {
-				spin_unlock(&drawctxt->lock);
-				goto done;
-			} else if (ret) {
-				spin_unlock(&drawctxt->lock);
-				return ret;
-			}
-			break;
-		case CMDOBJ_TYPE:
-			ret = _queue_cmdobj(adreno_dev, drawctxt,
-						CMDOBJ(drawobj[i]),
-						timestamp, user_ts);
-			if (ret) {
-				spin_unlock(&drawctxt->lock);
-				return ret;
-			}
-			break;
-		case SYNCOBJ_TYPE:
-			_queue_syncobj(drawctxt, SYNCOBJ(drawobj[i]),
-						timestamp);
-			break;
-		case SPARSEOBJ_TYPE:
-			ret = _queue_sparseobj(adreno_dev, drawctxt,
-					SPARSEOBJ(drawobj[i]),
-					timestamp, user_ts);
-			if (ret == 1) {
-				spin_unlock(&drawctxt->lock);
-				_retire_sparseobj(SPARSEOBJ(drawobj[i]),
-						drawctxt);
-				return 0;
-			} else if (ret) {
-				spin_unlock(&drawctxt->lock);
-				return ret;
-			}
-			break;
-		default:
-			spin_unlock(&drawctxt->lock);
-			return -EINVAL;
-		}
+	/*
+	 * Set the fault tolerance policy for the command batch - assuming the
+	 * context hasn't disabled FT use the current device policy
+	 */
+
+	if (drawctxt->base.flags & KGSL_CONTEXT_NO_FAULT_TOLERANCE)
+		set_bit(KGSL_FT_DISABLE, &cmdbatch->fault_policy);
+	else
+		cmdbatch->fault_policy = adreno_dev->ft_policy;
+
+	/* Put the command into the queue */
+	drawctxt->cmdqueue[drawctxt->cmdqueue_tail] = cmdbatch;
+	drawctxt->cmdqueue_tail = (drawctxt->cmdqueue_tail + 1) %
+		ADRENO_CONTEXT_CMDQUEUE_SIZE;
+
+	/*
+	 * If this is a real command then we need to force any markers queued
+	 * before it to dispatch to keep time linear - set the skip bit so
+	 * the commands get NOPed.
+	 */
 
+	if (!(cmdbatch->flags & KGSL_CMDBATCH_MARKER)) {
+		unsigned int i = drawctxt->cmdqueue_head;
+
+		while (i != drawctxt->cmdqueue_tail) {
+			if (drawctxt->cmdqueue[i]->flags & KGSL_CMDBATCH_MARKER)
+				set_bit(CMDBATCH_FLAG_SKIP,
+					&drawctxt->cmdqueue[i]->priv);
+
+			i = CMDQUEUE_NEXT(i, ADRENO_CONTEXT_CMDQUEUE_SIZE);
+		}
 	}
 
-	dispatch_q = ADRENO_DRAWOBJ_DISPATCH_DRAWQUEUE(drawobj[0]);
+	drawctxt->queued++;
+	trace_adreno_cmdbatch_queued(cmdbatch, drawctxt->queued);
 
-	_track_context(adreno_dev, dispatch_q, drawctxt);
+	_track_context(dispatch_q, drawctxt->base.id);
 
 	spin_unlock(&drawctxt->lock);
 
-	if (device->pwrctrl.l2pc_update_queue)
-		kgsl_pwrctrl_update_l2pc(&adreno_dev->dev,
-				KGSL_L2PC_QUEUE_TIMEOUT);
+	kgsl_pwrctrl_update_l2pc(&adreno_dev->dev);
 
 	/* Add the context to the dispatcher pending list */
 	dispatcher_queue_context(adreno_dev, drawctxt);
@@ -1498,11 +1192,8 @@ int adreno_dispatcher_queue_cmds(struct kgsl_device_private *dev_priv,
 	 * queue will try to schedule new commands anyway.
 	 */
 
-	if (dispatch_q->inflight < _context_drawobj_burst)
+	if (dispatch_q->inflight < _context_cmdbatch_burst)
 		adreno_dispatcher_issuecmds(adreno_dev);
-done:
-	if (test_and_clear_bit(ADRENO_CONTEXT_FAULT, &context->priv))
-		return -EPROTO;
 
 	return 0;
 }
@@ -1546,15 +1237,15 @@ static void mark_guilty_context(struct kgsl_device *device, unsigned int id)
 }
 
 /*
- * If an IB inside of the drawobj has a gpuaddr that matches the base
+ * If an IB inside of the command batch has a gpuaddr that matches the base
  * passed in then zero the size which effectively skips it when it is submitted
  * in the ringbuffer.
  */
-static void _skip_ib(struct kgsl_drawobj_cmd *cmdobj, uint64_t base)
+static void cmdbatch_skip_ib(struct kgsl_cmdbatch *cmdbatch, uint64_t base)
 {
 	struct kgsl_memobj_node *ib;
 
-	list_for_each_entry(ib, &cmdobj->cmdlist, node) {
+	list_for_each_entry(ib, &cmdbatch->cmdlist, node) {
 		if (ib->gpuaddr == base) {
 			ib->priv |= MEMOBJ_SKIP;
 			if (base)
@@ -1563,11 +1254,10 @@ static void _skip_ib(struct kgsl_drawobj_cmd *cmdobj, uint64_t base)
 	}
 }
 
-static void _skip_cmd(struct kgsl_drawobj_cmd *cmdobj,
-	struct kgsl_drawobj_cmd **replay, int count)
+static void cmdbatch_skip_cmd(struct kgsl_cmdbatch *cmdbatch,
+	struct kgsl_cmdbatch **replay, int count)
 {
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-	struct adreno_context *drawctxt = ADRENO_CONTEXT(drawobj->context);
+	struct adreno_context *drawctxt = ADRENO_CONTEXT(cmdbatch->context);
 	int i;
 
 	/*
@@ -1582,9 +1272,9 @@ static void _skip_cmd(struct kgsl_drawobj_cmd *cmdobj,
 	 * b) force preamble for next commandbatch
 	 */
 	for (i = 1; i < count; i++) {
-		if (DRAWOBJ(replay[i])->context->id == drawobj->context->id) {
+		if (replay[i]->context->id == cmdbatch->context->id) {
 			replay[i]->fault_policy = replay[0]->fault_policy;
-			set_bit(CMDOBJ_FORCE_PREAMBLE, &replay[i]->priv);
+			set_bit(CMDBATCH_FLAG_FORCE_PREAMBLE, &replay[i]->priv);
 			set_bit(KGSL_FT_SKIPCMD, &replay[i]->fault_recovery);
 			break;
 		}
@@ -1601,44 +1291,41 @@ static void _skip_cmd(struct kgsl_drawobj_cmd *cmdobj,
 		drawctxt->fault_policy = replay[0]->fault_policy;
 	}
 
-	/* set the flags to skip this cmdobj */
-	set_bit(CMDOBJ_SKIP, &cmdobj->priv);
-	cmdobj->fault_recovery = 0;
+	/* set the flags to skip this cmdbatch */
+	set_bit(CMDBATCH_FLAG_SKIP, &cmdbatch->priv);
+	cmdbatch->fault_recovery = 0;
 }
 
-static void _skip_frame(struct kgsl_drawobj_cmd *cmdobj,
-	struct kgsl_drawobj_cmd **replay, int count)
+static void cmdbatch_skip_frame(struct kgsl_cmdbatch *cmdbatch,
+	struct kgsl_cmdbatch **replay, int count)
 {
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-	struct adreno_context *drawctxt = ADRENO_CONTEXT(drawobj->context);
+	struct adreno_context *drawctxt = ADRENO_CONTEXT(cmdbatch->context);
 	int skip = 1;
 	int i;
 
 	for (i = 0; i < count; i++) {
 
-		struct kgsl_drawobj *replay_obj = DRAWOBJ(replay[i]);
-
 		/*
-		 * Only operate on drawobj's that belong to the
+		 * Only operate on command batches that belong to the
 		 * faulting context
 		 */
 
-		if (replay_obj->context->id != drawobj->context->id)
+		if (replay[i]->context->id != cmdbatch->context->id)
 			continue;
 
 		/*
-		 * Skip all the drawobjs in this context until
+		 * Skip all the command batches in this context until
 		 * the EOF flag is seen.  If the EOF flag is seen then
 		 * force the preamble for the next command.
 		 */
 
 		if (skip) {
-			set_bit(CMDOBJ_SKIP, &replay[i]->priv);
+			set_bit(CMDBATCH_FLAG_SKIP, &replay[i]->priv);
 
-			if (replay_obj->flags & KGSL_DRAWOBJ_END_OF_FRAME)
+			if (replay[i]->flags & KGSL_CMDBATCH_END_OF_FRAME)
 				skip = 0;
 		} else {
-			set_bit(CMDOBJ_FORCE_PREAMBLE, &replay[i]->priv);
+			set_bit(CMDBATCH_FLAG_FORCE_PREAMBLE, &replay[i]->priv);
 			return;
 		}
 	}
@@ -1660,28 +1347,26 @@ static void _skip_frame(struct kgsl_drawobj_cmd *cmdobj,
 		set_bit(ADRENO_CONTEXT_FORCE_PREAMBLE, &drawctxt->base.priv);
 }
 
-static void remove_invalidated_cmdobjs(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd **replay, int count)
+static void remove_invalidated_cmdbatches(struct kgsl_device *device,
+		struct kgsl_cmdbatch **replay, int count)
 {
 	int i;
 
 	for (i = 0; i < count; i++) {
-		struct kgsl_drawobj_cmd *cmdobj = replay[i];
-		struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-
-		if (cmdobj == NULL)
+		struct kgsl_cmdbatch *cmd = replay[i];
+		if (cmd == NULL)
 			continue;
 
-		if (kgsl_context_detached(drawobj->context) ||
-			kgsl_context_invalid(drawobj->context)) {
+		if (kgsl_context_detached(cmd->context) ||
+			kgsl_context_invalid(cmd->context)) {
 			replay[i] = NULL;
 
 			mutex_lock(&device->mutex);
 			kgsl_cancel_events_timestamp(device,
-				&drawobj->context->events, drawobj->timestamp);
+				&cmd->context->events, cmd->timestamp);
 			mutex_unlock(&device->mutex);
 
-			kgsl_drawobj_destroy(drawobj);
+			kgsl_cmdbatch_destroy(cmd);
 		}
 	}
 }
@@ -1705,14 +1390,13 @@ static inline const char *_kgsl_context_comm(struct kgsl_context *context)
 
 
 static void adreno_fault_header(struct kgsl_device *device,
-		struct adreno_ringbuffer *rb, struct kgsl_drawobj_cmd *cmdobj)
+		struct adreno_ringbuffer *rb, struct kgsl_cmdbatch *cmdbatch)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
 	unsigned int status, rptr, wptr, ib1sz, ib2sz;
 	uint64_t ib1base, ib2base;
 
-	adreno_readreg(adreno_dev, ADRENO_REG_RBBM_STATUS, &status);
+	adreno_readreg(adreno_dev , ADRENO_REG_RBBM_STATUS, &status);
 	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR, &rptr);
 	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_WPTR, &wptr);
 	adreno_readreg64(adreno_dev, ADRENO_REG_CP_IB1_BASE,
@@ -1722,24 +1406,24 @@ static void adreno_fault_header(struct kgsl_device *device,
 					   ADRENO_REG_CP_IB2_BASE_HI, &ib2base);
 	adreno_readreg(adreno_dev, ADRENO_REG_CP_IB2_BUFSZ, &ib2sz);
 
-	if (drawobj != NULL) {
+	if (cmdbatch != NULL) {
 		struct adreno_context *drawctxt =
-			ADRENO_CONTEXT(drawobj->context);
+			ADRENO_CONTEXT(cmdbatch->context);
 
-		trace_adreno_gpu_fault(drawobj->context->id,
-			drawobj->timestamp,
+		trace_adreno_gpu_fault(cmdbatch->context->id,
+			cmdbatch->timestamp,
 			status, rptr, wptr, ib1base, ib1sz,
 			ib2base, ib2sz, drawctxt->rb->id);
 
-		pr_fault(device, drawobj,
+		pr_fault(device, cmdbatch,
 			"gpu fault ctx %d ts %d status %8.8X rb %4.4x/%4.4x ib1 %16.16llX/%4.4x ib2 %16.16llX/%4.4x\n",
-			drawobj->context->id, drawobj->timestamp, status,
+			cmdbatch->context->id, cmdbatch->timestamp, status,
 			rptr, wptr, ib1base, ib1sz, ib2base, ib2sz);
 
 		if (rb != NULL)
-			pr_fault(device, drawobj,
+			pr_fault(device, cmdbatch,
 				"gpu fault rb %d rb sw r/w %4.4x/%4.4x\n",
-				rb->id, rptr, rb->wptr);
+				rb->id, rb->rptr, rb->wptr);
 	} else {
 		int id = (rb != NULL) ? rb->id : -1;
 
@@ -1750,40 +1434,39 @@ static void adreno_fault_header(struct kgsl_device *device,
 		if (rb != NULL)
 			dev_err(device->dev,
 				"RB[%d] gpu fault rb sw r/w %4.4x/%4.4x\n",
-				rb->id, rptr, rb->wptr);
+				rb->id, rb->rptr, rb->wptr);
 	}
 }
 
 void adreno_fault_skipcmd_detached(struct adreno_device *adreno_dev,
 				 struct adreno_context *drawctxt,
-				 struct kgsl_drawobj *drawobj)
+				 struct kgsl_cmdbatch *cmdbatch)
 {
 	if (test_bit(ADRENO_CONTEXT_SKIP_CMD, &drawctxt->base.priv) &&
 			kgsl_context_detached(&drawctxt->base)) {
-		pr_context(KGSL_DEVICE(adreno_dev), drawobj->context,
-			"gpu detached context %d\n", drawobj->context->id);
+		pr_context(KGSL_DEVICE(adreno_dev), cmdbatch->context,
+			"gpu detached context %d\n", cmdbatch->context->id);
 		clear_bit(ADRENO_CONTEXT_SKIP_CMD, &drawctxt->base.priv);
 	}
 }
 
 /**
- * process_cmdobj_fault() - Process a cmdobj for fault policies
- * @device: Device on which the cmdobj caused a fault
- * @replay: List of cmdobj's that are to be replayed on the device. The
- * first command in the replay list is the faulting command and the remaining
- * cmdobj's in the list are commands that were submitted to the same queue
+ * process_cmdbatch_fault() - Process a cmdbatch for fault policies
+ * @device: Device on which the cmdbatch caused a fault
+ * @replay: List of cmdbatches that are to be replayed on the device. The
+ * faulting cmdbatch is the first command in the replay list and the remaining
+ * cmdbatches in the list are commands that were submitted to the same queue
  * as the faulting one.
- * @count: Number of cmdobj's in replay
+ * @count: Number of cmdbatches in replay
  * @base: The IB1 base at the time of fault
  * @fault: The fault type
  */
-static void process_cmdobj_fault(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd **replay, int count,
+static void process_cmdbatch_fault(struct kgsl_device *device,
+		struct kgsl_cmdbatch **replay, int count,
 		unsigned int base,
 		int fault)
 {
-	struct kgsl_drawobj_cmd *cmdobj = replay[0];
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
+	struct kgsl_cmdbatch *cmdbatch = replay[0];
 	int i;
 	char *state = "failed";
 
@@ -1797,18 +1480,18 @@ static void process_cmdobj_fault(struct kgsl_device *device,
 	 * where 1st and 4th gpu hang are more than 3 seconds apart we
 	 * won't disable GFT and invalidate the context.
 	 */
-	if (test_bit(KGSL_FT_THROTTLE, &cmdobj->fault_policy)) {
-		if (time_after(jiffies, (drawobj->context->fault_time
+	if (test_bit(KGSL_FT_THROTTLE, &cmdbatch->fault_policy)) {
+		if (time_after(jiffies, (cmdbatch->context->fault_time
 				+ msecs_to_jiffies(_fault_throttle_time)))) {
-			drawobj->context->fault_time = jiffies;
-			drawobj->context->fault_count = 1;
+			cmdbatch->context->fault_time = jiffies;
+			cmdbatch->context->fault_count = 1;
 		} else {
-			drawobj->context->fault_count++;
-			if (drawobj->context->fault_count >
+			cmdbatch->context->fault_count++;
+			if (cmdbatch->context->fault_count >
 					_fault_throttle_burst) {
 				set_bit(KGSL_FT_DISABLE,
-						&cmdobj->fault_policy);
-				pr_context(device, drawobj->context,
+						&cmdbatch->fault_policy);
+				pr_context(device, cmdbatch->context,
 					 "gpu fault threshold exceeded %d faults in %d msecs\n",
 					 _fault_throttle_burst,
 					 _fault_throttle_time);
@@ -1817,46 +1500,45 @@ static void process_cmdobj_fault(struct kgsl_device *device,
 	}
 
 	/*
-	 * If FT is disabled for this cmdobj invalidate immediately
+	 * If FT is disabled for this cmdbatch invalidate immediately
 	 */
 
-	if (test_bit(KGSL_FT_DISABLE, &cmdobj->fault_policy) ||
-		test_bit(KGSL_FT_TEMP_DISABLE, &cmdobj->fault_policy)) {
+	if (test_bit(KGSL_FT_DISABLE, &cmdbatch->fault_policy) ||
+		test_bit(KGSL_FT_TEMP_DISABLE, &cmdbatch->fault_policy)) {
 		state = "skipped";
-		bitmap_zero(&cmdobj->fault_policy, BITS_PER_LONG);
+		bitmap_zero(&cmdbatch->fault_policy, BITS_PER_LONG);
 	}
 
 	/* If the context is detached do not run FT on context */
-	if (kgsl_context_detached(drawobj->context)) {
+	if (kgsl_context_detached(cmdbatch->context)) {
 		state = "detached";
-		bitmap_zero(&cmdobj->fault_policy, BITS_PER_LONG);
+		bitmap_zero(&cmdbatch->fault_policy, BITS_PER_LONG);
 	}
 
 	/*
-	 * Set a flag so we don't print another PM dump if the cmdobj fails
+	 * Set a flag so we don't print another PM dump if the cmdbatch fails
 	 * again on replay
 	 */
 
-	set_bit(KGSL_FT_SKIP_PMDUMP, &cmdobj->fault_policy);
+	set_bit(KGSL_FT_SKIP_PMDUMP, &cmdbatch->fault_policy);
 
 	/*
 	 * A hardware fault generally means something was deterministically
-	 * wrong with the cmdobj - no point in trying to replay it
+	 * wrong with the command batch - no point in trying to replay it
 	 * Clear the replay bit and move on to the next policy level
 	 */
 
 	if (fault & ADRENO_HARD_FAULT)
-		clear_bit(KGSL_FT_REPLAY, &(cmdobj->fault_policy));
+		clear_bit(KGSL_FT_REPLAY, &(cmdbatch->fault_policy));
 
 	/*
 	 * A timeout fault means the IB timed out - clear the policy and
 	 * invalidate - this will clear the FT_SKIP_PMDUMP bit but that is okay
-	 * because we won't see this cmdobj again
+	 * because we won't see this cmdbatch again
 	 */
 
-	if ((fault & ADRENO_TIMEOUT_FAULT) ||
-				(fault & ADRENO_CTX_DETATCH_TIMEOUT_FAULT))
-		bitmap_zero(&cmdobj->fault_policy, BITS_PER_LONG);
+	if (fault & ADRENO_TIMEOUT_FAULT)
+		bitmap_zero(&cmdbatch->fault_policy, BITS_PER_LONG);
 
 	/*
 	 * If the context had a GPU page fault then it is likely it would fault
@@ -1864,84 +1546,83 @@ static void process_cmdobj_fault(struct kgsl_device *device,
 	 */
 
 	if (test_bit(KGSL_CONTEXT_PRIV_PAGEFAULT,
-		     &drawobj->context->priv)) {
+		     &cmdbatch->context->priv)) {
 		/* we'll need to resume the mmu later... */
-		clear_bit(KGSL_FT_REPLAY, &cmdobj->fault_policy);
+		clear_bit(KGSL_FT_REPLAY, &cmdbatch->fault_policy);
 		clear_bit(KGSL_CONTEXT_PRIV_PAGEFAULT,
-			  &drawobj->context->priv);
+			  &cmdbatch->context->priv);
 	}
 
 	/*
-	 * Execute the fault tolerance policy. Each cmdobj stores the
+	 * Execute the fault tolerance policy. Each command batch stores the
 	 * current fault policy that was set when it was queued.
 	 * As the options are tried in descending priority
 	 * (REPLAY -> SKIPIBS -> SKIPFRAME -> NOTHING) the bits are cleared
-	 * from the cmdobj policy so the next thing can be tried if the
+	 * from the cmdbatch policy so the next thing can be tried if the
 	 * change comes around again
 	 */
 
-	/* Replay the hanging cmdobj again */
-	if (test_and_clear_bit(KGSL_FT_REPLAY, &cmdobj->fault_policy)) {
-		trace_adreno_cmdbatch_recovery(cmdobj, BIT(KGSL_FT_REPLAY));
-		set_bit(KGSL_FT_REPLAY, &cmdobj->fault_recovery);
+	/* Replay the hanging command batch again */
+	if (test_and_clear_bit(KGSL_FT_REPLAY, &cmdbatch->fault_policy)) {
+		trace_adreno_cmdbatch_recovery(cmdbatch, BIT(KGSL_FT_REPLAY));
+		set_bit(KGSL_FT_REPLAY, &cmdbatch->fault_recovery);
 		return;
 	}
 
 	/*
 	 * Skip the last IB1 that was played but replay everything else.
-	 * Note that the last IB1 might not be in the "hung" cmdobj
+	 * Note that the last IB1 might not be in the "hung" command batch
 	 * because the CP may have caused a page-fault while it was prefetching
 	 * the next IB1/IB2. walk all outstanding commands and zap the
 	 * supposedly bad IB1 where ever it lurks.
 	 */
 
-	if (test_and_clear_bit(KGSL_FT_SKIPIB, &cmdobj->fault_policy)) {
-		trace_adreno_cmdbatch_recovery(cmdobj, BIT(KGSL_FT_SKIPIB));
-		set_bit(KGSL_FT_SKIPIB, &cmdobj->fault_recovery);
+	if (test_and_clear_bit(KGSL_FT_SKIPIB, &cmdbatch->fault_policy)) {
+		trace_adreno_cmdbatch_recovery(cmdbatch, BIT(KGSL_FT_SKIPIB));
+		set_bit(KGSL_FT_SKIPIB, &cmdbatch->fault_recovery);
 
 		for (i = 0; i < count; i++) {
 			if (replay[i] != NULL &&
-				DRAWOBJ(replay[i])->context->id ==
-					drawobj->context->id)
-				_skip_ib(replay[i], base);
+				replay[i]->context->id == cmdbatch->context->id)
+				cmdbatch_skip_ib(replay[i], base);
 		}
 
 		return;
 	}
 
-	/* Skip the faulted cmdobj submission */
-	if (test_and_clear_bit(KGSL_FT_SKIPCMD, &cmdobj->fault_policy)) {
-		trace_adreno_cmdbatch_recovery(cmdobj, BIT(KGSL_FT_SKIPCMD));
+	/* Skip the faulted command batch submission */
+	if (test_and_clear_bit(KGSL_FT_SKIPCMD, &cmdbatch->fault_policy)) {
+		trace_adreno_cmdbatch_recovery(cmdbatch, BIT(KGSL_FT_SKIPCMD));
 
-		/* Skip faulting cmdobj */
-		_skip_cmd(cmdobj, replay, count);
+		/* Skip faulting command batch */
+		cmdbatch_skip_cmd(cmdbatch, replay, count);
 
 		return;
 	}
 
-	if (test_and_clear_bit(KGSL_FT_SKIPFRAME, &cmdobj->fault_policy)) {
-		trace_adreno_cmdbatch_recovery(cmdobj,
+	if (test_and_clear_bit(KGSL_FT_SKIPFRAME, &cmdbatch->fault_policy)) {
+		trace_adreno_cmdbatch_recovery(cmdbatch,
 			BIT(KGSL_FT_SKIPFRAME));
-		set_bit(KGSL_FT_SKIPFRAME, &cmdobj->fault_recovery);
+		set_bit(KGSL_FT_SKIPFRAME, &cmdbatch->fault_recovery);
 
 		/*
-		 * Skip all the pending cmdobj's for this context until
+		 * Skip all the pending command batches for this context until
 		 * the EOF frame is seen
 		 */
-		_skip_frame(cmdobj, replay, count);
+		cmdbatch_skip_frame(cmdbatch, replay, count);
 		return;
 	}
 
 	/* If we get here then all the policies failed */
 
-	pr_context(device, drawobj->context, "gpu %s ctx %d ts %d\n",
-		state, drawobj->context->id, drawobj->timestamp);
+	pr_context(device, cmdbatch->context, "gpu %s ctx %d ts %d\n",
+		state, cmdbatch->context->id, cmdbatch->timestamp);
 
 	/* Mark the context as failed */
-	mark_guilty_context(device, drawobj->context->id);
+	mark_guilty_context(device, cmdbatch->context->id);
 
 	/* Invalidate the context */
-	adreno_drawctxt_invalidate(device, drawobj->context);
+	adreno_drawctxt_invalidate(device, cmdbatch->context);
 }
 
 /**
@@ -1953,35 +1634,33 @@ static void process_cmdobj_fault(struct kgsl_device *device,
  * @base: The IB1 base during the fault
  */
 static void recover_dispatch_q(struct kgsl_device *device,
-		struct adreno_dispatcher_drawqueue *dispatch_q,
+		struct adreno_dispatcher_cmdqueue *dispatch_q,
 		int fault,
 		unsigned int base)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct kgsl_drawobj_cmd **replay;
+	struct kgsl_cmdbatch **replay = NULL;
 	unsigned int ptr;
 	int first = 0;
 	int count = 0;
 	int i;
 
 	/* Allocate memory to store the inflight commands */
-	replay = kcalloc(dispatch_q->inflight, sizeof(*replay), GFP_KERNEL);
+	replay = kzalloc(sizeof(*replay) * dispatch_q->inflight, GFP_KERNEL);
 
 	if (replay == NULL) {
 		unsigned int ptr = dispatch_q->head;
 
 		/* Recovery failed - mark everybody on this q guilty */
 		while (ptr != dispatch_q->tail) {
-			struct kgsl_drawobj_cmd *cmdobj =
-						dispatch_q->cmd_q[ptr];
-			struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
+			struct kgsl_context *context =
+				dispatch_q->cmd_q[ptr]->context;
 
-			mark_guilty_context(device, drawobj->context->id);
-			adreno_drawctxt_invalidate(device, drawobj->context);
-			kgsl_drawobj_destroy(drawobj);
+			mark_guilty_context(device, context->id);
+			adreno_drawctxt_invalidate(device, context);
+			kgsl_cmdbatch_destroy(dispatch_q->cmd_q[ptr]);
 
-			ptr = DRAWQUEUE_NEXT(ptr,
-				ADRENO_DISPATCH_DRAWQUEUE_SIZE);
+			ptr = CMDQUEUE_NEXT(ptr, ADRENO_DISPATCH_CMDQUEUE_SIZE);
 		}
 
 		/*
@@ -1993,22 +1672,22 @@ static void recover_dispatch_q(struct kgsl_device *device,
 		goto replay;
 	}
 
-	/* Copy the inflight cmdobj's into the temporary storage */
+	/* Copy the inflight command batches into the temporary storage */
 	ptr = dispatch_q->head;
 
 	while (ptr != dispatch_q->tail) {
 		replay[count++] = dispatch_q->cmd_q[ptr];
-		ptr = DRAWQUEUE_NEXT(ptr, ADRENO_DISPATCH_DRAWQUEUE_SIZE);
+		ptr = CMDQUEUE_NEXT(ptr, ADRENO_DISPATCH_CMDQUEUE_SIZE);
 	}
 
 	if (fault && count)
-		process_cmdobj_fault(device, replay,
+		process_cmdbatch_fault(device, replay,
 					count, base, fault);
 replay:
 	dispatch_q->inflight = 0;
 	dispatch_q->head = dispatch_q->tail = 0;
-	/* Remove any pending cmdobj's that have been invalidated */
-	remove_invalidated_cmdobjs(device, replay, count);
+	/* Remove any pending command batches that have been invalidated */
+	remove_invalidated_cmdbatches(device, replay, count);
 
 	/* Replay the pending command buffers */
 	for (i = 0; i < count; i++) {
@@ -2024,16 +1703,16 @@ static void recover_dispatch_q(struct kgsl_device *device,
 		 */
 
 		if (first == 0) {
-			set_bit(CMDOBJ_FORCE_PREAMBLE, &replay[i]->priv);
+			set_bit(CMDBATCH_FLAG_FORCE_PREAMBLE, &replay[i]->priv);
 			first = 1;
 		}
 
 		/*
-		 * Force each cmdobj to wait for idle - this avoids weird
+		 * Force each command batch to wait for idle - this avoids weird
 		 * CP parse issues
 		 */
 
-		set_bit(CMDOBJ_WFI, &replay[i]->priv);
+		set_bit(CMDBATCH_FLAG_WFI, &replay[i]->priv);
 
 		ret = sendcmd(adreno_dev, replay[i]);
 
@@ -2043,18 +1722,15 @@ static void recover_dispatch_q(struct kgsl_device *device,
 		 */
 
 		if (ret) {
-			pr_context(device, replay[i]->base.context,
+			pr_context(device, replay[i]->context,
 				"gpu reset failed ctx %d ts %d\n",
-				replay[i]->base.context->id,
-				replay[i]->base.timestamp);
+				replay[i]->context->id, replay[i]->timestamp);
 
 			/* Mark this context as guilty (failed recovery) */
-			mark_guilty_context(device,
-				replay[i]->base.context->id);
+			mark_guilty_context(device, replay[i]->context->id);
 
-			adreno_drawctxt_invalidate(device,
-				replay[i]->base.context);
-			remove_invalidated_cmdobjs(device, &replay[i],
+			adreno_drawctxt_invalidate(device, replay[i]->context);
+			remove_invalidated_cmdbatches(device, &replay[i],
 				count - i);
 		}
 	}
@@ -2065,86 +1741,32 @@ static void recover_dispatch_q(struct kgsl_device *device,
 	kfree(replay);
 }
 
-static void do_header_and_snapshot(struct kgsl_device *device, int fault,
-		struct adreno_ringbuffer *rb, struct kgsl_drawobj_cmd *cmdobj)
-{
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-
-	/* Always dump the snapshot on a non-drawobj failure */
-	if (cmdobj == NULL) {
-		adreno_fault_header(device, rb, NULL);
-		kgsl_device_snapshot(device, NULL, fault & ADRENO_GMU_FAULT);
-		return;
-	}
-
-	/* Skip everything if the PMDUMP flag is set */
-	if (test_bit(KGSL_FT_SKIP_PMDUMP, &cmdobj->fault_policy))
-		return;
-
-	/* Print the fault header */
-	adreno_fault_header(device, rb, cmdobj);
-
-	if (!(drawobj->context->flags & KGSL_CONTEXT_NO_SNAPSHOT))
-		kgsl_device_snapshot(device, drawobj->context,
-					fault & ADRENO_GMU_FAULT);
-}
-
 static int dispatcher_do_fault(struct adreno_device *adreno_dev)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
-	struct adreno_dispatcher_drawqueue *dispatch_q = NULL, *dispatch_q_temp;
+	struct adreno_dispatcher_cmdqueue *dispatch_q = NULL, *dispatch_q_temp;
 	struct adreno_ringbuffer *rb;
 	struct adreno_ringbuffer *hung_rb = NULL;
 	unsigned int reg;
-	uint64_t base = 0;
-	struct kgsl_drawobj_cmd *cmdobj = NULL;
+	uint64_t base;
+	struct kgsl_cmdbatch *cmdbatch = NULL;
 	int ret, i;
 	int fault;
 	int halt;
-	bool gx_on = true;
 
 	fault = atomic_xchg(&dispatcher->fault, 0);
 	if (fault == 0)
 		return 0;
 
-	/* Mask all GMU interrupts */
-	if (kgsl_gmu_isenabled(device)) {
-		adreno_write_gmureg(adreno_dev,
-			ADRENO_REG_GMU_AO_HOST_INTERRUPT_MASK,
-			0xFFFFFFFF);
-		adreno_write_gmureg(adreno_dev,
-			ADRENO_REG_GMU_GMU2HOST_INTR_MASK,
-			0xFFFFFFFF);
-	}
-
-	if (gpudev->gx_is_on)
-		gx_on = gpudev->gx_is_on(adreno_dev);
-
-	/*
-	 * In the very unlikely case that the power is off, do nothing - the
-	 * state will be reset on power up and everybody will be happy
-	 */
-
-	if (!kgsl_state_is_awake(device) && (fault & ADRENO_SOFT_FAULT)) {
-		/* Clear the existing register values */
-		memset(adreno_ft_regs_val, 0,
-				adreno_ft_regs_num * sizeof(unsigned int));
-		return 0;
-	}
-
 	/*
-	 * On A5xx and A6xx, read RBBM_STATUS3:SMMU_STALLED_ON_FAULT (BIT 24)
-	 * to tell if this function was entered after a pagefault. If so, only
+	 * On A5xx, read RBBM_STATUS3:SMMU_STALLED_ON_FAULT (BIT 24) to
+	 * tell if this function was entered after a pagefault. If so, only
 	 * proceed if the fault handler has already run in the IRQ thread,
 	 * else return early to give the fault handler a chance to run.
 	 */
-	if (!(fault & ADRENO_IOMMU_PAGE_FAULT) &&
-		(adreno_is_a5xx(adreno_dev) || adreno_is_a6xx(adreno_dev)) &&
-		gx_on) {
+	if (!(fault & ADRENO_IOMMU_PAGE_FAULT) && adreno_is_a5xx(adreno_dev)) {
 		unsigned int val;
-
 		mutex_lock(&device->mutex);
 		adreno_readreg(adreno_dev, ADRENO_REG_RBBM_STATUS3, &val);
 		mutex_unlock(&device->mutex);
@@ -2155,41 +1777,40 @@ static int dispatcher_do_fault(struct adreno_device *adreno_dev)
 	/* Turn off all the timers */
 	del_timer_sync(&dispatcher->timer);
 	del_timer_sync(&dispatcher->fault_timer);
-	/*
-	 * Deleting uninitialized timer will block for ever on kernel debug
-	 * disable build. Hence skip del timer if it is not initialized.
-	 */
-	if (adreno_is_preemption_enabled(adreno_dev))
-		del_timer_sync(&adreno_dev->preempt.timer);
+	del_timer_sync(&dispatcher->preempt_timer);
 
 	mutex_lock(&device->mutex);
 
-	if (gx_on)
-		adreno_readreg64(adreno_dev, ADRENO_REG_CP_RB_BASE,
-			ADRENO_REG_CP_RB_BASE_HI, &base);
+	/* hang opcode */
+	kgsl_cffdump_hang(device);
+
+	adreno_readreg64(adreno_dev, ADRENO_REG_CP_RB_BASE,
+		ADRENO_REG_CP_RB_BASE_HI, &base);
 
 	/*
 	 * Force the CP off for anything but a hard fault to make sure it is
 	 * good and stopped
 	 */
-	if (!(fault & ADRENO_HARD_FAULT) && gx_on) {
+	if (!(fault & ADRENO_HARD_FAULT)) {
 		adreno_readreg(adreno_dev, ADRENO_REG_CP_ME_CNTL, &reg);
-		if (adreno_is_a5xx(adreno_dev) || adreno_is_a6xx(adreno_dev))
+		if (adreno_is_a5xx(adreno_dev))
 			reg |= 1 | (1 << 1);
 		else
 			reg |= (1 << 27) | (1 << 28);
 		adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_CNTL, reg);
 	}
 	/*
-	 * retire cmdobj's from all the dispatch_q's before starting recovery
+	 * retire cmdbatches from all the dispatch_q's before starting recovery
 	 */
 	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		adreno_dispatch_retire_drawqueue(adreno_dev,
-			&(rb->dispatch_q));
+		adreno_dispatch_process_cmdqueue(adreno_dev,
+			&(rb->dispatch_q), 0);
 		/* Select the active dispatch_q */
 		if (base == rb->buffer_desc.gpuaddr) {
 			dispatch_q = &(rb->dispatch_q);
 			hung_rb = rb;
+			adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR,
+				&hung_rb->rptr);
 			if (adreno_dev->cur_rb != hung_rb) {
 				adreno_dev->prev_rb = adreno_dev->cur_rb;
 				adreno_dev->cur_rb = hung_rb;
@@ -2203,20 +1824,25 @@ static int dispatcher_do_fault(struct adreno_device *adreno_dev)
 		}
 	}
 
-	if (dispatch_q && !adreno_drawqueue_is_empty(dispatch_q)) {
-		cmdobj = dispatch_q->cmd_q[dispatch_q->head];
-		trace_adreno_cmdbatch_fault(cmdobj, fault);
+	if (dispatch_q && (dispatch_q->tail != dispatch_q->head)) {
+		cmdbatch = dispatch_q->cmd_q[dispatch_q->head];
+		trace_adreno_cmdbatch_fault(cmdbatch, fault);
 	}
 
-	if (gx_on)
-		adreno_readreg64(adreno_dev, ADRENO_REG_CP_IB1_BASE,
-			ADRENO_REG_CP_IB1_BASE_HI, &base);
+	adreno_readreg64(adreno_dev, ADRENO_REG_CP_IB1_BASE,
+		ADRENO_REG_CP_IB1_BASE_HI, &base);
 
-	do_header_and_snapshot(device, fault, hung_rb, cmdobj);
+	/*
+	 * Dump the snapshot information if this is the first
+	 * detected fault for the oldest active command batch
+	 */
 
-	/* Turn off the KEEPALIVE vote from the ISR for hard fault */
-	if (gpudev->gpu_keepalive && fault & ADRENO_HARD_FAULT)
-		gpudev->gpu_keepalive(adreno_dev, false);
+	if (cmdbatch == NULL ||
+		!test_bit(KGSL_FT_SKIP_PMDUMP, &cmdbatch->fault_policy)) {
+		adreno_fault_header(device, hung_rb, cmdbatch);
+		kgsl_device_snapshot(device,
+			cmdbatch ? cmdbatch->context : NULL);
+	}
 
 	/* Terminate the stalled transaction and resume the IOMMU */
 	if (fault & ADRENO_IOMMU_PAGE_FAULT)
@@ -2224,6 +1850,8 @@ static int dispatcher_do_fault(struct adreno_device *adreno_dev)
 
 	/* Reset the dispatcher queue */
 	dispatcher->inflight = 0;
+	atomic_set(&dispatcher->preemption_state,
+		ADRENO_DISPATCHER_PREEMPT_CLEAR);
 
 	/* Reset the GPU and make sure halt is not set during recovery */
 	halt = adreno_gpu_halt(adreno_dev);
@@ -2237,22 +1865,18 @@ static int dispatcher_do_fault(struct adreno_device *adreno_dev)
 
 	if (hung_rb != NULL) {
 		kgsl_sharedmem_writel(device, &device->memstore,
-				MEMSTORE_RB_OFFSET(hung_rb, soptimestamp),
-				hung_rb->timestamp);
+			KGSL_MEMSTORE_OFFSET(KGSL_MEMSTORE_MAX + hung_rb->id,
+				soptimestamp), hung_rb->timestamp);
 
 		kgsl_sharedmem_writel(device, &device->memstore,
-				MEMSTORE_RB_OFFSET(hung_rb, eoptimestamp),
-				hung_rb->timestamp);
+			KGSL_MEMSTORE_OFFSET(KGSL_MEMSTORE_MAX + hung_rb->id,
+				eoptimestamp), hung_rb->timestamp);
 
 		/* Schedule any pending events to be run */
 		kgsl_process_event_group(device, &hung_rb->events);
 	}
 
-	if (gpudev->reset)
-		ret = gpudev->reset(device, fault);
-	else
-		ret = adreno_reset(device, fault);
-
+	ret = adreno_reset(device, fault);
 	mutex_unlock(&device->mutex);
 	/* if any other fault got in until reset then ignore */
 	atomic_set(&dispatcher->fault, 0);
@@ -2271,35 +1895,26 @@ static int dispatcher_do_fault(struct adreno_device *adreno_dev)
 
 	atomic_add(halt, &adreno_dev->halt);
 
-	/*
-	 * At this point it is safe to assume that we recovered. Setting
-	 * this field allows us to take a new snapshot for the next failure
-	 * if we are prioritizing the first unrecoverable snapshot.
-	 */
-	if (device->snapshot)
-		device->snapshot->recovered = true;
-
 	return 1;
 }
 
-static inline int drawobj_consumed(struct kgsl_drawobj *drawobj,
+static inline int cmdbatch_consumed(struct kgsl_cmdbatch *cmdbatch,
 		unsigned int consumed, unsigned int retired)
 {
-	return ((timestamp_cmp(drawobj->timestamp, consumed) >= 0) &&
-		(timestamp_cmp(retired, drawobj->timestamp) < 0));
+	return ((timestamp_cmp(cmdbatch->timestamp, consumed) >= 0) &&
+		(timestamp_cmp(retired, cmdbatch->timestamp) < 0));
 }
 
 static void _print_recovery(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj)
+		struct kgsl_cmdbatch *cmdbatch)
 {
 	static struct {
 		unsigned int mask;
 		const char *str;
 	} flags[] = { ADRENO_FT_TYPES };
 
-	int i, nr = find_first_bit(&cmdobj->fault_recovery, BITS_PER_LONG);
+	int i, nr = find_first_bit(&cmdbatch->fault_recovery, BITS_PER_LONG);
 	char *result = "unknown";
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
 
 	for (i = 0; i < ARRAY_SIZE(flags); i++) {
 		if (flags[i].mask == BIT(nr)) {
@@ -2308,201 +1923,159 @@ static void _print_recovery(struct kgsl_device *device,
 		}
 	}
 
-	pr_context(device, drawobj->context,
+	pr_context(device, cmdbatch->context,
 		"gpu %s ctx %d ts %d policy %lX\n",
-		result, drawobj->context->id, drawobj->timestamp,
-		cmdobj->fault_recovery);
+		result, cmdbatch->context->id, cmdbatch->timestamp,
+		cmdbatch->fault_recovery);
 }
 
-static void cmdobj_profile_ticks(struct adreno_device *adreno_dev,
-	struct kgsl_drawobj_cmd *cmdobj, uint64_t *start, uint64_t *retire)
+static void cmdbatch_profile_ticks(struct adreno_device *adreno_dev,
+	struct kgsl_cmdbatch *cmdbatch, uint64_t *start, uint64_t *retire)
 {
-	void *ptr = adreno_dev->profile_buffer.hostptr;
-	struct adreno_drawobj_profile_entry *entry;
+	void *ptr = adreno_dev->cmdbatch_profile_buffer.hostptr;
+	struct adreno_cmdbatch_profile_entry *entry;
 
-	entry = (struct adreno_drawobj_profile_entry *)
-		(ptr + (cmdobj->profile_index * sizeof(*entry)));
+	entry = (struct adreno_cmdbatch_profile_entry *)
+		(ptr + (cmdbatch->profile_index * sizeof(*entry)));
 
-	/* get updated values of started and retired */
 	rmb();
 	*start = entry->started;
 	*retire = entry->retired;
 }
 
-static void retire_cmdobj(struct adreno_device *adreno_dev,
-		struct kgsl_drawobj_cmd *cmdobj)
+int adreno_dispatch_process_cmdqueue(struct adreno_device *adreno_dev,
+				struct adreno_dispatcher_cmdqueue *dispatch_q,
+				int long_ib_detect)
 {
-	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-	struct adreno_context *drawctxt = ADRENO_CONTEXT(drawobj->context);
-	uint64_t start = 0, end = 0;
-
-	if (cmdobj->fault_recovery != 0) {
-		set_bit(ADRENO_CONTEXT_FAULT, &drawobj->context->priv);
-		_print_recovery(KGSL_DEVICE(adreno_dev), cmdobj);
-	}
-
-	if (test_bit(CMDOBJ_PROFILE, &cmdobj->priv))
-		cmdobj_profile_ticks(adreno_dev, cmdobj, &start, &end);
+	struct adreno_dispatcher *dispatcher = &(adreno_dev->dispatcher);
+	uint64_t start_ticks = 0, retire_ticks = 0;
 
-	/*
-	 * For A3xx we still get the rptr from the CP_RB_RPTR instead of
-	 * rptr scratch out address. At this point GPU clocks turned off.
-	 * So avoid reading GPU register directly for A3xx.
-	 */
-	if (adreno_is_a3xx(adreno_dev))
-		trace_adreno_cmdbatch_retired(drawobj,
-			(int) dispatcher->inflight, start, end,
-			ADRENO_DRAWOBJ_RB(drawobj), 0, cmdobj->fault_recovery);
-	else
-		trace_adreno_cmdbatch_retired(drawobj,
-			(int) dispatcher->inflight, start, end,
-			ADRENO_DRAWOBJ_RB(drawobj),
-			adreno_get_rptr(drawctxt->rb), cmdobj->fault_recovery);
-
-	drawctxt->submit_retire_ticks[drawctxt->ticks_index] =
-		end - cmdobj->submit_ticks;
-
-	drawctxt->ticks_index = (drawctxt->ticks_index + 1) %
-		SUBMIT_RETIRE_TICKS_SIZE;
-
-	kgsl_drawobj_destroy(drawobj);
-}
-
-static int adreno_dispatch_retire_drawqueue(struct adreno_device *adreno_dev,
-		struct adreno_dispatcher_drawqueue *drawqueue)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+	struct adreno_dispatcher_cmdqueue *active_q =
+			&(adreno_dev->cur_rb->dispatch_q);
 	int count = 0;
 
-	while (!adreno_drawqueue_is_empty(drawqueue)) {
-		struct kgsl_drawobj_cmd *cmdobj =
-			drawqueue->cmd_q[drawqueue->head];
-		struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-
-		if (!kgsl_check_timestamp(device, drawobj->context,
-			drawobj->timestamp))
-			break;
-
-		retire_cmdobj(adreno_dev, cmdobj);
-
-		dispatcher->inflight--;
-		drawqueue->inflight--;
-
-		drawqueue->cmd_q[drawqueue->head] = NULL;
+	while (dispatch_q->head != dispatch_q->tail) {
+		struct kgsl_cmdbatch *cmdbatch =
+			dispatch_q->cmd_q[dispatch_q->head];
+		struct adreno_context *drawctxt;
+		BUG_ON(cmdbatch == NULL);
 
-		drawqueue->head = DRAWQUEUE_NEXT(drawqueue->head,
-			ADRENO_DISPATCH_DRAWQUEUE_SIZE);
+		drawctxt = ADRENO_CONTEXT(cmdbatch->context);
 
-		count++;
-	}
+		/*
+		 * First try to expire the timestamp. This happens if the
+		 * context is valid and the timestamp expired normally or if the
+		 * context was destroyed before the command batch was finished
+		 * in the GPU.  Either way retire the command batch advance the
+		 * pointers and continue processing the queue
+		 */
 
-	return count;
-}
+		if (kgsl_check_timestamp(KGSL_DEVICE(adreno_dev),
+			cmdbatch->context, cmdbatch->timestamp)) {
 
-static void _adreno_dispatch_check_timeout(struct adreno_device *adreno_dev,
-		struct adreno_dispatcher_drawqueue *drawqueue)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct kgsl_drawobj *drawobj =
-			DRAWOBJ(drawqueue->cmd_q[drawqueue->head]);
+			/*
+			 * If the cmdbatch in question had faulted announce its
+			 * successful completion to the world
+			 */
 
-	/* Don't timeout if the timer hasn't expired yet (duh) */
-	if (time_is_after_jiffies(drawqueue->expires))
-		return;
+			if (cmdbatch->fault_recovery != 0) {
+				/* Mark the context as faulted and recovered */
+				set_bit(ADRENO_CONTEXT_FAULT,
+					&cmdbatch->context->priv);
 
-	/* Don't timeout if the IB timeout is disabled globally */
-	if (!adreno_long_ib_detect(adreno_dev))
-		return;
+				_print_recovery(KGSL_DEVICE(adreno_dev),
+					cmdbatch);
+			}
 
-	/* Don't time out if the context has disabled it */
-	if (drawobj->context->flags & KGSL_CONTEXT_NO_FAULT_TOLERANCE)
-		return;
+			/* Reduce the number of inflight command batches */
+			dispatcher->inflight--;
+			dispatch_q->inflight--;
 
-	pr_context(device, drawobj->context, "gpu timeout ctx %d ts %d\n",
-		drawobj->context->id, drawobj->timestamp);
+			/*
+			 * If kernel profiling is enabled get the submit and
+			 * retired ticks from the buffer
+			 */
 
-	adreno_set_gpu_fault(adreno_dev, ADRENO_TIMEOUT_FAULT);
-}
+			if (test_bit(CMDBATCH_FLAG_PROFILE, &cmdbatch->priv))
+				cmdbatch_profile_ticks(adreno_dev, cmdbatch,
+					&start_ticks, &retire_ticks);
 
-static int adreno_dispatch_process_drawqueue(struct adreno_device *adreno_dev,
-		struct adreno_dispatcher_drawqueue *drawqueue)
-{
-	int count = adreno_dispatch_retire_drawqueue(adreno_dev, drawqueue);
+			trace_adreno_cmdbatch_retired(cmdbatch,
+				(int) dispatcher->inflight, start_ticks,
+				retire_ticks, ADRENO_CMDBATCH_RB(cmdbatch));
 
-	/* Nothing to do if there are no pending commands */
-	if (adreno_drawqueue_is_empty(drawqueue))
-		return count;
+			/* Record the delta between submit and retire ticks */
+			drawctxt->submit_retire_ticks[drawctxt->ticks_index] =
+				retire_ticks - cmdbatch->submit_ticks;
 
-	/* Don't update the drawqueue timeout if it isn't active */
-	if (!drawqueue_is_current(drawqueue))
-		return count;
+			drawctxt->ticks_index = (drawctxt->ticks_index + 1)
+				% SUBMIT_RETIRE_TICKS_SIZE;
 
-	/*
-	 * If the current ringbuffer retired any commands then universally
-	 * reset the timeout
-	 */
+			/* Zero the old entry*/
+			dispatch_q->cmd_q[dispatch_q->head] = NULL;
 
-	if (count) {
-		drawqueue->expires = jiffies +
-			msecs_to_jiffies(adreno_drawobj_timeout);
-		return count;
-	}
+			/* Advance the buffer head */
+			dispatch_q->head = CMDQUEUE_NEXT(dispatch_q->head,
+				ADRENO_DISPATCH_CMDQUEUE_SIZE);
 
-	/*
-	 * If we get here then 1) the ringbuffer is current and 2) we haven't
-	 * retired anything.  Check to see if the timeout if valid for the
-	 * current drawobj and fault if it has expired
-	 */
-	_adreno_dispatch_check_timeout(adreno_dev, drawqueue);
-	return 0;
-}
+			/* Destroy the retired command batch */
+			kgsl_cmdbatch_destroy(cmdbatch);
 
-/* Update the dispatcher timers */
-static void _dispatcher_update_timers(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+			/* Update the expire time for the next command batch */
 
-	/* Kick the idle timer */
-	mutex_lock(&device->mutex);
-	kgsl_pwrscale_update(device);
-	mod_timer(&device->idle_timer,
-		jiffies + device->pwrctrl.interval_timeout);
-	mutex_unlock(&device->mutex);
+			if (dispatch_q->inflight > 0 &&
+				dispatch_q == active_q) {
+				cmdbatch =
+					dispatch_q->cmd_q[dispatch_q->head];
+				cmdbatch->expires = jiffies +
+					msecs_to_jiffies(
+					adreno_cmdbatch_timeout);
+			}
 
-	/* Check to see if we need to update the command timer */
-	if (adreno_in_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE)) {
-		struct adreno_dispatcher_drawqueue *drawqueue =
-			DRAWQUEUE(adreno_dev->cur_rb);
+			count++;
+			continue;
+		}
+		/*
+		 * Break here if fault detection is disabled for the context or
+		 * if the long running IB detection is disaled device wide or
+		 * if the dispatch q is not active
+		 * Long running command buffers will be allowed to run to
+		 * completion - but badly behaving command buffers (infinite
+		 * shaders etc) can end up running forever.
+		 */
 
-		if (!adreno_drawqueue_is_empty(drawqueue))
-			mod_timer(&dispatcher->timer, drawqueue->expires);
-	}
-}
+		if (!long_ib_detect ||
+			drawctxt->base.flags & KGSL_CONTEXT_NO_FAULT_TOLERANCE
+			|| dispatch_q != active_q)
+			break;
 
-/* Take down the dispatcher and release any power states */
-static void _dispatcher_power_down(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
+		/*
+		 * The last line of defense is to check if the command batch has
+		 * timed out. If we get this far but the timeout hasn't expired
+		 * yet then the GPU is still ticking away
+		 */
 
-	mutex_lock(&device->mutex);
+		if (time_is_after_jiffies(cmdbatch->expires))
+			break;
 
-	if (test_and_clear_bit(ADRENO_DISPATCHER_ACTIVE, &dispatcher->priv))
-		complete_all(&dispatcher->idle_gate);
+		/* Boom goes the dynamite */
 
-	del_timer_sync(&dispatcher->fault_timer);
+		pr_context(KGSL_DEVICE(adreno_dev), cmdbatch->context,
+			"gpu timeout ctx %d ts %d\n",
+			cmdbatch->context->id, cmdbatch->timestamp);
 
-	if (test_bit(ADRENO_DISPATCHER_POWER, &dispatcher->priv)) {
-		kgsl_active_count_put(device);
-		clear_bit(ADRENO_DISPATCHER_POWER, &dispatcher->priv);
+		adreno_set_gpu_fault(adreno_dev, ADRENO_TIMEOUT_FAULT);
+		break;
 	}
-
-	mutex_unlock(&device->mutex);
+	return count;
 }
 
+/**
+ * adreno_dispatcher_work() - Master work handler for the dispatcher
+ * @work: Pointer to the work struct for the current work queue
+ *
+ * Process expired commands and send new ones.
+ */
 static void adreno_dispatcher_work(struct kthread_work *work)
 {
 	struct adreno_dispatcher *dispatcher =
@@ -2512,48 +2085,94 @@ static void adreno_dispatcher_work(struct kthread_work *work)
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	int count = 0;
-	unsigned int i = 0;
+	int cur_rb_id = adreno_dev->cur_rb->id;
 
 	mutex_lock(&dispatcher->mutex);
 
-	/*
-	 * As long as there are inflight commands, process retired comamnds from
-	 * all drawqueues
-	 */
-	for (i = 0; i < adreno_dev->num_ringbuffers; i++) {
-		struct adreno_dispatcher_drawqueue *drawqueue =
-			DRAWQUEUE(&adreno_dev->ringbuffers[i]);
+	if (ADRENO_DISPATCHER_PREEMPT_CLEAR ==
+		atomic_read(&dispatcher->preemption_state))
+		/* process the active q*/
+		count = adreno_dispatch_process_cmdqueue(adreno_dev,
+			&(adreno_dev->cur_rb->dispatch_q),
+			adreno_long_ib_detect(adreno_dev));
 
-		count += adreno_dispatch_process_drawqueue(adreno_dev,
-			drawqueue);
-		if (dispatcher->inflight == 0)
-			break;
-	}
+	else if (ADRENO_DISPATCHER_PREEMPT_TRIGGERED ==
+		atomic_read(&dispatcher->preemption_state))
+		count = adreno_dispatch_process_cmdqueue(adreno_dev,
+			&(adreno_dev->cur_rb->dispatch_q), 0);
 
-	kgsl_process_event_groups(device);
+	/* Check if gpu fault occurred */
+	if (dispatcher_do_fault(adreno_dev))
+		goto done;
+
+	if (gpudev->preemption_schedule)
+		gpudev->preemption_schedule(adreno_dev);
 
+	if (cur_rb_id != adreno_dev->cur_rb->id) {
+		struct adreno_dispatcher_cmdqueue *dispatch_q =
+			&(adreno_dev->cur_rb->dispatch_q);
+		/* active level switched, clear new level cmdbatches */
+		count = adreno_dispatch_process_cmdqueue(adreno_dev,
+			dispatch_q,
+			adreno_long_ib_detect(adreno_dev));
+		/*
+		 * If GPU has already completed all the commands in new incoming
+		 * RB then we may not get another interrupt due to which
+		 * dispatcher may not run again. Schedule dispatcher here so
+		 * we can come back and process the other RB's if required
+		 */
+		if (dispatch_q->head == dispatch_q->tail)
+			adreno_dispatcher_schedule(device);
+	}
 	/*
-	 * dispatcher_do_fault() returns 0 if no faults occurred. If that is the
-	 * case, then clean up preemption and try to schedule more work
+	 * If inflight went to 0, queue back up the event processor to catch
+	 * stragglers
 	 */
-	if (dispatcher_do_fault(adreno_dev) == 0) {
+	if (dispatcher->inflight == 0 && count)
+		kgsl_schedule_work(&device->event_work);
 
-		/* Clean up after preemption */
-		if (gpudev->preemption_schedule)
-			gpudev->preemption_schedule(adreno_dev);
+	/* Try to dispatch new commands */
+	_adreno_dispatcher_issuecmds(adreno_dev);
 
-		/* Run the scheduler for to dispatch new commands */
-		_adreno_dispatcher_issuecmds(adreno_dev);
-	}
+done:
+	/* Either update the timer for the next command batch or disable it */
+	if (dispatcher->inflight) {
+		struct kgsl_cmdbatch *cmdbatch =
+			adreno_dev->cur_rb->dispatch_q.cmd_q[
+				adreno_dev->cur_rb->dispatch_q.head];
+		if (cmdbatch && adreno_preempt_state(adreno_dev,
+					ADRENO_DISPATCHER_PREEMPT_CLEAR))
+			/* Update the timeout timer for the next cmdbatch */
+			mod_timer(&dispatcher->timer, cmdbatch->expires);
+
+		/* There are still things in flight - update the idle counts */
+		mutex_lock(&device->mutex);
+		kgsl_pwrscale_update(device);
+		mod_timer(&device->idle_timer, jiffies +
+				device->pwrctrl.interval_timeout);
+		mutex_unlock(&device->mutex);
+	} else {
+		/* There is nothing left in the pipeline.  Shut 'er down boys */
+		mutex_lock(&device->mutex);
 
-	/*
-	 * If there are commands pending, update the timers, otherwise release
-	 * the power state to prepare for power down
-	 */
-	if (dispatcher->inflight > 0)
-		_dispatcher_update_timers(adreno_dev);
-	else
-		_dispatcher_power_down(adreno_dev);
+		if (test_and_clear_bit(ADRENO_DISPATCHER_ACTIVE,
+			&dispatcher->priv))
+			complete_all(&dispatcher->idle_gate);
+
+		/*
+		 * Stop the fault timer before decrementing the active count to
+		 * avoid reading the hardware registers while we are trying to
+		 * turn clocks off
+		 */
+		del_timer_sync(&dispatcher->fault_timer);
+
+		if (test_bit(ADRENO_DISPATCHER_POWER, &dispatcher->priv)) {
+			kgsl_active_count_put(device);
+			clear_bit(ADRENO_DISPATCHER_POWER, &dispatcher->priv);
+		}
+
+		mutex_unlock(&device->mutex);
+	}
 
 	mutex_unlock(&dispatcher->mutex);
 }
@@ -2585,7 +2204,7 @@ void adreno_dispatcher_queue_context(struct kgsl_device *device,
 }
 
 /*
- * This is called on a regular basis while cmdobj's are inflight.  Fault
+ * This is called on a regular basis while command batches are inflight.  Fault
  * detection registers are read and compared to the existing values - if they
  * changed then the GPU is still running.  If they are the same between
  * subsequent calls then the GPU may have faulted
@@ -2613,7 +2232,7 @@ static void adreno_dispatcher_fault_timer(unsigned long data)
 	if (!fault_detect_read_compare(adreno_dev)) {
 		adreno_set_gpu_fault(adreno_dev, ADRENO_SOFT_FAULT);
 		adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
-	} else if (dispatcher->inflight > 0) {
+	} else {
 		mod_timer(&dispatcher->fault_timer,
 			jiffies + msecs_to_jiffies(_fault_timer_interval));
 	}
@@ -2637,7 +2256,7 @@ static void adreno_dispatcher_timer(unsigned long data)
  */
 void adreno_dispatcher_start(struct kgsl_device *device)
 {
-	complete_all(&device->halt_gate);
+	complete_all(&device->cmdbatch_gate);
 
 	/* Schedule the work loop to get things going */
 	adreno_dispatcher_schedule(device);
@@ -2657,20 +2276,6 @@ void adreno_dispatcher_stop(struct adreno_device *adreno_dev)
 	del_timer_sync(&dispatcher->fault_timer);
 }
 
-/**
- * adreno_dispatcher_stop_fault_timer() - stop the dispatcher fault timer
- * @device: pointer to the KGSL device structure
- *
- * Stop the dispatcher fault timer
- */
-void adreno_dispatcher_stop_fault_timer(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
-
-	del_timer_sync(&dispatcher->fault_timer);
-}
-
 /**
  * adreno_dispatcher_close() - close the dispatcher
  * @adreno_dev: pointer to the adreno device structure
@@ -2688,13 +2293,13 @@ void adreno_dispatcher_close(struct adreno_device *adreno_dev)
 	del_timer_sync(&dispatcher->fault_timer);
 
 	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		struct adreno_dispatcher_drawqueue *dispatch_q =
+		struct adreno_dispatcher_cmdqueue *dispatch_q =
 			&(rb->dispatch_q);
-		while (!adreno_drawqueue_is_empty(dispatch_q)) {
-			kgsl_drawobj_destroy(
-				DRAWOBJ(dispatch_q->cmd_q[dispatch_q->head]));
+		while (dispatch_q->head != dispatch_q->tail) {
+			kgsl_cmdbatch_destroy(
+				dispatch_q->cmd_q[dispatch_q->head]);
 			dispatch_q->head = (dispatch_q->head + 1)
-				% ADRENO_DISPATCH_DRAWQUEUE_SIZE;
+				% ADRENO_DISPATCH_CMDQUEUE_SIZE;
 		}
 	}
 
@@ -2753,23 +2358,23 @@ static ssize_t _show_uint(struct adreno_dispatcher *dispatcher,
 		*((unsigned int *) attr->value));
 }
 
-static DISPATCHER_UINT_ATTR(inflight, 0644, ADRENO_DISPATCH_DRAWQUEUE_SIZE,
+static DISPATCHER_UINT_ATTR(inflight, 0644, ADRENO_DISPATCH_CMDQUEUE_SIZE,
 	_dispatcher_q_inflight_hi);
 
 static DISPATCHER_UINT_ATTR(inflight_low_latency, 0644,
-	ADRENO_DISPATCH_DRAWQUEUE_SIZE, _dispatcher_q_inflight_lo);
+	ADRENO_DISPATCH_CMDQUEUE_SIZE, _dispatcher_q_inflight_lo);
 /*
  * Our code that "puts back" a command from the context is much cleaner
  * if we are sure that there will always be enough room in the
  * ringbuffer so restrict the maximum size of the context queue to
- * ADRENO_CONTEXT_DRAWQUEUE_SIZE - 1
+ * ADRENO_CONTEXT_CMDQUEUE_SIZE - 1
  */
-static DISPATCHER_UINT_ATTR(context_drawqueue_size, 0644,
-	ADRENO_CONTEXT_DRAWQUEUE_SIZE - 1, _context_drawqueue_size);
+static DISPATCHER_UINT_ATTR(context_cmdqueue_size, 0644,
+	ADRENO_CONTEXT_CMDQUEUE_SIZE - 1, _context_cmdqueue_size);
 static DISPATCHER_UINT_ATTR(context_burst_count, 0644, 0,
-	_context_drawobj_burst);
-static DISPATCHER_UINT_ATTR(drawobj_timeout, 0644, 0,
-	adreno_drawobj_timeout);
+	_context_cmdbatch_burst);
+static DISPATCHER_UINT_ATTR(cmdbatch_timeout, 0644, 0,
+	adreno_cmdbatch_timeout);
 static DISPATCHER_UINT_ATTR(context_queue_wait, 0644, 0, _context_queue_wait);
 static DISPATCHER_UINT_ATTR(fault_detect_interval, 0644, 0,
 	_fault_timer_interval);
@@ -2780,16 +2385,16 @@ static DISPATCHER_UINT_ATTR(fault_throttle_burst, 0644, 0,
 static DISPATCHER_UINT_ATTR(disp_preempt_fair_sched, 0644, 0,
 	adreno_disp_preempt_fair_sched);
 static DISPATCHER_UINT_ATTR(dispatch_time_slice, 0644, 0,
-	adreno_dispatch_time_slice);
+	_dispatch_time_slice);
 static DISPATCHER_UINT_ATTR(dispatch_starvation_time, 0644, 0,
-	adreno_dispatch_starvation_time);
+	_dispatch_starvation_time);
 
 static struct attribute *dispatcher_attrs[] = {
 	&dispatcher_attr_inflight.attr,
 	&dispatcher_attr_inflight_low_latency.attr,
-	&dispatcher_attr_context_drawqueue_size.attr,
+	&dispatcher_attr_context_cmdqueue_size.attr,
 	&dispatcher_attr_context_burst_count.attr,
-	&dispatcher_attr_drawobj_timeout.attr,
+	&dispatcher_attr_cmdbatch_timeout.attr,
 	&dispatcher_attr_context_queue_wait.attr,
 	&dispatcher_attr_fault_detect_interval.attr,
 	&dispatcher_attr_fault_throttle_time.attr,
@@ -2859,6 +2464,9 @@ int adreno_dispatcher_init(struct adreno_device *adreno_dev)
 	setup_timer(&dispatcher->fault_timer, adreno_dispatcher_fault_timer,
 		(unsigned long) adreno_dev);
 
+	setup_timer(&dispatcher->preempt_timer, adreno_dispatcher_preempt_timer,
+		(unsigned long) adreno_dev);
+
 	kthread_init_work(&dispatcher->work, adreno_dispatcher_work);
 
 	init_completion(&dispatcher->idle_gate);
@@ -2867,22 +2475,15 @@ int adreno_dispatcher_init(struct adreno_device *adreno_dev)
 	plist_head_init(&dispatcher->pending);
 	spin_lock_init(&dispatcher->plist_lock);
 
+	atomic_set(&dispatcher->preemption_state,
+		ADRENO_DISPATCHER_PREEMPT_CLEAR);
+
 	ret = kobject_init_and_add(&dispatcher->kobj, &ktype_dispatcher,
 		&device->dev->kobj, "dispatch");
 
 	return ret;
 }
 
-void adreno_dispatcher_halt(struct kgsl_device *device)
-{
-	adreno_get_gpu_halt(ADRENO_DEVICE(device));
-}
-
-void adreno_dispatcher_unhalt(struct kgsl_device *device)
-{
-	adreno_put_gpu_halt(ADRENO_DEVICE(device));
-}
-
 /*
  * adreno_dispatcher_idle() - Wait for dispatcher to idle
  * @adreno_dev: Adreno device whose dispatcher needs to idle
@@ -2890,7 +2491,6 @@ void adreno_dispatcher_unhalt(struct kgsl_device *device)
  * Signal dispatcher to stop sending more commands and complete
  * the commands that have already been submitted. This function
  * should not be called when dispatcher mutex is held.
- * The caller must hold the device mutex.
  */
 int adreno_dispatcher_idle(struct adreno_device *adreno_dev)
 {
@@ -2898,6 +2498,7 @@ int adreno_dispatcher_idle(struct adreno_device *adreno_dev)
 	struct adreno_dispatcher *dispatcher = &adreno_dev->dispatcher;
 	int ret;
 
+	BUG_ON(!mutex_is_locked(&device->mutex));
 	if (!test_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv))
 		return 0;
 
@@ -2907,19 +2508,12 @@ int adreno_dispatcher_idle(struct adreno_device *adreno_dev)
 	 */
 	if (mutex_is_locked(&dispatcher->mutex) &&
 		dispatcher->mutex.owner == current)
-		return -EDEADLK;
+		BUG_ON(1);
 
 	adreno_get_gpu_halt(adreno_dev);
 
 	mutex_unlock(&device->mutex);
 
-	/*
-	 * Flush the worker to make sure all executing
-	 * or pending dispatcher works on worker are
-	 * finished
-	 */
-	kthread_flush_worker(&kgsl_driver.worker);
-
 	ret = wait_for_completion_timeout(&dispatcher->idle_gate,
 			msecs_to_jiffies(ADRENO_IDLE_TIMEOUT));
 	if (ret == 0) {
@@ -2940,3 +2534,50 @@ int adreno_dispatcher_idle(struct adreno_device *adreno_dev)
 	adreno_dispatcher_schedule(device);
 	return ret;
 }
+
+void adreno_preempt_process_dispatch_queue(struct adreno_device *adreno_dev,
+	struct adreno_dispatcher_cmdqueue *dispatch_q)
+{
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct kgsl_cmdbatch *cmdbatch;
+
+	if (dispatch_q->head != dispatch_q->tail) {
+		/*
+		 * retire cmdbacthes from previous q, and don't check for
+		 * timeout since the cmdbatch may have been preempted
+		 */
+		adreno_dispatch_process_cmdqueue(adreno_dev,
+							dispatch_q, 0);
+	}
+
+	/* set the timer for the first cmdbatch of active dispatch_q */
+	dispatch_q = &(adreno_dev->cur_rb->dispatch_q);
+	if (dispatch_q->head != dispatch_q->tail) {
+		cmdbatch = dispatch_q->cmd_q[dispatch_q->head];
+		cmdbatch->expires = jiffies +
+			msecs_to_jiffies(adreno_cmdbatch_timeout);
+	}
+	kgsl_schedule_work(&device->event_work);
+}
+
+/**
+ * adreno_dispatcher_preempt_callback() - Callback funcion for CP_SW interrupt
+ * @adreno_dev: The device on which the interrupt occurred
+ * @bit: Interrupt bit in the interrupt status register
+ */
+void adreno_dispatcher_preempt_callback(struct adreno_device *adreno_dev,
+					int bit)
+{
+	struct adreno_dispatcher *dispatcher = &(adreno_dev->dispatcher);
+	if (ADRENO_DISPATCHER_PREEMPT_TRIGGERED !=
+			atomic_read(&dispatcher->preemption_state)) {
+		KGSL_DRV_CRIT_RATELIMIT(KGSL_DEVICE(adreno_dev),
+			"Preemption interrupt generated w/o trigger!\n");
+		return;
+	}
+	trace_adreno_hw_preempt_trig_to_comp_int(adreno_dev->cur_rb,
+			      adreno_dev->next_rb);
+	atomic_set(&dispatcher->preemption_state,
+			ADRENO_DISPATCHER_PREEMPT_COMPLETE);
+	adreno_dispatcher_schedule(KGSL_DEVICE(adreno_dev));
+}
diff --git a/drivers/gpu/msm/adreno_dispatch.h b/drivers/gpu/msm/adreno_dispatch.h
index 61bd06f4d373..8d56b70d916c 100644
--- a/drivers/gpu/msm/adreno_dispatch.h
+++ b/drivers/gpu/msm/adreno_dispatch.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2008-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -11,13 +11,29 @@
  *
  */
 
+
 #ifndef ____ADRENO_DISPATCHER_H
 #define ____ADRENO_DISPATCHER_H
 
+/* Time to allow preemption to complete (in ms) */
+#define ADRENO_DISPATCH_PREEMPT_TIMEOUT 10000
+
 extern unsigned int adreno_disp_preempt_fair_sched;
-extern unsigned int adreno_drawobj_timeout;
-extern unsigned int adreno_dispatch_starvation_time;
-extern unsigned int adreno_dispatch_time_slice;
+extern unsigned int adreno_cmdbatch_timeout;
+
+/**
+ * enum adreno_dispatcher_preempt_states - States of dispatcher for ringbuffer
+ * preemption
+ * @ADRENO_DISPATCHER_PREEMPT_CLEAR: No preemption is underway,
+ * only 1 preemption can be underway at any point
+ * @ADRENO_DISPATCHER_PREEMPT_TRIGGERED: A preemption is underway
+ * @ADRENO_DISPATCHER_PREEMPT_COMPLETE: A preemption has just completed
+ */
+enum adreno_dispatcher_preempt_states {
+	ADRENO_DISPATCHER_PREEMPT_CLEAR = 0,
+	ADRENO_DISPATCHER_PREEMPT_TRIGGERED,
+	ADRENO_DISPATCHER_PREEMPT_COMPLETE,
+};
 
 /**
  * enum adreno_dispatcher_starve_timer_states - Starvation control states of
@@ -44,40 +60,52 @@ enum adreno_dispatcher_starve_timer_states {
  * sizes that can be chosen at runtime
  */
 
-#define ADRENO_DISPATCH_DRAWQUEUE_SIZE 128
+#define ADRENO_DISPATCH_CMDQUEUE_SIZE 128
+
+#define CMDQUEUE_NEXT(_i, _s) (((_i) + 1) % (_s))
 
-#define DRAWQUEUE_NEXT(_i, _s) (((_i) + 1) % (_s))
+#define ACTIVE_CONTEXT_LIST_MAX 2
+
+struct adreno_context_list {
+	unsigned int id;
+	unsigned long jiffies;
+};
 
 /**
- * struct adreno_dispatcher_drawqueue - List of commands for a RB level
- * @cmd_q: List of command obj's submitted to dispatcher
+ * struct adreno_dispatcher_cmdqueue - List of commands for a RB level
+ * @cmd_q: List of command batches submitted to dispatcher
  * @inflight: Number of commands inflight in this q
  * @head: Head pointer to the q
  * @tail: Queues tail pointer
- * @active_context_count: Number of active contexts seen in this rb drawqueue
- * @expires: The jiffies value at which this drawqueue has run too long
+ * @active_contexts: List of most recently seen contexts
+ * @active_context_count: Number of active contexts in the active_contexts list
  */
-struct adreno_dispatcher_drawqueue {
-	struct kgsl_drawobj_cmd *cmd_q[ADRENO_DISPATCH_DRAWQUEUE_SIZE];
+struct adreno_dispatcher_cmdqueue {
+	struct kgsl_cmdbatch *cmd_q[ADRENO_DISPATCH_CMDQUEUE_SIZE];
 	unsigned int inflight;
 	unsigned int head;
 	unsigned int tail;
+	struct adreno_context_list active_contexts[ACTIVE_CONTEXT_LIST_MAX];
 	int active_context_count;
-	unsigned long expires;
 };
 
 /**
  * struct adreno_dispatcher - container for the adreno GPU dispatcher
  * @mutex: Mutex to protect the structure
  * @state: Current state of the dispatcher (active or paused)
- * @timer: Timer to monitor the progress of the drawobjs
- * @inflight: Number of drawobj operations pending in the ringbuffer
+ * @timer: Timer to monitor the progress of the command batches
+ * @inflight: Number of command batch operations pending in the ringbuffer
  * @fault: Non-zero if a fault was detected.
- * @pending: Priority list of contexts waiting to submit drawobjs
+ * @pending: Priority list of contexts waiting to submit command batches
  * @plist_lock: Spin lock to protect the pending queue
  * @work: work_struct to put the dispatcher in a work queue
  * @kobj: kobject for the dispatcher directory in the device sysfs node
  * @idle_gate: Gate to wait on for dispatcher to idle
+ * @preemption_state: Indicated what state the dispatcher is in, states are
+ * defined by enum adreno_dispatcher_preempt_states
+ * @preempt_token_submit: Indicates if a preempt token has been subnitted in
+ * current ringbuffer.
+ * @preempt_timer: Timer to track if preemption occured within specified time
  * @disp_preempt_fair_sched: If set then dispatcher will try to be fair to
  * starving RB's by scheduling them in and enforcing a minimum time slice
  * for every RB that is scheduled to run on the device
@@ -94,6 +122,9 @@ struct adreno_dispatcher {
 	struct kthread_work work;
 	struct kobject kobj;
 	struct completion idle_gate;
+	atomic_t preemption_state;
+	int preempt_token_submit;
+	struct timer_list preempt_timer;
 	unsigned int disp_preempt_fair_sched;
 };
 
@@ -103,18 +134,15 @@ enum adreno_dispatcher_flags {
 };
 
 void adreno_dispatcher_start(struct kgsl_device *device);
-void adreno_dispatcher_halt(struct kgsl_device *device);
-void adreno_dispatcher_unhalt(struct kgsl_device *device);
 int adreno_dispatcher_init(struct adreno_device *adreno_dev);
 void adreno_dispatcher_close(struct adreno_device *adreno_dev);
 int adreno_dispatcher_idle(struct adreno_device *adreno_dev);
 void adreno_dispatcher_irq_fault(struct adreno_device *adreno_dev);
 void adreno_dispatcher_stop(struct adreno_device *adreno_dev);
-void adreno_dispatcher_stop_fault_timer(struct kgsl_device *device);
 
-int adreno_dispatcher_queue_cmds(struct kgsl_device_private *dev_priv,
-		struct kgsl_context *context, struct kgsl_drawobj *drawobj[],
-		uint32_t count, uint32_t *timestamp);
+int adreno_dispatcher_queue_cmd(struct adreno_device *adreno_dev,
+		struct adreno_context *drawctxt, struct kgsl_cmdbatch *cmdbatch,
+		uint32_t *timestamp);
 
 void adreno_dispatcher_schedule(struct kgsl_device *device);
 void adreno_dispatcher_pause(struct adreno_device *adreno_dev);
@@ -122,12 +150,12 @@ void adreno_dispatcher_queue_context(struct kgsl_device *device,
 		struct adreno_context *drawctxt);
 void adreno_dispatcher_preempt_callback(struct adreno_device *adreno_dev,
 					int bit);
+struct adreno_ringbuffer *adreno_dispatcher_get_highest_busy_rb(
+					struct adreno_device *adreno_dev);
+int adreno_dispatch_process_cmdqueue(struct adreno_device *adreno_dev,
+				struct adreno_dispatcher_cmdqueue *dispatch_q,
+				int long_ib_detect);
 void adreno_preempt_process_dispatch_queue(struct adreno_device *adreno_dev,
-	struct adreno_dispatcher_drawqueue *dispatch_q);
+	struct adreno_dispatcher_cmdqueue *dispatch_q);
 
-static inline bool adreno_drawqueue_is_empty(
-		struct adreno_dispatcher_drawqueue *drawqueue)
-{
-	return (drawqueue != NULL && drawqueue->head == drawqueue->tail);
-}
 #endif /* __ADRENO_DISPATCHER_H */
diff --git a/drivers/gpu/msm/adreno_drawctxt.c b/drivers/gpu/msm/adreno_drawctxt.c
index 6876796bf124..f94fbc813d48 100644
--- a/drivers/gpu/msm/adreno_drawctxt.c
+++ b/drivers/gpu/msm/adreno_drawctxt.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -21,11 +21,12 @@
 #include "adreno.h"
 #include "adreno_trace.h"
 
+#define KGSL_INIT_REFTIMESTAMP		0x7FFFFFFF
+
 static void wait_callback(struct kgsl_device *device,
 		struct kgsl_event_group *group, void *priv, int result)
 {
 	struct adreno_context *drawctxt = priv;
-
 	wake_up_all(&drawctxt->waiting);
 }
 
@@ -60,14 +61,14 @@ void adreno_drawctxt_dump(struct kgsl_device *device,
 	kgsl_readtimestamp(device, context, KGSL_TIMESTAMP_RETIRED, &retire);
 
 	/*
-	 * We may have kgsl sync obj timer running, which also uses same
+	 * We may have cmdbatch timer running, which also uses same
 	 * lock, take a lock with software interrupt disabled (bh)
 	 * to avoid spin lock recursion.
 	 *
 	 * Use Spin trylock because dispatcher can acquire drawctxt->lock
 	 * if context is pending and the fence it is waiting on just got
 	 * signalled. Dispatcher acquires drawctxt->lock and tries to
-	 * delete the sync obj timer using del_timer_sync().
+	 * delete the cmdbatch timer using del_timer_sync().
 	 * del_timer_sync() waits till timer and its pending handlers
 	 * are deleted. But if the timer expires at the same time,
 	 * timer handler could be waiting on drawctxt->lock leading to a
@@ -84,33 +85,24 @@ void adreno_drawctxt_dump(struct kgsl_device *device,
 		context->id, queue, drawctxt->submitted_timestamp,
 		start, retire);
 
-	if (drawctxt->drawqueue_head != drawctxt->drawqueue_tail) {
-		struct kgsl_drawobj *drawobj =
-			drawctxt->drawqueue[drawctxt->drawqueue_head];
+	if (drawctxt->cmdqueue_head != drawctxt->cmdqueue_tail) {
+		struct kgsl_cmdbatch *cmdbatch =
+			drawctxt->cmdqueue[drawctxt->cmdqueue_head];
 
-		if (test_bit(ADRENO_CONTEXT_FENCE_LOG, &context->priv)) {
+		if (test_bit(CMDBATCH_FLAG_FENCE_LOG, &cmdbatch->priv)) {
 			dev_err(device->dev,
 				"  possible deadlock. Context %d might be blocked for itself\n",
 				context->id);
 			goto stats;
 		}
 
-		if (!kref_get_unless_zero(&drawobj->refcount))
-			goto stats;
-
-		if (drawobj->type == SYNCOBJ_TYPE) {
-			struct kgsl_drawobj_sync *syncobj = SYNCOBJ(drawobj);
-
-			if (kgsl_drawobj_events_pending(syncobj)) {
-				dev_err(device->dev,
-					"  context[%d] (ts=%d) Active sync points:\n",
-					context->id, drawobj->timestamp);
+		if (kgsl_cmdbatch_events_pending(cmdbatch)) {
+			dev_err(device->dev,
+				"  context[%d] (ts=%d) Active sync points:\n",
+				context->id, cmdbatch->timestamp);
 
-				kgsl_dump_syncpoints(device, syncobj);
-			}
+			kgsl_dump_syncpoints(device, cmdbatch);
 		}
-
-		kgsl_drawobj_put(drawobj);
 	}
 
 stats:
@@ -210,22 +202,23 @@ int adreno_drawctxt_wait(struct adreno_device *adreno_dev,
  * @context: The context which subbmitted command to RB
  * @timestamp: The RB timestamp of last command submitted to RB by context
  * @timeout: Timeout value for the wait
- * Caller must hold the device mutex
  */
 static int adreno_drawctxt_wait_rb(struct adreno_device *adreno_dev,
 		struct kgsl_context *context,
 		uint32_t timestamp, unsigned int timeout)
 {
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct adreno_context *drawctxt = ADRENO_CONTEXT(context);
 	int ret = 0;
 
+	/* Needs to hold the device mutex */
+	BUG_ON(!mutex_is_locked(&device->mutex));
+
 	/*
-	 * If the context is invalid (OR) not submitted commands to GPU
-	 * then return immediately - we may end up waiting for a timestamp
-	 * that will never come
+	 * If the context is invalid then return immediately - we may end up
+	 * waiting for a timestamp that will never come
 	 */
-	if (kgsl_context_invalid(context) ||
-			!test_bit(KGSL_CONTEXT_PRIV_SUBMITTED, &context->priv))
+	if (kgsl_context_invalid(context))
 		goto done;
 
 	trace_adreno_drawctxt_wait_start(drawctxt->rb->id, context->id,
@@ -238,19 +231,19 @@ static int adreno_drawctxt_wait_rb(struct adreno_device *adreno_dev,
 	return ret;
 }
 
-static int drawctxt_detach_drawobjs(struct adreno_context *drawctxt,
-		struct kgsl_drawobj **list)
+static int drawctxt_detach_cmdbatches(struct adreno_context *drawctxt,
+		struct kgsl_cmdbatch **list)
 {
 	int count = 0;
 
-	while (drawctxt->drawqueue_head != drawctxt->drawqueue_tail) {
-		struct kgsl_drawobj *drawobj =
-			drawctxt->drawqueue[drawctxt->drawqueue_head];
+	while (drawctxt->cmdqueue_head != drawctxt->cmdqueue_tail) {
+		struct kgsl_cmdbatch *cmdbatch =
+			drawctxt->cmdqueue[drawctxt->cmdqueue_head];
 
-		drawctxt->drawqueue_head = (drawctxt->drawqueue_head + 1) %
-			ADRENO_CONTEXT_DRAWQUEUE_SIZE;
+		drawctxt->cmdqueue_head = (drawctxt->cmdqueue_head + 1) %
+			ADRENO_CONTEXT_CMDQUEUE_SIZE;
 
-		list[count++] = drawobj;
+		list[count++] = cmdbatch;
 	}
 
 	return count;
@@ -268,7 +261,7 @@ void adreno_drawctxt_invalidate(struct kgsl_device *device,
 		struct kgsl_context *context)
 {
 	struct adreno_context *drawctxt = ADRENO_CONTEXT(context);
-	struct kgsl_drawobj *list[ADRENO_CONTEXT_DRAWQUEUE_SIZE];
+	struct kgsl_cmdbatch *list[ADRENO_CONTEXT_CMDQUEUE_SIZE];
 	int i, count;
 
 	trace_adreno_drawctxt_invalidate(drawctxt);
@@ -289,13 +282,13 @@ void adreno_drawctxt_invalidate(struct kgsl_device *device,
 			drawctxt->timestamp);
 
 	/* Get rid of commands still waiting in the queue */
-	count = drawctxt_detach_drawobjs(drawctxt, list);
+	count = drawctxt_detach_cmdbatches(drawctxt, list);
 	spin_unlock(&drawctxt->lock);
 
 	for (i = 0; i < count; i++) {
 		kgsl_cancel_events_timestamp(device, &context->events,
 			list[i]->timestamp);
-		kgsl_drawobj_destroy(list[i]);
+		kgsl_cmdbatch_destroy(list[i]);
 	}
 
 	/* Make sure all pending events are processed or cancelled */
@@ -304,7 +297,6 @@ void adreno_drawctxt_invalidate(struct kgsl_device *device,
 	/* Give the bad news to everybody waiting around */
 	wake_up_all(&drawctxt->waiting);
 	wake_up_all(&drawctxt->wq);
-	wake_up_all(&drawctxt->timeout);
 }
 
 /*
@@ -342,25 +334,21 @@ adreno_drawctxt_create(struct kgsl_device_private *dev_priv,
 	struct adreno_context *drawctxt;
 	struct kgsl_device *device = dev_priv->device;
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	int ret;
-	unsigned int local;
+	unsigned long local;
 
 	local = *flags & (KGSL_CONTEXT_PREAMBLE |
 		KGSL_CONTEXT_NO_GMEM_ALLOC |
 		KGSL_CONTEXT_PER_CONTEXT_TS |
 		KGSL_CONTEXT_USER_GENERATED_TS |
 		KGSL_CONTEXT_NO_FAULT_TOLERANCE |
-		KGSL_CONTEXT_INVALIDATE_ON_FAULT |
 		KGSL_CONTEXT_CTX_SWITCH |
 		KGSL_CONTEXT_PRIORITY_MASK |
 		KGSL_CONTEXT_TYPE_MASK |
 		KGSL_CONTEXT_PWR_CONSTRAINT |
 		KGSL_CONTEXT_IFH_NOP |
 		KGSL_CONTEXT_SECURE |
-		KGSL_CONTEXT_PREEMPT_STYLE_MASK |
-		KGSL_CONTEXT_NO_SNAPSHOT |
-		KGSL_CONTEXT_SPARSE);
+		KGSL_CONTEXT_PREEMPT_STYLE_MASK);
 
 	/* Check for errors before trying to initialize */
 
@@ -399,7 +387,6 @@ adreno_drawctxt_create(struct kgsl_device_private *dev_priv,
 	spin_lock_init(&drawctxt->lock);
 	init_waitqueue_head(&drawctxt->wq);
 	init_waitqueue_head(&drawctxt->waiting);
-	init_waitqueue_head(&drawctxt->timeout);
 
 	/* Set the context priority */
 	_set_context_priority(drawctxt);
@@ -433,16 +420,6 @@ adreno_drawctxt_create(struct kgsl_device_private *dev_priv,
 
 	adreno_context_debugfs_init(ADRENO_DEVICE(device), drawctxt);
 
-	INIT_LIST_HEAD(&drawctxt->active_node);
-
-	if (gpudev->preemption_context_init) {
-		ret = gpudev->preemption_context_init(&drawctxt->base);
-		if (ret != 0) {
-			kgsl_context_detach(&drawctxt->base);
-			return ERR_PTR(ret);
-		}
-	}
-
 	/* copy back whatever flags we dediced were valid */
 	*flags = drawctxt->base.flags;
 	return &drawctxt->base;
@@ -475,7 +452,7 @@ void adreno_drawctxt_detach(struct kgsl_context *context)
 	struct adreno_context *drawctxt;
 	struct adreno_ringbuffer *rb;
 	int ret, count, i;
-	struct kgsl_drawobj *list[ADRENO_CONTEXT_DRAWQUEUE_SIZE];
+	struct kgsl_cmdbatch *list[ADRENO_CONTEXT_CMDQUEUE_SIZE];
 
 	if (context == NULL)
 		return;
@@ -485,12 +462,22 @@ void adreno_drawctxt_detach(struct kgsl_context *context)
 	drawctxt = ADRENO_CONTEXT(context);
 	rb = drawctxt->rb;
 
-	spin_lock(&adreno_dev->active_list_lock);
-	list_del_init(&drawctxt->active_node);
-	spin_unlock(&adreno_dev->active_list_lock);
+	/* deactivate context */
+	mutex_lock(&device->mutex);
+	if (rb->drawctxt_active == drawctxt) {
+		if (adreno_dev->cur_rb == rb) {
+			if (!kgsl_active_count_get(device)) {
+				adreno_drawctxt_switch(adreno_dev, rb, NULL, 0);
+				kgsl_active_count_put(device);
+			} else
+				BUG();
+		} else
+			adreno_drawctxt_switch(adreno_dev, rb, NULL, 0);
+	}
+	mutex_unlock(&device->mutex);
 
 	spin_lock(&drawctxt->lock);
-	count = drawctxt_detach_drawobjs(drawctxt, list);
+	count = drawctxt_detach_cmdbatches(drawctxt, list);
 	spin_unlock(&drawctxt->lock);
 
 	for (i = 0; i < count; i++) {
@@ -500,11 +487,9 @@ void adreno_drawctxt_detach(struct kgsl_context *context)
 		 * detached status here.
 		 */
 		adreno_fault_skipcmd_detached(adreno_dev, drawctxt, list[i]);
-		kgsl_drawobj_destroy(list[i]);
+		kgsl_cmdbatch_destroy(list[i]);
 	}
 
-	debugfs_remove_recursive(drawctxt->debug_root);
-
 	/*
 	 * internal_timestamp is set in adreno_ringbuffer_addcmds,
 	 * which holds the device mutex.
@@ -522,33 +507,14 @@ void adreno_drawctxt_detach(struct kgsl_context *context)
 		drawctxt->internal_timestamp, 30 * 1000);
 
 	/*
-	 * If the wait for global fails due to timeout then mark it as
-	 * context detach timeout fault and schedule dispatcher to kick
-	 * in GPU recovery. For a ADRENO_CTX_DETATCH_TIMEOUT_FAULT we clear
-	 * the policy and invalidate the context. If EAGAIN error is returned
-	 * then recovery will kick in and there will be no more commands in the
-	 * RB pipe from this context which is what we are waiting for, so ignore
-	 * -EAGAIN error.
+	 * If the wait for global fails due to timeout then nothing after this
+	 * point is likely to work very well - BUG_ON() so we can take advantage
+	 * of the debug tools to figure out what the h - e - double hockey
+	 * sticks happened. If EAGAIN error is returned then recovery will kick
+	 * in and there will be no more commands in the RB pipe from this
+	 * context which is waht we are waiting for, so ignore -EAGAIN error
 	 */
-	if (ret && ret != -EAGAIN) {
-		KGSL_DRV_ERR(device,
-				"Wait for global ctx=%d ts=%d type=%d error=%d\n",
-				drawctxt->base.id, drawctxt->internal_timestamp,
-				drawctxt->type, ret);
-
-		adreno_set_gpu_fault(adreno_dev,
-				ADRENO_CTX_DETATCH_TIMEOUT_FAULT);
-		mutex_unlock(&device->mutex);
-
-		/* Schedule dispatcher to kick in recovery */
-		adreno_dispatcher_schedule(device);
-
-		/* Wait for context to be invalidated and release context */
-		wait_event_interruptible_timeout(drawctxt->timeout,
-					kgsl_context_invalid(&drawctxt->base),
-					msecs_to_jiffies(5000));
-		return;
-	}
+	BUG_ON(ret && ret != -EAGAIN);
 
 	kgsl_sharedmem_writel(device, &device->memstore,
 			KGSL_MEMSTORE_OFFSET(context->id, soptimestamp),
@@ -570,37 +536,20 @@ void adreno_drawctxt_detach(struct kgsl_context *context)
 void adreno_drawctxt_destroy(struct kgsl_context *context)
 {
 	struct adreno_context *drawctxt;
-	struct adreno_device *adreno_dev;
-	struct adreno_gpudev *gpudev;
-
 	if (context == NULL)
 		return;
 
-	adreno_dev = ADRENO_DEVICE(context->device);
-	gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	if (gpudev->preemption_context_destroy)
-		gpudev->preemption_context_destroy(context);
-
 	drawctxt = ADRENO_CONTEXT(context);
+	debugfs_remove_recursive(drawctxt->debug_root);
 	kfree(drawctxt);
 }
 
-static void _drawctxt_switch_wait_callback(struct kgsl_device *device,
-		struct kgsl_event_group *group,
-		void *priv, int result)
-{
-	struct adreno_context *drawctxt = (struct adreno_context *) priv;
-
-	kgsl_context_put(&drawctxt->base);
-}
-
 /**
  * adreno_drawctxt_switch - switch the current draw context in a given RB
  * @adreno_dev - The 3D device that owns the context
  * @rb: The ringubffer pointer on which the current context is being changed
  * @drawctxt - the 3D context to switch to
- * @flags: Control flags for the switch
+ * @flags - Flags to accompany the switch (from user space)
  *
  * Switch the current draw context in given RB
  */
@@ -615,23 +564,14 @@ int adreno_drawctxt_switch(struct adreno_device *adreno_dev,
 	int ret = 0;
 
 	/* We always expect a valid rb */
-	if (!rb)
-		return -EINVAL;
+	BUG_ON(!rb);
 
 	/* already current? */
 	if (rb->drawctxt_active == drawctxt)
 		return ret;
 
-	/*
-	 * Submitting pt switch commands from a detached context can
-	 * lead to a race condition where the pt is destroyed before
-	 * the pt switch commands get executed by the GPU, leading to
-	 * pagefaults.
-	 */
-	if (drawctxt != NULL && kgsl_context_detached(&drawctxt->base))
-		return -ENOENT;
-
-	trace_adreno_drawctxt_switch(rb, drawctxt);
+	trace_adreno_drawctxt_switch(rb,
+		drawctxt, flags);
 
 	/* Get a refcount to the new instance */
 	if (drawctxt) {
@@ -643,19 +583,17 @@ int adreno_drawctxt_switch(struct adreno_device *adreno_dev,
 		 /* No context - set the default pagetable and thats it. */
 		new_pt = device->mmu.defaultpagetable;
 	}
-	ret = adreno_ringbuffer_set_pt_ctx(rb, new_pt, drawctxt, flags);
-	if (ret)
+	ret = adreno_ringbuffer_set_pt_ctx(rb, new_pt, drawctxt);
+	if (ret) {
+		KGSL_DRV_ERR(device,
+			"Failed to set pagetable on rb %d\n", rb->id);
 		return ret;
-
-	if (rb->drawctxt_active) {
-		/* Wait for the timestamp to expire */
-		if (kgsl_add_event(device, &rb->events, rb->timestamp,
-			_drawctxt_switch_wait_callback,
-			rb->drawctxt_active)) {
-			kgsl_context_put(&rb->drawctxt_active->base);
-		}
 	}
 
+	/* Put the old instance of the active drawctxt */
+	if (rb->drawctxt_active)
+		kgsl_context_put(&rb->drawctxt_active->base);
+
 	rb->drawctxt_active = drawctxt;
 	return 0;
 }
diff --git a/drivers/gpu/msm/adreno_drawctxt.h b/drivers/gpu/msm/adreno_drawctxt.h
index eef506ff46f0..d50460a544b1 100644
--- a/drivers/gpu/msm/adreno_drawctxt.h
+++ b/drivers/gpu/msm/adreno_drawctxt.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -18,7 +18,7 @@ struct adreno_context_type {
 	const char *str;
 };
 
-#define ADRENO_CONTEXT_DRAWQUEUE_SIZE 128
+#define ADRENO_CONTEXT_CMDQUEUE_SIZE 128
 #define SUBMIT_RETIRE_TICKS_SIZE 7
 
 struct kgsl_device;
@@ -32,27 +32,23 @@ struct kgsl_context;
  * @internal_timestamp: Global timestamp of the last issued command
  *			NOTE: guarded by device->mutex, not drawctxt->mutex!
  * @type: Context type (GL, CL, RS)
- * @mutex: Mutex to protect the drawqueue
- * @drawqueue: Queue of drawobjs waiting to be dispatched for this
- *			context
- * @drawqueue_head: Head of the drawqueue queue
- * @drawqueue_tail: Tail of the drawqueue queue
+ * @mutex: Mutex to protect the cmdqueue
+ * @cmdqueue: Queue of command batches waiting to be dispatched for this context
+ * @cmdqueue_head: Head of the cmdqueue queue
+ * @cmdqueue_tail: Tail of the cmdqueue queue
  * @pending: Priority list node for the dispatcher list of pending contexts
  * @wq: Workqueue structure for contexts to sleep pending room in the queue
  * @waiting: Workqueue structure for contexts waiting for a timestamp or event
- * @timeout: Workqueue structure for contexts waiting to invalidate
- * @queued: Number of commands queued in the drawqueue
- * @fault_policy: GFT fault policy set in _skip_cmd();
+ * @queued: Number of commands queued in the cmdqueue
+ * @fault_policy: GFT fault policy set in cmdbatch_skip_cmd();
  * @debug_root: debugfs entry for this context.
  * @queued_timestamp: The last timestamp that was queued on this context
  * @rb: The ringbuffer in which this context submits commands.
  * @submitted_timestamp: The last timestamp that was submitted for this context
- * @submit_retire_ticks: Array to hold command obj execution times from submit
+ * @submit_retire_ticks: Array to hold cmdbatch execution times from submit
  *                       to retire
  * @ticks_index: The index into submit_retire_ticks[] where the new delta will
  *		 be written.
- * @active_node: Linkage for nodes in active_list
- * @active_time: Time when this context last seen
  */
 struct adreno_context {
 	struct kgsl_context base;
@@ -62,14 +58,13 @@ struct adreno_context {
 	spinlock_t lock;
 
 	/* Dispatcher */
-	struct kgsl_drawobj *drawqueue[ADRENO_CONTEXT_DRAWQUEUE_SIZE];
-	unsigned int drawqueue_head;
-	unsigned int drawqueue_tail;
+	struct kgsl_cmdbatch *cmdqueue[ADRENO_CONTEXT_CMDQUEUE_SIZE];
+	unsigned int cmdqueue_head;
+	unsigned int cmdqueue_tail;
 
 	struct plist_node pending;
 	wait_queue_head_t wq;
 	wait_queue_head_t waiting;
-	wait_queue_head_t timeout;
 
 	int queued;
 	unsigned int fault_policy;
@@ -79,13 +74,13 @@ struct adreno_context {
 	unsigned int submitted_timestamp;
 	uint64_t submit_retire_ticks[SUBMIT_RETIRE_TICKS_SIZE];
 	int ticks_index;
-
-	struct list_head active_node;
-	unsigned long active_time;
 };
 
 /* Flag definitions for flag field in adreno_context */
 
+/* Set when sync timer of cmdbatch belonging to the context times out */
+#define ADRENO_CONTEXT_CMDBATCH_FLAG_FENCE_LOG	BIT(0)
+
 /**
  * enum adreno_context_priv - Private flags for an adreno draw context
  * @ADRENO_CONTEXT_FAULT - set if the context has faulted (and recovered)
@@ -95,9 +90,8 @@ struct adreno_context {
  * @ADRENO_CONTEXT_SKIP_EOF - Context skip IBs until the next end of frame
  *      marker.
  * @ADRENO_CONTEXT_FORCE_PREAMBLE - Force the preamble for the next submission.
- * @ADRENO_CONTEXT_SKIP_CMD - Context's drawobj's skipped during
+ * @ADRENO_CONTEXT_SKIP_CMD - Context's command batch is skipped during
 	fault tolerance.
- * @ADRENO_CONTEXT_FENCE_LOG - Dump fences on this context.
  */
 enum adreno_context_priv {
 	ADRENO_CONTEXT_FAULT = KGSL_CONTEXT_PRIV_DEVICE_SPECIFIC,
@@ -106,14 +100,9 @@ enum adreno_context_priv {
 	ADRENO_CONTEXT_SKIP_EOF,
 	ADRENO_CONTEXT_FORCE_PREAMBLE,
 	ADRENO_CONTEXT_SKIP_CMD,
-	ADRENO_CONTEXT_FENCE_LOG,
 };
 
-/* Flags for adreno_drawctxt_switch() */
-#define ADRENO_CONTEXT_SWITCH_FORCE_GPU BIT(0)
-
-struct kgsl_context *adreno_drawctxt_create(
-			struct kgsl_device_private *dev_priv,
+struct kgsl_context *adreno_drawctxt_create(struct kgsl_device_private *,
 			uint32_t *flags);
 
 void adreno_drawctxt_detach(struct kgsl_context *context);
diff --git a/drivers/gpu/msm/adreno_ioctl.c b/drivers/gpu/msm/adreno_ioctl.c
index 82629c6fcf23..13d3353946ca 100644
--- a/drivers/gpu/msm/adreno_ioctl.c
+++ b/drivers/gpu/msm/adreno_ioctl.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -16,50 +16,6 @@
 #include "adreno.h"
 #include "adreno_a5xx.h"
 
-/*
- * Add a perfcounter to the per-fd list.
- * Call with the device mutex held
- */
-static int adreno_process_perfcounter_add(struct kgsl_device_private *dev_priv,
-	unsigned int groupid, unsigned int countable)
-{
-	struct adreno_device_private *adreno_priv = container_of(dev_priv,
-		struct adreno_device_private, dev_priv);
-	struct adreno_perfcounter_list_node *perfctr;
-
-	perfctr = kmalloc(sizeof(*perfctr), GFP_KERNEL);
-	if (!perfctr)
-		return -ENOMEM;
-
-	perfctr->groupid = groupid;
-	perfctr->countable = countable;
-
-	/* add the pair to process perfcounter list */
-	list_add(&perfctr->node, &adreno_priv->perfcounter_list);
-	return 0;
-}
-
-/*
- * Remove a perfcounter from the per-fd list.
- * Call with the device mutex held
- */
-static int adreno_process_perfcounter_del(struct kgsl_device_private *dev_priv,
-	unsigned int groupid, unsigned int countable)
-{
-	struct adreno_device_private *adreno_priv = container_of(dev_priv,
-		struct adreno_device_private, dev_priv);
-	struct adreno_perfcounter_list_node *p;
-
-	list_for_each_entry(p, &adreno_priv->perfcounter_list, node) {
-		if (p->groupid == groupid && p->countable == countable) {
-			list_del(&p->node);
-			kfree(p);
-			return 0;
-		}
-	}
-	return -ENODEV;
-}
-
 long adreno_ioctl_perfcounter_get(struct kgsl_device_private *dev_priv,
 	unsigned int cmd, void *data)
 {
@@ -75,28 +31,14 @@ long adreno_ioctl_perfcounter_get(struct kgsl_device_private *dev_priv,
 	 * during start(), so it is not safe to take an
 	 * active count inside that function.
 	 */
+	result = kgsl_active_count_get(device);
 
-	result = adreno_perfcntr_active_oob_get(adreno_dev);
-	if (result) {
-		mutex_unlock(&device->mutex);
-		return (long)result;
-	}
-
-	result = adreno_perfcounter_get(adreno_dev,
+	if (result == 0) {
+		result = adreno_perfcounter_get(adreno_dev,
 			get->groupid, get->countable, &get->offset,
 			&get->offset_hi, PERFCOUNTER_FLAG_NONE);
-
-	/* Add the perfcounter into the list */
-	if (!result) {
-		result = adreno_process_perfcounter_add(dev_priv, get->groupid,
-				get->countable);
-		if (result)
-			adreno_perfcounter_put(adreno_dev, get->groupid,
-				get->countable, PERFCOUNTER_FLAG_NONE);
+		kgsl_active_count_put(device);
 	}
-
-	adreno_perfcntr_active_oob_put(adreno_dev);
-
 	mutex_unlock(&device->mutex);
 
 	return (long) result;
@@ -111,15 +53,8 @@ long adreno_ioctl_perfcounter_put(struct kgsl_device_private *dev_priv,
 	int result;
 
 	mutex_lock(&device->mutex);
-
-	/* Delete the perfcounter from the process list */
-	result = adreno_process_perfcounter_del(dev_priv, put->groupid,
-		put->countable);
-
-	/* Put the perfcounter refcount */
-	if (!result)
-		adreno_perfcounter_put(adreno_dev, put->groupid,
-			put->countable, PERFCOUNTER_FLAG_NONE);
+	result = adreno_perfcounter_put(adreno_dev, put->groupid,
+		put->countable, PERFCOUNTER_FLAG_NONE);
 	mutex_unlock(&device->mutex);
 
 	return (long) result;
@@ -168,7 +103,7 @@ static long adreno_ioctl_preemption_counters_query(
 		levels_to_copy = gpudev->num_prio_levels;
 
 	if (copy_to_user((void __user *) (uintptr_t) read->counters,
-			adreno_dev->preempt.counters.hostptr,
+			adreno_dev->preemption_counters.hostptr,
 			levels_to_copy * size_level))
 		return -EFAULT;
 
@@ -182,14 +117,10 @@ long adreno_ioctl_helper(struct kgsl_device_private *dev_priv,
 		unsigned int cmd, unsigned long arg,
 		const struct kgsl_ioctl *cmds, int len)
 {
-	unsigned char data[128] = { 0 };
+	unsigned char data[128];
 	long ret;
 	int i;
 
-	static DEFINE_RATELIMIT_STATE(_rs,
-			DEFAULT_RATELIMIT_INTERVAL,
-			DEFAULT_RATELIMIT_BURST);
-
 	for (i = 0; i < len; i++) {
 		if (_IOC_NR(cmd) == _IOC_NR(cmds[i].cmd))
 			break;
@@ -201,12 +132,7 @@ long adreno_ioctl_helper(struct kgsl_device_private *dev_priv,
 		return -ENOIOCTLCMD;
 	}
 
-	if (WARN_ON(_IOC_SIZE(cmds[i].cmd) > sizeof(data))) {
-		if (__ratelimit(&_rs))
-			WARN(1, "data too big for ioctl 0x%08X: %d/%zu\n",
-				cmd, _IOC_SIZE(cmds[i].cmd), sizeof(data));
-		return -EINVAL;
-	}
+	BUG_ON(_IOC_SIZE(cmds[i].cmd) > sizeof(data));
 
 	if (_IOC_SIZE(cmds[i].cmd)) {
 		ret = kgsl_ioctl_copy_in(cmds[i].cmd, cmd, arg, data);
diff --git a/drivers/gpu/msm/adreno_iommu.c b/drivers/gpu/msm/adreno_iommu.c
index db6dff212b64..f1168d96b5de 100644
--- a/drivers/gpu/msm/adreno_iommu.c
+++ b/drivers/gpu/msm/adreno_iommu.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -68,10 +68,9 @@ static unsigned int  _iommu_lock(struct adreno_device *adreno_dev,
 	/*
 	 * If we don't have this register, probe should have forced
 	 * global pagetables and we shouldn't get here.
+	 * BUG() so we don't debug a bad register write.
 	 */
-	if (WARN_ONCE(iommu->micro_mmu_ctrl == UINT_MAX,
-		"invalid GPU IOMMU lock sequence\n"))
-		return 0;
+	BUG_ON(iommu->micro_mmu_ctrl == UINT_MAX);
 
 	/*
 	 * glue commands together until next
@@ -104,13 +103,7 @@ static unsigned int _iommu_unlock(struct adreno_device *adreno_dev,
 	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
 	unsigned int *start = cmds;
 
-	/*
-	 * If we don't have this register, probe should have forced
-	 * global pagetables and we shouldn't get here.
-	 */
-	if (WARN_ONCE(iommu->micro_mmu_ctrl == UINT_MAX,
-		"invalid GPU IOMMU unlock sequence\n"))
-		return 0;
+	BUG_ON(iommu->micro_mmu_ctrl == UINT_MAX);
 
 	/* unlock the IOMMU lock */
 	*cmds++ = cp_packet(adreno_dev, CP_REG_RMW, 3);
@@ -146,7 +139,7 @@ static unsigned int _vbif_lock(struct adreno_device *adreno_dev,
 	/* OR to set the HALT bit */
 	*cmds++ = 0x1;
 
-	/* Wait for acknowledgment */
+	/* Wait for acknowledgement */
 	cmds += _wait_reg(adreno_dev, cmds,
 			A3XX_VBIF_DDR_OUTPUT_RECOVERABLE_HALT_CTRL1,
 			1, 0xFFFFFFFF, 0xF);
@@ -185,7 +178,6 @@ static unsigned int _cp_smmu_reg(struct adreno_device *adreno_dev,
 	offset = kgsl_mmu_get_reg_ahbaddr(KGSL_MMU(adreno_dev),
 					  KGSL_IOMMU_CONTEXT_USER, reg) >> 2;
 
-	/* Required for a3x, a4x, a5x families */
 	if (adreno_is_a5xx(adreno_dev) || iommu->version == 1) {
 		*cmds++ = cp_register(adreno_dev, offset, num);
 	} else if (adreno_is_a3xx(adreno_dev)) {
@@ -194,6 +186,8 @@ static unsigned int _cp_smmu_reg(struct adreno_device *adreno_dev,
 	} else if (adreno_is_a4xx(adreno_dev)) {
 		*cmds++ = cp_packet(adreno_dev, CP_WIDE_REG_WRITE, num + 1);
 		*cmds++ = offset;
+	} else  {
+		BUG();
 	}
 	return cmds - start;
 }
@@ -261,7 +255,7 @@ static void _invalidate_uche_cpu(struct adreno_device *adreno_dev)
 			ADRENO_REG_UCHE_INVALIDATE1,
 			0x90000000);
 	} else {
-		WARN_ONCE(1, "GPU UCHE invalidate sequence not defined\n");
+		BUG();
 	}
 }
 
@@ -281,7 +275,6 @@ static bool _ctx_switch_use_cpu_path(
 				struct adreno_ringbuffer *rb)
 {
 	struct kgsl_mmu *mmu = KGSL_MMU(adreno_dev);
-
 	/*
 	 * If rb is current, we can use cpu path when GPU is
 	 * idle and we are switching to default pt.
@@ -291,7 +284,7 @@ static bool _ctx_switch_use_cpu_path(
 	if (adreno_dev->cur_rb == rb)
 		return adreno_isidle(KGSL_DEVICE(adreno_dev)) &&
 			(new_pt == mmu->defaultpagetable);
-	else if (adreno_rb_empty(rb) &&
+	else if ((rb->wptr == rb->rptr) &&
 			(new_pt == mmu->defaultpagetable))
 		return true;
 
@@ -336,8 +329,7 @@ static inline int _adreno_iommu_add_idle_indirect_cmds(
 	 * Adding an indirect buffer ensures that the prefetch stalls until
 	 * the commands in indirect buffer have completed. We need to stall
 	 * prefetch with a nop indirect buffer when updating pagetables
-	 * because it provides stabler synchronization.
-	 */
+	 * because it provides stabler synchronization */
 	cmds += cp_wait_for_me(adreno_dev, cmds);
 	*cmds++ = cp_mem_packet(adreno_dev, CP_INDIRECT_BUFFER_PFE, 2, 1);
 	cmds += cp_gpuaddr(adreno_dev, cmds, nop_gpuaddr);
@@ -368,7 +360,8 @@ static unsigned int _adreno_mmu_set_pt_update_condition(
 	 */
 	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
 	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(switch_pt_enable)));
+		offsetof(struct adreno_ringbuffer_pagetable_info,
+		switch_pt_enable)));
 	*cmds++ = 1;
 	*cmds++ = cp_packet(adreno_dev, CP_WAIT_MEM_WRITES, 1);
 	*cmds++ = 0;
@@ -382,11 +375,14 @@ static unsigned int _adreno_mmu_set_pt_update_condition(
 	*cmds++ = (1 << 8) | (1 << 4) | 3;
 	cmds += cp_gpuaddr(adreno_dev, cmds,
 	   (adreno_dev->ringbuffers[0].pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(current_global_ptname)));
+		   offsetof(struct adreno_ringbuffer_pagetable_info,
+		   current_global_ptname)));
 	*cmds++ = ptname;
 	*cmds++ = 0xFFFFFFFF;
-	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(switch_pt_enable)));
+	cmds += cp_gpuaddr(adreno_dev, cmds,
+		   (rb->pagetable_desc.gpuaddr +
+		   offsetof(struct adreno_ringbuffer_pagetable_info,
+		   switch_pt_enable)));
 	*cmds++ = 0;
 	*cmds++ = cp_packet(adreno_dev, CP_WAIT_MEM_WRITES, 1);
 	*cmds++ = 0;
@@ -410,18 +406,23 @@ static unsigned int _adreno_iommu_pt_update_pid_to_mem(
 	unsigned int *cmds_orig = cmds;
 
 	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
-	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(current_rb_ptname)));
+	cmds += cp_gpuaddr(adreno_dev, cmds,
+		  (rb->pagetable_desc.gpuaddr +
+		  offsetof(struct adreno_ringbuffer_pagetable_info,
+		  current_rb_ptname)));
 	*cmds++ = ptname;
 	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
 	cmds += cp_gpuaddr(adreno_dev, cmds,
-		(adreno_dev->ringbuffers[0].pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(current_global_ptname)));
+		  (adreno_dev->ringbuffers[0].pagetable_desc.gpuaddr +
+		  offsetof(struct adreno_ringbuffer_pagetable_info,
+		  current_global_ptname)));
 	*cmds++ = ptname;
 	/* pagetable switch done, Housekeeping: set the switch_pt_enable to 0 */
 	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
-	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(switch_pt_enable)));
+	cmds += cp_gpuaddr(adreno_dev, cmds,
+			(rb->pagetable_desc.gpuaddr +
+			offsetof(struct adreno_ringbuffer_pagetable_info,
+			switch_pt_enable)));
 	*cmds++ = 0;
 	*cmds++ = cp_packet(adreno_dev, CP_WAIT_MEM_WRITES, 1);
 	*cmds++ = 0;
@@ -443,10 +444,14 @@ static unsigned int _adreno_iommu_set_pt_v1(struct adreno_ringbuffer *rb,
 	/* set flag that indicates whether pt switch is required*/
 	cmds += _adreno_mmu_set_pt_update_condition(rb, cmds, ptname);
 	*cmds++ = cp_mem_packet(adreno_dev, CP_COND_EXEC, 4, 2);
-	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(switch_pt_enable)));
-	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(switch_pt_enable)));
+	cmds += cp_gpuaddr(adreno_dev, cmds,
+			(rb->pagetable_desc.gpuaddr +
+			offsetof(struct adreno_ringbuffer_pagetable_info,
+			switch_pt_enable)));
+	cmds += cp_gpuaddr(adreno_dev, cmds,
+			(rb->pagetable_desc.gpuaddr +
+			offsetof(struct adreno_ringbuffer_pagetable_info,
+			switch_pt_enable)));
 	*cmds++ = 1;
 	/* Exec count to be filled later */
 	cond_exec_ptr = cmds;
@@ -561,41 +566,7 @@ static unsigned int _adreno_iommu_set_pt_v2_a5xx(struct kgsl_device *device,
 
 	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 4, 1);
 	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(ttbr0)));
-	*cmds++ = lower_32_bits(ttbr0);
-	*cmds++ = upper_32_bits(ttbr0);
-	*cmds++ = contextidr;
-
-	/* release all commands with wait_for_me */
-	cmds += cp_wait_for_me(adreno_dev, cmds);
-
-	cmds += _adreno_iommu_add_idle_cmds(adreno_dev, cmds);
-
-	return cmds - cmds_orig;
-}
-
-static unsigned int _adreno_iommu_set_pt_v2_a6xx(struct kgsl_device *device,
-					unsigned int *cmds_orig,
-					u64 ttbr0, u32 contextidr,
-					struct adreno_ringbuffer *rb,
-					unsigned int cb_num)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	unsigned int *cmds = cmds_orig;
-
-	cmds += _adreno_iommu_add_idle_cmds(adreno_dev, cmds);
-	cmds += cp_wait_for_me(adreno_dev, cmds);
-
-	/* CP switches the pagetable and flushes the Caches */
-	*cmds++ = cp_packet(adreno_dev, CP_SMMU_TABLE_UPDATE, 4);
-	*cmds++ = lower_32_bits(ttbr0);
-	*cmds++ = upper_32_bits(ttbr0);
-	*cmds++ = contextidr;
-	*cmds++ = cb_num;
-
-	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 4, 1);
-	cmds += cp_gpuaddr(adreno_dev, cmds, (rb->pagetable_desc.gpuaddr +
-		PT_INFO_OFFSET(ttbr0)));
+		offsetof(struct adreno_ringbuffer_pagetable_info, ttbr0)));
 	*cmds++ = lower_32_bits(ttbr0);
 	*cmds++ = upper_32_bits(ttbr0);
 	*cmds++ = contextidr;
@@ -622,7 +593,6 @@ unsigned int adreno_iommu_set_pt_generate_cmds(
 	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct kgsl_iommu *iommu = KGSL_IOMMU_PRIV(device);
-	struct kgsl_iommu_context *ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
 	u64 ttbr0;
 	u32 contextidr;
 	unsigned int *cmds_orig = cmds;
@@ -636,11 +606,7 @@ unsigned int adreno_iommu_set_pt_generate_cmds(
 		iommu->setstate.gpuaddr + KGSL_IOMMU_SETSTATE_NOP_OFFSET);
 
 	if (iommu->version >= 2) {
-		if (adreno_is_a6xx(adreno_dev))
-			cmds += _adreno_iommu_set_pt_v2_a6xx(device, cmds,
-						ttbr0, contextidr, rb,
-						ctx->cb_num);
-		else if (adreno_is_a5xx(adreno_dev))
+		if (adreno_is_a5xx(adreno_dev))
 			cmds += _adreno_iommu_set_pt_v2_a5xx(device, cmds,
 						ttbr0, contextidr, rb);
 		else if (adreno_is_a4xx(adreno_dev))
@@ -650,8 +616,7 @@ unsigned int adreno_iommu_set_pt_generate_cmds(
 			cmds += _adreno_iommu_set_pt_v2_a3xx(device, cmds,
 						ttbr0, contextidr);
 		else
-			WARN_ONCE(1,
-			"GPU IOMMU set pagetable sequence not defined\n");
+			BUG(); /* new GPU family? */
 	} else {
 		cmds += _adreno_iommu_set_pt_v1(rb, cmds, ttbr0, contextidr,
 						pt->name);
@@ -686,21 +651,18 @@ static unsigned int __add_curr_ctxt_cmds(struct adreno_ringbuffer *rb,
 	*cmds++ = KGSL_CONTEXT_TO_MEM_IDENTIFIER;
 
 	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
-	cmds += cp_gpuaddr(adreno_dev, cmds,
-			MEMSTORE_RB_GPU_ADDR(device, rb, current_context));
+	cmds += cp_gpuaddr(adreno_dev, cmds, device->memstore.gpuaddr +
+			   KGSL_MEMSTORE_RB_OFFSET(rb, current_context));
 	*cmds++ = (drawctxt ? drawctxt->base.id : 0);
 
 	*cmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
-	cmds += cp_gpuaddr(adreno_dev, cmds,
-			MEMSTORE_ID_GPU_ADDR(device,
-				KGSL_MEMSTORE_GLOBAL, current_context));
+	cmds += cp_gpuaddr(adreno_dev, cmds, device->memstore.gpuaddr +
+			KGSL_MEMSTORE_OFFSET(KGSL_MEMSTORE_GLOBAL,
+			current_context));
 	*cmds++ = (drawctxt ? drawctxt->base.id : 0);
 
 	/* Invalidate UCHE for new context */
-	if (adreno_is_a6xx(adreno_dev)) {
-		*cmds++ = cp_packet(adreno_dev, CP_EVENT_WRITE, 1);
-		*cmds++ = 0x31; /* CACHE_INVALIDATE */
-	} else if (adreno_is_a5xx(adreno_dev)) {
+	if (adreno_is_a5xx(adreno_dev)) {
 		*cmds++ = cp_register(adreno_dev,
 			adreno_getreg(adreno_dev,
 		ADRENO_REG_UCHE_INVALIDATE0), 1);
@@ -718,7 +680,7 @@ static unsigned int __add_curr_ctxt_cmds(struct adreno_ringbuffer *rb,
 		*cmds++ = 0;
 		*cmds++ = 0x90000000;
 	} else
-		WARN_ONCE(1, "GPU UCHE invalidate sequence not defined\n");
+		BUG();
 
 	return cmds - cmds_orig;
 }
@@ -744,7 +706,7 @@ static void _set_ctxt_cpu(struct adreno_ringbuffer *rb,
 	}
 	/* Update rb memstore with current context */
 	kgsl_sharedmem_writel(device, &device->memstore,
-		MEMSTORE_RB_OFFSET(rb, current_context),
+		KGSL_MEMSTORE_RB_OFFSET(rb, current_context),
 		drawctxt ? drawctxt->base.id : 0);
 }
 
@@ -761,7 +723,7 @@ static int _set_ctxt_gpu(struct adreno_ringbuffer *rb,
 
 	cmds = &link[0];
 	cmds += __add_curr_ctxt_cmds(rb, cmds, drawctxt);
-	result = adreno_ringbuffer_issue_internal_cmds(rb, 0, link,
+	result = adreno_ringbuffer_issuecmds(rb, 0, link,
 			(unsigned int)(cmds - link));
 	return result;
 }
@@ -784,11 +746,26 @@ static int _set_pagetable_cpu(struct adreno_ringbuffer *rb,
 		if (result)
 			return result;
 		/* write the new pt set to memory var */
-		adreno_ringbuffer_set_global(adreno_dev, new_pt->name);
+		kgsl_sharedmem_writel(device,
+			&adreno_dev->ringbuffers[0].pagetable_desc,
+			offsetof(
+			struct adreno_ringbuffer_pagetable_info,
+			current_global_ptname), new_pt->name);
 	}
 
 	/* Update the RB pagetable info here */
-	adreno_ringbuffer_set_pagetable(rb, new_pt);
+	kgsl_sharedmem_writel(device, &rb->pagetable_desc,
+		offsetof(
+		struct adreno_ringbuffer_pagetable_info,
+		current_rb_ptname), new_pt->name);
+	kgsl_sharedmem_writeq(device, &rb->pagetable_desc,
+		offsetof(
+		struct adreno_ringbuffer_pagetable_info,
+		ttbr0), kgsl_mmu_pagetable_get_ttbr0(new_pt));
+	kgsl_sharedmem_writel(device, &rb->pagetable_desc,
+		offsetof(
+		struct adreno_ringbuffer_pagetable_info,
+		contextidr), kgsl_mmu_pagetable_get_contextidr(new_pt));
 
 	return 0;
 }
@@ -818,26 +795,33 @@ static int _set_pagetable_gpu(struct adreno_ringbuffer *rb,
 		return 0;
 	}
 
+	kgsl_mmu_enable_clk(KGSL_MMU(adreno_dev));
+
 	cmds += adreno_iommu_set_pt_generate_cmds(rb, cmds, new_pt);
 
 	if ((unsigned int) (cmds - link) > (PAGE_SIZE / sizeof(unsigned int))) {
 		KGSL_DRV_ERR(KGSL_DEVICE(adreno_dev),
 			"Temp command buffer overflow\n");
-
-		/*
-		 * Temp buffer not large enough for pagetable switch commands.
-		 * Increase the size allocated above.
-		 */
 		BUG();
 	}
 	/*
 	 * This returns the per context timestamp but we need to
 	 * use the global timestamp for iommu clock disablement
 	 */
-	result = adreno_ringbuffer_issue_internal_cmds(rb,
+	result = adreno_ringbuffer_issuecmds(rb,
 			KGSL_CMD_FLAGS_PMODE, link,
 			(unsigned int)(cmds - link));
 
+	/*
+	 * On error disable the IOMMU clock right away otherwise turn it off
+	 * after the command has been retired
+	 */
+	if (result)
+		kgsl_mmu_disable_clk(KGSL_MMU(adreno_dev));
+	else
+		adreno_ringbuffer_mmu_disable_clk_on_ts(KGSL_DEVICE(adreno_dev),
+			rb, rb->timestamp);
+
 	kfree(link);
 	return result;
 }
@@ -883,10 +867,6 @@ int adreno_iommu_init(struct adreno_device *adreno_dev)
 		}
 	}
 
-	/* Enable guard page MMU feature for A3xx and A4xx targets only */
-	if (adreno_is_a3xx(adreno_dev) || adreno_is_a4xx(adreno_dev))
-		device->mmu.features |= KGSL_MMU_NEED_GUARD_PAGE;
-
 	return 0;
 }
 
@@ -902,8 +882,7 @@ int adreno_iommu_init(struct adreno_device *adreno_dev)
  */
 int adreno_iommu_set_pt_ctx(struct adreno_ringbuffer *rb,
 			struct kgsl_pagetable *new_pt,
-			struct adreno_context *drawctxt,
-			unsigned long flags)
+			struct adreno_context *drawctxt)
 {
 	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
@@ -911,22 +890,10 @@ int adreno_iommu_set_pt_ctx(struct adreno_ringbuffer *rb,
 	int result = 0;
 	int cpu_path = 0;
 
-	/* Just do the context switch incase of NOMMU */
-	if (kgsl_mmu_get_mmutype(device) == KGSL_MMU_TYPE_NONE) {
-		if ((!(flags & ADRENO_CONTEXT_SWITCH_FORCE_GPU)) &&
-			adreno_isidle(device) && !adreno_is_a6xx(adreno_dev))
-			_set_ctxt_cpu(rb, drawctxt);
-		else
-			result = _set_ctxt_gpu(rb, drawctxt);
-
-		return result;
-	}
-
 	if (rb->drawctxt_active)
 		cur_pt = rb->drawctxt_active->base.proc_priv->pagetable;
 
-	cpu_path = !(flags & ADRENO_CONTEXT_SWITCH_FORCE_GPU) &&
-		_ctx_switch_use_cpu_path(adreno_dev, new_pt, rb);
+	cpu_path = _ctx_switch_use_cpu_path(adreno_dev, new_pt, rb);
 
 	/* Pagetable switch */
 	if (new_pt != cur_pt) {
@@ -936,14 +903,19 @@ int adreno_iommu_set_pt_ctx(struct adreno_ringbuffer *rb,
 			result = _set_pagetable_gpu(rb, new_pt);
 	}
 
-	if (result)
+	if (result) {
+		KGSL_DRV_ERR(device, "Error switching pagetable %d\n", result);
 		return result;
+	}
 
 	/* Context switch */
-	if (cpu_path && !adreno_is_a6xx(adreno_dev))
+	if (cpu_path)
 		_set_ctxt_cpu(rb, drawctxt);
 	else
 		result = _set_ctxt_gpu(rb, drawctxt);
 
+	if (result)
+		KGSL_DRV_ERR(device, "Error switching context %d\n", result);
+
 	return result;
 }
diff --git a/drivers/gpu/msm/adreno_iommu.h b/drivers/gpu/msm/adreno_iommu.h
index 5a6c2c549370..c557c65bb4c9 100644
--- a/drivers/gpu/msm/adreno_iommu.h
+++ b/drivers/gpu/msm/adreno_iommu.h
@@ -17,8 +17,7 @@
 #ifdef CONFIG_QCOM_KGSL_IOMMU
 int adreno_iommu_set_pt_ctx(struct adreno_ringbuffer *rb,
 			struct kgsl_pagetable *new_pt,
-			struct adreno_context *drawctxt,
-			unsigned long flags);
+			struct adreno_context *drawctxt);
 
 int adreno_iommu_init(struct adreno_device *adreno_dev);
 
@@ -34,8 +33,7 @@ static inline int adreno_iommu_init(struct adreno_device *adreno_dev)
 
 static inline int adreno_iommu_set_pt_ctx(struct adreno_ringbuffer *rb,
 			struct kgsl_pagetable *new_pt,
-			struct adreno_context *drawctxt,
-			unsigned long flags)
+			struct adreno_context *drawctxt)
 {
 	return 0;
 }
diff --git a/drivers/gpu/msm/adreno_llc.h b/drivers/gpu/msm/adreno_llc.h
deleted file mode 100644
index 90dff82f3059..000000000000
--- a/drivers/gpu/msm/adreno_llc.h
+++ /dev/null
@@ -1,98 +0,0 @@
-/* Copyright (c) 2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-#ifndef __ADRENO_LLC_H
-#define __ADRENO_LLC_H
-
-#ifdef CONFIG_QCOM_LLCC
-#include "adreno.h"
-#include <linux/soc/qcom/llcc-qcom.h>
-
-static inline bool adreno_llc_supported(void)
-{
-	return true;
-}
-
-static inline void *adreno_llc_getd(struct device *dev, const char *name)
-{
-	return llcc_slice_getd(dev, name);
-}
-
-static inline void adreno_llc_putd(void *desc)
-{
-	llcc_slice_putd(desc);
-}
-
-static inline int adreno_llc_deactivate_slice(void *desc)
-{
-	return llcc_slice_deactivate(desc);
-}
-
-static inline int adreno_llc_get_scid(void *desc)
-{
-	return llcc_get_slice_id(desc);
-
-}
-
-static inline void adreno_llc_setup(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	if (adreno_dev->gpu_llc_slice && adreno_dev->gpu_llc_slice_enable)
-		if (!llcc_slice_activate(adreno_dev->gpu_llc_slice)) {
-			if (gpudev->llc_configure_gpu_scid)
-				gpudev->llc_configure_gpu_scid(adreno_dev);
-		}
-
-	if (adreno_dev->gpuhtw_llc_slice && adreno_dev->gpuhtw_llc_slice_enable)
-		if (!llcc_slice_activate(adreno_dev->gpuhtw_llc_slice)) {
-			if (gpudev->llc_configure_gpuhtw_scid)
-				gpudev->llc_configure_gpuhtw_scid(adreno_dev);
-		}
-
-	if (gpudev->llc_enable_overrides)
-		gpudev->llc_enable_overrides(adreno_dev);
-}
-
-#else
-static inline bool adreno_llc_supported(void)
-{
-	return false;
-}
-
-static inline void *adreno_llc_getd(struct device *dev,
-		const char *name)
-{
-	return NULL;
-}
-
-static inline void adreno_llc_putd(void *desc)
-{
-}
-
-static inline int adreno_llc_deactivate_slice(void *desc)
-{
-	return 0;
-}
-
-static inline int adreno_llc_get_scid(void *desc)
-{
-	return 0;
-}
-
-static inline void adreno_llc_setup(struct kgsl_device *device)
-{
-}
-#endif
-
-#endif /* __ADRENO_LLC_H */
diff --git a/drivers/gpu/msm/adreno_perfcounter.c b/drivers/gpu/msm/adreno_perfcounter.c
index 50207509d783..1779e8e7c0b7 100644
--- a/drivers/gpu/msm/adreno_perfcounter.c
+++ b/drivers/gpu/msm/adreno_perfcounter.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -27,23 +27,13 @@
 #define VBIF2_PERF_CLR_REG_SEL_OFF 8
 /* offset of enable register from select register */
 #define VBIF2_PERF_EN_REG_SEL_OFF 16
-
-/* offset of clear register from select register for GBIF */
-#define GBIF_PERF_CLR_REG_SEL_OFF 1
-
-/* offset of enable register from select register for GBIF*/
-#define GBIF_PERF_EN_REG_SEL_OFF  2
-
-/* offset of clear register from the power enable register for GBIF*/
-#define GBIF_PWR_CLR_REG_EN_OFF    1
-
-/* */
-#define GBIF_PERF_RMW_MASK   0xFF
-/* */
-#define GBIF_PWR_RMW_MASK    0x10000
+/* offset of high counter from low counter value */
+#define VBIF2_PERF_HIGH_REG_LOW_OFF 8
 
 /* offset of clear register from the enable register */
 #define VBIF2_PERF_PWR_CLR_REG_EN_OFF 8
+/* offset of high counter from low counter value */
+#define VBIF2_PERF_PWR_HIGH_REG_LOW_OFF 8
 
 #define REG_64BIT_VAL(hi, lo, val) (((((uint64_t) hi) << 32) | lo) + val)
 /*
@@ -171,24 +161,13 @@ void adreno_perfcounter_restore(struct adreno_device *adreno_dev)
  */
 inline void adreno_perfcounter_save(struct adreno_device *adreno_dev)
 {
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	struct adreno_perfcounters *counters = ADRENO_PERFCOUNTERS(adreno_dev);
 	struct adreno_perfcount_group *group;
 	unsigned int counter, groupid;
-	int ret = 0;
 
 	if (counters == NULL)
 		return;
 
-	if (gpudev->oob_set)
-		ret = gpudev->oob_set(adreno_dev, OOB_PERFCNTR_SET_MASK,
-				OOB_PERFCNTR_CHECK_MASK,
-				OOB_PERFCNTR_CLEAR_MASK);
-
-	/* if oob_set timeout, clear the mask and return */
-	if (ret)
-		goto done;
-
 	for (groupid = 0; groupid < counters->group_count; groupid++) {
 		group = &(counters->groups[groupid]);
 
@@ -208,10 +187,6 @@ inline void adreno_perfcounter_save(struct adreno_device *adreno_dev)
 								counter);
 		}
 	}
-
-done:
-	if (gpudev->oob_clear)
-		gpudev->oob_clear(adreno_dev, OOB_PERFCNTR_CLEAR_MASK);
 }
 
 static int adreno_perfcounter_enable(struct adreno_device *adreno_dev,
@@ -232,7 +207,7 @@ void adreno_perfcounter_start(struct adreno_device *adreno_dev)
 	struct adreno_perfcount_group *group;
 	unsigned int i, j;
 
-	if (counters == NULL)
+	if (NULL == counters)
 		return;
 	/* group id iter */
 	for (i = 0; i < counters->group_count; i++) {
@@ -277,14 +252,14 @@ int adreno_perfcounter_read_group(struct adreno_device *adreno_dev,
 	unsigned int i, j;
 	int ret = 0;
 
-	if (counters == NULL)
+	if (NULL == counters)
 		return -EINVAL;
 
 	/* sanity check params passed in */
 	if (reads == NULL || count == 0 || count > 100)
 		return -EINVAL;
 
-	list = kmalloc_array(count, sizeof(struct kgsl_perfcounter_read_group),
+	list = kmalloc(sizeof(struct kgsl_perfcounter_read_group) * count,
 			GFP_KERNEL);
 	if (!list)
 		return -ENOMEM;
@@ -296,8 +271,7 @@ int adreno_perfcounter_read_group(struct adreno_device *adreno_dev,
 	}
 
 	mutex_lock(&device->mutex);
-
-	ret = adreno_perfcntr_active_oob_get(adreno_dev);
+	ret = kgsl_active_count_get(device);
 	if (ret) {
 		mutex_unlock(&device->mutex);
 		goto done;
@@ -326,8 +300,7 @@ int adreno_perfcounter_read_group(struct adreno_device *adreno_dev,
 		}
 	}
 
-	adreno_perfcntr_active_oob_put(adreno_dev);
-
+	kgsl_active_count_put(device);
 	mutex_unlock(&device->mutex);
 
 	/* write the data */
@@ -437,7 +410,7 @@ int adreno_perfcounter_query_group(struct adreno_device *adreno_dev,
 
 	t = min_t(unsigned int, group->reg_count, count);
 
-	buf = kmalloc_array(t, sizeof(unsigned int), GFP_KERNEL);
+	buf = kmalloc(t * sizeof(unsigned int), GFP_KERNEL);
 	if (buf == NULL) {
 		mutex_unlock(&device->mutex);
 		return -ENOMEM;
@@ -501,7 +474,7 @@ int adreno_perfcounter_get(struct adreno_device *adreno_dev,
 	if (offset_hi)
 		*offset_hi = 0;
 
-	if (counters == NULL)
+	if (NULL == counters)
 		return -EINVAL;
 
 	if (groupid >= counters->group_count)
@@ -521,9 +494,9 @@ int adreno_perfcounter_get(struct adreno_device *adreno_dev,
 		/* If it is already reserved, just increase the refcounts */
 		if ((group->regs[countable].kernelcount != 0) ||
 			(group->regs[countable].usercount != 0)) {
-			refcount_group(group, countable, flags,
-				offset, offset_hi);
-			return 0;
+				refcount_group(group, countable, flags,
+					offset, offset_hi);
+				return 0;
 		}
 
 		empty = countable;
@@ -553,18 +526,12 @@ int adreno_perfcounter_get(struct adreno_device *adreno_dev,
 	if (empty == -1)
 		return -EBUSY;
 
-	/* initialize the new counter */
-	group->regs[empty].countable = countable;
-
 	/* enable the new counter */
 	ret = adreno_perfcounter_enable(adreno_dev, groupid, empty, countable);
-	if (ret) {
-		/* Put back the perfcounter */
-		if (!(group->flags & ADRENO_PERFCOUNTER_GROUP_FIXED))
-			group->regs[empty].countable =
-				KGSL_PERFCOUNTER_NOT_USED;
+	if (ret)
 		return ret;
-	}
+	/* initialize the new counter */
+	group->regs[empty].countable = countable;
 
 	/* set initial kernel and user count */
 	if (flags & PERFCOUNTER_FLAG_KERNEL) {
@@ -635,6 +602,28 @@ int adreno_perfcounter_put(struct adreno_device *adreno_dev,
 	return -EINVAL;
 }
 
+static int _perfcounter_enable_pwr(struct adreno_device *adreno_dev,
+	unsigned int counter)
+{
+	/* PWR counters enabled by default on A3XX/A4XX so nothing to do */
+	if (adreno_is_a3xx(adreno_dev) || adreno_is_a4xx(adreno_dev))
+		return 0;
+
+	/*
+	 * On 5XX we have to emulate the PWR counters which are physically
+	 * missing. Program countable 6 on RBBM_PERFCTR_RBBM_0 as a substitute
+	 * for PWR:1. Don't emulate PWR:0 as nobody uses it and we don't want
+	 * to take away too many of the generic RBBM counters.
+	 */
+
+	if (counter == 0)
+		return -EINVAL;
+
+	kgsl_regwrite(KGSL_DEVICE(adreno_dev), A5XX_RBBM_PERFCTR_RBBM_SEL_0, 6);
+
+	return 0;
+}
+
 static void _perfcounter_enable_vbif(struct adreno_device *adreno_dev,
 		struct adreno_perfcounters *counters, unsigned int counter,
 		unsigned int countable)
@@ -643,40 +632,12 @@ static void _perfcounter_enable_vbif(struct adreno_device *adreno_dev,
 	struct adreno_perfcount_register *reg;
 
 	reg = &counters->groups[KGSL_PERFCOUNTER_GROUP_VBIF].regs[counter];
-
-	if (adreno_has_gbif(adreno_dev)) {
-		unsigned int shift = counter << 3;
-		unsigned int perfctr_mask = 1 << counter;
-		/*
-		 * Write 1, followed by 0 to CLR register for
-		 * clearing the counter
-		 */
-		kgsl_regrmw(device, reg->select - GBIF_PERF_CLR_REG_SEL_OFF,
-			perfctr_mask, perfctr_mask);
-		kgsl_regrmw(device, reg->select - GBIF_PERF_CLR_REG_SEL_OFF,
-			perfctr_mask, 0);
-		/* select the desired countable */
-		kgsl_regrmw(device, reg->select,
-			GBIF_PERF_RMW_MASK << shift, countable << shift);
-		/* enable counter */
-		kgsl_regrmw(device, reg->select - GBIF_PERF_EN_REG_SEL_OFF,
-			perfctr_mask, perfctr_mask);
-
-	} else {
-		/*
-		 * Write 1, followed by 0 to CLR register for
-		 * clearing the counter
-		 */
-		kgsl_regwrite(device,
-			reg->select - VBIF2_PERF_CLR_REG_SEL_OFF, 1);
-		kgsl_regwrite(device,
-			reg->select - VBIF2_PERF_CLR_REG_SEL_OFF, 0);
-		kgsl_regwrite(device,
-			reg->select, countable & VBIF2_PERF_CNT_SEL_MASK);
-		/* enable reg is 8 DWORDS before select reg */
-		kgsl_regwrite(device,
-			reg->select - VBIF2_PERF_EN_REG_SEL_OFF, 1);
-	}
+	/* Write 1, followed by 0 to CLR register for clearing the counter */
+	kgsl_regwrite(device, reg->select - VBIF2_PERF_CLR_REG_SEL_OFF, 1);
+	kgsl_regwrite(device, reg->select - VBIF2_PERF_CLR_REG_SEL_OFF, 0);
+	kgsl_regwrite(device, reg->select, countable & VBIF2_PERF_CNT_SEL_MASK);
+	/* enable reg is 8 DWORDS before select reg */
+	kgsl_regwrite(device, reg->select - VBIF2_PERF_EN_REG_SEL_OFF, 1);
 	reg->value = 0;
 }
 
@@ -687,39 +648,16 @@ static void _perfcounter_enable_vbif_pwr(struct adreno_device *adreno_dev,
 	struct adreno_perfcount_register *reg;
 
 	reg = &counters->groups[KGSL_PERFCOUNTER_GROUP_VBIF_PWR].regs[counter];
-
-	if (adreno_has_gbif(adreno_dev)) {
-		unsigned int perfctr_mask = GBIF_PWR_RMW_MASK << counter;
-		/*
-		 * Write 1, followed by 0 to CLR register for
-		 * clearing the counter
-		 */
-		kgsl_regrmw(device, reg->select + GBIF_PWR_CLR_REG_EN_OFF,
-			perfctr_mask, perfctr_mask);
-		kgsl_regrmw(device, reg->select + GBIF_PWR_CLR_REG_EN_OFF,
-			perfctr_mask, 0);
-		/* Enable the counter */
-		kgsl_regrmw(device, reg->select, perfctr_mask, perfctr_mask);
-	} else {
-		/*
-		 * Write 1, followed by 0 to CLR register for
-		 * clearing the counter
-		 */
-		kgsl_regwrite(device, reg->select +
-			VBIF2_PERF_PWR_CLR_REG_EN_OFF, 1);
-		kgsl_regwrite(device, reg->select +
-			VBIF2_PERF_PWR_CLR_REG_EN_OFF, 0);
-		kgsl_regwrite(device, reg->select, 1);
-	}
+	/* Write 1, followed by 0 to CLR register for clearing the counter */
+	kgsl_regwrite(device, reg->select + VBIF2_PERF_PWR_CLR_REG_EN_OFF, 1);
+	kgsl_regwrite(device, reg->select + VBIF2_PERF_PWR_CLR_REG_EN_OFF, 0);
+	kgsl_regwrite(device, reg->select, 1);
 	reg->value = 0;
 }
 
 static void _power_counter_enable_alwayson(struct adreno_device *adreno_dev,
 				struct adreno_perfcounters *counters)
 {
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_GPMU))
-		return;
-
 	kgsl_regwrite(KGSL_DEVICE(adreno_dev),
 		A5XX_GPMU_ALWAYS_ON_COUNTER_RESET, 1);
 	counters->groups[KGSL_PERFCOUNTER_GROUP_ALWAYSON_PWR].regs[0].value = 0;
@@ -731,7 +669,6 @@ static void _power_counter_enable_gpmu(struct adreno_device *adreno_dev,
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct adreno_perfcount_register *reg;
-	unsigned int shift = (counter << 3) % (sizeof(unsigned int) * 8);
 
 	if (adreno_is_a530(adreno_dev)) {
 		if (countable > 43)
@@ -739,16 +676,18 @@ static void _power_counter_enable_gpmu(struct adreno_device *adreno_dev,
 	} else if (adreno_is_a540(adreno_dev)) {
 		if (countable > 47)
 			return;
-	} else if (adreno_is_a6xx(adreno_dev)) {
-		if (countable > 34)
-			return;
 	} else
 		/* return on platforms that have no GPMU */
 		return;
 
 	reg = &counters->groups[group].regs[counter];
-	kgsl_regrmw(device, reg->select, 0xff << shift, countable << shift);
-	adreno_writereg(adreno_dev, ADRENO_REG_GPMU_POWER_COUNTER_ENABLE, 1);
+
+	/* Move the countable to the correct byte offset */
+	countable = countable << ((counter % 4) * 8);
+
+	kgsl_regwrite(device, reg->select, countable);
+
+	kgsl_regwrite(device, A5XX_GPMU_POWER_COUNTER_ENABLE, 1);
 	reg->value = 0;
 }
 
@@ -759,30 +698,12 @@ static void _power_counter_enable_default(struct adreno_device *adreno_dev,
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct adreno_perfcount_register *reg;
 
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_GPMU))
-		return;
-
 	reg = &counters->groups[group].regs[counter];
 	kgsl_regwrite(device, reg->select, countable);
-	adreno_writereg(adreno_dev, ADRENO_REG_GPMU_POWER_COUNTER_ENABLE, 1);
+	kgsl_regwrite(device, A5XX_GPMU_POWER_COUNTER_ENABLE, 1);
 	reg->value = 0;
 }
 
-static inline bool _perfcounter_inline_update(
-	struct adreno_device *adreno_dev, unsigned int group)
-{
-	if (adreno_is_a6xx(adreno_dev)) {
-		if ((group == KGSL_PERFCOUNTER_GROUP_HLSQ) ||
-			(group == KGSL_PERFCOUNTER_GROUP_SP) ||
-			(group == KGSL_PERFCOUNTER_GROUP_TP))
-			return true;
-		else
-			return false;
-	}
-
-	return true;
-}
-
 static int _perfcounter_enable_default(struct adreno_device *adreno_dev,
 		struct adreno_perfcounters *counters, unsigned int group,
 		unsigned int counter, unsigned int countable)
@@ -790,7 +711,6 @@ static int _perfcounter_enable_default(struct adreno_device *adreno_dev,
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	struct adreno_perfcount_register *reg;
-	struct adreno_perfcount_group *grp;
 	int i;
 	int ret = 0;
 
@@ -805,26 +725,19 @@ static int _perfcounter_enable_default(struct adreno_device *adreno_dev,
 			if (countable == invalid_countable.countables[i])
 				return -EACCES;
 	}
-	grp = &(counters->groups[group]);
-	reg = &(grp->regs[counter]);
+	reg = &(counters->groups[group].regs[counter]);
 
-	if (_perfcounter_inline_update(adreno_dev, group) &&
-		test_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv)) {
+	if (test_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv)) {
 		struct adreno_ringbuffer *rb = &adreno_dev->ringbuffers[0];
 		unsigned int buf[4];
 		unsigned int *cmds = buf;
 		int ret;
 
-		if (gpudev->perfcounter_update && (grp->flags &
-				ADRENO_PERFCOUNTER_GROUP_RESTORE))
-			gpudev->perfcounter_update(adreno_dev, reg, false);
-
 		cmds += cp_wait_for_idle(adreno_dev, cmds);
 		*cmds++ = cp_register(adreno_dev, reg->select, 1);
 		*cmds++ = countable;
 		/* submit to highest priority RB always */
-		ret = adreno_ringbuffer_issue_internal_cmds(rb,
-				KGSL_CMD_FLAGS_PMODE, buf, cmds-buf);
+		ret = adreno_ringbuffer_issuecmds(rb, 0, buf, cmds-buf);
 		if (ret)
 			return ret;
 		/*
@@ -837,34 +750,18 @@ static int _perfcounter_enable_default(struct adreno_device *adreno_dev,
 		/* wait for the above commands submitted to complete */
 		ret = adreno_ringbuffer_waittimestamp(rb, rb->timestamp,
 				ADRENO_IDLE_TIMEOUT);
-		if (ret) {
-			/*
-			 * If we were woken up because of cancelling rb events
-			 * either due to soft reset or adreno_stop, ignore the
-			 * error and return 0 here. The perfcounter is already
-			 * set up in software and it will be programmed in
-			 * hardware when we wake up or come up after soft reset,
-			 * by adreno_perfcounter_restore.
-			 */
-			if (ret == -EAGAIN)
-				ret = 0;
-			else
-				KGSL_DRV_ERR(device,
-				"Perfcounter %u/%u/%u start via commands failed %d\n",
-				group, counter, countable, ret);
-		}
+		if (ret)
+			KGSL_DRV_ERR(device,
+			"Perfcounter %u/%u/%u start via commands failed %d\n",
+			group, counter, countable, ret);
 	} else {
 		/* Select the desired perfcounter */
-		if (gpudev->perfcounter_update && (grp->flags &
-				ADRENO_PERFCOUNTER_GROUP_RESTORE))
-			ret = gpudev->perfcounter_update(adreno_dev, reg, true);
-		else
-			kgsl_regwrite(device, reg->select, countable);
+		kgsl_regwrite(device, reg->select, countable);
 	}
 
 	if (!ret)
 		reg->value = 0;
-	return ret;
+	return 0;
 }
 
 /**
@@ -882,7 +779,6 @@ static int adreno_perfcounter_enable(struct adreno_device *adreno_dev,
 	unsigned int group, unsigned int counter, unsigned int countable)
 {
 	struct adreno_perfcounters *counters = ADRENO_PERFCOUNTERS(adreno_dev);
-	struct adreno_gpudev *gpudev  = ADRENO_GPU_DEVICE(adreno_dev);
 
 	if (counters == NULL)
 		return -EINVAL;
@@ -898,9 +794,7 @@ static int adreno_perfcounter_enable(struct adreno_device *adreno_dev,
 		/* alwayson counter is global, so init value is 0 */
 		break;
 	case KGSL_PERFCOUNTER_GROUP_PWR:
-		if (gpudev->enable_pwr_counters)
-			return gpudev->enable_pwr_counters(adreno_dev, counter);
-		return 0;
+		return _perfcounter_enable_pwr(adreno_dev, counter);
 	case KGSL_PERFCOUNTER_GROUP_VBIF:
 		if (countable > VBIF2_PERF_CNT_SEL_MASK)
 			return -EINVAL;
@@ -926,11 +820,6 @@ static int adreno_perfcounter_enable(struct adreno_device *adreno_dev,
 	case KGSL_PERFCOUNTER_GROUP_ALWAYSON_PWR:
 		_power_counter_enable_alwayson(adreno_dev, counters);
 		break;
-	case KGSL_PERFCOUNTER_GROUP_RBBM:
-		/* The following rbbm countable is not reliable on a540 */
-		if (adreno_is_a540(adreno_dev))
-			if (countable == A5XX_RBBM_ALWAYS_COUNT)
-				return -EINVAL;
 	default:
 		return _perfcounter_enable_default(adreno_dev, counters, group,
 				counter, countable);
@@ -966,7 +855,7 @@ static uint64_t _perfcounter_read_pwr(struct adreno_device *adreno_dev,
 
 	if (adreno_is_a3xx(adreno_dev)) {
 		/* On A3XX we need to freeze the counter so we can read it */
-		if (counter == 0)
+		if (0 == counter)
 			enable_bit = 0x00010000;
 		else
 			enable_bit = 0x00020000;
@@ -1042,9 +931,6 @@ static uint64_t _perfcounter_read_pwrcntr(struct adreno_device *adreno_dev,
 	struct adreno_perfcount_register *reg;
 	unsigned int lo = 0, hi = 0;
 
-	if (!ADRENO_FEATURE(adreno_dev, ADRENO_GPMU))
-		return 0;
-
 	reg = &group->regs[counter];
 
 	kgsl_regread(device, reg->offset, &lo);
diff --git a/drivers/gpu/msm/adreno_perfcounter.h b/drivers/gpu/msm/adreno_perfcounter.h
index bcbc73894029..8c4db38983b1 100644
--- a/drivers/gpu/msm/adreno_perfcounter.h
+++ b/drivers/gpu/msm/adreno_perfcounter.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008-2015, 2017 The Linux Foundation. All rights reserved.
+/* Copyright (c) 2008-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -70,13 +70,6 @@ struct adreno_perfcount_group {
 
 #define ADRENO_PERFCOUNTER_GROUP_FIXED BIT(0)
 
-/*
- * ADRENO_PERFCOUNTER_GROUP_RESTORE indicates CP needs to restore the select
- * registers of this perfcounter group as part of preemption and IFPC
- */
-#define ADRENO_PERFCOUNTER_GROUP_RESTORE BIT(1)
-
-
 /**
  * adreno_perfcounts: all available perfcounter groups
  * @groups: available groups for this device
diff --git a/drivers/gpu/msm/adreno_pm4types.h b/drivers/gpu/msm/adreno_pm4types.h
index 2a330b4474aa..f81c0f20e10b 100644
--- a/drivers/gpu/msm/adreno_pm4types.h
+++ b/drivers/gpu/msm/adreno_pm4types.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -15,10 +15,12 @@
 
 #include "adreno.h"
 
-#define CP_TYPE0_PKT	(0 << 30)
-#define CP_TYPE3_PKT	(3 << 30)
-#define CP_TYPE4_PKT	(4 << 28)
-#define CP_TYPE7_PKT	(7 << 28)
+#define CP_PKT_MASK	0xc0000000
+
+#define CP_TYPE0_PKT	((unsigned int)0 << 30)
+#define CP_TYPE3_PKT	((unsigned int)3 << 30)
+#define CP_TYPE4_PKT    ((unsigned int)4 << 28)
+#define CP_TYPE7_PKT    ((unsigned int)7 << 28)
 
 #define PM4_TYPE4_PKT_SIZE_MAX  128
 
@@ -30,10 +32,21 @@
 #define CP_PREEMPT_TOKEN 0x1E
 /* Bit to set in CP_PREEMPT_TOKEN ordinal for interrupt on preemption */
 #define CP_PREEMPT_ORDINAL_INTERRUPT 24
+/* copy from ME scratch RAM to a register */
+#define CP_SCRATCH_TO_REG      0x4d
+
+/* Copy from REG to ME scratch RAM */
+#define CP_REG_TO_SCRATCH      0x4a
 
 /* Wait for memory writes to complete */
 #define CP_WAIT_MEM_WRITES     0x12
 
+/* Conditional execution based on register comparison */
+#define CP_COND_REG_EXEC       0x47
+
+/* Memory to REG copy */
+#define CP_MEM_TO_REG          0x42
+
 /* initialize CP's micro-engine */
 #define CP_ME_INIT		0x48
 
@@ -55,11 +68,11 @@
 /* switches SMMU pagetable, used on a5xx only */
 #define CP_SMMU_TABLE_UPDATE 0x53
 
-/*  Set internal CP registers, used to indicate context save data addresses */
-#define CP_SET_PSEUDO_REGISTER      0x56
+/* wait until a read completes */
+#define CP_WAIT_UNTIL_READ	0x5c
 
-/* Tell CP the current operation mode, indicates save and restore procedure */
-#define CP_SET_MARKER  0x65
+/* wait until all base/size writes from an IB_PFD packet have completed */
+#define CP_WAIT_IB_PFD_COMPLETE 0x5d
 
 /* register read/modify/write */
 #define CP_REG_RMW		0x21
@@ -73,6 +86,9 @@
 /* write N 32-bit words to memory */
 #define CP_MEM_WRITE		0x3d
 
+/* write CP_PROG_COUNTER value to memory */
+#define CP_MEM_WRITE_CNTR	0x4f
+
 /* conditional execution of a sequence of packets */
 #define CP_COND_EXEC		0x44
 
@@ -82,21 +98,69 @@
 /* generate an event that creates a write to memory when completed */
 #define CP_EVENT_WRITE		0x46
 
+/* generate a VS|PS_done event */
+#define CP_EVENT_WRITE_SHD	0x58
+
+/* generate a cache flush done event */
+#define CP_EVENT_WRITE_CFL	0x59
+
+/* generate a z_pass done event */
+#define CP_EVENT_WRITE_ZPD	0x5b
+
+
 /* initiate fetch of index buffer and draw */
 #define CP_DRAW_INDX		0x22
 
+/* draw using supplied indices in packet */
+#define CP_DRAW_INDX_2		0x36
+
+/* initiate fetch of index buffer and binIDs and draw */
+#define CP_DRAW_INDX_BIN	0x34
+
+/* initiate fetch of bin IDs and draw using supplied indices */
+#define CP_DRAW_INDX_2_BIN	0x35
+
 /* New draw packets defined for A4XX */
 #define CP_DRAW_INDX_OFFSET	0x38
 #define CP_DRAW_INDIRECT	0x28
 #define CP_DRAW_INDX_INDIRECT	0x29
 #define CP_DRAW_AUTO		0x24
 
+/* begin/end initiator for viz query extent processing */
+#define CP_VIZ_QUERY		0x23
+
+/* fetch state sub-blocks and initiate shader code DMAs */
+#define CP_SET_STATE		0x25
+
 /* load constant into chip and to memory */
 #define CP_SET_CONSTANT	0x2d
 
+/* load sequencer instruction memory (pointer-based) */
+#define CP_IM_LOAD		0x27
+
+/* load sequencer instruction memory (code embedded in packet) */
+#define CP_IM_LOAD_IMMEDIATE	0x2b
+
+/* load constants from a location in memory */
+#define CP_LOAD_CONSTANT_CONTEXT 0x2e
+
 /* selective invalidation of state pointers */
 #define CP_INVALIDATE_STATE	0x3b
 
+
+/* dynamically changes shader instruction memory partition */
+#define CP_SET_SHADER_BASES	0x4A
+
+/* sets the 64-bit BIN_MASK register in the PFP */
+#define CP_SET_BIN_MASK	0x50
+
+/* sets the 64-bit BIN_SELECT register in the PFP */
+#define CP_SET_BIN_SELECT	0x51
+
+
+/* updates the current context, if needed */
+#define CP_CONTEXT_UPDATE	0x5e
+
 /* generate interrupt from the command stream */
 #define CP_INTERRUPT		0x40
 
@@ -115,6 +179,12 @@
 /* Inform CP about current render mode (needed for a5xx preemption) */
 #define CP_SET_RENDER_MODE          0x6C
 
+/* copy sequencer instruction memory to system memory */
+#define CP_IM_STORE            0x2c
+
+/* test 2 memory locations to dword values specified */
+#define CP_TEST_TWO_MEMS	0x71
+
 /* Write register, ignoring context state for context sensitive registers */
 #define CP_REG_WR_NO_CTXT  0x78
 
@@ -128,6 +198,9 @@
 /* PFP waits until the FIFO between the PFP and the ME is empty */
 #define CP_WAIT_FOR_ME		0x13
 
+/* Record the real-time when this packet is processed by PFP */
+#define CP_RECORD_PFP_TIMESTAMP	0x11
+
 #define CP_SET_PROTECTED_MODE  0x5f /* sets the register protection mode */
 
 /* Used to switch GPU between secure and non-secure modes */
diff --git a/drivers/gpu/msm/adreno_profile.c b/drivers/gpu/msm/adreno_profile.c
index 2985f24c9f6a..c4fab8a5528a 100644
--- a/drivers/gpu/msm/adreno_profile.c
+++ b/drivers/gpu/msm/adreno_profile.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -22,6 +22,7 @@
 #include "adreno.h"
 #include "adreno_profile.h"
 #include "kgsl_sharedmem.h"
+#include "kgsl_cffdump.h"
 #include "adreno_pm4types.h"
 
 #define ASSIGNS_STR_FORMAT "%.8s:%u "
@@ -82,7 +83,6 @@ static struct adreno_context_type ctxt_type_table[] = {KGSL_CONTEXT_TYPES};
 static const char *get_api_type_str(unsigned int type)
 {
 	int i;
-
 	for (i = 0; i < ARRAY_SIZE(ctxt_type_table) - 1; i++) {
 		if (ctxt_type_table[i].type == type)
 			return ctxt_type_table[i].str;
@@ -447,7 +447,6 @@ static void transfer_results(struct adreno_profile *profile,
 				SIZE_LOG_ENTRY(cnt)) {
 			unsigned int size_tail;
 			uintptr_t boff;
-
 			size_tail = SIZE_LOG_ENTRY(0xffff &
 					*(profile->log_tail));
 			boff = ((uintptr_t) profile->log_tail -
@@ -739,7 +738,7 @@ static ssize_t profile_assignments_write(struct file *filep,
 		goto error_unlock;
 	}
 
-	ret = adreno_perfcntr_active_oob_get(adreno_dev);
+	ret = kgsl_active_count_get(device);
 	if (ret) {
 		size = ret;
 		goto error_unlock;
@@ -747,7 +746,7 @@ static ssize_t profile_assignments_write(struct file *filep,
 
 	/*
 	 * When adding/removing assignments, ensure that the GPU is done with
-	 * all it's work.  This helps to synchronize the work flow to the
+	 * all it's work.  This helps to syncronize the work flow to the
 	 * GPU and avoid racey conditions.
 	 */
 	if (adreno_idle(device)) {
@@ -786,7 +785,7 @@ static ssize_t profile_assignments_write(struct file *filep,
 	size = len;
 
 error_put:
-	adreno_perfcntr_active_oob_put(adreno_dev);
+	kgsl_active_count_put(device);
 error_unlock:
 	mutex_unlock(&device->mutex);
 error_free:
@@ -976,10 +975,10 @@ static ssize_t profile_pipe_print(struct file *filep, char __user *ubuf,
 			if (profile->shared_tail != profile->shared_head) {
 				status = _pipe_print_pending(usr_buf, max);
 				break;
+			} else {
+				status = 0;
+				break;
 			}
-
-			status = 0;
-			break;
 		}
 
 		mutex_unlock(&device->mutex);
@@ -1072,8 +1071,7 @@ void adreno_profile_init(struct adreno_device *adreno_dev)
 	/* allocate shared_buffer, which includes pre_ib and post_ib */
 	profile->shared_size = ADRENO_PROFILE_SHARED_BUF_SIZE_DWORDS;
 	ret = kgsl_allocate_global(device, &profile->shared_buffer,
-			profile->shared_size * sizeof(unsigned int),
-			0, 0, "profile");
+			profile->shared_size * sizeof(unsigned int), 0, 0);
 
 	if (ret) {
 		profile->shared_size = 0;
diff --git a/drivers/gpu/msm/adreno_ringbuffer.c b/drivers/gpu/msm/adreno_ringbuffer.c
index f2b1b2dfe7ea..42fb4659c1f2 100644
--- a/drivers/gpu/msm/adreno_ringbuffer.c
+++ b/drivers/gpu/msm/adreno_ringbuffer.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2019, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -18,6 +18,7 @@
 
 #include "kgsl.h"
 #include "kgsl_sharedmem.h"
+#include "kgsl_cffdump.h"
 #include "kgsl_trace.h"
 #include "kgsl_pwrctrl.h"
 
@@ -25,11 +26,12 @@
 #include "adreno_iommu.h"
 #include "adreno_pm4types.h"
 #include "adreno_ringbuffer.h"
-#include "adreno_trace.h"
 
 #include "a3xx_reg.h"
 #include "adreno_a5xx.h"
 
+#define GSL_RB_NOP_SIZEDWORDS				2
+
 #define RB_HOSTPTR(_rb, _pos) \
 	((unsigned int *) ((_rb)->buffer_desc.hostptr + \
 		((_pos) * sizeof(unsigned int))))
@@ -37,193 +39,291 @@
 #define RB_GPUADDR(_rb, _pos) \
 	((_rb)->buffer_desc.gpuaddr + ((_pos) * sizeof(unsigned int)))
 
-static inline bool is_internal_cmds(unsigned int flags)
+static void _cff_write_ringbuffer(struct adreno_ringbuffer *rb)
 {
-	return (flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE);
-}
+	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	uint64_t gpuaddr;
+	unsigned int *hostptr;
+	size_t size;
+
+	if (device->cff_dump_enable == 0)
+		return;
 
-static void adreno_get_submit_time(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb,
-		struct adreno_submit_time *time)
-{
-	unsigned long flags;
 	/*
-	 * Here we are attempting to create a mapping between the
-	 * GPU time domain (alwayson counter) and the CPU time domain
-	 * (local_clock) by sampling both values as close together as
-	 * possible. This is useful for many types of debugging and
-	 * profiling. In order to make this mapping as accurate as
-	 * possible, we must turn off interrupts to avoid running
-	 * interrupt handlers between the two samples.
+	 * This code is predicated on the fact that we write a full block of
+	 * stuff without wrapping
 	 */
+	BUG_ON(rb->wptr < rb->last_wptr);
 
-	local_irq_save(flags);
+	size = (rb->wptr - rb->last_wptr) * sizeof(unsigned int);
 
-	/* Read always on registers */
-	if (!adreno_is_a3xx(adreno_dev)) {
-		adreno_readreg64(adreno_dev,
-			ADRENO_REG_RBBM_ALWAYSON_COUNTER_LO,
-			ADRENO_REG_RBBM_ALWAYSON_COUNTER_HI,
-			&time->ticks);
+	hostptr = RB_HOSTPTR(rb, rb->last_wptr);
+	gpuaddr = RB_GPUADDR(rb, rb->last_wptr);
 
-		/* Mask hi bits as they may be incorrect on some targets */
-		if (ADRENO_GPUREV(adreno_dev) >= 400 &&
-				ADRENO_GPUREV(adreno_dev) <= ADRENO_REV_A530)
-			time->ticks &= 0xFFFFFFFF;
-	} else
-		time->ticks = 0;
-
-	/* Trace the GPU time to create a mapping to ftrace time */
-	trace_adreno_cmdbatch_sync(rb->drawctxt_active, time->ticks);
+	kgsl_cffdump_memcpy(device, gpuaddr, hostptr, size);
+}
 
-	/* Get the kernel clock for time since boot */
-	time->ktime = local_clock();
+void adreno_ringbuffer_submit(struct adreno_ringbuffer *rb,
+		struct adreno_submit_time *time)
+{
+	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
+	BUG_ON(rb->wptr == 0);
 
-	/* Get the timeofday for the wall time (for the user) */
-	getnstimeofday(&time->utime);
+	/* Write the changes to CFF if so enabled */
+	_cff_write_ringbuffer(rb);
 
-	local_irq_restore(flags);
-}
+	/*
+	 * Read the current GPU ticks and wallclock for most accurate
+	 * profiling
+	 */
 
-static void adreno_ringbuffer_wptr(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb)
-{
-	unsigned long flags;
-	int ret = 0;
+	if (time != NULL) {
+		/*
+		 * Here we are attempting to create a mapping between the
+		 * GPU time domain (alwayson counter) and the CPU time domain
+		 * (local_clock) by sampling both values as close together as
+		 * possible. This is useful for many types of debugging and
+		 * profiling. In order to make this mapping as accurate as
+		 * possible, we must turn off interrupts to avoid running
+		 * interrupt handlers between the two samples.
+		 */
+		unsigned long flags;
+		local_irq_save(flags);
 
-	spin_lock_irqsave(&rb->preempt_lock, flags);
-	if (adreno_in_preempt_state(adreno_dev, ADRENO_PREEMPT_NONE)) {
+		/* Read always on registers */
+		if (!adreno_is_a3xx(adreno_dev)) {
+			adreno_readreg64(adreno_dev,
+				ADRENO_REG_RBBM_ALWAYSON_COUNTER_LO,
+				ADRENO_REG_RBBM_ALWAYSON_COUNTER_HI,
+				&time->ticks);
 
-		if (adreno_dev->cur_rb == rb) {
 			/*
-			 * Let the pwrscale policy know that new commands have
-			 * been submitted.
+			 * Mask hi bits as they may be incorrect on
+			 * a4x and some a5x
 			 */
-			kgsl_pwrscale_busy(KGSL_DEVICE(adreno_dev));
+			if (ADRENO_GPUREV(adreno_dev) >= 400 &&
+				ADRENO_GPUREV(adreno_dev) <= ADRENO_REV_A530)
+				time->ticks &= 0xFFFFFFFF;
+		}
+		else
+			time->ticks = 0;
 
-			/*
-			 * Ensure the write posted after a possible
-			 * GMU wakeup (write could have dropped during wakeup)
-			 */
-			ret = adreno_gmu_fenced_write(adreno_dev,
-				ADRENO_REG_CP_RB_WPTR, rb->_wptr,
-				FENCE_STATUS_WRITEDROPPED0_MASK);
+		/* Get the kernel clock for time since boot */
+		time->ktime = local_clock();
 
-		}
-	}
+		/* Get the timeofday for the wall time (for the user) */
+		getnstimeofday(&time->utime);
 
-	rb->wptr = rb->_wptr;
-	spin_unlock_irqrestore(&rb->preempt_lock, flags);
+		local_irq_restore(flags);
+	}
 
-	if (ret)
-		kgsl_device_snapshot(KGSL_DEVICE(adreno_dev), NULL, false);
+	/* Memory barrier before informing the hardware of new commands */
+	mb();
 
+	if (adreno_preempt_state(adreno_dev, ADRENO_DISPATCHER_PREEMPT_CLEAR) &&
+		(adreno_dev->cur_rb == rb)) {
+		/*
+		 * Let the pwrscale policy know that new commands have
+		 * been submitted.
+		 */
+		kgsl_pwrscale_busy(KGSL_DEVICE(adreno_dev));
+		adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_WPTR, rb->wptr);
+	}
 }
 
-static void adreno_profile_submit_time(struct adreno_submit_time *time)
+int adreno_ringbuffer_submit_spin(struct adreno_ringbuffer *rb,
+		struct adreno_submit_time *time, unsigned int timeout)
 {
-	struct kgsl_drawobj *drawobj;
-	struct kgsl_drawobj_cmd *cmdobj;
-	struct kgsl_mem_entry *entry;
+	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
 
-	if (time == NULL)
-		return;
+	adreno_ringbuffer_submit(rb, NULL);
+	return adreno_spin_idle(adreno_dev, timeout);
+}
 
-	drawobj = time->drawobj;
+static int
+adreno_ringbuffer_waitspace(struct adreno_ringbuffer *rb,
+				unsigned int numcmds, int wptr_ahead)
+{
+	int nopcount = 0;
+	unsigned int freecmds;
+	unsigned int wptr = rb->wptr;
+	unsigned int *cmds = NULL;
+	uint64_t gpuaddr;
+	unsigned long wait_time;
+	unsigned long wait_timeout = msecs_to_jiffies(ADRENO_IDLE_TIMEOUT);
+	unsigned int rptr;
+	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
 
-	if (drawobj == NULL)
-		return;
+	/* if wptr ahead, fill the remaining with NOPs */
+	if (wptr_ahead) {
+		/* -1 for header */
+		nopcount = KGSL_RB_DWORDS - rb->wptr - 1;
+
+		cmds = RB_HOSTPTR(rb, rb->wptr);
+		gpuaddr = RB_GPUADDR(rb, rb->wptr);
+
+		rptr = adreno_get_rptr(rb);
+		/* For non current rb we don't expect the rptr to move */
+		if ((adreno_dev->cur_rb != rb ||
+				!adreno_preempt_state(adreno_dev,
+				ADRENO_DISPATCHER_PREEMPT_CLEAR)) &&
+			!rptr)
+			return -ENOSPC;
+
+		/* Make sure that rptr is not 0 before submitting
+		 * commands at the end of ringbuffer. We do not
+		 * want the rptr and wptr to become equal when
+		 * the ringbuffer is not empty */
+		wait_time = jiffies + wait_timeout;
+		while (!rptr) {
+			rptr = adreno_get_rptr(rb);
+			if (time_after(jiffies, wait_time))
+				return -ETIMEDOUT;
+		}
 
-	cmdobj = CMDOBJ(drawobj);
-	entry = cmdobj->profiling_buf_entry;
+		rb->wptr = 0;
+	}
 
-	if (entry) {
-		struct kgsl_drawobj_profiling_buffer *profile_buffer;
+	rptr = adreno_get_rptr(rb);
+	freecmds = rptr - rb->wptr;
+	if (freecmds == 0 || freecmds > numcmds)
+		goto done;
 
-		profile_buffer = kgsl_gpuaddr_to_vaddr(&entry->memdesc,
-					cmdobj->profiling_buffer_gpuaddr);
+	/* non current rptr will not advance anyway or if preemption underway */
+	if (adreno_dev->cur_rb != rb ||
+		!adreno_preempt_state(adreno_dev,
+			ADRENO_DISPATCHER_PREEMPT_CLEAR)) {
+		rb->wptr = wptr;
+		return -ENOSPC;
+	}
 
-		if (profile_buffer == NULL)
-			return;
+	wait_time = jiffies + wait_timeout;
+	/* wait for space in ringbuffer */
+	while (1) {
+		rptr = adreno_get_rptr(rb);
 
-		/* Return kernel clock time to the the client if requested */
-		if (drawobj->flags & KGSL_DRAWOBJ_PROFILING_KTIME) {
-			uint64_t secs = time->ktime;
+		freecmds = rptr - rb->wptr;
 
-			profile_buffer->wall_clock_ns =
-				do_div(secs, NSEC_PER_SEC);
-			profile_buffer->wall_clock_s = secs;
-		} else {
-			profile_buffer->wall_clock_s = time->utime.tv_sec;
-			profile_buffer->wall_clock_ns = time->utime.tv_nsec;
-		}
+		if (freecmds == 0 || freecmds > numcmds)
+			break;
 
-		profile_buffer->gpu_ticks_queued = time->ticks;
+		if (time_after(jiffies, wait_time)) {
+			KGSL_DRV_ERR(KGSL_DEVICE(adreno_dev),
+			"Timed out waiting for freespace in RB rptr: 0x%x, wptr: 0x%x, rb id %d\n",
+			rptr, wptr, rb->id);
+			return -ETIMEDOUT;
+		}
+	}
+done:
+	if (wptr_ahead) {
+		*cmds = cp_packet(adreno_dev, CP_NOP, nopcount);
+		kgsl_cffdump_write(KGSL_DEVICE(adreno_dev), gpuaddr, *cmds);
 
-		kgsl_memdesc_unmap(&entry->memdesc);
 	}
+	return 0;
 }
 
-void adreno_ringbuffer_submit(struct adreno_ringbuffer *rb,
-		struct adreno_submit_time *time)
+unsigned int *adreno_ringbuffer_allocspace(struct adreno_ringbuffer *rb,
+					unsigned int numcmds)
 {
-	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
-
-	if (time != NULL) {
-		adreno_get_submit_time(adreno_dev, rb, time);
-		/* Put the timevalues in the profiling buffer */
-		adreno_profile_submit_time(time);
+	unsigned int *ptr = NULL;
+	int ret = 0;
+	unsigned int rptr;
+	BUG_ON(numcmds >= KGSL_RB_DWORDS);
+
+	rptr = adreno_get_rptr(rb);
+	/* check for available space */
+	if (rb->wptr >= rptr) {
+		/* wptr ahead or equal to rptr */
+		/* reserve dwords for nop packet */
+		if ((rb->wptr + numcmds) > (KGSL_RB_DWORDS -
+				GSL_RB_NOP_SIZEDWORDS))
+			ret = adreno_ringbuffer_waitspace(rb, numcmds, 1);
+	} else {
+		/* wptr behind rptr */
+		if ((rb->wptr + numcmds) >= rptr)
+			ret = adreno_ringbuffer_waitspace(rb, numcmds, 0);
+		/* check for remaining space */
+		/* reserve dwords for nop packet */
+		if (!ret && (rb->wptr + numcmds) > (KGSL_RB_DWORDS -
+				GSL_RB_NOP_SIZEDWORDS))
+			ret = adreno_ringbuffer_waitspace(rb, numcmds, 1);
 	}
 
-	adreno_ringbuffer_wptr(adreno_dev, rb);
-}
+	if (!ret) {
+		rb->last_wptr = rb->wptr;
 
-int adreno_ringbuffer_submit_spin(struct adreno_ringbuffer *rb,
-		struct adreno_submit_time *time, unsigned int timeout)
-{
-	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
+		ptr = (unsigned int *)rb->buffer_desc.hostptr + rb->wptr;
+		rb->wptr += numcmds;
+	} else
+		ptr = ERR_PTR(ret);
 
-	adreno_ringbuffer_submit(rb, time);
-	return adreno_spin_idle(adreno_dev, timeout);
+	return ptr;
 }
 
-unsigned int *adreno_ringbuffer_allocspace(struct adreno_ringbuffer *rb,
-		unsigned int dwords)
+/**
+ * _ringbuffer_setup_common() - Ringbuffer start
+ * @adreno_dev: Pointer to an adreno_device
+ *
+ * Setup ringbuffer for GPU.
+ */
+static void _ringbuffer_setup_common(struct adreno_device *adreno_dev)
 {
-	struct adreno_device *adreno_dev = ADRENO_RB_DEVICE(rb);
-	unsigned int rptr = adreno_get_rptr(rb);
-	unsigned int ret;
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	struct adreno_ringbuffer *rb;
+	int i;
 
-	if (rptr <= rb->_wptr) {
-		unsigned int *cmds;
+	/* Initialize all of the ringbuffers */
+	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
+		kgsl_sharedmem_set(device, &(rb->buffer_desc), 0,
+			0xAA, KGSL_RB_SIZE);
+		rb->wptr = 0;
+		rb->rptr = 0;
+		rb->wptr_preempt_end = 0xFFFFFFFF;
+		rb->starve_timer_state =
+		ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT;
+	}
 
-		if (rb->_wptr + dwords <= (KGSL_RB_DWORDS - 2)) {
-			ret = rb->_wptr;
-			rb->_wptr = (rb->_wptr + dwords) % KGSL_RB_DWORDS;
-			return RB_HOSTPTR(rb, ret);
-		}
+	/* Continue setting up the current ringbuffer */
+	rb = ADRENO_CURRENT_RINGBUFFER(adreno_dev);
 
-		/*
-		 * There isn't enough space toward the end of ringbuffer. So
-		 * look for space from the beginning of ringbuffer upto the
-		 * read pointer.
-		 */
-		if (dwords < rptr) {
-			cmds = RB_HOSTPTR(rb, rb->_wptr);
-			*cmds = cp_packet(adreno_dev, CP_NOP,
-				KGSL_RB_DWORDS - rb->_wptr - 1);
-			rb->_wptr = dwords;
-			return RB_HOSTPTR(rb, 0);
-		}
-	}
+	/*
+	 * The size of the ringbuffer in the hardware is the log2
+	 * representation of the size in quadwords (sizedwords / 2).
+	 * Also disable the host RPTR shadow register as it might be unreliable
+	 * in certain circumstances.
+	 */
 
-	if (rb->_wptr + dwords < rptr) {
-		ret = rb->_wptr;
-		rb->_wptr = (rb->_wptr + dwords) % KGSL_RB_DWORDS;
-		return RB_HOSTPTR(rb, ret);
-	}
+	adreno_writereg(adreno_dev, ADRENO_REG_CP_RB_CNTL,
+		(ilog2(KGSL_RB_DWORDS >> 1) & 0x3F) |
+		(1 << 27));
 
-	return ERR_PTR(-ENOSPC);
+	adreno_writereg64(adreno_dev, ADRENO_REG_CP_RB_BASE,
+			  ADRENO_REG_CP_RB_BASE_HI, rb->buffer_desc.gpuaddr);
+}
+
+/**
+ * _ringbuffer_start_common() - Ringbuffer start
+ * @adreno_dev: Pointer to an adreno device
+ *
+ * Start ringbuffer for GPU.
+ */
+static int _ringbuffer_start_common(struct adreno_device *adreno_dev)
+{
+	int status;
+	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	struct adreno_ringbuffer *rb = ADRENO_CURRENT_RINGBUFFER(adreno_dev);
+
+	/* clear ME_HALT to start micro engine */
+	adreno_writereg(adreno_dev, ADRENO_REG_CP_ME_CNTL, 0);
+
+	/* ME init is GPU specific, so jump into the sub-function */
+	status = gpudev->rb_init(adreno_dev, rb);
+	if (status)
+		return status;
+
+	return status;
 }
 
 /**
@@ -234,27 +334,16 @@ unsigned int *adreno_ringbuffer_allocspace(struct adreno_ringbuffer *rb,
 int adreno_ringbuffer_start(struct adreno_device *adreno_dev,
 	unsigned int start_type)
 {
+	int status;
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_ringbuffer *rb;
-	int i;
 
-	/* Setup the ringbuffers state before we start */
-	FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
-		kgsl_sharedmem_set(device, &(rb->buffer_desc),
-				0, 0xAA, KGSL_RB_SIZE);
-		if (!adreno_is_a3xx(adreno_dev))
-			kgsl_sharedmem_writel(device, &device->scratch,
-					SCRATCH_RPTR_OFFSET(rb->id), 0);
-		rb->wptr = 0;
-		rb->_wptr = 0;
-		rb->wptr_preempt_end = 0xFFFFFFFF;
-		rb->starve_timer_state =
-			ADRENO_DISPATCHER_RB_STARVE_TIMER_UNINIT;
-	}
+	_ringbuffer_setup_common(adreno_dev);
+
+	status = gpudev->microcode_load(adreno_dev, start_type);
+	if (status)
+		return status;
 
-	/* start is specific GPU rb */
-	return gpudev->rb_start(adreno_dev, start_type);
+	return _ringbuffer_start_common(adreno_dev);
 }
 
 void adreno_ringbuffer_stop(struct adreno_device *adreno_dev)
@@ -274,7 +363,7 @@ static int _rb_readtimestamp(struct kgsl_device *device,
 		timestamp);
 }
 
-static int _adreno_ringbuffer_probe(struct adreno_device *adreno_dev,
+static int _adreno_ringbuffer_init(struct adreno_device *adreno_dev,
 		int id)
 {
 	struct adreno_ringbuffer *rb = &adreno_dev->ringbuffers[id];
@@ -289,39 +378,24 @@ static int _adreno_ringbuffer_probe(struct adreno_device *adreno_dev,
 	rb->timestamp = 0;
 	init_waitqueue_head(&rb->ts_expire_waitq);
 
-	spin_lock_init(&rb->preempt_lock);
-
 	/*
 	 * Allocate mem for storing RB pagetables and commands to
 	 * switch pagetable
 	 */
 	ret = kgsl_allocate_global(KGSL_DEVICE(adreno_dev), &rb->pagetable_desc,
-		PAGE_SIZE, 0, KGSL_MEMDESC_PRIVILEGED, "pagetable_desc");
+		PAGE_SIZE, 0, KGSL_MEMDESC_PRIVILEGED);
 	if (ret)
 		return ret;
 
-	/* allocate a chunk of memory to create user profiling IB1s */
-	kgsl_allocate_global(KGSL_DEVICE(adreno_dev), &rb->profile_desc,
-		PAGE_SIZE, KGSL_MEMFLAGS_GPUREADONLY, 0, "profile_desc");
-
 	return kgsl_allocate_global(KGSL_DEVICE(adreno_dev), &rb->buffer_desc,
-			KGSL_RB_SIZE, KGSL_MEMFLAGS_GPUREADONLY,
-			0, "ringbuffer");
+			KGSL_RB_SIZE, KGSL_MEMFLAGS_GPUREADONLY, 0);
 }
 
-int adreno_ringbuffer_probe(struct adreno_device *adreno_dev, bool nopreempt)
+int adreno_ringbuffer_init(struct adreno_device *adreno_dev, bool nopreempt)
 {
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
+	int status = 0;
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 	int i;
-	int status = -ENOMEM;
-
-	if (!adreno_is_a3xx(adreno_dev)) {
-		status = kgsl_allocate_global(device, &device->scratch,
-				PAGE_SIZE, 0, KGSL_MEMDESC_RANDOM, "scratch");
-		if (status != 0)
-			return status;
-	}
 
 	if (nopreempt == false && ADRENO_FEATURE(adreno_dev, ADRENO_PREEMPTION))
 		adreno_dev->num_ringbuffers = gpudev->num_prio_levels;
@@ -329,7 +403,7 @@ int adreno_ringbuffer_probe(struct adreno_device *adreno_dev, bool nopreempt)
 		adreno_dev->num_ringbuffers = 1;
 
 	for (i = 0; i < adreno_dev->num_ringbuffers; i++) {
-		status = _adreno_ringbuffer_probe(adreno_dev, i);
+		status = _adreno_ringbuffer_init(adreno_dev, i);
 		if (status != 0)
 			break;
 	}
@@ -349,7 +423,7 @@ static void _adreno_ringbuffer_close(struct adreno_device *adreno_dev,
 
 	kgsl_free_global(device, &rb->pagetable_desc);
 	kgsl_free_global(device, &rb->preemption_desc);
-	kgsl_free_global(device, &rb->profile_desc);
+
 	kgsl_free_global(device, &rb->buffer_desc);
 	kgsl_del_event_group(&rb->events);
 	memset(rb, 0, sizeof(struct adreno_ringbuffer));
@@ -357,13 +431,9 @@ static void _adreno_ringbuffer_close(struct adreno_device *adreno_dev,
 
 void adreno_ringbuffer_close(struct adreno_device *adreno_dev)
 {
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 	struct adreno_ringbuffer *rb;
 	int i;
 
-	if (!adreno_is_a3xx(adreno_dev))
-		kgsl_free_global(device, &device->scratch);
-
 	FOR_EACH_RINGBUFFER(adreno_dev, rb, i)
 		_adreno_ringbuffer_close(adreno_dev, rb);
 }
@@ -419,18 +489,6 @@ int cp_secure_mode(struct adreno_device *adreno_dev, uint *cmds,
 	return cmds - start;
 }
 
-static inline int cp_mem_write(struct adreno_device *adreno_dev,
-		unsigned int *cmds, uint64_t gpuaddr, unsigned int value)
-{
-	int dwords = 0;
-
-	cmds[dwords++] = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
-	dwords += cp_gpuaddr(adreno_dev, &cmds[dwords], gpuaddr);
-	cmds[dwords++] = value;
-
-	return dwords;
-}
-
 static int
 adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 				unsigned int flags, unsigned int *cmds,
@@ -444,25 +502,22 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	unsigned int total_sizedwords = sizedwords;
 	unsigned int i;
 	unsigned int context_id = 0;
+	uint64_t gpuaddr = device->memstore.gpuaddr;
 	bool profile_ready;
 	struct adreno_context *drawctxt = rb->drawctxt_active;
 	struct kgsl_context *context = NULL;
 	bool secured_ctxt = false;
+	uint64_t cond_addr;
 	static unsigned int _seq_cnt;
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_SQE);
 
 	if (drawctxt != NULL && kgsl_context_detached(&drawctxt->base) &&
-		!is_internal_cmds(flags))
+		!(flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE))
 		return -ENOENT;
 
-	/* On fault return error so that we don't keep submitting */
-	if (adreno_gpu_fault(adreno_dev) != 0)
-		return -EPROTO;
-
 	rb->timestamp++;
 
 	/* If this is a internal IB, use the global timestamp for it */
-	if (!drawctxt || is_internal_cmds(flags))
+	if (!drawctxt || (flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE))
 		timestamp = rb->timestamp;
 	else {
 		context_id = drawctxt->base.id;
@@ -491,23 +546,21 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	 */
 	profile_ready = drawctxt &&
 		adreno_profile_assignments_ready(&adreno_dev->profile) &&
-		!is_internal_cmds(flags);
+		!(flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE);
 
-	/*
-	 * reserve space to temporarily turn off protected mode
-	 * error checking if needed
-	 */
+	/* reserve space to temporarily turn off protected mode
+	*  error checking if needed
+	*/
 	total_sizedwords += flags & KGSL_CMD_FLAGS_PMODE ? 4 : 0;
 	/* 2 dwords to store the start of command sequence */
 	total_sizedwords += 2;
 	/* internal ib command identifier for the ringbuffer */
-	total_sizedwords += is_internal_cmds(flags) ? 2 : 0;
+	total_sizedwords += (flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE) ? 2 : 0;
 
 	total_sizedwords += (secured_ctxt) ? 26 : 0;
 
 	/* _seq mem write for each submission */
-	if (adreno_is_a5xx(adreno_dev))
-		total_sizedwords += 4;
+	total_sizedwords += 4;
 
 	/* context rollover */
 	if (adreno_is_a3xx(adreno_dev))
@@ -518,12 +571,12 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 		total_sizedwords += 4;
 
 	if (gpudev->preemption_pre_ibsubmit &&
-			adreno_is_preemption_enabled(adreno_dev))
-		total_sizedwords += 27;
+				adreno_is_preemption_enabled(adreno_dev))
+		total_sizedwords += 22;
 
 	if (gpudev->preemption_post_ibsubmit &&
-			adreno_is_preemption_enabled(adreno_dev))
-		total_sizedwords += 10;
+				adreno_is_preemption_enabled(adreno_dev))
+		total_sizedwords += 13;
 
 	/*
 	 * a5xx uses 64 bit memory address. pm4 commands that involve read/write
@@ -532,12 +585,12 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	 * required in ringbuffer and adjust the write pointer depending on
 	 * gpucore at the end of this function.
 	 */
-	total_sizedwords += 8; /* sop timestamp */
+	total_sizedwords += 4; /* sop timestamp */
 	total_sizedwords += 5; /* eop timestamp */
 
-	if (drawctxt && !is_internal_cmds(flags)) {
-		/* global timestamp without cache flush for non-zero context */
-		total_sizedwords += 4;
+	if (drawctxt && !(flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE)) {
+		total_sizedwords += 4; /* global timestamp without cache
+					* flush for non-zero context */
 	}
 
 	if (flags & KGSL_CMD_FLAGS_WFI)
@@ -550,20 +603,12 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	if (flags & KGSL_CMD_FLAGS_PWRON_FIXUP)
 		total_sizedwords += 9;
 
-	/* Don't insert any commands if stall on fault is not supported. */
-	if ((ADRENO_GPUREV(adreno_dev) > 500) && !adreno_is_a510(adreno_dev)) {
-		/*
-		 * WAIT_MEM_WRITES - needed in the stall on fault case
-		 * to prevent out of order CP operations that can result
-		 * in a CACHE_FLUSH_TS interrupt storm
-		 */
-		if (test_bit(KGSL_FT_PAGEFAULT_GPUHALT_ENABLE,
+	/* WAIT_MEM_WRITES - needed in the stall on fault case
+	 * to prevent out of order CP operations that can result
+	 * in a CACHE_FLUSH_TS interrupt storm */
+	if (test_bit(KGSL_FT_PAGEFAULT_GPUHALT_ENABLE,
 				&adreno_dev->ft_pf_policy))
-			total_sizedwords += 1;
-	}
-
-	if (gpudev->set_marker)
-		total_sizedwords += 4;
+		total_sizedwords += 1;
 
 	ringcmds = adreno_ringbuffer_allocspace(rb, total_sizedwords);
 	if (IS_ERR(ringcmds))
@@ -575,23 +620,20 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	*ringcmds++ = KGSL_CMD_IDENTIFIER;
 
 	if (adreno_is_preemption_enabled(adreno_dev) &&
-				gpudev->preemption_pre_ibsubmit)
+				gpudev->preemption_pre_ibsubmit) {
+		cond_addr = device->memstore.gpuaddr +
+					KGSL_MEMSTORE_OFFSET(context_id,
+					 preempted);
 		ringcmds += gpudev->preemption_pre_ibsubmit(
-					adreno_dev, rb, ringcmds, context);
+					adreno_dev, rb, ringcmds, context,
+					cond_addr, NULL);
+	}
 
-	if (is_internal_cmds(flags)) {
+	if (flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE) {
 		*ringcmds++ = cp_packet(adreno_dev, CP_NOP, 1);
 		*ringcmds++ = KGSL_CMD_INTERNAL_IDENTIFIER;
 	}
 
-	if (gpudev->set_marker) {
-		/* Firmware versions before 1.49 do not support IFPC markers */
-		if (adreno_is_a6xx(adreno_dev) && (fw->version & 0xFFF) < 0x149)
-			ringcmds += gpudev->set_marker(ringcmds, IB1LIST_START);
-		else
-			ringcmds += gpudev->set_marker(ringcmds, IFPC_DISABLE);
-	}
-
 	if (flags & KGSL_CMD_FLAGS_PWRON_FIXUP) {
 		/* Disable protected mode for the fixup */
 		*ringcmds++ = cp_packet(adreno_dev, CP_SET_PROTECTED_MODE, 1);
@@ -615,15 +657,16 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 		adreno_profile_preib_processing(adreno_dev, drawctxt,
 				&flags, &ringcmds);
 
-	/* start-of-pipeline timestamp for the context */
-	if (drawctxt && !is_internal_cmds(flags))
-		ringcmds += cp_mem_write(adreno_dev, ringcmds,
-			MEMSTORE_ID_GPU_ADDR(device, context_id, soptimestamp),
-			timestamp);
-
-	/* start-of-pipeline timestamp for the ringbuffer */
-	ringcmds += cp_mem_write(adreno_dev, ringcmds,
-		MEMSTORE_RB_GPU_ADDR(device, rb, soptimestamp), rb->timestamp);
+	/* start-of-pipeline timestamp */
+	*ringcmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
+	if (drawctxt && !(flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE))
+		ringcmds += cp_gpuaddr(adreno_dev, ringcmds,
+			gpuaddr + KGSL_MEMSTORE_OFFSET(context_id,
+			soptimestamp));
+	else
+		ringcmds += cp_gpuaddr(adreno_dev, ringcmds,
+			gpuaddr + KGSL_MEMSTORE_RB_OFFSET(rb, soptimestamp));
+	*ringcmds++ = timestamp;
 
 	if (secured_ctxt)
 		ringcmds += cp_secure_mode(adreno_dev, ringcmds, 1);
@@ -653,36 +696,30 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 		ringcmds += cp_wait_for_idle(adreno_dev, ringcmds);
 	}
 
-	/*
-	 * Add any postIB required for profiling if it is enabled and has
-	 * assigned counters
-	 */
+	/* Add any postIB required for profiling if it is enabled and has
+	   assigned counters */
 	if (profile_ready)
 		adreno_profile_postib_processing(adreno_dev, &flags, &ringcmds);
 
-	/* Don't insert any commands if stall on fault is not supported. */
-	if ((ADRENO_GPUREV(adreno_dev) > 500) && !adreno_is_a510(adreno_dev)) {
-		/*
-		 * WAIT_MEM_WRITES - needed in the stall on fault case
-		 * to prevent out of order CP operations that can result
-		 * in a CACHE_FLUSH_TS interrupt storm
-		 */
-		if (test_bit(KGSL_FT_PAGEFAULT_GPUHALT_ENABLE,
+	/*
+	 * WAIT_MEM_WRITES - needed in the stall on fault case to prevent
+	 * out of order CP operations that can result in a CACHE_FLUSH_TS
+	 * interrupt storm
+	 */
+	if (test_bit(KGSL_FT_PAGEFAULT_GPUHALT_ENABLE,
 				&adreno_dev->ft_pf_policy))
-			*ringcmds++ = cp_packet(adreno_dev,
-						CP_WAIT_MEM_WRITES, 0);
-	}
+		*ringcmds++ = cp_packet(adreno_dev, CP_WAIT_MEM_WRITES, 0);
 
 	/*
 	 * Do a unique memory write from the GPU. This can be used in
 	 * early detection of timestamp interrupt storms to stave
 	 * off system collapse.
 	 */
-	if (adreno_is_a5xx(adreno_dev))
-		ringcmds += cp_mem_write(adreno_dev, ringcmds,
-				MEMSTORE_ID_GPU_ADDR(device,
-				KGSL_MEMSTORE_GLOBAL,
-				ref_wait_ts), ++_seq_cnt);
+	*ringcmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
+	ringcmds += cp_gpuaddr(adreno_dev, ringcmds, gpuaddr +
+			KGSL_MEMSTORE_OFFSET(KGSL_MEMSTORE_GLOBAL,
+				ref_wait_ts));
+	*ringcmds++ = ++_seq_cnt;
 
 	/*
 	 * end-of-pipeline timestamp.  If per context timestamps is not
@@ -690,33 +727,25 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	 * set and hence the rb timestamp will be used in else statement below.
 	 */
 	*ringcmds++ = cp_mem_packet(adreno_dev, CP_EVENT_WRITE, 3, 1);
-	if (drawctxt || is_internal_cmds(flags))
+	if (drawctxt || (flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE))
 		*ringcmds++ = CACHE_FLUSH_TS | (1 << 31);
 	else
 		*ringcmds++ = CACHE_FLUSH_TS;
 
-	if (drawctxt && !is_internal_cmds(flags)) {
-		ringcmds += cp_gpuaddr(adreno_dev, ringcmds,
-			MEMSTORE_ID_GPU_ADDR(device, context_id, eoptimestamp));
+	if (drawctxt && !(flags & KGSL_CMD_FLAGS_INTERNAL_ISSUE)) {
+		ringcmds += cp_gpuaddr(adreno_dev, ringcmds, gpuaddr +
+				KGSL_MEMSTORE_OFFSET(context_id, eoptimestamp));
 		*ringcmds++ = timestamp;
-
-		/* Write the end of pipeline timestamp to the ringbuffer too */
-		ringcmds += cp_mem_write(adreno_dev, ringcmds,
-			MEMSTORE_RB_GPU_ADDR(device, rb, eoptimestamp),
-			rb->timestamp);
+		*ringcmds++ = cp_mem_packet(adreno_dev, CP_MEM_WRITE, 2, 1);
+		ringcmds += cp_gpuaddr(adreno_dev, ringcmds, gpuaddr +
+				KGSL_MEMSTORE_RB_OFFSET(rb, eoptimestamp));
+		*ringcmds++ = rb->timestamp;
 	} else {
-		ringcmds += cp_gpuaddr(adreno_dev, ringcmds,
-			MEMSTORE_RB_GPU_ADDR(device, rb, eoptimestamp));
+		ringcmds += cp_gpuaddr(adreno_dev, ringcmds, gpuaddr +
+				KGSL_MEMSTORE_RB_OFFSET(rb, eoptimestamp));
 		*ringcmds++ = timestamp;
 	}
 
-	if (gpudev->set_marker) {
-		if (adreno_is_a6xx(adreno_dev) && (fw->version & 0xFFF) < 0x149)
-			ringcmds += gpudev->set_marker(ringcmds, IB1LIST_END);
-		else
-			ringcmds += gpudev->set_marker(ringcmds, IFPC_ENABLE);
-	}
-
 	if (adreno_is_a3xx(adreno_dev)) {
 		/* Dummy set-constant to trigger context rollover */
 		*ringcmds++ = cp_packet(adreno_dev, CP_SET_CONSTANT, 2);
@@ -725,8 +754,9 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 		*ringcmds++ = 0;
 	}
 
-	if (flags & KGSL_CMD_FLAGS_WFI)
+	if (flags & KGSL_CMD_FLAGS_WFI) {
 		ringcmds += cp_wait_for_idle(adreno_dev, ringcmds);
+	}
 
 	if (secured_ctxt)
 		ringcmds += cp_secure_mode(adreno_dev, ringcmds, 0);
@@ -734,7 +764,7 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	if (gpudev->preemption_post_ibsubmit &&
 				adreno_is_preemption_enabled(adreno_dev))
 		ringcmds += gpudev->preemption_post_ibsubmit(adreno_dev,
-			ringcmds);
+					rb, ringcmds, &drawctxt->base);
 
 	/*
 	 * If we have more ringbuffer commands than space reserved
@@ -748,7 +778,7 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 	 *  required. If we have commands less than the space reserved in RB
 	 *  adjust the wptr accordingly.
 	 */
-	rb->_wptr = rb->_wptr - (total_sizedwords - (ringcmds - start));
+	rb->wptr = rb->wptr - (total_sizedwords - (ringcmds - start));
 
 	adreno_ringbuffer_submit(rb, time);
 
@@ -756,7 +786,7 @@ adreno_ringbuffer_addcmds(struct adreno_ringbuffer *rb,
 }
 
 int
-adreno_ringbuffer_issue_internal_cmds(struct adreno_ringbuffer *rb,
+adreno_ringbuffer_issuecmds(struct adreno_ringbuffer *rb,
 				unsigned int flags,
 				unsigned int *cmds,
 				int sizedwords)
@@ -767,54 +797,93 @@ adreno_ringbuffer_issue_internal_cmds(struct adreno_ringbuffer *rb,
 		sizedwords, 0, NULL);
 }
 
+/**
+ * _ringbuffer_verify_ib() - Check if an IB's size is within a permitted limit
+ * @device: The kgsl device pointer
+ * @ibdesc: Pointer to the IB descriptor
+ */
+static inline bool _ringbuffer_verify_ib(struct kgsl_device_private *dev_priv,
+		struct kgsl_context *context, struct kgsl_memobj_node *ib)
+{
+	struct kgsl_device *device = dev_priv->device;
+	struct kgsl_process_private *private = dev_priv->process_priv;
+
+	/* The maximum allowable size for an IB in the CP is 0xFFFFF dwords */
+	if (ib->size == 0 || ((ib->size >> 2) > 0xFFFFF)) {
+		pr_context(device, context, "ctxt %d invalid ib size %lld\n",
+			context->id, ib->size);
+		return false;
+	}
 
+	/* Make sure that the address is mapped */
+	if (!kgsl_mmu_gpuaddr_in_range(private->pagetable, ib->gpuaddr)) {
+		pr_context(device, context, "ctxt %d invalid ib gpuaddr %llX\n",
+			context->id, ib->gpuaddr);
+		return false;
+	}
 
-static void adreno_ringbuffer_set_constraint(struct kgsl_device *device,
-			struct kgsl_drawobj *drawobj)
+	return true;
+}
+
+int
+adreno_ringbuffer_issueibcmds(struct kgsl_device_private *dev_priv,
+				struct kgsl_context *context,
+				struct kgsl_cmdbatch *cmdbatch,
+				uint32_t *timestamp)
 {
-	struct kgsl_context *context = drawobj->context;
-	unsigned long flags = drawobj->flags;
+	struct kgsl_device *device = dev_priv->device;
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+	struct adreno_context *drawctxt = ADRENO_CONTEXT(context);
+	struct kgsl_memobj_node *ib;
+	int ret;
+
+	if (kgsl_context_invalid(context))
+		return -EDEADLK;
+
+	/* Verify the IBs before they get queued */
+	list_for_each_entry(ib, &cmdbatch->cmdlist, node)
+		if (_ringbuffer_verify_ib(dev_priv, context, ib) == false)
+			return -EINVAL;
+
+	/* wait for the suspend gate */
+	wait_for_completion(&device->cmdbatch_gate);
 
+	/*
+	 * Clear the wake on touch bit to indicate an IB has been
+	 * submitted since the last time we set it. But only clear
+	 * it when we have rendering commands.
+	 */
+	if (!(cmdbatch->flags & KGSL_CMDBATCH_MARKER)
+		&& !(cmdbatch->flags & KGSL_CMDBATCH_SYNC))
+		device->flags &= ~KGSL_FLAG_WAKE_ON_TOUCH;
+
+	/* Queue the command in the ringbuffer */
+	ret = adreno_dispatcher_queue_cmd(adreno_dev, drawctxt, cmdbatch,
+		timestamp);
+
+	/*
+	 * Return -EPROTO if the device has faulted since the last time we
+	 * checked - userspace uses this to perform post-fault activities
+	 */
+	if (!ret && test_and_clear_bit(ADRENO_CONTEXT_FAULT, &context->priv))
+		ret = -EPROTO;
+
+	return ret;
+}
+
+static void adreno_ringbuffer_set_constraint(struct kgsl_device *device,
+			struct kgsl_cmdbatch *cmdbatch)
+{
+	struct kgsl_context *context = cmdbatch->context;
 	/*
 	 * Check if the context has a constraint and constraint flags are
 	 * set.
 	 */
 	if (context->pwr_constraint.type &&
 		((context->flags & KGSL_CONTEXT_PWR_CONSTRAINT) ||
-			(drawobj->flags & KGSL_CONTEXT_PWR_CONSTRAINT)))
+			(cmdbatch->flags & KGSL_CONTEXT_PWR_CONSTRAINT)))
 		kgsl_pwrctrl_set_constraint(device, &context->pwr_constraint,
 						context->id);
-
-	if (context->l3_pwr_constraint.type &&
-			((context->flags & KGSL_CONTEXT_PWR_CONSTRAINT) ||
-				(flags & KGSL_CONTEXT_PWR_CONSTRAINT))) {
-
-		if (IS_ERR_OR_NULL(device->l3_clk)) {
-			KGSL_DEV_ERR_ONCE(device, "Cannot set L3 constraint\n");
-			return;
-		}
-
-		switch (context->l3_pwr_constraint.type) {
-
-		case KGSL_CONSTRAINT_L3_PWRLEVEL: {
-			unsigned int sub_type;
-
-			sub_type = context->l3_pwr_constraint.sub_type;
-
-			if (sub_type == KGSL_CONSTRAINT_L3_PWR_MED)
-				clk_set_rate(device->l3_clk,
-						device->l3_freq[1]);
-
-			if (sub_type == KGSL_CONSTRAINT_L3_PWR_MAX)
-				clk_set_rate(device->l3_clk,
-						device->l3_freq[2]);
-			}
-			break;
-		case KGSL_CONSTRAINT_L3_NONE:
-			clk_set_rate(device->l3_clk, device->l3_freq[0]);
-			break;
-		}
-	}
 }
 
 static inline int _get_alwayson_counter(struct adreno_device *adreno_dev,
@@ -842,45 +911,11 @@ static inline int _get_alwayson_counter(struct adreno_device *adreno_dev,
 	return (unsigned int)(p - cmds);
 }
 
-/* This is the maximum possible size for 64 bit targets */
-#define PROFILE_IB_DWORDS 4
-#define PROFILE_IB_SLOTS (PAGE_SIZE / (PROFILE_IB_DWORDS << 2))
-
-static int set_user_profiling(struct adreno_device *adreno_dev,
-		struct adreno_ringbuffer *rb, u32 *cmds, u64 gpuaddr)
-{
-	int dwords, index = 0;
-	u64 ib_gpuaddr;
-	u32 *ib;
-
-	if (!rb->profile_desc.hostptr)
-		return 0;
-
-	ib = ((u32 *) rb->profile_desc.hostptr) +
-		(rb->profile_index * PROFILE_IB_DWORDS);
-	ib_gpuaddr = rb->profile_desc.gpuaddr +
-		(rb->profile_index * (PROFILE_IB_DWORDS << 2));
-
-	dwords = _get_alwayson_counter(adreno_dev, ib, gpuaddr);
-
-	/* Make an indirect buffer for the request */
-	cmds[index++] = cp_mem_packet(adreno_dev, CP_INDIRECT_BUFFER_PFE, 2, 1);
-	index += cp_gpuaddr(adreno_dev, &cmds[index], ib_gpuaddr);
-	cmds[index++] = dwords;
-
-	rb->profile_index = (rb->profile_index + 1) % PROFILE_IB_SLOTS;
-
-	return index;
-}
-
 /* adreno_rindbuffer_submitcmd - submit userspace IBs to the GPU */
 int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
-		struct kgsl_drawobj_cmd *cmdobj,
-		struct adreno_submit_time *time)
+		struct kgsl_cmdbatch *cmdbatch, struct adreno_submit_time *time)
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
 	struct kgsl_memobj_node *ib;
 	unsigned int numibs = 0;
 	unsigned int *link;
@@ -888,23 +923,25 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 	struct kgsl_context *context;
 	struct adreno_context *drawctxt;
 	bool use_preamble = true;
-	bool user_profiling = false;
-	bool kernel_profiling = false;
+	bool cmdbatch_user_profiling = false;
+	bool cmdbatch_kernel_profiling = false;
 	int flags = KGSL_CMD_FLAGS_NONE;
 	int ret;
 	struct adreno_ringbuffer *rb;
+	struct kgsl_cmdbatch_profiling_buffer *profile_buffer = NULL;
 	unsigned int dwords = 0;
 	struct adreno_submit_time local;
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_SQE);
-	bool set_ib1list_marker = false;
 
-	memset(&local, 0x0, sizeof(local));
+	struct kgsl_mem_entry *entry = cmdbatch->profiling_buf_entry;
+	if (entry)
+		profile_buffer = kgsl_gpuaddr_to_vaddr(&entry->memdesc,
+					cmdbatch->profiling_buffer_gpuaddr);
 
-	context = drawobj->context;
+	context = cmdbatch->context;
 	drawctxt = ADRENO_CONTEXT(context);
 
 	/* Get the total IBs in the list */
-	list_for_each_entry(ib, &cmdobj->cmdlist, node)
+	list_for_each_entry(ib, &cmdbatch->cmdlist, node)
 		numibs++;
 
 	rb = drawctxt->rb;
@@ -921,28 +958,26 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 	 * c) force preamble for commandbatch
 	 */
 	if (test_bit(ADRENO_CONTEXT_SKIP_CMD, &drawctxt->base.priv) &&
-		(!test_bit(CMDOBJ_SKIP, &cmdobj->priv))) {
+		(!test_bit(CMDBATCH_FLAG_SKIP, &cmdbatch->priv))) {
 
-		set_bit(KGSL_FT_SKIPCMD, &cmdobj->fault_recovery);
-		cmdobj->fault_policy = drawctxt->fault_policy;
-		set_bit(CMDOBJ_FORCE_PREAMBLE, &cmdobj->priv);
+		set_bit(KGSL_FT_SKIPCMD, &cmdbatch->fault_recovery);
+		cmdbatch->fault_policy = drawctxt->fault_policy;
+		set_bit(CMDBATCH_FLAG_FORCE_PREAMBLE, &cmdbatch->priv);
 
 		/* if context is detached print fault recovery */
-		adreno_fault_skipcmd_detached(adreno_dev, drawctxt, drawobj);
+		adreno_fault_skipcmd_detached(adreno_dev, drawctxt, cmdbatch);
 
 		/* clear the drawctxt flags */
 		clear_bit(ADRENO_CONTEXT_SKIP_CMD, &drawctxt->base.priv);
 		drawctxt->fault_policy = 0;
 	}
 
-	/*
-	 * When preamble is enabled, the preamble buffer with state restoration
-	 * commands are stored in the first node of the IB chain.
-	 * We can skip that if a context switch hasn't occurred.
-	 */
+	/*When preamble is enabled, the preamble buffer with state restoration
+	commands are stored in the first node of the IB chain. We can skip that
+	if a context switch hasn't occured */
 
 	if ((drawctxt->base.flags & KGSL_CONTEXT_PREAMBLE) &&
-		!test_bit(CMDOBJ_FORCE_PREAMBLE, &cmdobj->priv) &&
+		!test_bit(CMDBATCH_FLAG_FORCE_PREAMBLE, &cmdbatch->priv) &&
 		(rb->drawctxt_active == drawctxt))
 		use_preamble = false;
 
@@ -952,7 +987,7 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 	 * the accounting sane. Set start_index and numibs to 0 to just
 	 * generate the start and end markers and skip everything else
 	 */
-	if (test_bit(CMDOBJ_SKIP, &cmdobj->priv)) {
+	if (test_bit(CMDBATCH_FLAG_SKIP, &cmdbatch->priv)) {
 		use_preamble = false;
 		numibs = 0;
 	}
@@ -969,16 +1004,17 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 	/* Each IB takes up 30 dwords in worst case */
 	dwords += (numibs * 30);
 
-	if (drawobj->flags & KGSL_DRAWOBJ_PROFILING &&
-		!adreno_is_a3xx(adreno_dev) &&
-		(cmdobj->profiling_buf_entry != NULL)) {
-		user_profiling = true;
+	if (cmdbatch->flags & KGSL_CMDBATCH_PROFILING &&
+		!adreno_is_a3xx(adreno_dev) && profile_buffer) {
+		cmdbatch_user_profiling = true;
+		dwords += 6;
 
 		/*
-		 * User side profiling uses two IB1s, one before with 4 dwords
-		 * per INDIRECT_BUFFER_PFE call
+		 * REG_TO_MEM packet on A5xx needs another ordinal.
+		 * Add 2 more dwords since we do profiling before and after.
 		 */
-		dwords += 8;
+		if (adreno_is_a5xx(adreno_dev))
+			dwords += 2;
 
 		/*
 		 * we want to use an adreno_submit_time struct to get the
@@ -989,36 +1025,16 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 
 		if (time == NULL)
 			time = &local;
-
-		time->drawobj = drawobj;
 	}
 
-	if (test_bit(CMDOBJ_PROFILE, &cmdobj->priv)) {
-		kernel_profiling = true;
+	if (test_bit(CMDBATCH_FLAG_PROFILE, &cmdbatch->priv)) {
+		cmdbatch_kernel_profiling = true;
 		dwords += 6;
-		if (!ADRENO_LEGACY_PM4(adreno_dev))
+		if (adreno_is_a5xx(adreno_dev))
 			dwords += 2;
 	}
 
-	if (adreno_is_preemption_enabled(adreno_dev))
-		if (gpudev->preemption_yield_enable)
-			dwords += 8;
-
-	/*
-	 * Prior to SQE FW version 1.49, there was only one marker for
-	 * both preemption and IFPC. Only include the IB1LIST markers if
-	 * we are using a firmware that supports them.
-	 */
-	if (gpudev->set_marker && numibs && adreno_is_a6xx(adreno_dev) &&
-			((fw->version & 0xFFF) >= 0x149)) {
-		set_ib1list_marker = true;
-		dwords += 4;
-	}
-
-	if (gpudev->ccu_invalidate)
-		dwords += 4;
-
-	link = kcalloc(dwords, sizeof(unsigned int), GFP_KERNEL);
+	link = kzalloc(sizeof(unsigned int) *  dwords, GFP_KERNEL);
 	if (!link) {
 		ret = -ENOMEM;
 		goto done;
@@ -1029,29 +1045,26 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 	*cmds++ = cp_packet(adreno_dev, CP_NOP, 1);
 	*cmds++ = KGSL_START_OF_IB_IDENTIFIER;
 
-	if (kernel_profiling) {
+	if (cmdbatch_kernel_profiling) {
 		cmds += _get_alwayson_counter(adreno_dev, cmds,
-			adreno_dev->profile_buffer.gpuaddr +
-			ADRENO_DRAWOBJ_PROFILE_OFFSET(cmdobj->profile_index,
+			adreno_dev->cmdbatch_profile_buffer.gpuaddr +
+			ADRENO_CMDBATCH_PROFILE_OFFSET(cmdbatch->profile_index,
 				started));
 	}
 
 	/*
-	 * Add IB1 to read the GPU ticks at the start of command obj and
-	 * write it into the appropriate command obj profiling buffer offset
+	 * Add cmds to read the GPU ticks at the start of the cmdbatch and
+	 * write it into the appropriate cmdbatch profiling buffer offset
 	 */
-	if (user_profiling) {
-		cmds += set_user_profiling(adreno_dev, rb, cmds,
-			cmdobj->profiling_buffer_gpuaddr +
-			offsetof(struct kgsl_drawobj_profiling_buffer,
+	if (cmdbatch_user_profiling) {
+		cmds += _get_alwayson_counter(adreno_dev, cmds,
+			cmdbatch->profiling_buffer_gpuaddr +
+			offsetof(struct kgsl_cmdbatch_profiling_buffer,
 			gpu_ticks_submitted));
 	}
 
 	if (numibs) {
-		if (set_ib1list_marker)
-			cmds += gpudev->set_marker(cmds, IB1LIST_START);
-
-		list_for_each_entry(ib, &cmdobj->cmdlist, node) {
+		list_for_each_entry(ib, &cmdbatch->cmdlist, node) {
 			/*
 			 * Skip 0 sized IBs - these are presumed to have been
 			 * removed from consideration by the FT policy
@@ -1069,59 +1082,39 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 			/* preamble is required on only for first command */
 			use_preamble = false;
 		}
-
-		if (set_ib1list_marker)
-			cmds += gpudev->set_marker(cmds, IB1LIST_END);
 	}
 
-	if (gpudev->ccu_invalidate)
-		cmds += gpudev->ccu_invalidate(adreno_dev, cmds);
-
-	if (adreno_is_preemption_enabled(adreno_dev))
-		if (gpudev->preemption_yield_enable)
-			cmds += gpudev->preemption_yield_enable(cmds);
-
-	if (kernel_profiling) {
+	if (cmdbatch_kernel_profiling) {
 		cmds += _get_alwayson_counter(adreno_dev, cmds,
-			adreno_dev->profile_buffer.gpuaddr +
-			ADRENO_DRAWOBJ_PROFILE_OFFSET(cmdobj->profile_index,
+			adreno_dev->cmdbatch_profile_buffer.gpuaddr +
+			ADRENO_CMDBATCH_PROFILE_OFFSET(cmdbatch->profile_index,
 				retired));
 	}
 
 	/*
-	 * Add IB1 to read the GPU ticks at the end of command obj and
-	 * write it into the appropriate command obj profiling buffer offset
+	 * Add cmds to read the GPU ticks at the end of the cmdbatch and
+	 * write it into the appropriate cmdbatch profiling buffer offset
 	 */
-	if (user_profiling) {
-		cmds += set_user_profiling(adreno_dev, rb, cmds,
-			cmdobj->profiling_buffer_gpuaddr +
-			offsetof(struct kgsl_drawobj_profiling_buffer,
+	if (cmdbatch_user_profiling) {
+		cmds += _get_alwayson_counter(adreno_dev, cmds,
+			cmdbatch->profiling_buffer_gpuaddr +
+			offsetof(struct kgsl_cmdbatch_profiling_buffer,
 			gpu_ticks_retired));
 	}
 
 	*cmds++ = cp_packet(adreno_dev, CP_NOP, 1);
 	*cmds++ = KGSL_END_OF_IB_IDENTIFIER;
 
-	/* Context switches commands should *always* be on the GPU */
-	ret = adreno_drawctxt_switch(adreno_dev, rb, drawctxt,
-		ADRENO_CONTEXT_SWITCH_FORCE_GPU);
+	ret = adreno_drawctxt_switch(adreno_dev, rb, drawctxt, cmdbatch->flags);
 
 	/*
 	 * In the unlikely event of an error in the drawctxt switch,
 	 * treat it like a hang
 	 */
-	if (ret) {
-		/*
-		 * It is "normal" to get a -ENOSPC or a -ENOENT. Don't log it,
-		 * the upper layers know how to handle it
-		 */
-		if (ret != -ENOSPC && ret != -ENOENT)
-			KGSL_DRV_ERR(device,
-				"Unable to switch draw context: %d\n", ret);
+	if (ret)
 		goto done;
-	}
 
-	if (test_bit(CMDOBJ_WFI, &cmdobj->priv))
+	if (test_bit(CMDBATCH_FLAG_WFI, &cmdbatch->priv))
 		flags = KGSL_CMD_FLAGS_WFI;
 
 	/*
@@ -1134,25 +1127,96 @@ int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
 		flags |= KGSL_CMD_FLAGS_PWRON_FIXUP;
 
 	/* Set the constraints before adding to ringbuffer */
-	adreno_ringbuffer_set_constraint(device, drawobj);
+	adreno_ringbuffer_set_constraint(device, cmdbatch);
+
+	/* CFF stuff executed only if CFF is enabled */
+	kgsl_cffdump_capture_ib_desc(device, context, cmdbatch);
+
 
 	ret = adreno_ringbuffer_addcmds(rb, flags,
 					&link[0], (cmds - link),
-					drawobj->timestamp, time);
+					cmdbatch->timestamp, time);
 
 	if (!ret) {
-		set_bit(KGSL_CONTEXT_PRIV_SUBMITTED, &context->priv);
-		cmdobj->global_ts = drawctxt->internal_timestamp;
+		cmdbatch->global_ts = drawctxt->internal_timestamp;
+
+		/* Put the timevalues in the profiling buffer */
+		if (cmdbatch_user_profiling) {
+			/*
+			* Return kernel clock time to the the client
+			* if requested
+			*/
+			if (cmdbatch->flags & KGSL_CMDBATCH_PROFILING_KTIME) {
+				uint64_t secs = time->ktime;
+
+				profile_buffer->wall_clock_ns =
+					do_div(secs, NSEC_PER_SEC);
+				profile_buffer->wall_clock_s = secs;
+			} else {
+				profile_buffer->wall_clock_s =
+					time->utime.tv_sec;
+				profile_buffer->wall_clock_ns =
+					time->utime.tv_nsec;
+			}
+			profile_buffer->gpu_ticks_queued = time->ticks;
+		}
 	}
 
+	kgsl_cffdump_regpoll(device,
+		adreno_getreg(adreno_dev, ADRENO_REG_RBBM_STATUS) << 2,
+		0x00000000, 0x80000000);
 done:
-	trace_kgsl_issueibcmds(device, context->id, numibs, drawobj->timestamp,
-			drawobj->flags, ret, drawctxt->type);
+	/* Corresponding unmap to the memdesc map of profile_buffer */
+	if (entry)
+		kgsl_memdesc_unmap(&entry->memdesc);
+
+
+	trace_kgsl_issueibcmds(device, context->id, cmdbatch,
+			numibs, cmdbatch->timestamp,
+			cmdbatch->flags, ret, drawctxt->type);
 
 	kfree(link);
 	return ret;
 }
 
+/**
+ * adreno_ringbuffer_mmu_clk_disable_event() - Callback function that
+ * disables the MMU clocks.
+ * @device: Device pointer
+ * @context: The ringbuffer context pointer
+ * @data: Pointer containing the adreno_mmu_disable_clk_param structure
+ * @type: The event call type (RETIRED or CANCELLED)
+ */
+static void adreno_ringbuffer_mmu_clk_disable_event(struct kgsl_device *device,
+			struct kgsl_event_group *group, void *data, int type)
+{
+	kgsl_mmu_disable_clk(&device->mmu);
+}
+
+/*
+ * adreno_ringbuffer_mmu_disable_clk_on_ts() - Sets up event to disable MMU
+ * clocks
+ * @device - The kgsl device pointer
+ * @rb: The ringbuffer in whose event list the event is added
+ * @timestamp: The timestamp on which the event should trigger
+ *
+ * Creates an event to disable the MMU clocks on timestamp and if event
+ * already exists then updates the timestamp of disabling the MMU clocks
+ * with the passed in ts if it is greater than the current value at which
+ * the clocks will be disabled
+ * Return - void
+ */
+void
+adreno_ringbuffer_mmu_disable_clk_on_ts(struct kgsl_device *device,
+			struct adreno_ringbuffer *rb, unsigned int timestamp)
+{
+	if (kgsl_add_event(device, &(rb->events), timestamp,
+		adreno_ringbuffer_mmu_clk_disable_event, NULL)) {
+		KGSL_DRV_ERR(device,
+			"Failed to add IOMMU disable clk event\n");
+	}
+}
+
 /**
  * adreno_ringbuffer_wait_callback() - Callback function for event registered
  * on a ringbuffer timestamp
@@ -1166,7 +1230,6 @@ static void adreno_ringbuffer_wait_callback(struct kgsl_device *device,
 		void *priv, int result)
 {
 	struct adreno_ringbuffer *rb = group->priv;
-
 	wake_up_all(&rb->ts_expire_waitq);
 }
 
@@ -1198,10 +1261,8 @@ int adreno_ringbuffer_waittimestamp(struct adreno_ringbuffer *rb,
 	int ret;
 	unsigned long wait_time;
 
-	/* check immediately if timeout is 0 */
-	if (msecs == 0)
-		return adreno_ringbuffer_check_timestamp(rb,
-			timestamp, KGSL_TIMESTAMP_RETIRED) ? 0 : -EBUSY;
+	/* force a timeout from caller for the wait */
+	BUG_ON(0 == msecs);
 
 	ret = kgsl_add_event(device, &rb->events, timestamp,
 		adreno_ringbuffer_wait_callback, NULL);
@@ -1211,10 +1272,10 @@ int adreno_ringbuffer_waittimestamp(struct adreno_ringbuffer *rb,
 	mutex_unlock(&device->mutex);
 
 	wait_time = msecs_to_jiffies(msecs);
-	if (wait_event_timeout(rb->ts_expire_waitq,
+	if (0 == wait_event_timeout(rb->ts_expire_waitq,
 		!kgsl_event_pending(device, &rb->events, timestamp,
 				adreno_ringbuffer_wait_callback, NULL),
-		wait_time) == 0)
+		wait_time))
 		ret  = -ETIMEDOUT;
 
 	mutex_lock(&device->mutex);
diff --git a/drivers/gpu/msm/adreno_ringbuffer.h b/drivers/gpu/msm/adreno_ringbuffer.h
index 8e0c32108f96..5507c6d9b0f0 100644
--- a/drivers/gpu/msm/adreno_ringbuffer.h
+++ b/drivers/gpu/msm/adreno_ringbuffer.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2019, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -43,13 +43,11 @@ struct kgsl_device_private;
  * @ticks: GPU ticks at submit time (from the 19.2Mhz timer)
  * @ktime: local clock time (in nanoseconds)
  * @utime: Wall clock time
- * @drawobj: the object that we want to profile
  */
 struct adreno_submit_time {
 	uint64_t ticks;
 	u64 ktime;
 	struct timespec utime;
-	struct kgsl_drawobj *drawobj;
 };
 
 /**
@@ -75,16 +73,13 @@ struct adreno_ringbuffer_pagetable_info {
 	unsigned int contextidr;
 };
 
-#define PT_INFO_OFFSET(_field) \
-	offsetof(struct adreno_ringbuffer_pagetable_info, _field)
-
 /**
  * struct adreno_ringbuffer - Definition for an adreno ringbuffer object
  * @flags: Internal control flags for the ringbuffer
- * @buffer_desc: Pointer to the ringbuffer memory descripto
- * @_wptr: The next value of wptr to be written to the hardware on submit
- * @wptr: Local copy of the wptr offset last written to hardware
- * @last_wptr: offset of the last wptr that was written to CFF
+ * @buffer_desc: Pointer to the ringbuffer memory descriptor
+ * @wptr: Local copy of the wptr offset
+ * @rptr: Read pointer offset in dwords from baseaddr
+ * @last_wptr: offset of the last H/W committed wptr
  * @rb_ctx: The context that represents a ringbuffer
  * @id: Priority level of the ringbuffer, also used as an ID
  * @fault_detect_ts: The last retired global timestamp read during fault detect
@@ -94,10 +89,6 @@ struct adreno_ringbuffer_pagetable_info {
  * @drawctxt_active: The last pagetable that this ringbuffer is set to
  * @preemption_desc: The memory descriptor containing
  * preemption info written/read by CP
- * @secure_preemption_desc: The memory descriptor containing
- * preemption info written/read by CP for secure contexts
- * @perfcounter_save_restore_desc: Used by CP to save/restore the perfcounter
- * values across preemption
  * @pagetable_desc: Memory to hold information about the pagetables being used
  * and the commands to switch pagetable on the RB
  * @dispatch_q: The dispatcher side queue for this ringbuffer
@@ -110,13 +101,13 @@ struct adreno_ringbuffer_pagetable_info {
  * @sched_timer: Timer that tracks how long RB has been waiting to be scheduled
  * or how long it has been scheduled for after preempting in
  * @starve_timer_state: Indicates the state of the wait.
- * @preempt_lock: Lock to protect the wptr pointer while it is being updated
  */
 struct adreno_ringbuffer {
 	uint32_t flags;
 	struct kgsl_memdesc buffer_desc;
-	unsigned int _wptr;
+	unsigned int sizedwords;
 	unsigned int wptr;
+	unsigned int rptr;
 	unsigned int last_wptr;
 	int id;
 	unsigned int fault_detect_ts;
@@ -124,46 +115,44 @@ struct adreno_ringbuffer {
 	struct kgsl_event_group events;
 	struct adreno_context *drawctxt_active;
 	struct kgsl_memdesc preemption_desc;
-	struct kgsl_memdesc secure_preemption_desc;
-	struct kgsl_memdesc perfcounter_save_restore_desc;
 	struct kgsl_memdesc pagetable_desc;
-	struct adreno_dispatcher_drawqueue dispatch_q;
+	struct adreno_dispatcher_cmdqueue dispatch_q;
 	wait_queue_head_t ts_expire_waitq;
 	unsigned int wptr_preempt_end;
 	unsigned int gpr11;
 	int preempted_midway;
 	unsigned long sched_timer;
 	enum adreno_dispatcher_starve_timer_states starve_timer_state;
-	spinlock_t preempt_lock;
-	/**
-	 * @profile_desc: global memory to construct IB1s to do user side
-	 * profiling
-	 */
-	struct kgsl_memdesc profile_desc;
-	/**
-	 * @profile_index: Pointer to the next "slot" in profile_desc for a user
-	 * profiling IB1.  This allows for PAGE_SIZE / 16 = 256 simultaneous
-	 * commands per ringbuffer with user profiling enabled
-	 * enough.
-	 */
-	u32 profile_index;
 };
 
+/* enable timestamp (...scratch0) memory shadowing */
+#define GSL_RB_MEMPTRS_SCRATCH_MASK 0x1
+
+/*
+ * protected mode error checking below register address 0x800
+ * note: if CP_INTERRUPT packet is used then checking needs
+ * to change to below register address 0x7C8
+ */
+#define GSL_RB_PROTECTED_MODE_CONTROL		0x200001F2
+
 /* Returns the current ringbuffer */
 #define ADRENO_CURRENT_RINGBUFFER(a)	((a)->cur_rb)
 
+#define KGSL_MEMSTORE_RB_OFFSET(rb, field)	\
+	KGSL_MEMSTORE_OFFSET((rb->id + KGSL_MEMSTORE_MAX), field)
+
 int cp_secure_mode(struct adreno_device *adreno_dev, uint *cmds, int set);
 
 int adreno_ringbuffer_issueibcmds(struct kgsl_device_private *dev_priv,
 				struct kgsl_context *context,
-				struct kgsl_drawobj *drawobj,
+				struct kgsl_cmdbatch *cmdbatch,
 				uint32_t *timestamp);
 
 int adreno_ringbuffer_submitcmd(struct adreno_device *adreno_dev,
-		struct kgsl_drawobj_cmd *cmdobj,
+		struct kgsl_cmdbatch *cmdbatch,
 		struct adreno_submit_time *time);
 
-int adreno_ringbuffer_probe(struct adreno_device *adreno_dev, bool nopreempt);
+int adreno_ringbuffer_init(struct adreno_device *adreno_dev, bool nopreempt);
 
 int adreno_ringbuffer_start(struct adreno_device *adreno_dev,
 		unsigned int start_type);
@@ -172,7 +161,7 @@ void adreno_ringbuffer_stop(struct adreno_device *adreno_dev);
 
 void adreno_ringbuffer_close(struct adreno_device *adreno_dev);
 
-int adreno_ringbuffer_issue_internal_cmds(struct adreno_ringbuffer *rb,
+int adreno_ringbuffer_issuecmds(struct adreno_ringbuffer *rb,
 					unsigned int flags,
 					unsigned int *cmdaddr,
 					int sizedwords);
@@ -192,6 +181,9 @@ void adreno_ringbuffer_read_pfp_ucode(struct kgsl_device *device);
 
 void adreno_ringbuffer_read_pm4_ucode(struct kgsl_device *device);
 
+void adreno_ringbuffer_mmu_disable_clk_on_ts(struct kgsl_device *device,
+			struct adreno_ringbuffer *rb, unsigned int ts);
+
 int adreno_ringbuffer_waittimestamp(struct adreno_ringbuffer *rb,
 					unsigned int timestamp,
 					unsigned int msecs);
@@ -223,10 +215,9 @@ static inline unsigned int adreno_ringbuffer_dec_wrapped(unsigned int val,
 }
 
 static inline int adreno_ringbuffer_set_pt_ctx(struct adreno_ringbuffer *rb,
-		struct kgsl_pagetable *pt, struct adreno_context *context,
-		unsigned long flags)
+		struct kgsl_pagetable *pt, struct adreno_context *context)
 {
-	return adreno_iommu_set_pt_ctx(rb, pt, context, flags);
+	return adreno_iommu_set_pt_ctx(rb, pt, context);
 }
 
 #endif  /* __ADRENO_RINGBUFFER_H */
diff --git a/drivers/gpu/msm/adreno_snapshot.c b/drivers/gpu/msm/adreno_snapshot.c
index 089c46063bbc..fa30e32fed88 100644
--- a/drivers/gpu/msm/adreno_snapshot.c
+++ b/drivers/gpu/msm/adreno_snapshot.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2019 The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -21,12 +21,17 @@
 #include "adreno_snapshot.h"
 #include "adreno_a5xx.h"
 
+/* Number of dwords of ringbuffer history to record */
+#define NUM_DWORDS_OF_RINGBUFFER_HISTORY 100
+
 #define VPC_MEMORY_BANKS 4
 
 /* Maintain a list of the objects we see during parsing */
 
 #define SNAPSHOT_OBJ_BUFSIZE 64
 
+#define SNAPSHOT_OBJ_TYPE_IB 0
+
 /* Used to print error message if an IB has too many objects in it */
 static int ib_max_objs;
 
@@ -51,7 +56,8 @@ static inline int adreno_rb_ctxtswitch(struct adreno_device *adreno_dev,
 }
 
 /* Push a new buffer object onto the list */
-void kgsl_snapshot_push_object(struct kgsl_process_private *process,
+static void push_object(int type,
+	struct kgsl_process_private *process,
 	uint64_t gpuaddr, uint64_t dwords)
 {
 	int index;
@@ -70,19 +76,6 @@ void kgsl_snapshot_push_object(struct kgsl_process_private *process,
 	for (index = 0; index < objbufptr; index++) {
 		if (objbuf[index].gpuaddr == gpuaddr &&
 			objbuf[index].entry->priv == process) {
-			/*
-			 * Check if newly requested size is within the
-			 * allocated range or not, otherwise continue
-			 * with previous size.
-			 */
-			if (!kgsl_gpuaddr_in_memdesc(
-				&objbuf[index].entry->memdesc,
-				gpuaddr, dwords << 2)) {
-				KGSL_CORE_ERR(
-					"snapshot: IB 0x%016llx size is not within the memdesc range\n",
-					gpuaddr);
-				return;
-			}
 
 			objbuf[index].size = max_t(uint64_t,
 						objbuf[index].size,
@@ -111,26 +104,29 @@ void kgsl_snapshot_push_object(struct kgsl_process_private *process,
 	}
 
 	/* Put it on the list of things to parse */
+	objbuf[objbufptr].type = type;
 	objbuf[objbufptr].gpuaddr = gpuaddr;
 	objbuf[objbufptr].size = dwords << 2;
 	objbuf[objbufptr++].entry = entry;
 }
 
 /*
- * Returns index of the specified object is already on the list of buffers
+ * Return a 1 if the specified object is already on the list of buffers
  * to be dumped
  */
 
-static int find_object(uint64_t gpuaddr, struct kgsl_process_private *process)
+static int find_object(int type, uint64_t gpuaddr,
+		struct kgsl_process_private *process)
 {
 	int index;
 
 	for (index = 0; index < objbufptr; index++) {
 		if (objbuf[index].gpuaddr == gpuaddr &&
 			objbuf[index].entry->priv == process)
-			return index;
+			return 1;
 	}
-	return -ENOENT;
+
+	return 0;
 }
 
 /*
@@ -139,12 +135,14 @@ static int find_object(uint64_t gpuaddr, struct kgsl_process_private *process)
  * @snapshot: The snapshot data.
  * @process: The process to which the IB belongs
  * @ib_obj_list: List of the IB objects
+ * @ib2base: IB2 base address at time of the fault
  *
  * Returns 0 on success else error code
  */
 static int snapshot_freeze_obj_list(struct kgsl_snapshot *snapshot,
 		struct kgsl_process_private *process,
-		struct adreno_ib_object_list *ib_obj_list)
+		struct adreno_ib_object_list *ib_obj_list,
+		uint64_t ib2base)
 {
 	int ret = 0;
 	struct adreno_ib_object *ib_objs;
@@ -169,15 +167,21 @@ static int snapshot_freeze_obj_list(struct kgsl_snapshot *snapshot,
 		}
 
 		if (freeze) {
-			temp_ret = kgsl_snapshot_get_object(snapshot,
-					    process, ib_objs->gpuaddr,
-					    ib_objs->size,
-					    ib_objs->snapshot_obj_type);
-			if (temp_ret < 0) {
-				if (ret >= 0)
-					ret = temp_ret;
+			/* Save current IB2 statically */
+			if (ib2base == ib_objs->gpuaddr) {
+				push_object(SNAPSHOT_OBJ_TYPE_IB,
+				process, ib_objs->gpuaddr, ib_objs->size >> 2);
 			} else {
-				snapshot_frozen_objsize += temp_ret;
+				temp_ret = kgsl_snapshot_get_object(snapshot,
+					process, ib_objs->gpuaddr,
+					ib_objs->size,
+					ib_objs->snapshot_obj_type);
+				if (temp_ret < 0) {
+					if (ret >= 0)
+						ret = temp_ret;
+				} else {
+					snapshot_frozen_objsize += temp_ret;
+				}
 			}
 		}
 	}
@@ -195,6 +199,8 @@ static inline void parse_ib(struct kgsl_device *device,
 		struct kgsl_process_private *process,
 		uint64_t gpuaddr, uint64_t dwords)
 {
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+	uint64_t ib1base;
 	struct adreno_ib_object_list *ib_obj_list;
 
 	/*
@@ -202,8 +208,13 @@ static inline void parse_ib(struct kgsl_device *device,
 	 * then push it into the static blob otherwise put it in the dynamic
 	 * list
 	 */
-	if (gpuaddr == snapshot->ib1base) {
-		kgsl_snapshot_push_object(process, gpuaddr, dwords);
+
+	adreno_readreg64(adreno_dev, ADRENO_REG_CP_IB1_BASE,
+		ADRENO_REG_CP_IB1_BASE_HI, &ib1base);
+
+	if (gpuaddr == ib1base) {
+		push_object(SNAPSHOT_OBJ_TYPE_IB, process,
+			gpuaddr, dwords);
 		return;
 	}
 
@@ -212,8 +223,7 @@ static inline void parse_ib(struct kgsl_device *device,
 		return;
 
 	if (-E2BIG == adreno_ib_create_object_list(device, process,
-				gpuaddr, dwords, snapshot->ib2base,
-				&ib_obj_list))
+				gpuaddr, dwords, &ib_obj_list))
 		ib_max_objs = 1;
 
 	if (ib_obj_list)
@@ -283,17 +293,22 @@ static void dump_all_ibs(struct kgsl_device *device,
  * @snapshot: Pointer to information about the current snapshot being taken
  */
 static void snapshot_rb_ibs(struct kgsl_device *device,
-		struct adreno_ringbuffer *rb,
+		struct adreno_ringbuffer *rb, unsigned int *data,
 		struct kgsl_snapshot *snapshot)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	unsigned int rptr, *rbptr;
+	uint64_t ibbase;
 	int index, i;
 	int parse_ibs = 0, ib_parse_start;
 
 	/* Get the current read pointers for the RB */
 	adreno_readreg(adreno_dev, ADRENO_REG_CP_RB_RPTR, &rptr);
 
+	/* Address of the last processed IB */
+	adreno_readreg64(adreno_dev, ADRENO_REG_CP_IB1_BASE,
+				ADRENO_REG_CP_IB1_BASE_HI, &ibbase);
+
 	/*
 	 * Figure out the window of ringbuffer data to dump.  First we need to
 	 * find where the last processed IB ws submitted.  Start walking back
@@ -321,14 +336,14 @@ static void snapshot_rb_ibs(struct kgsl_device *device,
 
 		if (adreno_cmd_is_ib(adreno_dev, rbptr[index])) {
 			if (ADRENO_LEGACY_PM4(adreno_dev)) {
-				if (rbptr[index + 1] == snapshot->ib1base)
+				if (rbptr[index + 1] == ibbase)
 					break;
 			} else {
 				uint64_t ibaddr;
 
 				ibaddr = rbptr[index + 2];
 				ibaddr = ibaddr << 32 | rbptr[index + 1];
-				if (ibaddr == snapshot->ib1base)
+				if (ibaddr == ibbase)
 					break;
 			}
 		}
@@ -341,6 +356,7 @@ static void snapshot_rb_ibs(struct kgsl_device *device,
 	 */
 
 	if (index == rb->wptr) {
+		memcpy(data, rb->buffer_desc.hostptr, KGSL_RB_SIZE);
 		dump_all_ibs(device, rb, snapshot);
 		return;
 	}
@@ -383,12 +399,14 @@ static void snapshot_rb_ibs(struct kgsl_device *device,
 	ib_parse_start = index;
 
 	/*
-	 * Loop through the RB, looking for indirect buffers and MMU pagetable
-	 * changes
+	 * Loop through the RB, copying the data and looking for indirect
+	 * buffers and MMU pagetable changes
 	 */
 
 	index = rb->wptr;
 	for (i = 0; i < KGSL_RB_DWORDS; i++) {
+		*data = rbptr[index];
+
 		/*
 		 * Only parse IBs between the start and the rptr or the next
 		 * context switch, whichever comes first
@@ -413,8 +431,6 @@ static void snapshot_rb_ibs(struct kgsl_device *device,
 				ibsize = rbptr[index + 3];
 			}
 
-			index = (index + 1) % KGSL_RB_DWORDS;
-
 			/* Don't parse known global IBs */
 			if (iommu_is_setstate_addr(device, ibaddr, ibsize))
 				continue;
@@ -425,8 +441,14 @@ static void snapshot_rb_ibs(struct kgsl_device *device,
 
 			parse_ib(device, snapshot, snapshot->process,
 				ibaddr, ibsize);
-		} else
-			index = (index + 1) % KGSL_RB_DWORDS;
+		}
+
+		index = index + 1;
+
+		if (index == KGSL_RB_DWORDS)
+			index = 0;
+
+		data++;
 	}
 
 }
@@ -453,10 +475,10 @@ static size_t snapshot_rb(struct kgsl_device *device, u8 *buf,
 	}
 
 	/* Write the sub-header for the section */
-	header->start = 0;
-	header->end = KGSL_RB_DWORDS;
+	header->start = rb->wptr;
+	header->end = rb->wptr;
 	header->wptr = rb->wptr;
-	header->rptr = adreno_get_rptr(rb);
+	header->rptr = rb->rptr;
 	header->rbsize = KGSL_RB_DWORDS;
 	header->count = KGSL_RB_DWORDS;
 	adreno_rb_readtimestamp(adreno_dev, rb, KGSL_TIMESTAMP_QUEUED,
@@ -466,12 +488,12 @@ static size_t snapshot_rb(struct kgsl_device *device, u8 *buf,
 	header->gpuaddr = rb->buffer_desc.gpuaddr;
 	header->id = rb->id;
 
-	if (rb == adreno_dev->cur_rb)
-		snapshot_rb_ibs(device, rb, snapshot);
-
-	/* Just copy the ringbuffer, there are no active IBs */
-	memcpy(data, rb->buffer_desc.hostptr, KGSL_RB_SIZE);
-
+	if (rb == adreno_dev->cur_rb) {
+		snapshot_rb_ibs(device, rb, data, snapshot);
+	} else {
+		/* Just copy the ringbuffer, there are no active IBs */
+		memcpy(data, rb->buffer_desc.hostptr, KGSL_RB_SIZE);
+	}
 	/* Return the size of the section */
 	return KGSL_RB_SIZE + sizeof(*header);
 }
@@ -531,11 +553,11 @@ static size_t snapshot_capture_mem_list(struct kgsl_device *device,
 
 	header->num_entries = num_mem;
 	header->ptbase = kgsl_mmu_pagetable_get_ttbr0(process->pagetable);
-
 	/*
-	 * Walk through the memory list and store the
+	 * Walk throught the memory list and store the
 	 * tuples(gpuaddr, size, memtype) in snapshot
 	 */
+
 	idr_for_each(&process->mem_idr, _save_mem_entries, data);
 
 	ret = sizeof(*header) + (num_mem * sizeof(struct mem_entry));
@@ -553,66 +575,6 @@ struct snapshot_ib_meta {
 	uint64_t ib2size;
 };
 
-void kgsl_snapshot_add_active_ib_obj_list(struct kgsl_device *device,
-		struct kgsl_snapshot *snapshot)
-{
-	struct adreno_ib_object_list *ib_obj_list;
-	int index = -ENOENT;
-
-	if (!snapshot->ib1dumped)
-		index = find_object(snapshot->ib1base, snapshot->process);
-
-	/* only do this for IB1 because the IB2's are part of IB1 objects */
-	if ((index != -ENOENT) &&
-			(snapshot->ib1base == objbuf[index].gpuaddr)) {
-		if (-E2BIG == adreno_ib_create_object_list(device,
-					objbuf[index].entry->priv,
-					objbuf[index].gpuaddr,
-					objbuf[index].size >> 2,
-					snapshot->ib2base,
-					&ib_obj_list))
-			ib_max_objs = 1;
-		if (ib_obj_list) {
-			/* freeze the IB objects in the IB */
-			snapshot_freeze_obj_list(snapshot,
-					objbuf[index].entry->priv,
-					ib_obj_list);
-			adreno_ib_destroy_obj_list(ib_obj_list);
-		}
-	} else {
-		/* Get the IB2 index from parsed object */
-		index = find_object(snapshot->ib2base, snapshot->process);
-
-		if (index != -ENOENT)
-			parse_ib(device, snapshot, snapshot->process,
-				snapshot->ib2base, objbuf[index].size >> 2);
-	}
-}
-
-/*
- * active_ib_is_parsed() - Checks if active ib is already parsed
- * @gpuaddr: Active IB base address at the time of fault
- * @size: Active IB size
- * @process: The process to which the IB belongs
- *
- * Function returns true if the active is already is parsed
- * else false
- */
-static bool active_ib_is_parsed(uint64_t gpuaddr, uint64_t size,
-		struct kgsl_process_private *process)
-{
-	int  index;
-	/* go through the static list for gpuaddr is in list or not */
-	for (index = 0; index < objbufptr; index++) {
-		if ((objbuf[index].gpuaddr <= gpuaddr) &&
-				((objbuf[index].gpuaddr +
-				  (objbuf[index].size)) >=
-				 (gpuaddr + size)) &&
-				(objbuf[index].entry->priv == process))
-			return true;
-	}
-	return false;
-}
 /* Snapshot the memory for an indirect buffer */
 static size_t snapshot_ib(struct kgsl_device *device, u8 *buf,
 	size_t remain, void *priv)
@@ -624,7 +586,6 @@ static size_t snapshot_ib(struct kgsl_device *device, u8 *buf,
 	struct adreno_ib_object_list *ib_obj_list;
 	struct kgsl_snapshot *snapshot;
 	struct kgsl_snapshot_object *obj;
-	struct kgsl_memdesc *memdesc;
 
 	if (meta == NULL || meta->snapshot == NULL || meta->obj == NULL) {
 		KGSL_CORE_ERR("snapshot: bad metadata");
@@ -632,18 +593,13 @@ static size_t snapshot_ib(struct kgsl_device *device, u8 *buf,
 	}
 	snapshot = meta->snapshot;
 	obj = meta->obj;
-	memdesc = &obj->entry->memdesc;
-
-	/* If size is zero get it from the medesc size */
-	if (!obj->size)
-		obj->size = (memdesc->size - (obj->gpuaddr - memdesc->gpuaddr));
 
 	if (remain < (obj->size + sizeof(*header))) {
 		KGSL_CORE_ERR("snapshot: Not enough memory for the ib\n");
 		return 0;
 	}
 
-	src = kgsl_gpuaddr_to_vaddr(memdesc, obj->gpuaddr);
+	src = kgsl_gpuaddr_to_vaddr(&obj->entry->memdesc, obj->gpuaddr);
 	if (src == NULL) {
 		KGSL_DRV_ERR(device,
 			"snapshot: Unable to map GPU memory object 0x%016llX into the kernel\n",
@@ -651,31 +607,27 @@ static size_t snapshot_ib(struct kgsl_device *device, u8 *buf,
 		return 0;
 	}
 
+	if (remain < (obj->size + sizeof(*header))) {
+		KGSL_CORE_ERR("snapshot: Not enough memory for the ib\n");
+		return 0;
+	}
+
 	/* only do this for IB1 because the IB2's are part of IB1 objects */
 	if (meta->ib1base == obj->gpuaddr) {
-
-		snapshot->ib1dumped = active_ib_is_parsed(obj->gpuaddr,
-					obj->size, obj->entry->priv);
 		if (-E2BIG == adreno_ib_create_object_list(device,
 				obj->entry->priv,
 				obj->gpuaddr, obj->size >> 2,
-				snapshot->ib2base,
 				&ib_obj_list))
 			ib_max_objs = 1;
 		if (ib_obj_list) {
 			/* freeze the IB objects in the IB */
 			snapshot_freeze_obj_list(snapshot,
 						obj->entry->priv,
-						ib_obj_list);
+						ib_obj_list, meta->ib2base);
 			adreno_ib_destroy_obj_list(ib_obj_list);
 		}
 	}
 
-
-	if (meta->ib2base == obj->gpuaddr)
-		snapshot->ib2dumped = active_ib_is_parsed(obj->gpuaddr,
-					obj->size, obj->entry->priv);
-
 	/* Write the sub-header for the section */
 	header->gpuaddr = obj->gpuaddr;
 	header->ptbase =
@@ -691,22 +643,32 @@ static size_t snapshot_ib(struct kgsl_device *device, u8 *buf,
 
 /* Dump another item on the current pending list */
 static void dump_object(struct kgsl_device *device, int obj,
-		struct kgsl_snapshot *snapshot)
+		struct kgsl_snapshot *snapshot,
+		uint64_t ib1base, uint64_t ib1size,
+		uint64_t ib2base, uint64_t ib2size)
 {
 	struct snapshot_ib_meta meta;
 
-	meta.snapshot = snapshot;
-	meta.obj = &objbuf[obj];
-	meta.ib1base = snapshot->ib1base;
-	meta.ib1size = snapshot->ib1size;
-	meta.ib2base = snapshot->ib2base;
-	meta.ib2size = snapshot->ib2size;
+	switch (objbuf[obj].type) {
+	case SNAPSHOT_OBJ_TYPE_IB:
+		meta.snapshot = snapshot;
+		meta.obj = &objbuf[obj];
+		meta.ib1base = ib1base;
+		meta.ib1size = ib1size;
+		meta.ib2base = ib2base;
+		meta.ib2size = ib2size;
 
-	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_IB_V2,
+		kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_IB_V2,
 			snapshot, snapshot_ib, &meta);
-	if (objbuf[obj].entry) {
-		kgsl_memdesc_unmap(&(objbuf[obj].entry->memdesc));
-		kgsl_mem_entry_put(objbuf[obj].entry);
+		if (objbuf[obj].entry) {
+			kgsl_memdesc_unmap(&(objbuf[obj].entry->memdesc));
+			kgsl_mem_entry_put(objbuf[obj].entry);
+		}
+		break;
+	default:
+		KGSL_CORE_ERR("snapshot: Invalid snapshot object type: %d\n",
+			objbuf[obj].type);
+		break;
 	}
 }
 
@@ -783,15 +745,15 @@ static size_t snapshot_global(struct kgsl_device *device, u8 *buf,
 	}
 
 	if (memdesc->hostptr == NULL) {
-		KGSL_CORE_ERR(
-		"snapshot: no kernel mapping for global object 0x%016llX\n",
-		memdesc->gpuaddr);
+		KGSL_CORE_ERR("snapshot: no kernel mapping for global object 0x%016llX\n",
+				memdesc->gpuaddr);
 		return 0;
 	}
 
 	header->size = memdesc->size >> 2;
 	header->gpuaddr = memdesc->gpuaddr;
-	header->ptbase = MMU_DEFAULT_TTBR0(device);
+	header->ptbase =
+		kgsl_mmu_pagetable_get_ttbr0(device->mmu.defaultpagetable);
 	header->type = SNAPSHOT_GPU_OBJECT_GLOBAL;
 
 	memcpy(ptr, memdesc->hostptr, memdesc->size);
@@ -799,6 +761,46 @@ static size_t snapshot_global(struct kgsl_device *device, u8 *buf,
 	return memdesc->size + sizeof(*header);
 }
 
+/* Snapshot a preemption record buffer */
+static size_t snapshot_preemption_record(struct kgsl_device *device, u8 *buf,
+	size_t remain, void *priv)
+{
+	struct kgsl_memdesc *memdesc = priv;
+	struct a5xx_cp_preemption_record record;
+	int size = sizeof(record);
+
+	struct kgsl_snapshot_gpu_object_v2 *header =
+		(struct kgsl_snapshot_gpu_object_v2 *)buf;
+
+	u8 *ptr = buf + sizeof(*header);
+
+	if (size == 0)
+		return 0;
+
+	if (remain < (size + sizeof(*header))) {
+		KGSL_CORE_ERR(
+			"snapshot: Not enough memory for preemption record\n");
+		return 0;
+	}
+
+	if (memdesc->hostptr == NULL) {
+		KGSL_CORE_ERR(
+		"snapshot: no kernel mapping for preemption record 0x%016llX\n",
+				memdesc->gpuaddr);
+		return 0;
+	}
+
+	header->size = size >> 2;
+	header->gpuaddr = memdesc->gpuaddr;
+	header->ptbase =
+		kgsl_mmu_pagetable_get_ttbr0(device->mmu.defaultpagetable);
+	header->type = SNAPSHOT_GPU_OBJECT_GLOBAL;
+
+	memcpy(ptr, memdesc->hostptr, size);
+
+	return size + sizeof(*header);
+}
+
 /* Snapshot IOMMU specific buffers */
 static void adreno_snapshot_iommu(struct kgsl_device *device,
 		struct kgsl_snapshot *snapshot)
@@ -815,21 +817,6 @@ static void adreno_snapshot_iommu(struct kgsl_device *device,
 			snapshot, snapshot_global, &iommu->smmu_info);
 }
 
-static void adreno_snapshot_ringbuffer(struct kgsl_device *device,
-		struct kgsl_snapshot *snapshot, struct adreno_ringbuffer *rb)
-{
-	struct snapshot_rb_params params = {
-		.snapshot = snapshot,
-		.rb = rb,
-	};
-
-	if (rb == NULL)
-		return;
-
-	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_RB_V2, snapshot,
-		snapshot_rb, &params);
-}
-
 /* adreno_snapshot - Snapshot the Adreno GPU state
  * @device - KGSL device to snapshot
  * @snapshot - Pointer to the snapshot instance
@@ -842,8 +829,12 @@ void adreno_snapshot(struct kgsl_device *device, struct kgsl_snapshot *snapshot,
 			struct kgsl_context *context)
 {
 	unsigned int i;
+	uint64_t ib1base, ib2base;
+	unsigned int ib1size, ib2size;
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
+	struct adreno_ringbuffer *rb;
+	struct snapshot_rb_params snap_rb_params;
 
 	ib_max_objs = 0;
 	/* Reset the list of objects */
@@ -854,36 +845,36 @@ void adreno_snapshot(struct kgsl_device *device, struct kgsl_snapshot *snapshot,
 	setup_fault_process(device, snapshot,
 			context ? context->proc_priv : NULL);
 
-	/* Add GPU specific sections - registers mainly, but other stuff too */
-	if (gpudev->snapshot)
-		gpudev->snapshot(adreno_dev, snapshot);
+	/* Dump the current ringbuffer */
+	snap_rb_params.snapshot = snapshot;
+	snap_rb_params.rb = adreno_dev->cur_rb;
+	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_RB_V2, snapshot,
+			snapshot_rb, &snap_rb_params);
 
-	/* Dumping these buffers is useless if the GX is not on */
-	if (gpudev->gx_is_on)
-		if (!gpudev->gx_is_on(adreno_dev))
-			return;
+	/* Dump the prev ringbuffer */
+	if (adreno_dev->prev_rb) {
+		snap_rb_params.rb = adreno_dev->prev_rb;
+		kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_RB_V2,
+			snapshot, snapshot_rb, &snap_rb_params);
+	}
+
+	/* Dump next ringbuffer */
+	if (adreno_dev->next_rb) {
+		snap_rb_params.rb = adreno_dev->next_rb;
+		kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_RB_V2,
+			snapshot, snapshot_rb, &snap_rb_params);
+	}
 
 	adreno_readreg64(adreno_dev, ADRENO_REG_CP_IB1_BASE,
-			ADRENO_REG_CP_IB1_BASE_HI, &snapshot->ib1base);
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_IB1_BUFSZ, &snapshot->ib1size);
+			ADRENO_REG_CP_IB1_BASE_HI, &ib1base);
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_IB1_BUFSZ, &ib1size);
 	adreno_readreg64(adreno_dev, ADRENO_REG_CP_IB2_BASE,
-			ADRENO_REG_CP_IB2_BASE_HI, &snapshot->ib2base);
-	adreno_readreg(adreno_dev, ADRENO_REG_CP_IB2_BUFSZ, &snapshot->ib2size);
+			ADRENO_REG_CP_IB2_BASE_HI, &ib2base);
+	adreno_readreg(adreno_dev, ADRENO_REG_CP_IB2_BUFSZ, &ib2size);
 
-	snapshot->ib1dumped = false;
-	snapshot->ib2dumped = false;
-
-	adreno_snapshot_ringbuffer(device, snapshot, adreno_dev->cur_rb);
-
-	/* Dump the prev ringbuffer */
-	if (adreno_dev->prev_rb != adreno_dev->cur_rb)
-		adreno_snapshot_ringbuffer(device, snapshot,
-			adreno_dev->prev_rb);
-
-	if ((adreno_dev->next_rb != adreno_dev->prev_rb) &&
-		 (adreno_dev->next_rb != adreno_dev->cur_rb))
-		adreno_snapshot_ringbuffer(device, snapshot,
-			adreno_dev->next_rb);
+	/* Add GPU specific sections - registers mainly, but other stuff too */
+	if (gpudev->snapshot)
+		gpudev->snapshot(adreno_dev, snapshot);
 
 	/* Dump selected global buffers */
 	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_GPU_OBJECT_V2,
@@ -896,6 +887,15 @@ void adreno_snapshot(struct kgsl_device *device, struct kgsl_snapshot *snapshot,
 	if (kgsl_mmu_get_mmutype(device) == KGSL_MMU_TYPE_IOMMU)
 		adreno_snapshot_iommu(device, snapshot);
 
+	if (ADRENO_FEATURE(adreno_dev, ADRENO_PREEMPTION)) {
+		FOR_EACH_RINGBUFFER(adreno_dev, rb, i) {
+			kgsl_snapshot_add_section(device,
+				KGSL_SNAPSHOT_SECTION_GPU_OBJECT_V2,
+				snapshot, snapshot_preemption_record,
+				&rb->preemption_desc);
+		}
+	}
+
 	/*
 	 * Add a section that lists (gpuaddr, size, memtype) tuples of the
 	 * hanging process
@@ -914,13 +914,13 @@ void adreno_snapshot(struct kgsl_device *device, struct kgsl_snapshot *snapshot,
 	 * figure how often this really happens.
 	 */
 
-	if (-ENOENT == find_object(snapshot->ib1base, snapshot->process) &&
-			snapshot->ib1size) {
-		kgsl_snapshot_push_object(snapshot->process, snapshot->ib1base,
-				snapshot->ib1size);
+	if (!find_object(SNAPSHOT_OBJ_TYPE_IB, ib1base,
+			snapshot->process) && ib1size) {
+		push_object(SNAPSHOT_OBJ_TYPE_IB, snapshot->process,
+			ib1base, ib1size);
 		KGSL_CORE_ERR(
 		"CP_IB1_BASE not found in the ringbuffer.Dumping %x dwords of the buffer.\n",
-		snapshot->ib1size);
+		ib1size);
 	}
 
 	/*
@@ -931,9 +931,10 @@ void adreno_snapshot(struct kgsl_device *device, struct kgsl_snapshot *snapshot,
 	 * correct size.
 	 */
 
-	if (-ENOENT == find_object(snapshot->ib2base, snapshot->process)) {
-		kgsl_snapshot_push_object(snapshot->process, snapshot->ib2base,
-				snapshot->ib2size);
+	if (!find_object(SNAPSHOT_OBJ_TYPE_IB, ib2base,
+		snapshot->process) && ib2size) {
+		push_object(SNAPSHOT_OBJ_TYPE_IB, snapshot->process,
+			ib2base, ib2size);
 	}
 
 	/*
@@ -941,15 +942,8 @@ void adreno_snapshot(struct kgsl_device *device, struct kgsl_snapshot *snapshot,
 	 * are parsed, more objects might be found, and objbufptr will increase
 	 */
 	for (i = 0; i < objbufptr; i++)
-		dump_object(device, i, snapshot);
-
-	/*
-	 * Incase snapshot static blob is running out of memory, Add Active IB1
-	 * and IB2 entries to obj_list so that active ib's can be dumped to
-	 * snapshot dynamic blob.
-	 */
-	if (!snapshot->ib1dumped || !snapshot->ib2dumped)
-		kgsl_snapshot_add_active_ib_obj_list(device, snapshot);
+		dump_object(device, i, snapshot, ib1base, ib1size,
+			ib2base, ib2size);
 
 	if (ib_max_objs)
 		KGSL_CORE_ERR("Max objects found in IB\n");
@@ -959,24 +953,6 @@ void adreno_snapshot(struct kgsl_device *device, struct kgsl_snapshot *snapshot,
 
 }
 
-/* adreno_snapshot_gmu - Snapshot the Adreno GMU state
- * @device - KGSL device to snapshot
- * @snapshot - Pointer to the snapshot instance
- * This is a hook function called by kgsl_snapshot to snapshot the
- * Adreno specific information for the GMU snapshot.  In turn, this function
- * calls the GMU specific snapshot function to get core specific information.
- */
-void adreno_snapshot_gmu(struct kgsl_device *device,
-		struct kgsl_snapshot *snapshot)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-
-	/* Add GMU specific sections */
-	if (gpudev->snapshot_gmu)
-		gpudev->snapshot_gmu(adreno_dev, snapshot);
-}
-
 /*
  * adreno_snapshot_cp_roq - Dump CP merciu data in snapshot
  * @device: Device being snapshotted
@@ -1057,8 +1033,7 @@ size_t adreno_snapshot_cp_pm4_ram(struct kgsl_device *device, u8 *buf,
 	struct kgsl_snapshot_debug *header = (struct kgsl_snapshot_debug *)buf;
 	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
 	int i;
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_PM4);
-	size_t size = fw->size - 1;
+	size_t size = adreno_dev->pm4_fw_size - 1;
 
 	if (remain < DEBUG_SECTION_SZ(size)) {
 		SNAPSHOT_ERR_NOMEM(device, "CP PM4 RAM DEBUG");
@@ -1095,9 +1070,7 @@ size_t adreno_snapshot_cp_pfp_ram(struct kgsl_device *device, u8 *buf,
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	struct kgsl_snapshot_debug *header = (struct kgsl_snapshot_debug *)buf;
 	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
-	int i;
-	struct adreno_firmware *fw = ADRENO_FW(adreno_dev, ADRENO_FW_PFP);
-	int size = fw->size - 1;
+	int i, size = adreno_dev->pfp_fw_size - 1;
 
 	if (remain < DEBUG_SECTION_SZ(size)) {
 		SNAPSHOT_ERR_NOMEM(device, "CP PFP RAM DEBUG");
@@ -1149,7 +1122,6 @@ size_t adreno_snapshot_vpc_memory(struct kgsl_device *device, u8 *buf,
 	for (bank = 0; bank < VPC_MEMORY_BANKS; bank++) {
 		for (addr = 0; addr < vpc_mem_size; addr++) {
 			unsigned int val = bank | (addr << 4);
-
 			adreno_writereg(adreno_dev,
 				ADRENO_REG_VPC_DEBUG_RAM_SEL, val);
 			adreno_readreg(adreno_dev,
@@ -1202,8 +1174,7 @@ static const struct adreno_vbif_snapshot_registers *vbif_registers(
 	adreno_readreg(adreno_dev, ADRENO_REG_VBIF_VERSION, &version);
 
 	for (i = 0; i < count; i++) {
-		if ((list[i].version & list[i].mask) ==
-				(version & list[i].mask))
+		if (list[i].version == version)
 			return &list[i];
 	}
 
diff --git a/drivers/gpu/msm/adreno_sysfs.c b/drivers/gpu/msm/adreno_sysfs.c
index 022aa9f1f994..f9d16222177c 100644
--- a/drivers/gpu/msm/adreno_sysfs.c
+++ b/drivers/gpu/msm/adreno_sysfs.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2014-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2014-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -29,20 +29,12 @@ struct adreno_sysfs_attribute adreno_attr_##_name = { \
 	.store = _ ## _name ## _store, \
 }
 
-#define _ADRENO_SYSFS_ATTR_RO(_name, __show) \
-struct adreno_sysfs_attribute adreno_attr_##_name = { \
-	.attr = __ATTR(_name, 0444, __show, NULL), \
-	.show = _ ## _name ## _show, \
-	.store = NULL, \
-}
-
 #define ADRENO_SYSFS_ATTR(_a) \
 	container_of((_a), struct adreno_sysfs_attribute, attr)
 
 static struct adreno_device *_get_adreno_dev(struct device *dev)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	return device ? ADRENO_DEVICE(device) : NULL;
 }
 
@@ -58,55 +50,6 @@ static unsigned int _ft_policy_show(struct adreno_device *adreno_dev)
 	return adreno_dev->ft_policy;
 }
 
-static int _preempt_level_store(struct adreno_device *adreno_dev,
-		unsigned int val)
-{
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	if (val <= 2)
-		preempt->preempt_level = val;
-	return 0;
-}
-
-static unsigned int _preempt_level_show(struct adreno_device *adreno_dev)
-{
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	return preempt->preempt_level;
-}
-
-static int _usesgmem_store(struct adreno_device *adreno_dev,
-		unsigned int val)
-{
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	preempt->usesgmem = val ? 1 : 0;
-	return 0;
-}
-
-static unsigned int _usesgmem_show(struct adreno_device *adreno_dev)
-{
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	return preempt->usesgmem;
-}
-
-static int _skipsaverestore_store(struct adreno_device *adreno_dev,
-		unsigned int val)
-{
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	preempt->skipsaverestore = val ? 1 : 0;
-	return 0;
-}
-
-static unsigned int _skipsaverestore_show(struct adreno_device *adreno_dev)
-{
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	return preempt->skipsaverestore;
-}
-
 static int _ft_pagefault_policy_store(struct adreno_device *adreno_dev,
 		unsigned int val)
 {
@@ -133,29 +76,32 @@ static unsigned int _ft_pagefault_policy_show(struct adreno_device *adreno_dev)
 	return adreno_dev->ft_pf_policy;
 }
 
-static int _gpu_llc_slice_enable_store(struct adreno_device *adreno_dev,
+static int _ft_fast_hang_detect_store(struct adreno_device *adreno_dev,
 		unsigned int val)
 {
-	adreno_dev->gpu_llc_slice_enable = val ? true : false;
-	return 0;
-}
+	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
-static unsigned int _gpu_llc_slice_enable_show(struct adreno_device *adreno_dev)
-{
-	return adreno_dev->gpu_llc_slice_enable;
-}
+	if (!test_bit(ADRENO_DEVICE_SOFT_FAULT_DETECT, &adreno_dev->priv))
+		return 0;
+
+	mutex_lock(&device->mutex);
+
+	if (val) {
+		if (!kgsl_active_count_get(device)) {
+			adreno_fault_detect_start(adreno_dev);
+			kgsl_active_count_put(device);
+		}
+	} else
+		adreno_fault_detect_stop(adreno_dev);
+
+	mutex_unlock(&device->mutex);
 
-static int _gpuhtw_llc_slice_enable_store(struct adreno_device *adreno_dev,
-		unsigned int val)
-{
-	adreno_dev->gpuhtw_llc_slice_enable = val ? true : false;
 	return 0;
 }
 
-static unsigned int
-_gpuhtw_llc_slice_enable_show(struct adreno_device *adreno_dev)
+static unsigned int _ft_fast_hang_detect_show(struct adreno_device *adreno_dev)
 {
-	return adreno_dev->gpuhtw_llc_slice_enable;
+	return adreno_dev->fast_hang_detect;
 }
 
 static int _ft_long_ib_detect_store(struct adreno_device *adreno_dev,
@@ -184,6 +130,7 @@ static int _ft_hang_intr_status_store(struct adreno_device *adreno_dev,
 
 	if (test_bit(ADRENO_DEVICE_STARTED, &adreno_dev->priv)) {
 		kgsl_pwrctrl_change_state(device, KGSL_STATE_ACTIVE);
+		adreno_irqctrl(adreno_dev, 1);
 	} else if (device->state == KGSL_STATE_INIT) {
 		ret = -EACCES;
 		change_bit(ADRENO_DEVICE_HANG_INTR, &adreno_dev->priv);
@@ -223,14 +170,10 @@ static int _preemption_store(struct adreno_device *adreno_dev,
 {
 	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
 
-	mutex_lock(&device->mutex);
+	if (test_bit(ADRENO_DEVICE_PREEMPTION, &adreno_dev->priv) == val)
+			return 0;
 
-	if (!(ADRENO_FEATURE(adreno_dev, ADRENO_PREEMPTION)) ||
-		(test_bit(ADRENO_DEVICE_PREEMPTION,
-		&adreno_dev->priv) == val)) {
-		mutex_unlock(&device->mutex);
-		return 0;
-	}
+	mutex_lock(&device->mutex);
 
 	kgsl_pwrctrl_change_state(device, KGSL_STATE_SUSPEND);
 	change_bit(ADRENO_DEVICE_PREEMPTION, &adreno_dev->priv);
@@ -242,23 +185,6 @@ static int _preemption_store(struct adreno_device *adreno_dev,
 	return 0;
 }
 
-static int _gmu_idle_level_store(struct adreno_device *adreno_dev,
-		unsigned int val)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-
-	mutex_lock(&device->mutex);
-
-	/* Power down the GPU before changing the idle level */
-	kgsl_pwrctrl_change_state(device, KGSL_STATE_SUSPEND);
-	gmu->idle_level = val;
-	kgsl_pwrctrl_change_state(device, KGSL_STATE_SLUMBER);
-
-	mutex_unlock(&device->mutex);
-	return 0;
-}
-
 static unsigned int _preemption_show(struct adreno_device *adreno_dev)
 {
 	return adreno_is_preemption_enabled(adreno_dev);
@@ -275,17 +201,6 @@ static unsigned int _hwcg_show(struct adreno_device *adreno_dev)
 	return test_bit(ADRENO_HWCG_CTRL, &adreno_dev->pwrctrl_flag);
 }
 
-static int _throttling_store(struct adreno_device *adreno_dev,
-	unsigned int val)
-{
-	return _pwrctrl_store(adreno_dev, val, ADRENO_THROTTLING_CTRL);
-}
-
-static unsigned int _throttling_show(struct adreno_device *adreno_dev)
-{
-	return test_bit(ADRENO_THROTTLING_CTRL, &adreno_dev->pwrctrl_flag);
-}
-
 static int _sptp_pc_store(struct adreno_device *adreno_dev,
 		unsigned int val)
 {
@@ -307,47 +222,6 @@ static unsigned int _lm_show(struct adreno_device *adreno_dev)
 	return test_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag);
 }
 
-static int _ifpc_store(struct adreno_device *adreno_dev, unsigned int val)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-	unsigned int requested_idle_level;
-
-	if (!kgsl_gmu_isenabled(device) ||
-			!ADRENO_FEATURE(adreno_dev, ADRENO_IFPC))
-		return -EINVAL;
-
-	if ((val && gmu->idle_level >= GPU_HW_IFPC) ||
-			(!val && gmu->idle_level < GPU_HW_IFPC))
-		return 0;
-
-	if (val)
-		requested_idle_level = GPU_HW_IFPC;
-	else {
-		if (ADRENO_FEATURE(adreno_dev, ADRENO_SPTP_PC))
-			requested_idle_level = GPU_HW_SPTP_PC;
-		else
-			requested_idle_level = GPU_HW_ACTIVE;
-	}
-
-	return _gmu_idle_level_store(adreno_dev, requested_idle_level);
-}
-
-static unsigned int _ifpc_show(struct adreno_device *adreno_dev)
-{
-	struct kgsl_device *device = KGSL_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-
-	return kgsl_gmu_isenabled(device) && gmu->idle_level >= GPU_HW_IFPC;
-}
-
-static unsigned int _preempt_count_show(struct adreno_device *adreno_dev)
-{
-	struct adreno_preemption *preempt = &adreno_dev->preempt;
-
-	return preempt->count;
-}
-
 static ssize_t _sysfs_store_u32(struct device *dev,
 		struct device_attribute *attr,
 		const char *buf, size_t count)
@@ -428,19 +302,11 @@ static ssize_t _sysfs_show_bool(struct device *dev,
 #define ADRENO_SYSFS_U32(_name) \
 	_ADRENO_SYSFS_ATTR(_name, _sysfs_show_u32, _sysfs_store_u32)
 
-#define ADRENO_SYSFS_RO_U32(_name) \
-	_ADRENO_SYSFS_ATTR_RO(_name, _sysfs_show_u32)
-
 static ADRENO_SYSFS_U32(ft_policy);
 static ADRENO_SYSFS_U32(ft_pagefault_policy);
-static ADRENO_SYSFS_U32(preempt_level);
-static ADRENO_SYSFS_RO_U32(preempt_count);
-static ADRENO_SYSFS_BOOL(usesgmem);
-static ADRENO_SYSFS_BOOL(skipsaverestore);
+static ADRENO_SYSFS_BOOL(ft_fast_hang_detect);
 static ADRENO_SYSFS_BOOL(ft_long_ib_detect);
 static ADRENO_SYSFS_BOOL(ft_hang_intr_status);
-static ADRENO_SYSFS_BOOL(gpu_llc_slice_enable);
-static ADRENO_SYSFS_BOOL(gpuhtw_llc_slice_enable);
 
 static DEVICE_INT_ATTR(wake_nice, 0644, adreno_wake_nice);
 static DEVICE_INT_ATTR(wake_timeout, 0644, adreno_wake_timeout);
@@ -449,14 +315,12 @@ static ADRENO_SYSFS_BOOL(sptp_pc);
 static ADRENO_SYSFS_BOOL(lm);
 static ADRENO_SYSFS_BOOL(preemption);
 static ADRENO_SYSFS_BOOL(hwcg);
-static ADRENO_SYSFS_BOOL(throttling);
-static ADRENO_SYSFS_BOOL(ifpc);
-
 
 
 static const struct device_attribute *_attr_list[] = {
 	&adreno_attr_ft_policy.attr,
 	&adreno_attr_ft_pagefault_policy.attr,
+	&adreno_attr_ft_fast_hang_detect.attr,
 	&adreno_attr_ft_long_ib_detect.attr,
 	&adreno_attr_ft_hang_intr_status.attr,
 	&dev_attr_wake_nice.attr,
@@ -465,14 +329,6 @@ static const struct device_attribute *_attr_list[] = {
 	&adreno_attr_lm.attr,
 	&adreno_attr_preemption.attr,
 	&adreno_attr_hwcg.attr,
-	&adreno_attr_throttling.attr,
-	&adreno_attr_gpu_llc_slice_enable.attr,
-	&adreno_attr_gpuhtw_llc_slice_enable.attr,
-	&adreno_attr_preempt_level.attr,
-	&adreno_attr_usesgmem.attr,
-	&adreno_attr_skipsaverestore.attr,
-	&adreno_attr_ifpc.attr,
-	&adreno_attr_preempt_count.attr,
 	NULL,
 };
 
@@ -531,7 +387,6 @@ static ssize_t ppd_enable_show(struct kgsl_device *device,
 					char *buf)
 {
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
 	return snprintf(buf, PAGE_SIZE, "%u\n",
 		test_bit(ADRENO_PPD_CTRL, &adreno_dev->pwrctrl_flag));
 }
diff --git a/drivers/gpu/msm/adreno_trace.h b/drivers/gpu/msm/adreno_trace.h
index de028fad6a02..c0926cbb6a85 100644
--- a/drivers/gpu/msm/adreno_trace.h
+++ b/drivers/gpu/msm/adreno_trace.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -22,13 +22,10 @@
 #define TRACE_INCLUDE_FILE adreno_trace
 
 #include <linux/tracepoint.h>
-#include "adreno_a3xx.h"
-#include "adreno_a4xx.h"
-#include "adreno_a5xx.h"
 
 TRACE_EVENT(adreno_cmdbatch_queued,
-	TP_PROTO(struct kgsl_drawobj *drawobj, unsigned int queued),
-	TP_ARGS(drawobj, queued),
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, unsigned int queued),
+	TP_ARGS(cmdbatch, queued),
 	TP_STRUCT__entry(
 		__field(unsigned int, id)
 		__field(unsigned int, timestamp)
@@ -37,26 +34,26 @@ TRACE_EVENT(adreno_cmdbatch_queued,
 		__field(unsigned int, prio)
 	),
 	TP_fast_assign(
-		__entry->id = drawobj->context->id;
-		__entry->timestamp = drawobj->timestamp;
+		__entry->id = cmdbatch->context->id;
+		__entry->timestamp = cmdbatch->timestamp;
 		__entry->queued = queued;
-		__entry->flags = drawobj->flags;
-		__entry->prio = drawobj->context->priority;
+		__entry->flags = cmdbatch->flags;
+		__entry->prio = cmdbatch->context->priority;
 	),
 	TP_printk(
 		"ctx=%u ctx_prio=%u ts=%u queued=%u flags=%s",
 			__entry->id, __entry->prio,
 			__entry->timestamp, __entry->queued,
 			__entry->flags ? __print_flags(__entry->flags, "|",
-						KGSL_DRAWOBJ_FLAGS) : "none"
+						KGSL_CMDBATCH_FLAGS) : "none"
 	)
 );
 
 TRACE_EVENT(adreno_cmdbatch_submitted,
-	TP_PROTO(struct kgsl_drawobj *drawobj, int inflight, uint64_t ticks,
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, int inflight, uint64_t ticks,
 		unsigned long secs, unsigned long usecs,
-		struct adreno_ringbuffer *rb, unsigned int rptr),
-	TP_ARGS(drawobj, inflight, ticks, secs, usecs, rb, rptr),
+		struct adreno_ringbuffer *rb),
+	TP_ARGS(cmdbatch, inflight, ticks, secs, usecs, rb),
 	TP_STRUCT__entry(
 		__field(unsigned int, id)
 		__field(unsigned int, timestamp)
@@ -72,16 +69,16 @@ TRACE_EVENT(adreno_cmdbatch_submitted,
 		__field(int, q_inflight)
 	),
 	TP_fast_assign(
-		__entry->id = drawobj->context->id;
-		__entry->timestamp = drawobj->timestamp;
+		__entry->id = cmdbatch->context->id;
+		__entry->timestamp = cmdbatch->timestamp;
 		__entry->inflight = inflight;
-		__entry->flags = drawobj->flags;
+		__entry->flags = cmdbatch->flags;
 		__entry->ticks = ticks;
 		__entry->secs = secs;
 		__entry->usecs = usecs;
-		__entry->prio = drawobj->context->priority;
+		__entry->prio = cmdbatch->context->priority;
 		__entry->rb_id = rb->id;
-		__entry->rptr = rptr;
+		__entry->rptr = rb->rptr;
 		__entry->wptr = rb->wptr;
 		__entry->q_inflight = rb->dispatch_q.inflight;
 	),
@@ -90,7 +87,7 @@ TRACE_EVENT(adreno_cmdbatch_submitted,
 			__entry->id, __entry->prio, __entry->timestamp,
 			__entry->inflight,
 			__entry->flags ? __print_flags(__entry->flags, "|",
-				KGSL_DRAWOBJ_FLAGS) : "none",
+				KGSL_CMDBATCH_FLAGS) : "none",
 			__entry->ticks, __entry->secs, __entry->usecs,
 			__entry->rb_id, __entry->rptr, __entry->wptr,
 			__entry->q_inflight
@@ -98,11 +95,10 @@ TRACE_EVENT(adreno_cmdbatch_submitted,
 );
 
 TRACE_EVENT(adreno_cmdbatch_retired,
-	TP_PROTO(struct kgsl_drawobj *drawobj, int inflight,
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, int inflight,
 		uint64_t start, uint64_t retire,
-		struct adreno_ringbuffer *rb, unsigned int rptr,
-		unsigned long fault_recovery),
-	TP_ARGS(drawobj, inflight, start, retire, rb, rptr, fault_recovery),
+		struct adreno_ringbuffer *rb),
+	TP_ARGS(cmdbatch, inflight, start, retire, rb),
 	TP_STRUCT__entry(
 		__field(unsigned int, id)
 		__field(unsigned int, timestamp)
@@ -116,19 +112,18 @@ TRACE_EVENT(adreno_cmdbatch_retired,
 		__field(unsigned int, rptr)
 		__field(unsigned int, wptr)
 		__field(int, q_inflight)
-		__field(unsigned long, fault_recovery)
 	),
 	TP_fast_assign(
-		__entry->id = drawobj->context->id;
-		__entry->timestamp = drawobj->timestamp;
+		__entry->id = cmdbatch->context->id;
+		__entry->timestamp = cmdbatch->timestamp;
 		__entry->inflight = inflight;
-		__entry->recovery = fault_recovery;
-		__entry->flags = drawobj->flags;
+		__entry->recovery = cmdbatch->fault_recovery;
+		__entry->flags = cmdbatch->flags;
 		__entry->start = start;
 		__entry->retire = retire;
-		__entry->prio = drawobj->context->priority;
+		__entry->prio = cmdbatch->context->priority;
 		__entry->rb_id = rb->id;
-		__entry->rptr = rptr;
+		__entry->rptr = rb->rptr;
 		__entry->wptr = rb->wptr;
 		__entry->q_inflight = rb->dispatch_q.inflight;
 	),
@@ -140,7 +135,7 @@ TRACE_EVENT(adreno_cmdbatch_retired,
 				__print_flags(__entry->recovery, "|",
 				ADRENO_FT_TYPES) : "none",
 			__entry->flags ? __print_flags(__entry->flags, "|",
-				KGSL_DRAWOBJ_FLAGS) : "none",
+				KGSL_CMDBATCH_FLAGS) : "none",
 			__entry->start,
 			__entry->retire,
 			__entry->rb_id, __entry->rptr, __entry->wptr,
@@ -148,40 +143,17 @@ TRACE_EVENT(adreno_cmdbatch_retired,
 	)
 );
 
-TRACE_EVENT(adreno_cmdbatch_sync,
-	TP_PROTO(struct adreno_context *drawctxt,
-		uint64_t ticks),
-	TP_ARGS(drawctxt, ticks),
-	TP_STRUCT__entry(
-		__field(unsigned int, id)
-		__field(unsigned int, timestamp)
-		__field(uint64_t, ticks)
-		__field(int, prio)
-	),
-	TP_fast_assign(
-		__entry->id = drawctxt->base.id;
-		__entry->timestamp = drawctxt->timestamp;
-		__entry->ticks = ticks;
-		__entry->prio = drawctxt->base.priority;
-	),
-	TP_printk(
-		"ctx=%u ctx_prio=%d ts=%u ticks=%lld",
-			__entry->id, __entry->prio, __entry->timestamp,
-			__entry->ticks
-	)
-);
-
 TRACE_EVENT(adreno_cmdbatch_fault,
-	TP_PROTO(struct kgsl_drawobj_cmd *cmdobj, unsigned int fault),
-	TP_ARGS(cmdobj, fault),
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, unsigned int fault),
+	TP_ARGS(cmdbatch, fault),
 	TP_STRUCT__entry(
 		__field(unsigned int, id)
 		__field(unsigned int, timestamp)
 		__field(unsigned int, fault)
 	),
 	TP_fast_assign(
-		__entry->id = cmdobj->base.context->id;
-		__entry->timestamp = cmdobj->base.timestamp;
+		__entry->id = cmdbatch->context->id;
+		__entry->timestamp = cmdbatch->timestamp;
 		__entry->fault = fault;
 	),
 	TP_printk(
@@ -196,16 +168,16 @@ TRACE_EVENT(adreno_cmdbatch_fault,
 );
 
 TRACE_EVENT(adreno_cmdbatch_recovery,
-	TP_PROTO(struct kgsl_drawobj_cmd *cmdobj, unsigned int action),
-	TP_ARGS(cmdobj, action),
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, unsigned int action),
+	TP_ARGS(cmdbatch, action),
 	TP_STRUCT__entry(
 		__field(unsigned int, id)
 		__field(unsigned int, timestamp)
 		__field(unsigned int, action)
 	),
 	TP_fast_assign(
-		__entry->id = cmdobj->base.context->id;
-		__entry->timestamp = cmdobj->base.timestamp;
+		__entry->id = cmdbatch->context->id;
+		__entry->timestamp = cmdbatch->timestamp;
 		__entry->action = action;
 	),
 	TP_printk(
@@ -292,8 +264,9 @@ TRACE_EVENT(adreno_drawctxt_wait_done,
 
 TRACE_EVENT(adreno_drawctxt_switch,
 	TP_PROTO(struct adreno_ringbuffer *rb,
-		struct adreno_context *newctx),
-	TP_ARGS(rb, newctx),
+		struct adreno_context *newctx,
+		unsigned int flags),
+	TP_ARGS(rb, newctx, flags),
 	TP_STRUCT__entry(
 		__field(int, rb_level)
 		__field(unsigned int, oldctx)
@@ -307,8 +280,8 @@ TRACE_EVENT(adreno_drawctxt_switch,
 		__entry->newctx = newctx ? newctx->base.id : 0;
 	),
 	TP_printk(
-		"rb level=%d oldctx=%u newctx=%u",
-		__entry->rb_level, __entry->oldctx, __entry->newctx
+		"rb level=%d oldctx=%u newctx=%u flags=%X",
+		__entry->rb_level, __entry->oldctx, __entry->newctx, flags
 	)
 );
 
@@ -343,8 +316,7 @@ TRACE_EVENT(adreno_gpu_fault,
 		__entry->ib2size = ib2size;
 		__entry->rb_id = rb_id;
 	),
-	TP_printk(
-		"ctx=%d ts=%d rb_id=%d status=%X RB=%X/%X IB1=%X/%X IB2=%X/%X",
+	TP_printk("ctx=%d ts=%d rb_id=%d status=%X RB=%X/%X IB1=%X/%X IB2=%X/%X",
 		__entry->ctx, __entry->ts, __entry->rb_id, __entry->status,
 		__entry->wptr, __entry->rptr, __entry->ib1base,
 		__entry->ib1size, __entry->ib2base, __entry->ib2size)
@@ -392,7 +364,38 @@ TRACE_EVENT(kgsl_a3xx_irq_status,
 		"d_name=%s status=%s",
 		__get_str(device_name),
 		__entry->status ? __print_flags(__entry->status, "|",
-			A3XX_IRQ_FLAGS) : "None"
+			{ 1 << A3XX_INT_RBBM_GPU_IDLE, "RBBM_GPU_IDLE" },
+			{ 1 << A3XX_INT_RBBM_AHB_ERROR, "RBBM_AHB_ERR" },
+			{ 1 << A3XX_INT_RBBM_REG_TIMEOUT, "RBBM_REG_TIMEOUT" },
+			{ 1 << A3XX_INT_RBBM_ME_MS_TIMEOUT,
+				"RBBM_ME_MS_TIMEOUT" },
+			{ 1 << A3XX_INT_RBBM_PFP_MS_TIMEOUT,
+				"RBBM_PFP_MS_TIMEOUT" },
+			{ 1 << A3XX_INT_RBBM_ATB_BUS_OVERFLOW,
+				"RBBM_ATB_BUS_OVERFLOW" },
+			{ 1 << A3XX_INT_VFD_ERROR, "RBBM_VFD_ERROR" },
+			{ 1 << A3XX_INT_CP_SW_INT, "CP_SW" },
+			{ 1 << A3XX_INT_CP_T0_PACKET_IN_IB,
+				"CP_T0_PACKET_IN_IB" },
+			{ 1 << A3XX_INT_CP_OPCODE_ERROR, "CP_OPCODE_ERROR" },
+			{ 1 << A3XX_INT_CP_RESERVED_BIT_ERROR,
+				"CP_RESERVED_BIT_ERROR" },
+			{ 1 << A3XX_INT_CP_HW_FAULT, "CP_HW_FAULT" },
+			{ 1 << A3XX_INT_CP_DMA, "CP_DMA" },
+			{ 1 << A3XX_INT_CP_IB2_INT, "CP_IB2_INT" },
+			{ 1 << A3XX_INT_CP_IB1_INT, "CP_IB1_INT" },
+			{ 1 << A3XX_INT_CP_RB_INT, "CP_RB_INT" },
+			{ 1 << A3XX_INT_CP_REG_PROTECT_FAULT,
+				"CP_REG_PROTECT_FAULT" },
+			{ 1 << A3XX_INT_CP_RB_DONE_TS, "CP_RB_DONE_TS" },
+			{ 1 << A3XX_INT_CP_VS_DONE_TS, "CP_VS_DONE_TS" },
+			{ 1 << A3XX_INT_CP_PS_DONE_TS, "CP_PS_DONE_TS" },
+			{ 1 << A3XX_INT_CACHE_FLUSH_TS, "CACHE_FLUSH_TS" },
+			{ 1 << A3XX_INT_CP_AHB_ERROR_HALT,
+				"CP_AHB_ERROR_HALT" },
+			{ 1 << A3XX_INT_MISC_HANG_DETECT, "MISC_HANG_DETECT" },
+			{ 1 << A3XX_INT_UCHE_OOB_ACCESS, "UCHE_OOB_ACCESS" })
+		: "None"
 	)
 );
 
@@ -419,42 +422,56 @@ TRACE_EVENT(kgsl_a4xx_irq_status,
 		"d_name=%s status=%s",
 		__get_str(device_name),
 		__entry->status ? __print_flags(__entry->status, "|",
-			A4XX_IRQ_FLAGS) : "None"
-	)
-);
-
-/*
- * Tracepoint for a5xx irq. Includes status info
- */
-TRACE_EVENT(kgsl_a5xx_irq_status,
-
-	TP_PROTO(struct adreno_device *adreno_dev, unsigned int status),
-
-	TP_ARGS(adreno_dev, status),
-
-	TP_STRUCT__entry(
-		__string(device_name, adreno_dev->dev.name)
-		__field(unsigned int, status)
-	),
-
-	TP_fast_assign(
-		__assign_str(device_name, adreno_dev->dev.name);
-		__entry->status = status;
-	),
-
-	TP_printk(
-		"d_name=%s status=%s",
-		__get_str(device_name),
-		__entry->status ? __print_flags(__entry->status, "|",
-			A5XX_IRQ_FLAGS) : "None"
+			{ 1 << A4XX_INT_RBBM_GPU_IDLE, "RBBM_GPU_IDLE" },
+			{ 1 << A4XX_INT_RBBM_AHB_ERROR, "RBBM_AHB_ERR" },
+			{ 1 << A4XX_INT_RBBM_REG_TIMEOUT, "RBBM_REG_TIMEOUT" },
+			{ 1 << A4XX_INT_RBBM_ME_MS_TIMEOUT,
+				"RBBM_ME_MS_TIMEOUT" },
+			{ 1 << A4XX_INT_RBBM_PFP_MS_TIMEOUT,
+				"RBBM_PFP_MS_TIMEOUT" },
+			{ 1 << A4XX_INT_RBBM_ETS_MS_TIMEOUT,
+				"RBBM_ETS_MS_TIMEOUT" },
+			{ 1 << A4XX_INT_RBBM_ASYNC_OVERFLOW,
+				"RBBM_ASYNC_OVERFLOW" },
+			{ 1 << A4XX_INT_RBBM_GPC_ERR,
+				"RBBM_GPC_ERR" },
+			{ 1 << A4XX_INT_CP_SW, "CP_SW" },
+			{ 1 << A4XX_INT_CP_OPCODE_ERROR, "CP_OPCODE_ERROR" },
+			{ 1 << A4XX_INT_CP_RESERVED_BIT_ERROR,
+				"CP_RESERVED_BIT_ERROR" },
+			{ 1 << A4XX_INT_CP_HW_FAULT, "CP_HW_FAULT" },
+			{ 1 << A4XX_INT_CP_DMA, "CP_DMA" },
+			{ 1 << A4XX_INT_CP_IB2_INT, "CP_IB2_INT" },
+			{ 1 << A4XX_INT_CP_IB1_INT, "CP_IB1_INT" },
+			{ 1 << A4XX_INT_CP_RB_INT, "CP_RB_INT" },
+			{ 1 << A4XX_INT_CP_REG_PROTECT_FAULT,
+				"CP_REG_PROTECT_FAULT" },
+			{ 1 << A4XX_INT_CP_RB_DONE_TS, "CP_RB_DONE_TS" },
+			{ 1 << A4XX_INT_CP_VS_DONE_TS, "CP_VS_DONE_TS" },
+			{ 1 << A4XX_INT_CP_PS_DONE_TS, "CP_PS_DONE_TS" },
+			{ 1 << A4XX_INT_CACHE_FLUSH_TS, "CACHE_FLUSH_TS" },
+			{ 1 << A4XX_INT_CP_AHB_ERROR_HALT,
+				"CP_AHB_ERROR_HALT" },
+			{ 1 << A4XX_INT_RBBM_ATB_BUS_OVERFLOW,
+				"RBBM_ATB_BUS_OVERFLOW" },
+			{ 1 << A4XX_INT_MISC_HANG_DETECT, "MISC_HANG_DETECT" },
+			{ 1 << A4XX_INT_UCHE_OOB_ACCESS, "UCHE_OOB_ACCESS" },
+			{ 1 << A4XX_INT_RBBM_DPM_CALC_ERR,
+				"RBBM_DPM_CALC_ERR" },
+			{ 1 << A4XX_INT_RBBM_DPM_EPOCH_ERR,
+				"RBBM_DPM_CALC_ERR" },
+			{ 1 << A4XX_INT_RBBM_DPM_THERMAL_YELLOW_ERR,
+				"RBBM_DPM_THERMAL_YELLOW_ERR" },
+			{ 1 << A4XX_INT_RBBM_DPM_THERMAL_RED_ERR,
+				"RBBM_DPM_THERMAL_RED_ERR" })
+		: "None"
 	)
 );
 
 DECLARE_EVENT_CLASS(adreno_hw_preempt_template,
 	TP_PROTO(struct adreno_ringbuffer *cur_rb,
-		struct adreno_ringbuffer *new_rb,
-		unsigned int cur_rptr, unsigned int new_rptr),
-	TP_ARGS(cur_rb, new_rb, cur_rptr, new_rptr),
+		struct adreno_ringbuffer *new_rb),
+	TP_ARGS(cur_rb, new_rb),
 	TP_STRUCT__entry(__field(int, cur_level)
 			__field(int, new_level)
 			__field(unsigned int, cur_rptr)
@@ -466,8 +483,8 @@ DECLARE_EVENT_CLASS(adreno_hw_preempt_template,
 	),
 	TP_fast_assign(__entry->cur_level = cur_rb->id;
 			__entry->new_level = new_rb->id;
-			__entry->cur_rptr = cur_rptr;
-			__entry->new_rptr = new_rptr;
+			__entry->cur_rptr = cur_rb->rptr;
+			__entry->new_rptr = new_rb->rptr;
 			__entry->cur_wptr = cur_rb->wptr;
 			__entry->new_wptr = new_rb->wptr;
 			__entry->cur_rbbase = cur_rb->buffer_desc.gpuaddr;
@@ -484,30 +501,26 @@ DECLARE_EVENT_CLASS(adreno_hw_preempt_template,
 
 DEFINE_EVENT(adreno_hw_preempt_template, adreno_hw_preempt_clear_to_trig,
 	TP_PROTO(struct adreno_ringbuffer *cur_rb,
-		struct adreno_ringbuffer *new_rb,
-		unsigned int cur_rptr, unsigned int new_rptr),
-	TP_ARGS(cur_rb, new_rb, cur_rptr, new_rptr)
+		struct adreno_ringbuffer *new_rb),
+	TP_ARGS(cur_rb, new_rb)
 );
 
 DEFINE_EVENT(adreno_hw_preempt_template, adreno_hw_preempt_trig_to_comp,
 	TP_PROTO(struct adreno_ringbuffer *cur_rb,
-		struct adreno_ringbuffer *new_rb,
-		unsigned int cur_rptr, unsigned int new_rptr),
-	TP_ARGS(cur_rb, new_rb, cur_rptr, new_rptr)
+		struct adreno_ringbuffer *new_rb),
+	TP_ARGS(cur_rb, new_rb)
 );
 
 DEFINE_EVENT(adreno_hw_preempt_template, adreno_hw_preempt_trig_to_comp_int,
 	TP_PROTO(struct adreno_ringbuffer *cur_rb,
-		struct adreno_ringbuffer *new_rb,
-		unsigned int cur_rptr, unsigned int new_rptr),
-	TP_ARGS(cur_rb, new_rb, cur_rptr, new_rptr)
+		struct adreno_ringbuffer *new_rb),
+	TP_ARGS(cur_rb, new_rb)
 );
 
 TRACE_EVENT(adreno_hw_preempt_comp_to_clear,
 	TP_PROTO(struct adreno_ringbuffer *cur_rb,
-		struct adreno_ringbuffer *new_rb,
-		unsigned int cur_rptr, unsigned int new_rptr),
-	TP_ARGS(cur_rb, new_rb, cur_rptr, new_rptr),
+		struct adreno_ringbuffer *new_rb),
+	TP_ARGS(cur_rb, new_rb),
 	TP_STRUCT__entry(__field(int, cur_level)
 			__field(int, new_level)
 			__field(unsigned int, cur_rptr)
@@ -520,8 +533,8 @@ TRACE_EVENT(adreno_hw_preempt_comp_to_clear,
 	),
 	TP_fast_assign(__entry->cur_level = cur_rb->id;
 			__entry->new_level = new_rb->id;
-			__entry->cur_rptr = cur_rptr;
-			__entry->new_rptr = new_rptr;
+			__entry->cur_rptr = cur_rb->rptr;
+			__entry->new_rptr = new_rb->rptr;
 			__entry->cur_wptr = cur_rb->wptr;
 			__entry->new_wptr_end = new_rb->wptr_preempt_end;
 			__entry->new_wptr = new_rb->wptr;
@@ -539,9 +552,8 @@ TRACE_EVENT(adreno_hw_preempt_comp_to_clear,
 
 TRACE_EVENT(adreno_hw_preempt_token_submit,
 	TP_PROTO(struct adreno_ringbuffer *cur_rb,
-		struct adreno_ringbuffer *new_rb,
-		unsigned int cur_rptr, unsigned int new_rptr),
-	TP_ARGS(cur_rb, new_rb, cur_rptr, new_rptr),
+		struct adreno_ringbuffer *new_rb),
+	TP_ARGS(cur_rb, new_rb),
 	TP_STRUCT__entry(__field(int, cur_level)
 		__field(int, new_level)
 		__field(unsigned int, cur_rptr)
@@ -554,8 +566,8 @@ TRACE_EVENT(adreno_hw_preempt_token_submit,
 	),
 	TP_fast_assign(__entry->cur_level = cur_rb->id;
 			__entry->new_level = new_rb->id;
-			__entry->cur_rptr = cur_rptr;
-			__entry->new_rptr = new_rptr;
+			__entry->cur_rptr = cur_rb->rptr;
+			__entry->new_rptr = new_rb->rptr;
 			__entry->cur_wptr = cur_rb->wptr;
 			__entry->cur_wptr_end = cur_rb->wptr_preempt_end;
 			__entry->new_wptr = new_rb->wptr;
@@ -572,43 +584,103 @@ TRACE_EVENT(adreno_hw_preempt_token_submit,
 	)
 );
 
-TRACE_EVENT(adreno_preempt_trigger,
-	TP_PROTO(struct adreno_ringbuffer *cur, struct adreno_ringbuffer *next,
-		unsigned int cntl),
-	TP_ARGS(cur, next, cntl),
-	TP_STRUCT__entry(
-		__field(struct adreno_ringbuffer *, cur)
-		__field(struct adreno_ringbuffer *, next)
-		__field(unsigned int, cntl)
+TRACE_EVENT(adreno_rb_starve,
+	TP_PROTO(struct adreno_ringbuffer *rb),
+	TP_ARGS(rb),
+	TP_STRUCT__entry(__field(int, id)
+		__field(unsigned int, rptr)
+		__field(unsigned int, wptr)
 	),
-	TP_fast_assign(
-		__entry->cur = cur;
-		__entry->next = next;
-		__entry->cntl = cntl;
+	TP_fast_assign(__entry->id = rb->id;
+		__entry->rptr = rb->rptr;
+		__entry->wptr = rb->wptr;
 	),
-	TP_printk("trigger from id=%d to id=%d cntl=%x",
-		__entry->cur->id, __entry->next->id, __entry->cntl
+	TP_printk(
+		"rb %d r/w %x/%x starved", __entry->id, __entry->rptr,
+		__entry->wptr
 	)
 );
 
-TRACE_EVENT(adreno_preempt_done,
-	TP_PROTO(struct adreno_ringbuffer *cur, struct adreno_ringbuffer *next,
-		unsigned int level),
-	TP_ARGS(cur, next, level),
+/*
+ * Tracepoint for a5xx irq. Includes status info
+ */
+TRACE_EVENT(kgsl_a5xx_irq_status,
+
+	TP_PROTO(struct adreno_device *adreno_dev, unsigned int status),
+
+	TP_ARGS(adreno_dev, status),
+
 	TP_STRUCT__entry(
-		__field(struct adreno_ringbuffer *, cur)
-		__field(struct adreno_ringbuffer *, next)
-		__field(unsigned int, level)
+		__string(device_name, adreno_dev->dev.name)
+		__field(unsigned int, status)
 	),
+
 	TP_fast_assign(
-		__entry->cur = cur;
-		__entry->next = next;
-		__entry->level = level;
+		__assign_str(device_name, adreno_dev->dev.name);
+		__entry->status = status;
 	),
-	TP_printk("done switch to id=%d from id=%d level=%x",
-		__entry->next->id, __entry->cur->id, __entry->level
+
+	TP_printk(
+		"d_name=%s status=%s",
+		__get_str(device_name),
+		__entry->status ? __print_flags(__entry->status, "|",
+			{ 1 << A5XX_INT_RBBM_GPU_IDLE, "RBBM_GPU_IDLE" },
+			{ 1 << A5XX_INT_RBBM_AHB_ERROR, "RBBM_AHB_ERR" },
+			{ 1 << A5XX_INT_RBBM_TRANSFER_TIMEOUT,
+				"RBBM_TRANSFER_TIMEOUT" },
+			{ 1 << A5XX_INT_RBBM_ME_MS_TIMEOUT,
+				"RBBM_ME_MS_TIMEOUT" },
+			{ 1 << A5XX_INT_RBBM_PFP_MS_TIMEOUT,
+				"RBBM_PFP_MS_TIMEOUT" },
+			{ 1 << A5XX_INT_RBBM_ETS_MS_TIMEOUT,
+				"RBBM_ETS_MS_TIMEOUT" },
+			{ 1 << A5XX_INT_RBBM_ATB_ASYNC_OVERFLOW,
+				"RBBM_ATB_ASYNC_OVERFLOW" },
+			{ 1 << A5XX_INT_RBBM_GPC_ERROR,
+				"RBBM_GPC_ERR" },
+			{ 1 << A5XX_INT_CP_SW, "CP_SW" },
+			{ 1 << A5XX_INT_CP_HW_ERROR, "CP_OPCODE_ERROR" },
+			{ 1 << A5XX_INT_CP_CCU_FLUSH_DEPTH_TS,
+				"CP_CCU_FLUSH_DEPTH_TS" },
+			{ 1 << A5XX_INT_CP_CCU_FLUSH_COLOR_TS,
+				"CP_CCU_FLUSH_COLOR_TS" },
+			{ 1 << A5XX_INT_CP_CCU_RESOLVE_TS,
+				"CP_CCU_RESOLVE_TS" },
+			{ 1 << A5XX_INT_CP_IB2, "CP_IB2_INT" },
+			{ 1 << A5XX_INT_CP_IB1, "CP_IB1_INT" },
+			{ 1 << A5XX_INT_CP_RB, "CP_RB_INT" },
+			{ 1 << A5XX_INT_CP_UNUSED_1, "CP_UNUSED_1" },
+			{ 1 << A5XX_INT_CP_RB_DONE_TS, "CP_RB_DONE_TS" },
+			{ 1 << A5XX_INT_CP_WT_DONE_TS, "CP_WT_DONE_TS" },
+			{ 1 << A5XX_INT_UNKNOWN_1, "UNKNOWN_1" },
+			{ 1 << A5XX_INT_CP_CACHE_FLUSH_TS,
+				"CP_CACHE_FLUSH_TS" },
+			{ 1 << A5XX_INT_UNUSED_2,
+				"UNUSED_2" },
+			{ 1 << A5XX_INT_RBBM_ATB_BUS_OVERFLOW,
+				"RBBM_ATB_BUS_OVERFLOW" },
+			{ 1 << A5XX_INT_MISC_HANG_DETECT,
+				"MISC_HANG_DETECT" },
+			{ 1 << A5XX_INT_UCHE_OOB_ACCESS,
+				"UCHE_OOB_ACCESS" },
+			{ 1 << A5XX_INT_UCHE_TRAP_INTR,
+				"UCHE_TRAP_INTR" },
+			{ 1 << A5XX_INT_DEBBUS_INTR_0,
+				"DEBBUS_INTR_0" },
+			{ 1 << A5XX_INT_DEBBUS_INTR_1,
+				"DEBBUS_INTR_1" },
+			{ 1 << A5XX_INT_GPMU_VOLTAGE_DROOP,
+				"GPMU_VOLTAGE_DROOP" },
+			{ 1 << A5XX_INT_GPMU_FIRMWARE,
+				"GPMU_FIRMWARE" },
+			{ 1 << A5XX_INT_ISDB_CPU_IRQ,
+				"ISDB_CPU_IRQ" },
+			{ 1 << A5XX_INT_ISDB_UNDER_DEBUG,
+				"ISDB_UNDER_DEBUG" })
+		: "None"
 	)
 );
+
 #endif /* _ADRENO_TRACE_H */
 
 /* This part must be outside protection */
diff --git a/drivers/gpu/msm/kgsl.c b/drivers/gpu/msm/kgsl.c
old mode 100755
new mode 100644
index 3520ddf350df..811cff41740b
--- a/drivers/gpu/msm/kgsl.c
+++ b/drivers/gpu/msm/kgsl.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008-2019, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2008-2018, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -30,19 +30,17 @@
 #include <linux/security.h>
 #include <linux/compat.h>
 #include <linux/ctype.h>
-#include <linux/mm.h>
-#include <asm/cacheflush.h>
 
 #include "kgsl.h"
 #include "kgsl_debugfs.h"
+#include "kgsl_cffdump.h"
 #include "kgsl_log.h"
 #include "kgsl_sharedmem.h"
-#include "kgsl_drawobj.h"
+#include "kgsl_cmdbatch.h"
 #include "kgsl_device.h"
 #include "kgsl_trace.h"
 #include "kgsl_sync.h"
 #include "kgsl_compat.h"
-#include "kgsl_pool.h"
 
 #undef MODULE_PARAM_PREFIX
 #define MODULE_PARAM_PREFIX "kgsl."
@@ -66,7 +64,7 @@
 #endif
 
 static char *kgsl_mmu_type;
-module_param_named(mmutype, kgsl_mmu_type, charp, 0000);
+module_param_named(mmutype, kgsl_mmu_type, charp, 0);
 MODULE_PARM_DESC(kgsl_mmu_type, "Type of MMU to be used for graphics");
 
 /* Mutex used for the IOMMU sync quirk */
@@ -79,16 +77,6 @@ struct kgsl_dma_buf_meta {
 	struct sg_table *table;
 };
 
-static inline struct kgsl_pagetable *_get_memdesc_pagetable(
-		struct kgsl_pagetable *pt, struct kgsl_mem_entry *entry)
-{
-	/* if a secured buffer, map it to secure global pagetable */
-	if (kgsl_memdesc_is_secured(&entry->memdesc))
-		return pt->mmu->securepagetable;
-
-	return pt;
-}
-
 static void kgsl_mem_entry_detach_process(struct kgsl_mem_entry *entry);
 
 static const struct file_operations kgsl_fops;
@@ -119,7 +107,7 @@ static struct {
 
 static int kgsl_memfree_init(void)
 {
-	memfree.list = kcalloc(MEMFREE_ENTRIES, sizeof(struct memfree_entry),
+	memfree.list = kzalloc(MEMFREE_ENTRIES * sizeof(struct memfree_entry),
 		GFP_KERNEL);
 
 	return (memfree.list) ? 0 : -ENOMEM;
@@ -261,7 +249,9 @@ kgsl_mem_entry_create(void)
 
 	if (entry != NULL) {
 		kref_init(&entry->refcount);
-		/* put this ref in userspace memory alloc and map ioctls */
+		INIT_WORK(&entry->work, _deferred_put);
+
+		/* put this ref in the caller functions after init */
 		kref_get(&entry->refcount);
 	}
 
@@ -327,7 +317,7 @@ kgsl_mem_entry_destroy(struct kref *kref)
 			    entry->memdesc.sgt->nents, i) {
 			page = sg_page(sg);
 			for (j = 0; j < (sg->length >> PAGE_SHIFT); j++)
-				set_page_dirty_lock(nth_page(page, j));
+				set_page_dirty(nth_page(page, j));
 		}
 	}
 
@@ -355,10 +345,8 @@ static int kgsl_mem_entry_track_gpuaddr(struct kgsl_device *device,
 	/*
 	 * If SVM is enabled for this object then the address needs to be
 	 * assigned elsewhere
-	 * Also do not proceed further in case of NoMMU.
 	 */
-	if (kgsl_memdesc_use_cpu_map(&entry->memdesc) ||
-		(kgsl_mmu_get_mmutype(device) == KGSL_MMU_TYPE_NONE))
+	if (kgsl_memdesc_use_cpu_map(&entry->memdesc))
 		return 0;
 
 	pagetable = kgsl_memdesc_is_secured(&entry->memdesc) ?
@@ -420,14 +408,7 @@ static int kgsl_mem_entry_attach_process(struct kgsl_device *device,
 	 * kgsl_mem_entry_track_gpuaddr() or via some other SVM process
 	 */
 	if (entry->memdesc.gpuaddr) {
-		if (entry->memdesc.flags & KGSL_MEMFLAGS_SPARSE_VIRT)
-			ret = kgsl_mmu_sparse_dummy_map(
-					entry->memdesc.pagetable,
-					&entry->memdesc, 0,
-					entry->memdesc.size);
-		else if (entry->memdesc.gpuaddr)
-			ret = kgsl_mmu_map(entry->memdesc.pagetable,
-					&entry->memdesc);
+		ret = kgsl_mmu_map(entry->memdesc.pagetable, &entry->memdesc);
 
 		if (ret)
 			kgsl_mem_entry_detach_process(entry);
@@ -447,10 +428,8 @@ static void kgsl_mem_entry_detach_process(struct kgsl_mem_entry *entry)
 	if (entry == NULL)
 		return;
 
-	/*
-	 * First remove the entry from mem_idr list
-	 * so that no one can operate on obsolete values
-	 */
+	kgsl_mmu_put_gpuaddr(&entry->memdesc);
+
 	spin_lock(&entry->priv->mem_lock);
 	if (entry->id != 0)
 		idr_remove(&entry->priv->mem_idr, entry->id);
@@ -458,14 +437,7 @@ static void kgsl_mem_entry_detach_process(struct kgsl_mem_entry *entry)
 
 	type = kgsl_memdesc_usermem_type(&entry->memdesc);
 	entry->priv->stats[type].cur -= entry->memdesc.size;
-
-	if (type != KGSL_MEM_ENTRY_ION)
-		entry->priv->gpumem_mapped -= entry->memdesc.mapsize;
-
 	spin_unlock(&entry->priv->mem_lock);
-
-	kgsl_mmu_put_gpuaddr(&entry->memdesc);
-
 	kgsl_process_private_put(entry->priv);
 
 	entry->priv = NULL;
@@ -533,23 +505,6 @@ int kgsl_context_init(struct kgsl_device_private *dev_priv,
 	struct kgsl_device *device = dev_priv->device;
 	char name[64];
 	int ret = 0, id;
-	struct kgsl_process_private  *proc_priv = dev_priv->process_priv;
-
-	/*
-	 * Read and increment the context count under lock to make sure
-	 * no process goes beyond the specified context limit.
-	 */
-	spin_lock(&proc_priv->ctxt_count_lock);
-	if (atomic_read(&proc_priv->ctxt_count) > KGSL_MAX_CONTEXTS_PER_PROC) {
-		KGSL_DRV_ERR_RATELIMIT(device,
-			"Per process context limit reached for pid %u",
-			dev_priv->process_priv->pid);
-		spin_unlock(&proc_priv->ctxt_count_lock);
-		return -ENOSPC;
-	}
-
-	atomic_inc(&proc_priv->ctxt_count);
-	spin_unlock(&proc_priv->ctxt_count_lock);
 
 	id = _kgsl_get_context_id(device);
 	if (id == -ENOSPC) {
@@ -568,7 +523,7 @@ int kgsl_context_init(struct kgsl_device_private *dev_priv,
 			KGSL_DRV_INFO(device,
 				"cannot have more than %zu contexts due to memstore limitation\n",
 				KGSL_MEMSTORE_MAX);
-		atomic_dec(&proc_priv->ctxt_count);
+
 		return id;
 	}
 
@@ -590,10 +545,8 @@ int kgsl_context_init(struct kgsl_device_private *dev_priv,
 	context->tid = task_pid_nr(current);
 
 	ret = kgsl_sync_timeline_create(context);
-	if (ret) {
-		kgsl_process_private_put(dev_priv->process_priv);
+	if (ret)
 		goto out;
-	}
 
 	snprintf(name, sizeof(name), "context-%d", id);
 	kgsl_add_event_group(&context->events, context, name,
@@ -601,7 +554,6 @@ int kgsl_context_init(struct kgsl_device_private *dev_priv,
 
 out:
 	if (ret) {
-		atomic_dec(&proc_priv->ctxt_count);
 		write_lock(&device->context_lock);
 		idr_remove(&dev_priv->device->context_idr, id);
 		write_unlock(&device->context_lock);
@@ -622,7 +574,7 @@ EXPORT_SYMBOL(kgsl_context_init);
  * detached by checking the KGSL_CONTEXT_PRIV_DETACHED bit in
  * context->priv.
  */
-void kgsl_context_detach(struct kgsl_context *context)
+static void kgsl_context_detach(struct kgsl_context *context)
 {
 	struct kgsl_device *device;
 
@@ -653,8 +605,6 @@ void kgsl_context_detach(struct kgsl_context *context)
 	/* Remove the event group from the list */
 	kgsl_del_event_group(&context->events);
 
-	kgsl_sync_timeline_put(context->ktimeline);
-
 	kgsl_context_put(context);
 }
 
@@ -667,10 +617,6 @@ kgsl_context_destroy(struct kref *kref)
 
 	trace_kgsl_context_destroy(device, context);
 
-	/*
-	 * It's not safe to destroy the context if it's not detached as GPU
-	 * may still be executing commands
-	 */
 	BUG_ON(!kgsl_context_detached(context));
 
 	write_lock(&device->context_lock);
@@ -691,7 +637,6 @@ kgsl_context_destroy(struct kref *kref)
 			device->pwrctrl.constraint.type = KGSL_CONSTRAINT_NONE;
 		}
 
-		atomic_dec(&context->proc_priv->ctxt_count);
 		idr_remove(&device->context_idr, context->id);
 		context->id = KGSL_CONTEXT_INVALID;
 	}
@@ -764,8 +709,6 @@ static int kgsl_suspend_device(struct kgsl_device *device, pm_message_t state)
 
 	mutex_lock(&device->mutex);
 	status = kgsl_pwrctrl_change_state(device, KGSL_STATE_SUSPEND);
-	if (status == 0 && device->state == KGSL_STATE_SUSPEND)
-		device->ftbl->dispatcher_halt(device);
 	mutex_unlock(&device->mutex);
 
 	KGSL_PWR_WARN(device, "suspend end\n");
@@ -780,7 +723,6 @@ static int kgsl_resume_device(struct kgsl_device *device)
 	KGSL_PWR_WARN(device, "resume start\n");
 	mutex_lock(&device->mutex);
 	if (device->state == KGSL_STATE_SUSPEND) {
-		device->ftbl->dispatcher_unhalt(device);
 		kgsl_pwrctrl_change_state(device, KGSL_STATE_SLUMBER);
 	} else if (device->state != KGSL_STATE_INIT) {
 		/*
@@ -806,14 +748,12 @@ static int kgsl_suspend(struct device *dev)
 
 	pm_message_t arg = {0};
 	struct kgsl_device *device = dev_get_drvdata(dev);
-
 	return kgsl_suspend_device(device, arg);
 }
 
 static int kgsl_resume(struct device *dev)
 {
 	struct kgsl_device *device = dev_get_drvdata(dev);
-
 	return kgsl_resume_device(device);
 }
 
@@ -839,7 +779,6 @@ int kgsl_suspend_driver(struct platform_device *pdev,
 					pm_message_t state)
 {
 	struct kgsl_device *device = dev_get_drvdata(&pdev->dev);
-
 	return kgsl_suspend_device(device, state);
 }
 EXPORT_SYMBOL(kgsl_suspend_driver);
@@ -847,7 +786,6 @@ EXPORT_SYMBOL(kgsl_suspend_driver);
 int kgsl_resume_driver(struct platform_device *pdev)
 {
 	struct kgsl_device *device = dev_get_drvdata(&pdev->dev);
-
 	return kgsl_resume_device(device);
 }
 EXPORT_SYMBOL(kgsl_resume_driver);
@@ -874,6 +812,7 @@ static void kgsl_destroy_process_private(struct kref *kref)
 		kgsl_mmu_putpagetable(private->pagetable);
 
 	kfree(private);
+	return;
 }
 
 void
@@ -932,7 +871,6 @@ static struct kgsl_process_private *kgsl_process_private_new(
 
 	spin_lock_init(&private->mem_lock);
 	spin_lock_init(&private->syncsource_lock);
-	spin_lock_init(&private->ctxt_count_lock);
 
 	idr_init(&private->mem_idr);
 	idr_init(&private->syncsource_idr);
@@ -981,6 +919,24 @@ static void process_release_memory(struct kgsl_process_private *private)
 	}
 }
 
+static void process_release_sync_sources(struct kgsl_process_private *private)
+{
+	struct kgsl_syncsource *syncsource;
+	int next = 0;
+
+	while (1) {
+		spin_lock(&private->syncsource_lock);
+		syncsource = idr_get_next(&private->syncsource_idr, &next);
+		spin_unlock(&private->syncsource_lock);
+
+		if (syncsource == NULL)
+			break;
+
+		kgsl_syncsource_put(syncsource);
+		next = next + 1;
+	}
+}
+
 static void kgsl_process_private_close(struct kgsl_device_private *dev_priv,
 		struct kgsl_process_private *private)
 {
@@ -998,9 +954,9 @@ static void kgsl_process_private_close(struct kgsl_device_private *dev_priv,
 	 */
 
 	kgsl_process_uninit_sysfs(private);
+	debugfs_remove_recursive(private->debug_root);
 
-	/* Release all syncsource objects from process private */
-	kgsl_syncsource_process_release_syncsources(private);
+	process_release_sync_sources(private);
 
 	/* When using global pagetables, do not detach global pagetable */
 	if (private->pagetable->name != KGSL_MMU_GLOBAL_PT)
@@ -1010,14 +966,12 @@ static void kgsl_process_private_close(struct kgsl_device_private *dev_priv,
 	list_del(&private->list);
 
 	/*
-	 * Unlock the mutex before releasing the memory and the debugfs
-	 * nodes - this prevents deadlocks with the IOMMU and debugfs
-	 * locks.
+	 * Unlock the mutex before releasing the memory - this prevents a
+	 * deadlock with the IOMMU mutex if a page fault occurs
 	 */
 	mutex_unlock(&kgsl_driver.process_mutex);
 
 	process_release_memory(private);
-	debugfs_remove_recursive(private->debug_root);
 
 	kgsl_process_private_put(private);
 }
@@ -1077,28 +1031,25 @@ static void device_release_contexts(struct kgsl_device_private *dev_priv)
 	struct kgsl_device *device = dev_priv->device;
 	struct kgsl_context *context;
 	int next = 0;
-	int result = 0;
 
 	while (1) {
 		read_lock(&device->context_lock);
 		context = idr_get_next(&device->context_idr, &next);
+		read_unlock(&device->context_lock);
 
-		if (context == NULL) {
-			read_unlock(&device->context_lock);
+		if (context == NULL)
 			break;
-		} else if (context->dev_priv == dev_priv) {
+
+		if (context->dev_priv == dev_priv) {
 			/*
 			 * Hold a reference to the context in case somebody
 			 * tries to put it while we are detaching
 			 */
-			result = _kgsl_context_get(context);
-		}
-		read_unlock(&device->context_lock);
 
-		if (result) {
-			kgsl_context_detach(context);
-			kgsl_context_put(context);
-			result = 0;
+			if (_kgsl_context_get(context)) {
+				kgsl_context_detach(context);
+				kgsl_context_put(context);
+			}
 		}
 
 		next = next + 1;
@@ -1119,8 +1070,7 @@ static int kgsl_release(struct inode *inodep, struct file *filep)
 	/* Close down the process wide resources for the file */
 	kgsl_process_private_close(dev_priv, dev_priv->process_priv);
 
-	/* Destroy the device-specific structure */
-	device->ftbl->device_private_destroy(dev_priv);
+	kfree(dev_priv);
 
 	result = kgsl_close_device(device);
 	pm_runtime_put(&device->pdev->dev);
@@ -1188,7 +1138,7 @@ static int kgsl_open(struct inode *inodep, struct file *filep)
 	}
 	result = 0;
 
-	dev_priv = device->ftbl->device_private_create();
+	dev_priv = kzalloc(sizeof(struct kgsl_device_private), GFP_KERNEL);
 	if (dev_priv == NULL) {
 		result = -ENOMEM;
 		goto err;
@@ -1250,8 +1200,7 @@ kgsl_sharedmem_find(struct kgsl_process_private *private, uint64_t gpuaddr)
 	spin_lock(&private->mem_lock);
 	idr_for_each_entry(&private->mem_idr, entry, id) {
 		if (GPUADDR_IN_MEMDESC(gpuaddr, &entry->memdesc)) {
-			if (!entry->pending_free)
-				ret = kgsl_mem_entry_get(entry);
+			ret = kgsl_mem_entry_get(entry);
 			break;
 		}
 	}
@@ -1261,24 +1210,6 @@ kgsl_sharedmem_find(struct kgsl_process_private *private, uint64_t gpuaddr)
 }
 EXPORT_SYMBOL(kgsl_sharedmem_find);
 
-struct kgsl_mem_entry * __must_check
-kgsl_sharedmem_find_id_flags(struct kgsl_process_private *process,
-		unsigned int id, uint64_t flags)
-{
-	int count = 0;
-	struct kgsl_mem_entry *entry;
-
-	spin_lock(&process->mem_lock);
-	entry = idr_find(&process->mem_idr, id);
-	if (entry)
-		if (!entry->pending_free &&
-				(flags & entry->memdesc.flags) == flags)
-			count = kgsl_mem_entry_get(entry);
-	spin_unlock(&process->mem_lock);
-
-	return (count == 0) ? NULL : entry;
-}
-
 /**
  * kgsl_sharedmem_find_id() - find a memory entry by id
  * @process: the owning process
@@ -1292,7 +1223,19 @@ kgsl_sharedmem_find_id_flags(struct kgsl_process_private *process,
 struct kgsl_mem_entry * __must_check
 kgsl_sharedmem_find_id(struct kgsl_process_private *process, unsigned int id)
 {
-	return kgsl_sharedmem_find_id_flags(process, id, 0);
+	int result;
+	struct kgsl_mem_entry *entry;
+
+	drain_workqueue(kgsl_driver.mem_workqueue);
+
+	spin_lock(&process->mem_lock);
+	entry = idr_find(&process->mem_idr, id);
+	result = kgsl_mem_entry_get(entry);
+	spin_unlock(&process->mem_lock);
+
+	if (result == 0)
+		return NULL;
+	return entry;
 }
 
 /**
@@ -1345,7 +1288,6 @@ long kgsl_ioctl_device_getproperty(struct kgsl_device_private *dev_priv,
 	case KGSL_PROP_VERSION:
 	{
 		struct kgsl_version version;
-
 		if (param->sizebytes != sizeof(version)) {
 			result = -EINVAL;
 			break;
@@ -1397,45 +1339,6 @@ long kgsl_ioctl_device_getproperty(struct kgsl_device_private *dev_priv,
 		kgsl_context_put(context);
 		break;
 	}
-	case KGSL_PROP_SECURE_BUFFER_ALIGNMENT:
-	{
-		unsigned int align;
-
-		if (param->sizebytes != sizeof(unsigned int)) {
-			result = -EINVAL;
-			break;
-		}
-		/*
-		 * XPUv2 impose the constraint of 1MB memory alignment,
-		 * on the other hand Hypervisor does not have such
-		 * constraints. So driver should fulfill such
-		 * requirements when allocating secure memory.
-		 */
-		align = MMU_FEATURE(&dev_priv->device->mmu,
-				KGSL_MMU_HYP_SECURE_ALLOC) ? PAGE_SIZE : SZ_1M;
-
-		if (copy_to_user(param->value, &align, sizeof(align)))
-			result = -EFAULT;
-
-		break;
-	}
-	case KGSL_PROP_SECURE_CTXT_SUPPORT:
-	{
-		unsigned int secure_ctxt;
-
-		if (param->sizebytes != sizeof(unsigned int)) {
-			result = -EINVAL;
-			break;
-		}
-
-		secure_ctxt = dev_priv->device->mmu.secured ? 1 : 0;
-
-		if (copy_to_user(param->value, &secure_ctxt,
-				sizeof(secure_ctxt)))
-			result = -EFAULT;
-
-		break;
-	}
 	default:
 		if (is_compat_task())
 			result = dev_priv->device->ftbl->getproperty_compat(
@@ -1503,34 +1406,17 @@ long kgsl_ioctl_device_waittimestamp_ctxtid(
 	return result;
 }
 
-static inline bool _check_context_is_sparse(struct kgsl_context *context,
-			uint64_t flags)
-{
-	if ((context->flags & KGSL_CONTEXT_SPARSE) ||
-		(flags & KGSL_DRAWOBJ_SPARSE))
-		return true;
-
-	return false;
-}
-
-
 long kgsl_ioctl_rb_issueibcmds(struct kgsl_device_private *dev_priv,
 				      unsigned int cmd, void *data)
 {
 	struct kgsl_ringbuffer_issueibcmds *param = data;
 	struct kgsl_device *device = dev_priv->device;
 	struct kgsl_context *context;
-	struct kgsl_drawobj *drawobj;
-	struct kgsl_drawobj_cmd *cmdobj;
+	struct kgsl_cmdbatch *cmdbatch = NULL;
 	long result = -EINVAL;
 
 	/* The legacy functions don't support synchronization commands */
-	if ((param->flags & (KGSL_DRAWOBJ_SYNC | KGSL_DRAWOBJ_MARKER)))
-		return -EINVAL;
-
-	/* Sanity check the number of IBs */
-	if (param->flags & KGSL_DRAWOBJ_SUBMIT_IB_LIST &&
-			(param->numibs == 0 || param->numibs > KGSL_MAX_NUMIBS))
+	if ((param->flags & (KGSL_CMDBATCH_SYNC | KGSL_CMDBATCH_MARKER)))
 		return -EINVAL;
 
 	/* Get the context */
@@ -1538,25 +1424,23 @@ long kgsl_ioctl_rb_issueibcmds(struct kgsl_device_private *dev_priv,
 	if (context == NULL)
 		return -EINVAL;
 
-	if (_check_context_is_sparse(context, param->flags)) {
-		kgsl_context_put(context);
-		return -EINVAL;
-	}
-
-	cmdobj = kgsl_drawobj_cmd_create(device, context, param->flags,
-					CMDOBJ_TYPE);
-	if (IS_ERR(cmdobj)) {
-		kgsl_context_put(context);
-		return PTR_ERR(cmdobj);
+	/* Create a command batch */
+	cmdbatch = kgsl_cmdbatch_create(device, context, param->flags);
+	if (IS_ERR(cmdbatch)) {
+		result = PTR_ERR(cmdbatch);
+		goto done;
 	}
 
-	drawobj = DRAWOBJ(cmdobj);
-
-	if (param->flags & KGSL_DRAWOBJ_SUBMIT_IB_LIST)
-		result = kgsl_drawobj_cmd_add_ibdesc_list(device, cmdobj,
+	if (param->flags & KGSL_CMDBATCH_SUBMIT_IB_LIST) {
+		/* Sanity check the number of IBs */
+		if (param->numibs == 0 || param->numibs > KGSL_MAX_NUMIBS) {
+			result = -EINVAL;
+			goto done;
+		}
+		result = kgsl_cmdbatch_add_ibdesc_list(device, cmdbatch,
 			(void __user *) param->ibdesc_addr,
 			param->numibs);
-	else {
+	} else {
 		struct kgsl_ibdesc ibdesc;
 		/* Ultra legacy path */
 
@@ -1564,125 +1448,83 @@ long kgsl_ioctl_rb_issueibcmds(struct kgsl_device_private *dev_priv,
 		ibdesc.sizedwords = param->numibs;
 		ibdesc.ctrl = 0;
 
-		result = kgsl_drawobj_cmd_add_ibdesc(device, cmdobj, &ibdesc);
+		result = kgsl_cmdbatch_add_ibdesc(device, cmdbatch, &ibdesc);
 	}
 
-	if (result == 0)
-		result = dev_priv->device->ftbl->queue_cmds(dev_priv, context,
-				&drawobj, 1, &param->timestamp);
+	if (result)
+		goto done;
+
+	result = dev_priv->device->ftbl->issueibcmds(dev_priv, context,
+		cmdbatch, &param->timestamp);
 
+done:
 	/*
 	 * -EPROTO is a "success" error - it just tells the user that the
 	 * context had previously faulted
 	 */
 	if (result && result != -EPROTO)
-		kgsl_drawobj_destroy(drawobj);
+		kgsl_cmdbatch_destroy(cmdbatch);
 
 	kgsl_context_put(context);
 	return result;
 }
 
-/* Returns 0 on failure.  Returns command type(s) on success */
-static unsigned int _process_command_input(struct kgsl_device *device,
-		unsigned int flags, unsigned int numcmds,
-		unsigned int numobjs, unsigned int numsyncs)
-{
-	if (numcmds > KGSL_MAX_NUMIBS ||
-			numobjs > KGSL_MAX_NUMIBS ||
-			numsyncs > KGSL_MAX_SYNCPOINTS)
-		return 0;
-
-	/*
-	 * The SYNC bit is supposed to identify a dummy sync object
-	 * so warn the user if they specified any IBs with it.
-	 * A MARKER command can either have IBs or not but if the
-	 * command has 0 IBs it is automatically assumed to be a marker.
-	 */
-
-	/* If they specify the flag, go with what they say */
-	if (flags & KGSL_DRAWOBJ_MARKER)
-		return MARKEROBJ_TYPE;
-	else if (flags & KGSL_DRAWOBJ_SYNC)
-		return SYNCOBJ_TYPE;
-
-	/* If not, deduce what they meant */
-	if (numsyncs && numcmds)
-		return SYNCOBJ_TYPE | CMDOBJ_TYPE;
-	else if (numsyncs)
-		return SYNCOBJ_TYPE;
-	else if (numcmds)
-		return CMDOBJ_TYPE;
-	else if (numcmds == 0)
-		return MARKEROBJ_TYPE;
-
-	return 0;
-}
-
 long kgsl_ioctl_submit_commands(struct kgsl_device_private *dev_priv,
 				      unsigned int cmd, void *data)
 {
 	struct kgsl_submit_commands *param = data;
 	struct kgsl_device *device = dev_priv->device;
 	struct kgsl_context *context;
-	struct kgsl_drawobj *drawobj[2];
-	unsigned int type;
-	long result;
-	unsigned int i = 0;
+	struct kgsl_cmdbatch *cmdbatch = NULL;
+	long result = -EINVAL;
 
-	type = _process_command_input(device, param->flags, param->numcmds, 0,
-			param->numsyncs);
-	if (!type)
-		return -EINVAL;
+	/*
+	 * The SYNC bit is supposed to identify a dummy sync object so warn the
+	 * user if they specified any IBs with it.  A MARKER command can either
+	 * have IBs or not but if the command has 0 IBs it is automatically
+	 * assumed to be a marker.  If none of the above make sure that the user
+	 * specified a sane number of IBs
+	 */
 
-	context = kgsl_context_get_owner(dev_priv, param->context_id);
-	if (context == NULL)
+	if ((param->flags & KGSL_CMDBATCH_SYNC) && param->numcmds)
+		KGSL_DEV_ERR_ONCE(device,
+			"Commands specified with the SYNC flag.  They will be ignored\n");
+	else if (param->numcmds > KGSL_MAX_NUMIBS)
 		return -EINVAL;
+	else if (!(param->flags & KGSL_CMDBATCH_SYNC) && param->numcmds == 0)
+		param->flags |= KGSL_CMDBATCH_MARKER;
 
-	if (_check_context_is_sparse(context, param->flags)) {
-		kgsl_context_put(context);
+	/* Make sure that we don't have too many syncpoints */
+	if (param->numsyncs > KGSL_MAX_SYNCPOINTS)
 		return -EINVAL;
-	}
 
-	if (type & SYNCOBJ_TYPE) {
-		struct kgsl_drawobj_sync *syncobj =
-				kgsl_drawobj_sync_create(device, context);
-		if (IS_ERR(syncobj)) {
-			result = PTR_ERR(syncobj);
-			goto done;
-		}
-
-		drawobj[i++] = DRAWOBJ(syncobj);
+	context = kgsl_context_get_owner(dev_priv, param->context_id);
+	if (context == NULL)
+		return -EINVAL;
 
-		result = kgsl_drawobj_sync_add_syncpoints(device, syncobj,
-				param->synclist, param->numsyncs);
-		if (result)
-			goto done;
+	/* Create a command batch */
+	cmdbatch = kgsl_cmdbatch_create(device, context, param->flags);
+	if (IS_ERR(cmdbatch)) {
+		result = PTR_ERR(cmdbatch);
+		goto done;
 	}
 
-	if (type & (CMDOBJ_TYPE | MARKEROBJ_TYPE)) {
-		struct kgsl_drawobj_cmd *cmdobj =
-				kgsl_drawobj_cmd_create(device,
-					context, param->flags, type);
-		if (IS_ERR(cmdobj)) {
-			result = PTR_ERR(cmdobj);
-			goto done;
-		}
-
-		drawobj[i++] = DRAWOBJ(cmdobj);
+	result = kgsl_cmdbatch_add_ibdesc_list(device, cmdbatch,
+		param->cmdlist, param->numcmds);
+	if (result)
+		goto done;
 
-		result = kgsl_drawobj_cmd_add_ibdesc_list(device, cmdobj,
-				param->cmdlist, param->numcmds);
-		if (result)
-			goto done;
+	result = kgsl_cmdbatch_add_syncpoints(device, cmdbatch,
+		param->synclist, param->numsyncs);
+	if (result)
+		goto done;
 
-		/* If no profiling buffer was specified, clear the flag */
-		if (cmdobj->profiling_buf_entry == NULL)
-			DRAWOBJ(cmdobj)->flags &=
-				~(unsigned long)KGSL_DRAWOBJ_PROFILING;
-	}
+	/* If no profiling buffer was specified, clear the flag */
+	if (cmdbatch->profiling_buf_entry == NULL)
+		cmdbatch->flags &= ~KGSL_CMDBATCH_PROFILING;
 
-	result = device->ftbl->queue_cmds(dev_priv, context, drawobj,
-			i, &param->timestamp);
+	result = dev_priv->device->ftbl->issueibcmds(dev_priv, context,
+		cmdbatch, &param->timestamp);
 
 done:
 	/*
@@ -1690,9 +1532,7 @@ long kgsl_ioctl_submit_commands(struct kgsl_device_private *dev_priv,
 	 * context had previously faulted
 	 */
 	if (result && result != -EPROTO)
-		while (i--)
-			kgsl_drawobj_destroy(drawobj[i]);
-
+		kgsl_cmdbatch_destroy(cmdbatch);
 
 	kgsl_context_put(context);
 	return result;
@@ -1704,75 +1544,63 @@ long kgsl_ioctl_gpu_command(struct kgsl_device_private *dev_priv,
 	struct kgsl_gpu_command *param = data;
 	struct kgsl_device *device = dev_priv->device;
 	struct kgsl_context *context;
-	struct kgsl_drawobj *drawobj[2];
-	unsigned int type;
-	long result;
-	unsigned int i = 0;
+	struct kgsl_cmdbatch *cmdbatch = NULL;
+
+	long result = -EINVAL;
 
-	type = _process_command_input(device, param->flags, param->numcmds,
-			param->numobjs, param->numsyncs);
-	if (!type)
+	/*
+	 * The SYNC bit is supposed to identify a dummy sync object so warn the
+	 * user if they specified any IBs with it.  A MARKER command can either
+	 * have IBs or not but if the command has 0 IBs it is automatically
+	 * assumed to be a marker.  If none of the above make sure that the user
+	 * specified a sane number of IBs
+	 */
+	if ((param->flags & KGSL_CMDBATCH_SYNC) && param->numcmds)
+		KGSL_DEV_ERR_ONCE(device,
+			"Commands specified with the SYNC flag.  They will be ignored\n");
+	else if (!(param->flags & KGSL_CMDBATCH_SYNC) && param->numcmds == 0)
+		param->flags |= KGSL_CMDBATCH_MARKER;
+
+	/* Make sure that the memobj and syncpoint count isn't too big */
+	if (param->numcmds > KGSL_MAX_NUMIBS ||
+		param->numobjs > KGSL_MAX_NUMIBS ||
+		param->numsyncs > KGSL_MAX_SYNCPOINTS)
 		return -EINVAL;
 
 	context = kgsl_context_get_owner(dev_priv, param->context_id);
 	if (context == NULL)
 		return -EINVAL;
 
-	if (_check_context_is_sparse(context, param->flags)) {
-		kgsl_context_put(context);
-		return -EINVAL;
-	}
-
-	if (type & SYNCOBJ_TYPE) {
-		struct kgsl_drawobj_sync *syncobj =
-				kgsl_drawobj_sync_create(device, context);
-
-		if (IS_ERR(syncobj)) {
-			result = PTR_ERR(syncobj);
-			goto done;
-		}
-
-		drawobj[i++] = DRAWOBJ(syncobj);
-
-		result = kgsl_drawobj_sync_add_synclist(device, syncobj,
-				to_user_ptr(param->synclist),
-				param->syncsize, param->numsyncs);
-		if (result)
-			goto done;
+	cmdbatch = kgsl_cmdbatch_create(device, context, param->flags);
+	if (IS_ERR(cmdbatch)) {
+		result = PTR_ERR(cmdbatch);
+		goto done;
 	}
 
-	if (type & (CMDOBJ_TYPE | MARKEROBJ_TYPE)) {
-		struct kgsl_drawobj_cmd *cmdobj =
-				kgsl_drawobj_cmd_create(device,
-					context, param->flags, type);
-
-		if (IS_ERR(cmdobj)) {
-			result = PTR_ERR(cmdobj);
-			goto done;
-		}
-
-		drawobj[i++] = DRAWOBJ(cmdobj);
+	result = kgsl_cmdbatch_add_cmdlist(device, cmdbatch,
+		to_user_ptr(param->cmdlist),
+		param->cmdsize, param->numcmds);
+	if (result)
+		goto done;
 
-		result = kgsl_drawobj_cmd_add_cmdlist(device, cmdobj,
-			to_user_ptr(param->cmdlist),
-			param->cmdsize, param->numcmds);
-		if (result)
-			goto done;
+	result = kgsl_cmdbatch_add_memlist(device, cmdbatch,
+		to_user_ptr(param->objlist),
+		param->objsize, param->numobjs);
+	if (result)
+		goto done;
 
-		result = kgsl_drawobj_cmd_add_memlist(device, cmdobj,
-			to_user_ptr(param->objlist),
-			param->objsize, param->numobjs);
-		if (result)
-			goto done;
+	result = kgsl_cmdbatch_add_synclist(device, cmdbatch,
+		to_user_ptr(param->synclist),
+		param->syncsize, param->numsyncs);
+	if (result)
+		goto done;
 
-		/* If no profiling buffer was specified, clear the flag */
-		if (cmdobj->profiling_buf_entry == NULL)
-			DRAWOBJ(cmdobj)->flags &=
-				~(unsigned long)KGSL_DRAWOBJ_PROFILING;
-	}
+	/* If no profiling buffer was specified, clear the flag */
+	if (cmdbatch->profiling_buf_entry == NULL)
+		cmdbatch->flags &= ~KGSL_CMDBATCH_PROFILING;
 
-	result = device->ftbl->queue_cmds(dev_priv, context, drawobj,
-				i, &param->timestamp);
+	result = dev_priv->device->ftbl->issueibcmds(dev_priv, context,
+		cmdbatch, &param->timestamp);
 
 done:
 	/*
@@ -1780,8 +1608,7 @@ long kgsl_ioctl_gpu_command(struct kgsl_device_private *dev_priv,
 	 * context had previously faulted
 	 */
 	if (result && result != -EPROTO)
-		while (i--)
-			kgsl_drawobj_destroy(drawobj[i]);
+		kgsl_cmdbatch_destroy(cmdbatch);
 
 	kgsl_context_put(context);
 	return result;
@@ -1853,17 +1680,20 @@ long kgsl_ioctl_drawctxt_destroy(struct kgsl_device_private *dev_priv,
 	return 0;
 }
 
-long gpumem_free_entry(struct kgsl_mem_entry *entry)
+static long gpumem_free_entry(struct kgsl_mem_entry *entry)
 {
+	pid_t ptname = 0;
+
 	if (!kgsl_mem_entry_set_pend(entry))
 		return -EBUSY;
 
 	trace_kgsl_mem_free(entry);
-	kgsl_memfree_add(entry->priv->pid,
-			entry->memdesc.pagetable ?
-				entry->memdesc.pagetable->name : 0,
-			entry->memdesc.gpuaddr, entry->memdesc.size,
-			entry->memdesc.flags);
+
+	if (entry->memdesc.pagetable != NULL)
+		ptname = entry->memdesc.pagetable->name;
+
+	kgsl_memfree_add(entry->priv->pid, ptname, entry->memdesc.gpuaddr,
+		entry->memdesc.size, entry->memdesc.flags);
 
 	kgsl_mem_entry_put(entry);
 
@@ -1882,12 +1712,6 @@ static void gpumem_free_func(struct kgsl_device *device,
 	/* Free the memory for all event types */
 	trace_kgsl_mem_timestamp_free(device, entry, KGSL_CONTEXT_ID(context),
 		timestamp, 0);
-	kgsl_memfree_add(entry->priv->pid,
-			entry->memdesc.pagetable ?
-				entry->memdesc.pagetable->name : 0,
-			entry->memdesc.gpuaddr, entry->memdesc.size,
-			entry->memdesc.flags);
-
 	kgsl_mem_entry_put(entry);
 }
 
@@ -1922,8 +1746,12 @@ long kgsl_ioctl_sharedmem_free(struct kgsl_device_private *dev_priv,
 	long ret;
 
 	entry = kgsl_sharedmem_find(private, (uint64_t) param->gpuaddr);
-	if (entry == NULL)
+	if (entry == NULL) {
+		KGSL_MEM_INFO(dev_priv->device,
+			"Invalid GPU address 0x%016llx\n",
+			(uint64_t) param->gpuaddr);
 		return -EINVAL;
+	}
 
 	ret = gpumem_free_entry(entry);
 	kgsl_mem_entry_put(entry);
@@ -1940,8 +1768,11 @@ long kgsl_ioctl_gpumem_free_id(struct kgsl_device_private *dev_priv,
 	long ret;
 
 	entry = kgsl_sharedmem_find_id(private, param->id);
-	if (entry == NULL)
+	if (entry == NULL) {
+		KGSL_MEM_INFO(dev_priv->device,
+			"Invalid GPU memory object ID %d\n", param->id);
 		return -EINVAL;
+	}
 
 	ret = gpumem_free_entry(entry);
 	kgsl_mem_entry_put(entry);
@@ -1977,26 +1808,15 @@ static long gpuobj_free_on_timestamp(struct kgsl_device_private *dev_priv,
 	return ret;
 }
 
-static bool gpuobj_free_fence_func(void *priv)
+static void gpuobj_free_fence_func(void *priv)
 {
-	struct kgsl_mem_entry *entry = priv;
-
-	trace_kgsl_mem_free(entry);
-	kgsl_memfree_add(entry->priv->pid,
-			entry->memdesc.pagetable ?
-				entry->memdesc.pagetable->name : 0,
-			entry->memdesc.gpuaddr, entry->memdesc.size,
-			entry->memdesc.flags);
-
-	INIT_WORK(&entry->work, _deferred_put);
-	queue_work(kgsl_driver.mem_workqueue, &entry->work);
-	return true;
+	kgsl_mem_entry_put_deferred((struct kgsl_mem_entry *) priv);
 }
 
 static long gpuobj_free_on_fence(struct kgsl_device_private *dev_priv,
 		struct kgsl_mem_entry *entry, struct kgsl_gpuobj_free *param)
 {
-	struct kgsl_sync_fence_cb *handle;
+	struct kgsl_sync_fence_waiter *handle;
 	struct kgsl_gpu_event_fence event;
 	long ret;
 
@@ -2018,17 +1838,17 @@ static long gpuobj_free_on_fence(struct kgsl_device_private *dev_priv,
 	}
 
 	handle = kgsl_sync_fence_async_wait(event.fd,
-		gpuobj_free_fence_func, entry, NULL, 0);
+		gpuobj_free_fence_func, entry);
+
+	/* if handle is NULL the fence has already signaled */
+	if (handle == NULL)
+		return gpumem_free_entry(entry);
 
 	if (IS_ERR(handle)) {
 		kgsl_mem_entry_unset_pend(entry);
 		return PTR_ERR(handle);
 	}
 
-	/* if handle is NULL the fence has already signaled */
-	if (handle == NULL)
-		gpuobj_free_fence_func(entry);
-
 	return 0;
 }
 
@@ -2041,8 +1861,11 @@ long kgsl_ioctl_gpuobj_free(struct kgsl_device_private *dev_priv,
 	long ret;
 
 	entry = kgsl_sharedmem_find_id(private, param->id);
-	if (entry == NULL)
+	if (entry == NULL) {
+		KGSL_MEM_ERR(dev_priv->device,
+			"Invalid GPU memory object ID %d\n", param->id);
 		return -EINVAL;
+	}
 
 	/* If no event is specified then free immediately */
 	if (!(param->flags & KGSL_GPUOBJ_FREE_ON_EVENT))
@@ -2077,14 +1900,17 @@ long kgsl_ioctl_cmdstream_freememontimestamp_ctxtid(
 	entry = kgsl_sharedmem_find(dev_priv->process_priv,
 		(uint64_t) param->gpuaddr);
 	if (entry == NULL) {
-		kgsl_context_put(context);
-		return -EINVAL;
+		KGSL_MEM_ERR(dev_priv->device,
+			"Invalid GPU address 0x%016llx\n",
+			(uint64_t) param->gpuaddr);
+		goto out;
 	}
 
 	ret = gpumem_free_entry_on_timestamp(dev_priv->device, entry,
 		context, param->timestamp);
 
 	kgsl_mem_entry_put(entry);
+out:
 	kgsl_context_put(context);
 
 	return ret;
@@ -2094,7 +1920,6 @@ static inline int _check_region(unsigned long start, unsigned long size,
 				uint64_t len)
 {
 	uint64_t end = ((uint64_t) start) + size;
-
 	return (end > len);
 }
 
@@ -2104,7 +1929,7 @@ static int check_vma_flags(struct vm_area_struct *vma,
 	unsigned long flags_requested = (VM_READ | VM_WRITE);
 
 	if (flags & KGSL_MEMFLAGS_GPUREADONLY)
-		flags_requested &= ~(unsigned long)VM_WRITE;
+		flags_requested &= ~VM_WRITE;
 
 	if ((vma->vm_flags & flags_requested) == flags_requested)
 		return 0;
@@ -2134,8 +1959,7 @@ static int memdesc_sg_virt(struct kgsl_memdesc *memdesc, struct file *vmfile)
 	long npages = 0, i;
 	size_t sglen = (size_t) (memdesc->size / PAGE_SIZE);
 	struct page **pages = NULL;
-	int write = ((memdesc->flags & KGSL_MEMFLAGS_GPUREADONLY) ? 0 :
-								FOLL_WRITE);
+	int write = ((memdesc->flags & KGSL_MEMFLAGS_GPUREADONLY) ? 0 : 1);
 
 	if (sglen == 0 || sglen >= LONG_MAX)
 		return -EINVAL;
@@ -2198,7 +2022,7 @@ static int kgsl_setup_anon_useraddr(struct kgsl_pagetable *pagetable,
 	entry->memdesc.pagetable = pagetable;
 	entry->memdesc.size = (uint64_t) size;
 	entry->memdesc.useraddr = hostptr;
-	entry->memdesc.flags |= (uint64_t)KGSL_MEMFLAGS_USERMEM_ADDR;
+	entry->memdesc.flags |= KGSL_MEMFLAGS_USERMEM_ADDR;
 
 	if (kgsl_memdesc_use_cpu_map(&entry->memdesc)) {
 		int ret;
@@ -2216,7 +2040,6 @@ static int kgsl_setup_anon_useraddr(struct kgsl_pagetable *pagetable,
 	return memdesc_sg_virt(&entry->memdesc, NULL);
 }
 
-#ifdef CONFIG_DMA_SHARED_BUFFER
 static int match_file(const void *p, struct file *file, unsigned int fd)
 {
 	/*
@@ -2229,12 +2052,12 @@ static int match_file(const void *p, struct file *file, unsigned int fd)
 static void _setup_cache_mode(struct kgsl_mem_entry *entry,
 		struct vm_area_struct *vma)
 {
-	uint64_t mode;
+	unsigned int mode;
 	pgprot_t pgprot = vma->vm_page_prot;
 
-	if (pgprot_val(pgprot) == pgprot_val(pgprot_noncached(pgprot)))
+	if (pgprot == pgprot_noncached(pgprot))
 		mode = KGSL_CACHEMODE_UNCACHED;
-	else if (pgprot_val(pgprot) == pgprot_val(pgprot_writecombine(pgprot)))
+	else if (pgprot == pgprot_writecombine(pgprot))
 		mode = KGSL_CACHEMODE_WRITECOMBINE;
 	else
 		mode = KGSL_CACHEMODE_WRITEBACK;
@@ -2242,6 +2065,7 @@ static void _setup_cache_mode(struct kgsl_mem_entry *entry,
 	entry->memdesc.flags |= (mode << KGSL_CACHEMODE_SHIFT);
 }
 
+#ifdef CONFIG_DMA_SHARED_BUFFER
 static int kgsl_setup_dma_buf(struct kgsl_device *device,
 				struct kgsl_pagetable *pagetable,
 				struct kgsl_mem_entry *entry,
@@ -2344,8 +2168,7 @@ static long _gpuobj_map_useraddr(struct kgsl_device *device,
 	param->flags &= KGSL_MEMFLAGS_GPUREADONLY
 		| KGSL_CACHEMODE_MASK
 		| KGSL_MEMTYPE_MASK
-		| KGSL_MEMFLAGS_FORCE_32BIT
-		| KGSL_MEMFLAGS_IOCOHERENT;
+		| KGSL_MEMFLAGS_FORCE_32BIT;
 
 	/* Specifying SECURE is an explicit error */
 	if (param->flags & KGSL_MEMFLAGS_SECURE)
@@ -2362,7 +2185,7 @@ static long _gpuobj_map_useraddr(struct kgsl_device *device,
 		return -EINVAL;
 
 	return kgsl_setup_useraddr(device, pagetable, entry,
-		(unsigned long) useraddr.virtaddr, 0, param->priv_len);
+		(unsigned long) useraddr.virtaddr, 0, 0);
 }
 
 #ifdef CONFIG_DMA_SHARED_BUFFER
@@ -2395,7 +2218,7 @@ static long _gpuobj_map_dma_buf(struct kgsl_device *device,
 	if (ret)
 		return ret;
 
-	if (buf.fd < 0)
+	if (buf.fd == 0)
 		return -EINVAL;
 
 	*fd = buf.fd;
@@ -2428,6 +2251,7 @@ long kgsl_ioctl_gpuobj_import(struct kgsl_device_private *dev_priv,
 	struct kgsl_gpuobj_import *param = data;
 	struct kgsl_mem_entry *entry;
 	int ret, fd = -1;
+	struct kgsl_mmu *mmu = &dev_priv->device->mmu;
 
 	entry = kgsl_mem_entry_create();
 	if (entry == NULL)
@@ -2438,13 +2262,13 @@ long kgsl_ioctl_gpuobj_import(struct kgsl_device_private *dev_priv,
 			| KGSL_MEMALIGN_MASK
 			| KGSL_MEMFLAGS_USE_CPU_MAP
 			| KGSL_MEMFLAGS_SECURE
-			| KGSL_MEMFLAGS_FORCE_32BIT
-			| KGSL_MEMFLAGS_IOCOHERENT;
+			| KGSL_MEMFLAGS_FORCE_32BIT;
 
-	if (kgsl_is_compat_task())
-		param->flags |= KGSL_MEMFLAGS_FORCE_32BIT;
+	entry->memdesc.flags = param->flags;
+
+	if (MMU_FEATURE(mmu, KGSL_MMU_NEED_GUARD_PAGE))
+		entry->memdesc.priv |= KGSL_MEMDESC_GUARD_PAGE;
 
-	kgsl_memdesc_init(dev_priv->device, &entry->memdesc, param->flags);
 	if (param->type == KGSL_USER_MEM_TYPE_ADDR)
 		ret = _gpuobj_map_useraddr(dev_priv->device, private->pagetable,
 			entry, param);
@@ -2481,9 +2305,8 @@ long kgsl_ioctl_gpuobj_import(struct kgsl_device_private *dev_priv,
 
 	kgsl_mem_entry_commit_process(entry);
 
-	/* Put the extra ref from kgsl_mem_entry_create() */
+	/* put the extra refcount for kgsl_mem_entry_create() */
 	kgsl_mem_entry_put(entry);
-
 	return 0;
 
 unmap:
@@ -2583,14 +2406,12 @@ static int kgsl_setup_dma_buf(struct kgsl_device *device,
 	meta->dmabuf = dmabuf;
 	meta->attach = attach;
 
-	attach->priv = entry;
-
 	entry->priv_data = meta;
 	entry->memdesc.pagetable = pagetable;
 	entry->memdesc.size = 0;
 	/* USE_CPU_MAP is not impemented for ION. */
 	entry->memdesc.flags &= ~((uint64_t) KGSL_MEMFLAGS_USE_CPU_MAP);
-	entry->memdesc.flags |= (uint64_t)KGSL_MEMFLAGS_USERMEM_ION;
+	entry->memdesc.flags |= KGSL_MEMFLAGS_USERMEM_ION;
 
 	sg_table = dma_buf_map_attachment(attach, DMA_TO_DEVICE);
 
@@ -2635,45 +2456,6 @@ static int kgsl_setup_dma_buf(struct kgsl_device *device,
 }
 #endif
 
-#ifdef CONFIG_DMA_SHARED_BUFFER
-void kgsl_get_egl_counts(struct kgsl_mem_entry *entry,
-		int *egl_surface_count, int *egl_image_count)
-{
-	struct kgsl_dma_buf_meta *meta = entry->priv_data;
-	struct dma_buf *dmabuf = meta->dmabuf;
-	struct dma_buf_attachment *mem_entry_buf_attachment = meta->attach;
-	struct device *buf_attachment_dev = mem_entry_buf_attachment->dev;
-	struct dma_buf_attachment *attachment = NULL;
-
-	mutex_lock(&dmabuf->lock);
-	list_for_each_entry(attachment, &dmabuf->attachments, node) {
-		struct kgsl_mem_entry *scan_mem_entry = NULL;
-
-		if (attachment->dev != buf_attachment_dev)
-			continue;
-
-		scan_mem_entry = attachment->priv;
-		if (!scan_mem_entry)
-			continue;
-
-		switch (kgsl_memdesc_get_memtype(&scan_mem_entry->memdesc)) {
-		case KGSL_MEMTYPE_EGL_SURFACE:
-			(*egl_surface_count)++;
-			break;
-		case KGSL_MEMTYPE_EGL_IMAGE:
-			(*egl_image_count)++;
-			break;
-		}
-	}
-	mutex_unlock(&dmabuf->lock);
-}
-#else
-void kgsl_get_egl_counts(struct kgsl_mem_entry *entry,
-		int *egl_surface_count, int *egl_image_count)
-{
-}
-#endif
-
 long kgsl_ioctl_map_user_mem(struct kgsl_device_private *dev_priv,
 				     unsigned int cmd, void *data)
 {
@@ -2683,7 +2465,6 @@ long kgsl_ioctl_map_user_mem(struct kgsl_device_private *dev_priv,
 	struct kgsl_process_private *private = dev_priv->process_priv;
 	struct kgsl_mmu *mmu = &dev_priv->device->mmu;
 	unsigned int memtype;
-	uint64_t flags;
 
 	/*
 	 * If content protection is not enabled and secure buffer
@@ -2720,17 +2501,22 @@ long kgsl_ioctl_map_user_mem(struct kgsl_device_private *dev_priv,
 	 * Note: CACHEMODE is ignored for this call. Caching should be
 	 * determined by type of allocation being mapped.
 	 */
-	flags = param->flags & (KGSL_MEMFLAGS_GPUREADONLY
-				| KGSL_MEMTYPE_MASK
-				| KGSL_MEMALIGN_MASK
-				| KGSL_MEMFLAGS_USE_CPU_MAP
-				| KGSL_MEMFLAGS_SECURE
-				| KGSL_MEMFLAGS_IOCOHERENT);
+	param->flags &= KGSL_MEMFLAGS_GPUREADONLY
+			| KGSL_MEMTYPE_MASK
+			| KGSL_MEMALIGN_MASK
+			| KGSL_MEMFLAGS_USE_CPU_MAP
+			| KGSL_MEMFLAGS_SECURE;
+	entry->memdesc.flags = ((uint64_t) param->flags)
+		| KGSL_MEMFLAGS_FORCE_32BIT;
 
-	if (kgsl_is_compat_task())
-		flags |= KGSL_MEMFLAGS_FORCE_32BIT;
+	if (!kgsl_mmu_use_cpu_map(mmu))
+		entry->memdesc.flags &= ~((uint64_t) KGSL_MEMFLAGS_USE_CPU_MAP);
 
-	kgsl_memdesc_init(dev_priv->device, &entry->memdesc, flags);
+	if (MMU_FEATURE(mmu, KGSL_MMU_NEED_GUARD_PAGE))
+		entry->memdesc.priv |= KGSL_MEMDESC_GUARD_PAGE;
+
+	if (param->flags & KGSL_MEMFLAGS_SECURE)
+		entry->memdesc.priv |= KGSL_MEMDESC_SECURE;
 
 	switch (memtype) {
 	case KGSL_MEM_ENTRY_USER:
@@ -2745,6 +2531,7 @@ long kgsl_ioctl_map_user_mem(struct kgsl_device_private *dev_priv,
 				private->pagetable, entry, param->fd);
 		break;
 	default:
+		KGSL_CORE_ERR("Invalid memory type: %x\n", memtype);
 		result = -EOPNOTSUPP;
 		break;
 	}
@@ -2754,6 +2541,10 @@ long kgsl_ioctl_map_user_mem(struct kgsl_device_private *dev_priv,
 
 	if ((param->flags & KGSL_MEMFLAGS_SECURE) &&
 		(entry->memdesc.size & mmu->secure_align_mask)) {
+			KGSL_DRV_ERR(dev_priv->device,
+				"Secure buffer size %lld not aligned to %x alignment",
+				entry->memdesc.size,
+				mmu->secure_align_mask + 1);
 		result = -EINVAL;
 		goto error_attach;
 	}
@@ -2787,9 +2578,8 @@ long kgsl_ioctl_map_user_mem(struct kgsl_device_private *dev_priv,
 
 	kgsl_mem_entry_commit_process(entry);
 
-	/* Put the extra ref from kgsl_mem_entry_create() */
+	/* put the extra refcount for kgsl_mem_entry_create() */
 	kgsl_mem_entry_put(entry);
-
 	return result;
 
 error_attach:
@@ -2817,10 +2607,6 @@ static int _kgsl_gpumem_sync_cache(struct kgsl_mem_entry *entry,
 	int cacheop;
 	int mode;
 
-	 /* Cache ops are not allowed on secure memory */
-	if (entry->memdesc.flags & KGSL_MEMFLAGS_SECURE)
-		return 0;
-
 	/*
 	 * Flush is defined as (clean | invalidate).  If both bits are set, then
 	 * do a flush, otherwise check for the individual bits and clean or inv
@@ -2865,13 +2651,24 @@ long kgsl_ioctl_gpumem_sync_cache(struct kgsl_device_private *dev_priv,
 	struct kgsl_mem_entry *entry = NULL;
 	long ret;
 
-	if (param->id != 0)
+	if (param->id != 0) {
 		entry = kgsl_sharedmem_find_id(private, param->id);
-	else if (param->gpuaddr != 0)
+		if (entry == NULL) {
+			KGSL_MEM_INFO(dev_priv->device, "can't find id %d\n",
+					param->id);
+			return -EINVAL;
+		}
+	} else if (param->gpuaddr != 0) {
 		entry = kgsl_sharedmem_find(private, (uint64_t) param->gpuaddr);
-
-	if (entry == NULL)
+		if (entry == NULL) {
+			KGSL_MEM_INFO(dev_priv->device,
+					"can't find gpuaddr 0x%08lX\n",
+					param->gpuaddr);
+			return -EINVAL;
+		}
+	} else {
 		return -EINVAL;
+	}
 
 	ret = _kgsl_gpumem_sync_cache(entry, (uint64_t) param->offset,
 					(uint64_t) param->length, param->op);
@@ -2882,7 +2679,6 @@ long kgsl_ioctl_gpumem_sync_cache(struct kgsl_device_private *dev_priv,
 static int mem_id_cmp(const void *_a, const void *_b)
 {
 	const unsigned int *a = _a, *b = _b;
-
 	if (*a == *b)
 		return 0;
 	return (*a > *b) ? 1 : -1;
@@ -2899,12 +2695,9 @@ static inline bool check_full_flush(size_t size, int op)
 static inline bool check_full_flush(size_t size, int op)
 {
 	/* If we exceed the breakeven point, flush the entire cache */
-	bool ret = (kgsl_driver.full_cache_threshold != 0) &&
+	return (kgsl_driver.full_cache_threshold != 0) &&
 		(size >= kgsl_driver.full_cache_threshold) &&
 		(op == KGSL_GPUMEM_CACHE_FLUSH);
-	if (ret)
-		flush_cache_all();
-	return ret;
 }
 #endif
 
@@ -2924,11 +2717,11 @@ long kgsl_ioctl_gpumem_sync_cache_bulk(struct kgsl_device_private *dev_priv,
 			|| param->count > (PAGE_SIZE / sizeof(unsigned int)))
 		return -EINVAL;
 
-	id_list = kcalloc(param->count, sizeof(unsigned int), GFP_KERNEL);
+	id_list = kzalloc(param->count * sizeof(unsigned int), GFP_KERNEL);
 	if (id_list == NULL)
 		return -ENOMEM;
 
-	entries = kcalloc(param->count, sizeof(*entries), GFP_KERNEL);
+	entries = kzalloc(param->count * sizeof(*entries), GFP_KERNEL);
 	if (entries == NULL) {
 		ret = -ENOMEM;
 		goto end;
@@ -2967,13 +2760,15 @@ long kgsl_ioctl_gpumem_sync_cache_bulk(struct kgsl_device_private *dev_priv,
 		entries[actual_count++] = entry;
 
 		full_flush  = check_full_flush(op_size, param->op);
-		if (full_flush) {
-			trace_kgsl_mem_sync_full_cache(actual_count, op_size);
+		if (full_flush)
 			break;
-		}
 
 		last_id = id;
 	}
+	if (full_flush) {
+		trace_kgsl_mem_sync_full_cache(actual_count, op_size);
+		flush_cache_all();
+	}
 
 	param->op &= ~KGSL_GPUMEM_CACHE_RANGE;
 
@@ -3001,8 +2796,12 @@ long kgsl_ioctl_sharedmem_flush_cache(struct kgsl_device_private *dev_priv,
 	long ret;
 
 	entry = kgsl_sharedmem_find(private, (uint64_t) param->gpuaddr);
-	if (entry == NULL)
+	if (entry == NULL) {
+		KGSL_MEM_INFO(dev_priv->device,
+				"can't find gpuaddr 0x%08lX\n",
+				param->gpuaddr);
 		return -EINVAL;
+	}
 
 	ret = _kgsl_gpumem_sync_cache(entry, 0, entry->memdesc.size,
 					KGSL_GPUMEM_CACHE_FLUSH);
@@ -3026,11 +2825,11 @@ long kgsl_ioctl_gpuobj_sync(struct kgsl_device_private *dev_priv,
 	if (param->count == 0 || param->count > 128)
 		return -EINVAL;
 
-	objs = kcalloc(param->count, sizeof(*objs), GFP_KERNEL);
+	objs = kzalloc(param->count * sizeof(*objs), GFP_KERNEL);
 	if (objs == NULL)
 		return -ENOMEM;
-
-	entries = kcalloc(param->count, sizeof(*entries), GFP_KERNEL);
+	
+	entries = kzalloc(param->count * sizeof(*entries), GFP_KERNEL);
 	if (entries == NULL) {
 		kfree(objs);
 		return -ENOMEM;
@@ -3091,9 +2890,8 @@ static uint64_t kgsl_filter_cachemode(uint64_t flags)
 	if ((flags & KGSL_CACHEMODE_MASK) >> KGSL_CACHEMODE_SHIFT ==
 					KGSL_CACHEMODE_WRITETHROUGH) {
 		flags &= ~((uint64_t) KGSL_CACHEMODE_MASK);
-		flags |= (uint64_t)((KGSL_CACHEMODE_WRITEBACK <<
-						KGSL_CACHEMODE_SHIFT) &
-					KGSL_CACHEMODE_MASK);
+		flags |= (KGSL_CACHEMODE_WRITEBACK << KGSL_CACHEMODE_SHIFT) &
+							KGSL_CACHEMODE_MASK;
 	}
 	return flags;
 }
@@ -3107,14 +2905,13 @@ static uint64_t kgsl_filter_cachemode(uint64_t flags)
 /* The largest allowable alignment for a GPU object is 32MB */
 #define KGSL_MAX_ALIGN (32 * SZ_1M)
 
-struct kgsl_mem_entry *gpumem_alloc_entry(
+static struct kgsl_mem_entry *gpumem_alloc_entry(
 		struct kgsl_device_private *dev_priv,
 		uint64_t size, uint64_t flags)
 {
 	int ret;
 	struct kgsl_process_private *private = dev_priv->process_priv;
 	struct kgsl_mem_entry *entry;
-	struct kgsl_mmu *mmu = &dev_priv->device->mmu;
 	unsigned int align;
 
 	flags &= KGSL_MEMFLAGS_GPUREADONLY
@@ -3123,17 +2920,24 @@ struct kgsl_mem_entry *gpumem_alloc_entry(
 		| KGSL_MEMALIGN_MASK
 		| KGSL_MEMFLAGS_USE_CPU_MAP
 		| KGSL_MEMFLAGS_SECURE
-		| KGSL_MEMFLAGS_FORCE_32BIT
-		| KGSL_MEMFLAGS_IOCOHERENT;
+		| KGSL_MEMFLAGS_FORCE_32BIT;
+
+	/* Turn off SVM if the system doesn't support it */
+	if (!kgsl_mmu_use_cpu_map(&dev_priv->device->mmu))
+		flags &= ~((uint64_t) KGSL_MEMFLAGS_USE_CPU_MAP);
 
 	/* Return not supported error if secure memory isn't enabled */
-	if (!kgsl_mmu_is_secured(mmu) &&
+	if (!kgsl_mmu_is_secured(&dev_priv->device->mmu) &&
 			(flags & KGSL_MEMFLAGS_SECURE)) {
 		dev_WARN_ONCE(dev_priv->device->dev, 1,
 				"Secure memory not supported");
 		return ERR_PTR(-EOPNOTSUPP);
 	}
 
+	/* Secure memory disables advanced addressing modes */
+	if (flags & KGSL_MEMFLAGS_SECURE)
+		flags &= ~((uint64_t) KGSL_MEMFLAGS_USE_CPU_MAP);
+
 	/* Cap the alignment bits to the highest number we can handle */
 	align = MEMFLAGS(flags, KGSL_MEMALIGN_MASK, KGSL_MEMALIGN_SHIFT);
 	if (align >= ilog2(KGSL_MAX_ALIGN)) {
@@ -3141,9 +2945,8 @@ struct kgsl_mem_entry *gpumem_alloc_entry(
 			KGSL_MAX_ALIGN >> 10);
 
 		flags &= ~((uint64_t) KGSL_MEMALIGN_MASK);
-		flags |= (uint64_t)((ilog2(KGSL_MAX_ALIGN) <<
-						KGSL_MEMALIGN_SHIFT) &
-					KGSL_MEMALIGN_MASK);
+		flags |= (ilog2(KGSL_MAX_ALIGN) << KGSL_MEMALIGN_SHIFT) &
+			KGSL_MEMALIGN_MASK;
 	}
 
 	/* For now only allow allocations up to 4G */
@@ -3156,6 +2959,12 @@ struct kgsl_mem_entry *gpumem_alloc_entry(
 	if (entry == NULL)
 		return ERR_PTR(-ENOMEM);
 
+	if (MMU_FEATURE(&dev_priv->device->mmu, KGSL_MMU_NEED_GUARD_PAGE))
+		entry->memdesc.priv |= KGSL_MEMDESC_GUARD_PAGE;
+
+	if (flags & KGSL_MEMFLAGS_SECURE)
+		entry->memdesc.priv |= KGSL_MEMDESC_SECURE;
+
 	ret = kgsl_allocate_user(dev_priv->device, &entry->memdesc,
 		size, flags);
 	if (ret != 0)
@@ -3207,9 +3016,6 @@ long kgsl_ioctl_gpuobj_alloc(struct kgsl_device_private *dev_priv,
 	struct kgsl_gpuobj_alloc *param = data;
 	struct kgsl_mem_entry *entry;
 
-	if (kgsl_is_compat_task())
-		param->flags |= KGSL_MEMFLAGS_FORCE_32BIT;
-
 	entry = gpumem_alloc_entry(dev_priv, param->size, param->flags);
 
 	if (IS_ERR(entry))
@@ -3222,9 +3028,8 @@ long kgsl_ioctl_gpuobj_alloc(struct kgsl_device_private *dev_priv,
 	param->mmapsize = kgsl_memdesc_footprint(&entry->memdesc);
 	param->id = entry->id;
 
-	/* Put the extra ref from kgsl_mem_entry_create() */
+	/* put the extra refcount for kgsl_mem_entry_create() */
 	kgsl_mem_entry_put(entry);
-
 	return 0;
 }
 
@@ -3237,9 +3042,7 @@ long kgsl_ioctl_gpumem_alloc(struct kgsl_device_private *dev_priv,
 
 	/* Legacy functions doesn't support these advanced features */
 	flags &= ~((uint64_t) KGSL_MEMFLAGS_USE_CPU_MAP);
-
-	if (kgsl_is_compat_task())
-		flags |= KGSL_MEMFLAGS_FORCE_32BIT;
+	flags |= KGSL_MEMFLAGS_FORCE_32BIT;
 
 	entry = gpumem_alloc_entry(dev_priv, (uint64_t) param->size, flags);
 
@@ -3250,9 +3053,8 @@ long kgsl_ioctl_gpumem_alloc(struct kgsl_device_private *dev_priv,
 	param->size = (size_t) entry->memdesc.size;
 	param->flags = (unsigned int) entry->memdesc.flags;
 
-	/* Put the extra ref from kgsl_mem_entry_create() */
+	/* put the extra refcount for kgsl_mem_entry_create() */
 	kgsl_mem_entry_put(entry);
-
 	return 0;
 }
 
@@ -3263,8 +3065,7 @@ long kgsl_ioctl_gpumem_alloc_id(struct kgsl_device_private *dev_priv,
 	struct kgsl_mem_entry *entry;
 	uint64_t flags = param->flags;
 
-	if (kgsl_is_compat_task())
-		flags |= KGSL_MEMFLAGS_FORCE_32BIT;
+	flags |= KGSL_MEMFLAGS_FORCE_32BIT;
 
 	entry = gpumem_alloc_entry(dev_priv, (uint64_t) param->size, flags);
 
@@ -3277,9 +3078,8 @@ long kgsl_ioctl_gpumem_alloc_id(struct kgsl_device_private *dev_priv,
 	param->mmapsize = (size_t) kgsl_memdesc_footprint(&entry->memdesc);
 	param->gpuaddr = (unsigned long) entry->memdesc.gpuaddr;
 
-	/* Put the extra ref from kgsl_mem_entry_create() */
+	/* put the extra refcount for kgsl_mem_entry_create() */
 	kgsl_mem_entry_put(entry);
-
 	return 0;
 }
 
@@ -3291,12 +3091,15 @@ long kgsl_ioctl_gpumem_get_info(struct kgsl_device_private *dev_priv,
 	struct kgsl_mem_entry *entry = NULL;
 	int result = 0;
 
-	if (param->id != 0)
+	if (param->id != 0) {
 		entry = kgsl_sharedmem_find_id(private, param->id);
-	else if (param->gpuaddr != 0)
+		if (entry == NULL)
+			return -EINVAL;
+	} else if (param->gpuaddr != 0) {
 		entry = kgsl_sharedmem_find(private, (uint64_t) param->gpuaddr);
-
-	if (entry == NULL)
+		if (entry == NULL)
+			return -EINVAL;
+	} else
 		return -EINVAL;
 
 	/*
@@ -3318,786 +3121,116 @@ long kgsl_ioctl_gpumem_get_info(struct kgsl_device_private *dev_priv,
 	return result;
 }
 
-static inline int _sparse_alloc_param_sanity_check(uint64_t size,
-		uint64_t pagesize)
-{
-	if (size == 0 || pagesize == 0)
-		return -EINVAL;
-
-	if (pagesize != PAGE_SIZE && pagesize != SZ_64K)
-		return -EINVAL;
-
-	if (pagesize > size || !IS_ALIGNED(size, pagesize))
-		return -EINVAL;
-
-	return 0;
-}
-
-long kgsl_ioctl_sparse_phys_alloc(struct kgsl_device_private *dev_priv,
-	unsigned int cmd, void *data)
+long kgsl_ioctl_gpuobj_info(struct kgsl_device_private *dev_priv,
+		unsigned int cmd, void *data)
 {
-	struct kgsl_process_private *process = dev_priv->process_priv;
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_sparse_phys_alloc *param = data;
+	struct kgsl_process_private *private = dev_priv->process_priv;
+	struct kgsl_gpuobj_info *param = data;
 	struct kgsl_mem_entry *entry;
-	uint64_t flags;
-	int ret;
-	int id;
 
-	if (!(device->flags & KGSL_FLAG_SPARSE))
-		return -ENOTSUPP;
-
-	ret = _sparse_alloc_param_sanity_check(param->size, param->pagesize);
-	if (ret)
-		return ret;
+	if (param->id == 0)
+		return -EINVAL;
 
-	entry = kgsl_mem_entry_create();
+	entry = kgsl_sharedmem_find_id(private, param->id);
 	if (entry == NULL)
-		return -ENOMEM;
-
-	ret = kgsl_process_private_get(process);
-	if (!ret) {
-		ret = -EBADF;
-		goto err_free_entry;
-	}
-
-	idr_preload(GFP_KERNEL);
-	spin_lock(&process->mem_lock);
-	/* Allocate the ID but don't attach the pointer just yet */
-	id = idr_alloc(&process->mem_idr, NULL, 1, 0, GFP_NOWAIT);
-	spin_unlock(&process->mem_lock);
-	idr_preload_end();
-
-	if (id < 0) {
-		ret = id;
-		goto err_put_proc_priv;
-	}
-
-	entry->id = id;
-	entry->priv = process;
-
-	flags = KGSL_MEMFLAGS_SPARSE_PHYS |
-		((ilog2(param->pagesize) << KGSL_MEMALIGN_SHIFT) &
-			KGSL_MEMALIGN_MASK);
-
-	ret = kgsl_allocate_user(dev_priv->device, &entry->memdesc,
-			param->size, flags);
-	if (ret)
-		goto err_remove_idr;
-
-	/* Sanity check to verify we got correct pagesize */
-	if (param->pagesize != PAGE_SIZE && entry->memdesc.sgt != NULL) {
-		struct scatterlist *s;
-		int i;
-
-		for_each_sg(entry->memdesc.sgt->sgl, s,
-				entry->memdesc.sgt->nents, i) {
-			if (!IS_ALIGNED(s->length, param->pagesize))
-				goto err_invalid_pages;
-		}
-	}
+		return -EINVAL;
 
 	param->id = entry->id;
+	param->gpuaddr = entry->memdesc.gpuaddr;
 	param->flags = entry->memdesc.flags;
+	param->size = entry->memdesc.size;
+	param->va_len = kgsl_memdesc_footprint(&entry->memdesc);
+	param->va_addr = (uint64_t) entry->memdesc.useraddr;
 
-	trace_sparse_phys_alloc(entry->id, param->size, param->pagesize);
-	kgsl_mem_entry_commit_process(entry);
-
-	/* Put the extra ref from kgsl_mem_entry_create() */
 	kgsl_mem_entry_put(entry);
-
 	return 0;
-
-err_invalid_pages:
-	kgsl_sharedmem_free(&entry->memdesc);
-err_remove_idr:
-	spin_lock(&process->mem_lock);
-	idr_remove(&process->mem_idr, entry->id);
-	spin_unlock(&process->mem_lock);
-err_put_proc_priv:
-	kgsl_process_private_put(process);
-err_free_entry:
-	kfree(entry);
-
-	return ret;
 }
 
-long kgsl_ioctl_sparse_phys_free(struct kgsl_device_private *dev_priv,
-	unsigned int cmd, void *data)
+long kgsl_ioctl_gpuobj_set_info(struct kgsl_device_private *dev_priv,
+		unsigned int cmd, void *data)
 {
-	struct kgsl_process_private *process = dev_priv->process_priv;
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_sparse_phys_free *param = data;
+	struct kgsl_process_private *private = dev_priv->process_priv;
+	struct kgsl_gpuobj_set_info *param = data;
 	struct kgsl_mem_entry *entry;
 
-	if (!(device->flags & KGSL_FLAG_SPARSE))
-		return -ENOTSUPP;
+	if (param->id == 0)
+		return -EINVAL;
 
-	entry = kgsl_sharedmem_find_id_flags(process, param->id,
-			KGSL_MEMFLAGS_SPARSE_PHYS);
+	entry = kgsl_sharedmem_find_id(private, param->id);
 	if (entry == NULL)
 		return -EINVAL;
 
-	if (!kgsl_mem_entry_set_pend(entry)) {
-		kgsl_mem_entry_put(entry);
-		return -EBUSY;
-	}
+	if (param->flags & KGSL_GPUOBJ_SET_INFO_METADATA)
+		copy_metadata(entry, param->metadata, param->metadata_len);
 
-	if (entry->memdesc.cur_bindings != 0) {
-		kgsl_mem_entry_unset_pend(entry);
-		kgsl_mem_entry_put(entry);
-		return -EINVAL;
+	if (param->flags & KGSL_GPUOBJ_SET_INFO_TYPE) {
+		entry->memdesc.flags &= ~((uint64_t) KGSL_MEMTYPE_MASK);
+		entry->memdesc.flags |= param->type << KGSL_MEMTYPE_SHIFT;
 	}
 
-	trace_sparse_phys_free(entry->id);
-
-	/* One put for find_id(), one put for the kgsl_mem_entry_create() */
 	kgsl_mem_entry_put(entry);
-	kgsl_mem_entry_put(entry);
-
 	return 0;
 }
 
-long kgsl_ioctl_sparse_virt_alloc(struct kgsl_device_private *dev_priv,
-	unsigned int cmd, void *data)
+long kgsl_ioctl_cff_syncmem(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data)
 {
+	struct kgsl_cff_syncmem *param = data;
 	struct kgsl_process_private *private = dev_priv->process_priv;
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_sparse_virt_alloc *param = data;
-	struct kgsl_mem_entry *entry;
-	int ret;
-
-	if (!(device->flags & KGSL_FLAG_SPARSE))
-		return -ENOTSUPP;
-
-	ret = _sparse_alloc_param_sanity_check(param->size, param->pagesize);
-	if (ret)
-		return ret;
-
-	entry = kgsl_mem_entry_create();
-	if (entry == NULL)
-		return -ENOMEM;
-
-	kgsl_memdesc_init(dev_priv->device, &entry->memdesc,
-			KGSL_MEMFLAGS_SPARSE_VIRT);
-	entry->memdesc.size = param->size;
-	entry->memdesc.cur_bindings = 0;
-	kgsl_memdesc_set_align(&entry->memdesc, ilog2(param->pagesize));
-
-	spin_lock_init(&entry->bind_lock);
-	entry->bind_tree = RB_ROOT;
-
-	ret = kgsl_mem_entry_attach_process(dev_priv->device, private, entry);
-	if (ret) {
-		kfree(entry);
-		return ret;
-	}
-
-	param->id = entry->id;
-	param->gpuaddr = entry->memdesc.gpuaddr;
-	param->flags = entry->memdesc.flags;
-
-	trace_sparse_virt_alloc(entry->id, param->size, param->pagesize);
-	kgsl_mem_entry_commit_process(entry);
-
-	/* Put the extra ref from kgsl_mem_entry_create() */
-	kgsl_mem_entry_put(entry);
-
-	return 0;
-}
-
-long kgsl_ioctl_sparse_virt_free(struct kgsl_device_private *dev_priv,
-	unsigned int cmd, void *data)
-{
-	struct kgsl_process_private *process = dev_priv->process_priv;
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_sparse_virt_free *param = data;
 	struct kgsl_mem_entry *entry = NULL;
+	uint64_t offset, len;
 
-	if (!(device->flags & KGSL_FLAG_SPARSE))
-		return -ENOTSUPP;
-
-	entry = kgsl_sharedmem_find_id_flags(process, param->id,
-			KGSL_MEMFLAGS_SPARSE_VIRT);
+	entry = kgsl_sharedmem_find(private, (uint64_t) param->gpuaddr);
 	if (entry == NULL)
 		return -EINVAL;
 
-	if (!kgsl_mem_entry_set_pend(entry)) {
-		kgsl_mem_entry_put(entry);
-		return -EBUSY;
-	}
-
-	if (entry->bind_tree.rb_node != NULL) {
-		kgsl_mem_entry_unset_pend(entry);
-		kgsl_mem_entry_put(entry);
-		return -EINVAL;
-	}
-
-	trace_sparse_virt_free(entry->id);
-
-	/* One put for find_id(), one put for the kgsl_mem_entry_create() */
-	kgsl_mem_entry_put(entry);
-	kgsl_mem_entry_put(entry);
-
-	return 0;
-}
-
-/* entry->bind_lock must be held by the caller */
-static int _sparse_add_to_bind_tree(struct kgsl_mem_entry *entry,
-		uint64_t v_offset,
-		struct kgsl_memdesc *memdesc,
-		uint64_t p_offset,
-		uint64_t size,
-		uint64_t flags)
-{
-	struct sparse_bind_object *new;
-	struct rb_node **node, *parent = NULL;
-
-	new = kzalloc(sizeof(*new), GFP_ATOMIC);
-	if (new == NULL)
-		return -ENOMEM;
-
-	new->v_off = v_offset;
-	new->p_off = p_offset;
-	new->p_memdesc = memdesc;
-	new->size = size;
-	new->flags = flags;
-
-	node = &entry->bind_tree.rb_node;
-
-	while (*node != NULL) {
-		struct sparse_bind_object *this;
-
-		parent = *node;
-		this = rb_entry(parent, struct sparse_bind_object, node);
-
-		if ((new->v_off < this->v_off) &&
-			((new->v_off + new->size) <= this->v_off))
-			node = &parent->rb_left;
-		else if ((new->v_off > this->v_off) &&
-			(new->v_off >= (this->v_off + this->size)))
-			node = &parent->rb_right;
-		else {
-			kfree(new);
-			return -EADDRINUSE;
-		}
-	}
-
-	rb_link_node(&new->node, parent, node);
-	rb_insert_color(&new->node, &entry->bind_tree);
-
-	return 0;
-}
-
-static int _sparse_rm_from_bind_tree(struct kgsl_mem_entry *entry,
-		struct sparse_bind_object *obj,
-		uint64_t v_offset, uint64_t size)
-{
-	if (v_offset == obj->v_off && size >= obj->size) {
-		/*
-		 * We are all encompassing, remove the entry and free
-		 * things up
-		 */
-		rb_erase(&obj->node, &entry->bind_tree);
-		kfree(obj);
-	} else if (v_offset == obj->v_off) {
-		/*
-		 * We are the front of the node, adjust the front of
-		 * the node
-		 */
-		obj->v_off += size;
-		obj->p_off += size;
-		obj->size -= size;
-	} else if ((v_offset + size) == (obj->v_off + obj->size)) {
-		/*
-		 * We are at the end of the obj, adjust the beginning
-		 * points
-		 */
-		obj->size -= size;
-	} else {
-		/*
-		 * We are in the middle of a node, split it up and
-		 * create a new mini node. Adjust this node's bounds
-		 * and add the new node to the list.
-		 */
-		uint64_t tmp_size = obj->size;
-		int ret;
-
-		obj->size = v_offset - obj->v_off;
-
-		ret = _sparse_add_to_bind_tree(entry, v_offset + size,
-				obj->p_memdesc,
-				obj->p_off + (v_offset - obj->v_off) + size,
-				tmp_size - (v_offset - obj->v_off) - size,
-				obj->flags);
-
-		return ret;
-	}
-
-	return 0;
-}
-
-/* entry->bind_lock must be held by the caller */
-static struct sparse_bind_object *_find_containing_bind_obj(
-		struct kgsl_mem_entry *entry,
-		uint64_t offset, uint64_t size)
-{
-	struct sparse_bind_object *obj = NULL;
-	struct rb_node *node = entry->bind_tree.rb_node;
-
-	while (node != NULL) {
-		obj = rb_entry(node, struct sparse_bind_object, node);
-
-		if (offset == obj->v_off) {
-			break;
-		} else if (offset < obj->v_off) {
-			if (offset + size > obj->v_off)
-				break;
-			node = node->rb_left;
-			obj = NULL;
-		} else if (offset > obj->v_off) {
-			if (offset < obj->v_off + obj->size)
-				break;
-			node = node->rb_right;
-			obj = NULL;
-		}
-	}
-
-	return obj;
-}
-
-/* entry->bind_lock must be held by the caller */
-static int _sparse_unbind(struct kgsl_mem_entry *entry,
-		struct sparse_bind_object *bind_obj,
-		uint64_t offset, uint64_t size)
-{
-	int ret;
-
-	ret = _sparse_rm_from_bind_tree(entry, bind_obj, offset, size);
-	if (ret == 0) {
-		atomic_long_sub(size, &kgsl_driver.stats.mapped);
-		trace_sparse_unbind(entry->id, offset, size);
-	}
-
-	return ret;
-}
-
-static long sparse_unbind_range(struct kgsl_sparse_binding_object *obj,
-	struct kgsl_mem_entry *virt_entry)
-{
-	struct sparse_bind_object *bind_obj;
-	struct kgsl_memdesc *memdesc;
-	struct kgsl_pagetable *pt;
-	int ret = 0;
-	uint64_t size = obj->size;
-	uint64_t tmp_size = obj->size;
-	uint64_t offset = obj->virtoffset;
-
-	while (size > 0 && ret == 0) {
-		tmp_size = size;
-
-		spin_lock(&virt_entry->bind_lock);
-		bind_obj = _find_containing_bind_obj(virt_entry, offset, size);
-
-		if (bind_obj == NULL) {
-			spin_unlock(&virt_entry->bind_lock);
-			return 0;
-		}
-
-		if (bind_obj->v_off > offset) {
-			tmp_size = size - bind_obj->v_off - offset;
-			if (tmp_size > bind_obj->size)
-				tmp_size = bind_obj->size;
-			offset = bind_obj->v_off;
-		} else if (bind_obj->v_off < offset) {
-			uint64_t diff = offset - bind_obj->v_off;
-
-			if (diff + size > bind_obj->size)
-				tmp_size = bind_obj->size - diff;
-		} else {
-			if (tmp_size > bind_obj->size)
-				tmp_size = bind_obj->size;
-		}
-
-		memdesc = bind_obj->p_memdesc;
-		pt = memdesc->pagetable;
-
-		if (memdesc->cur_bindings < (tmp_size / PAGE_SIZE)) {
-			spin_unlock(&virt_entry->bind_lock);
-			return -EINVAL;
-		}
-
-		memdesc->cur_bindings -= tmp_size / PAGE_SIZE;
-
-		ret = _sparse_unbind(virt_entry, bind_obj, offset, tmp_size);
-		spin_unlock(&virt_entry->bind_lock);
-
-		ret = kgsl_mmu_unmap_offset(pt, memdesc,
-				virt_entry->memdesc.gpuaddr, offset, tmp_size);
-		if (ret)
-			return ret;
-
-		ret = kgsl_mmu_sparse_dummy_map(pt, memdesc, offset, tmp_size);
-		if (ret)
-			return ret;
-
-		if (ret == 0) {
-			offset += tmp_size;
-			size -= tmp_size;
-		}
-	}
-
-	return ret;
-}
-
-static inline bool _is_phys_bindable(struct kgsl_mem_entry *phys_entry,
-		uint64_t offset, uint64_t size, uint64_t flags)
-{
-	struct kgsl_memdesc *memdesc = &phys_entry->memdesc;
-
-	if (!IS_ALIGNED(offset | size, kgsl_memdesc_get_pagesize(memdesc)))
-		return false;
-
-	if (offset + size < offset)
-		return false;
-
-	if (!(flags & KGSL_SPARSE_BIND_MULTIPLE_TO_PHYS) &&
-			offset + size > memdesc->size)
-		return false;
-
-	return true;
-}
-
-static int _sparse_bind(struct kgsl_process_private *process,
-		struct kgsl_mem_entry *virt_entry, uint64_t v_offset,
-		struct kgsl_mem_entry *phys_entry, uint64_t p_offset,
-		uint64_t size, uint64_t flags)
-{
-	int ret;
-	struct kgsl_pagetable *pagetable;
-	struct kgsl_memdesc *memdesc = &phys_entry->memdesc;
-
-	/* map the memory after unlocking if gpuaddr has been assigned */
-	if (memdesc->gpuaddr)
-		return -EINVAL;
-
-	if (memdesc->useraddr != 0)
-		return -EINVAL;
-
-	pagetable = memdesc->pagetable;
-
-	/* Clear out any mappings */
-	ret = kgsl_mmu_unmap_offset(pagetable, &virt_entry->memdesc,
-			virt_entry->memdesc.gpuaddr, v_offset, size);
-	if (ret)
-		return ret;
-
-	ret = kgsl_mmu_map_offset(pagetable, virt_entry->memdesc.gpuaddr,
-			v_offset, memdesc, p_offset, size, flags);
-	if (ret) {
-		/* Try to clean up, but not the end of the world */
-		kgsl_mmu_sparse_dummy_map(pagetable, &virt_entry->memdesc,
-				v_offset, size);
-		return ret;
-	}
-
-	spin_lock(&virt_entry->bind_lock);
-	ret = _sparse_add_to_bind_tree(virt_entry, v_offset, memdesc,
-			p_offset, size, flags);
-	spin_unlock(&virt_entry->bind_lock);
-
-	if (ret == 0)
-		memdesc->cur_bindings += size / PAGE_SIZE;
-
-	return ret;
-}
-
-static long sparse_bind_range(struct kgsl_process_private *private,
-		struct kgsl_sparse_binding_object *obj,
-		struct kgsl_mem_entry *virt_entry)
-{
-	struct kgsl_mem_entry *phys_entry;
-	int ret;
-
-	phys_entry = kgsl_sharedmem_find_id_flags(private, obj->id,
-			KGSL_MEMFLAGS_SPARSE_PHYS);
-	if (phys_entry == NULL)
-		return -EINVAL;
-
-	if (!_is_phys_bindable(phys_entry, obj->physoffset, obj->size,
-				obj->flags)) {
-		kgsl_mem_entry_put(phys_entry);
-		return -EINVAL;
-	}
-
-	if (kgsl_memdesc_get_align(&virt_entry->memdesc) !=
-			kgsl_memdesc_get_align(&phys_entry->memdesc)) {
-		kgsl_mem_entry_put(phys_entry);
-		return -EINVAL;
-	}
-
-	ret = sparse_unbind_range(obj, virt_entry);
-	if (ret) {
-		kgsl_mem_entry_put(phys_entry);
-		return -EINVAL;
-	}
-
-	ret = _sparse_bind(private, virt_entry, obj->virtoffset,
-			phys_entry, obj->physoffset, obj->size,
-			obj->flags & KGSL_SPARSE_BIND_MULTIPLE_TO_PHYS);
-	if (ret == 0) {
-		KGSL_STATS_ADD(obj->size, &kgsl_driver.stats.mapped,
-				&kgsl_driver.stats.mapped_max);
-
-		trace_sparse_bind(virt_entry->id, obj->virtoffset,
-				phys_entry->id, obj->physoffset,
-				obj->size, obj->flags);
-	}
-
-	kgsl_mem_entry_put(phys_entry);
-
-	return ret;
-}
-
-long kgsl_ioctl_sparse_bind(struct kgsl_device_private *dev_priv,
-		unsigned int cmd, void *data)
-{
-	struct kgsl_process_private *private = dev_priv->process_priv;
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_sparse_bind *param = data;
-	struct kgsl_sparse_binding_object obj;
-	struct kgsl_mem_entry *virt_entry;
-	int pg_sz;
-	void __user *ptr;
-	int ret = 0;
-	int i = 0;
-
-	if (!(device->flags & KGSL_FLAG_SPARSE))
-		return -ENOTSUPP;
-
-	ptr = (void __user *) (uintptr_t) param->list;
-
-	if (param->size > sizeof(struct kgsl_sparse_binding_object) ||
-		param->count == 0 || ptr == NULL)
-		return -EINVAL;
-
-	virt_entry = kgsl_sharedmem_find_id_flags(private, param->id,
-			KGSL_MEMFLAGS_SPARSE_VIRT);
-	if (virt_entry == NULL)
-		return -EINVAL;
-
-	pg_sz = kgsl_memdesc_get_pagesize(&virt_entry->memdesc);
-
-	for (i = 0; i < param->count; i++) {
-		memset(&obj, 0, sizeof(obj));
-		ret = _copy_from_user(&obj, ptr, sizeof(obj), param->size);
-		if (ret)
-			break;
-
-		/* Sanity check initial range */
-		if (obj.size == 0 || obj.virtoffset + obj.size < obj.size ||
-			obj.virtoffset + obj.size > virt_entry->memdesc.size ||
-			!(IS_ALIGNED(obj.virtoffset | obj.size, pg_sz))) {
-			ret = -EINVAL;
-			break;
-		}
-
-		if (obj.flags & KGSL_SPARSE_BIND)
-			ret = sparse_bind_range(private, &obj, virt_entry);
-		else if (obj.flags & KGSL_SPARSE_UNBIND)
-			ret = sparse_unbind_range(&obj, virt_entry);
-		else
-			ret = -EINVAL;
-		if (ret)
-			break;
-
-		ptr += sizeof(obj);
-	}
-
-	kgsl_mem_entry_put(virt_entry);
-
-	return ret;
-}
-
-long kgsl_ioctl_gpu_sparse_command(struct kgsl_device_private *dev_priv,
-		unsigned int cmd, void *data)
-{
-	struct kgsl_gpu_sparse_command *param = data;
-	struct kgsl_device *device = dev_priv->device;
-	struct kgsl_context *context;
-	struct kgsl_drawobj *drawobj[2];
-	struct kgsl_drawobj_sparse *sparseobj;
-	long result;
-	unsigned int i = 0;
-
-	if (!(device->flags & KGSL_FLAG_SPARSE))
-		return -ENOTSUPP;
-
-	/* Make sure sparse and syncpoint count isn't too big */
-	if (param->numsparse > KGSL_MAX_SPARSE ||
-		param->numsyncs > KGSL_MAX_SYNCPOINTS)
-		return -EINVAL;
-
-	/* Make sure there is atleast one sparse or sync */
-	if (param->numsparse == 0 && param->numsyncs == 0)
-		return -EINVAL;
-
-	/* Only Sparse commands are supported in this ioctl */
-	if (!(param->flags & KGSL_DRAWOBJ_SPARSE) || (param->flags &
-			(KGSL_DRAWOBJ_SUBMIT_IB_LIST | KGSL_DRAWOBJ_MARKER
-			| KGSL_DRAWOBJ_SYNC)))
-		return -EINVAL;
-
-	context = kgsl_context_get_owner(dev_priv, param->context_id);
-	if (context == NULL)
-		return -EINVAL;
-
-	/* Restrict bind commands to bind context */
-	if (!(context->flags & KGSL_CONTEXT_SPARSE)) {
-		kgsl_context_put(context);
-		return -EINVAL;
-	}
-
-	if (param->numsyncs) {
-		struct kgsl_drawobj_sync *syncobj = kgsl_drawobj_sync_create(
-				device, context);
-		if (IS_ERR(syncobj)) {
-			result = PTR_ERR(syncobj);
-			goto done;
-		}
-
-		drawobj[i++] = DRAWOBJ(syncobj);
-		result = kgsl_drawobj_sync_add_synclist(device, syncobj,
-				to_user_ptr(param->synclist),
-				param->syncsize, param->numsyncs);
-		if (result)
-			goto done;
-	}
-
-	if (param->numsparse) {
-		sparseobj = kgsl_drawobj_sparse_create(device, context,
-					param->flags);
-		if (IS_ERR(sparseobj)) {
-			result = PTR_ERR(sparseobj);
-			goto done;
-		}
-
-		sparseobj->id = param->id;
-		drawobj[i++] = DRAWOBJ(sparseobj);
-		result = kgsl_drawobj_sparse_add_sparselist(device, sparseobj,
-				param->id, to_user_ptr(param->sparselist),
-				param->sparsesize, param->numsparse);
-		if (result)
-			goto done;
-	}
-
-	result = dev_priv->device->ftbl->queue_cmds(dev_priv, context,
-					drawobj, i, &param->timestamp);
-
-done:
 	/*
-	 * -EPROTO is a "success" error - it just tells the user that the
-	 * context had previously faulted
+	 * Calculate the offset between the requested GPU address and the start
+	 * of the object
 	 */
-	if (result && result != -EPROTO)
-		while (i--)
-			kgsl_drawobj_destroy(drawobj[i]);
-
-	kgsl_context_put(context);
-	return result;
-}
-
-void kgsl_sparse_bind(struct kgsl_process_private *private,
-		struct kgsl_drawobj_sparse *sparseobj)
-{
-	struct kgsl_sparseobj_node *sparse_node;
-	struct kgsl_mem_entry *virt_entry = NULL;
-	long ret = 0;
-	char *name;
 
-	virt_entry = kgsl_sharedmem_find_id_flags(private, sparseobj->id,
-			KGSL_MEMFLAGS_SPARSE_VIRT);
-	if (virt_entry == NULL)
-		return;
+	offset = ((uint64_t) param->gpuaddr) - entry->memdesc.gpuaddr;
 
-	list_for_each_entry(sparse_node, &sparseobj->sparselist, node) {
-		if (sparse_node->obj.flags & KGSL_SPARSE_BIND) {
-			ret = sparse_bind_range(private, &sparse_node->obj,
-					virt_entry);
-			name = "bind";
-		} else {
-			ret = sparse_unbind_range(&sparse_node->obj,
-					virt_entry);
-			name = "unbind";
-		}
+	if ((offset + param->len) > entry->memdesc.size)
+		len = entry->memdesc.size - offset;
+	else
+		len = param->len;
 
-		if (ret)
-			KGSL_CORE_ERR("kgsl: Unable to '%s' ret %ld virt_id %d, phys_id %d, virt_offset %16.16llX, phys_offset %16.16llX, size %16.16llX, flags %16.16llX\n",
-				name, ret, sparse_node->virt_id,
-				sparse_node->obj.id,
-				sparse_node->obj.virtoffset,
-				sparse_node->obj.physoffset,
-				sparse_node->obj.size, sparse_node->obj.flags);
-	}
+	kgsl_cffdump_syncmem(dev_priv->device, entry, offset, len, true);
 
-	kgsl_mem_entry_put(virt_entry);
+	kgsl_mem_entry_put(entry);
+	return 0;
 }
-EXPORT_SYMBOL(kgsl_sparse_bind);
 
-long kgsl_ioctl_gpuobj_info(struct kgsl_device_private *dev_priv,
+long kgsl_ioctl_cff_sync_gpuobj(struct kgsl_device_private *dev_priv,
 		unsigned int cmd, void *data)
 {
+	struct kgsl_cff_sync_gpuobj *param = data;
 	struct kgsl_process_private *private = dev_priv->process_priv;
-	struct kgsl_gpuobj_info *param = data;
-	struct kgsl_mem_entry *entry;
-
-	if (param->id == 0)
-		return -EINVAL;
+	struct kgsl_mem_entry *entry = NULL;
 
 	entry = kgsl_sharedmem_find_id(private, param->id);
 	if (entry == NULL)
 		return -EINVAL;
 
-	param->id = entry->id;
-	param->gpuaddr = entry->memdesc.gpuaddr;
-	param->flags = entry->memdesc.flags;
-	param->size = entry->memdesc.size;
-	param->va_len = kgsl_memdesc_footprint(&entry->memdesc);
-	param->va_addr = (uint64_t) entry->memdesc.useraddr;
+	kgsl_cffdump_syncmem(dev_priv->device, entry, param->offset,
+		param->length, true);
 
 	kgsl_mem_entry_put(entry);
 	return 0;
 }
 
-long kgsl_ioctl_gpuobj_set_info(struct kgsl_device_private *dev_priv,
+long kgsl_ioctl_cff_user_event(struct kgsl_device_private *dev_priv,
 		unsigned int cmd, void *data)
 {
-	struct kgsl_process_private *private = dev_priv->process_priv;
-	struct kgsl_gpuobj_set_info *param = data;
-	struct kgsl_mem_entry *entry;
-	int ret = 0;
-
-	if (param->id == 0)
-		return -EINVAL;
-
-	entry = kgsl_sharedmem_find_id(private, param->id);
-	if (entry == NULL)
-		return -EINVAL;
-
-	if (param->flags & KGSL_GPUOBJ_SET_INFO_METADATA)
-		copy_metadata(entry, param->metadata, param->metadata_len);
+	int result = 0;
+	struct kgsl_cff_user_event *param = data;
 
-	if (param->flags & KGSL_GPUOBJ_SET_INFO_TYPE) {
-		if (param->type <= (KGSL_MEMTYPE_MASK >> KGSL_MEMTYPE_SHIFT)) {
-			entry->memdesc.flags &= ~((uint64_t) KGSL_MEMTYPE_MASK);
-			entry->memdesc.flags |= (uint64_t)((param->type <<
-				KGSL_MEMTYPE_SHIFT) & KGSL_MEMTYPE_MASK);
-		} else
-			ret = -EINVAL;
-	}
+	kgsl_cffdump_user_event(dev_priv->device, param->cff_opcode,
+			param->op1, param->op2,
+			param->op3, param->op4, param->op5);
 
-	kgsl_mem_entry_put(entry);
-	return ret;
+	return result;
 }
 
 /**
@@ -4145,7 +3278,7 @@ kgsl_mmap_memstore(struct kgsl_device *device, struct vm_area_struct *vma)
 		return -EINVAL;
 	}
 
-	vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	result = remap_pfn_range(vma, vma->vm_start,
 				device->memstore.physaddr >> PAGE_SHIFT,
@@ -4174,18 +3307,13 @@ static int
 kgsl_gpumem_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 {
 	struct kgsl_mem_entry *entry = vma->vm_private_data;
-	int ret;
 
 	if (!entry)
 		return VM_FAULT_SIGBUS;
 	if (!entry->memdesc.ops || !entry->memdesc.ops->vmfault)
 		return VM_FAULT_SIGBUS;
 
-	ret = entry->memdesc.ops->vmfault(&entry->memdesc, vma, vmf);
-	if ((ret == 0) || (ret == VM_FAULT_NOPAGE))
-		entry->priv->gpumem_mapped += PAGE_SIZE;
-
-	return ret;
+	return entry->memdesc.ops->vmfault(&entry->memdesc, vma, vmf);
 }
 
 static void
@@ -4200,7 +3328,7 @@ kgsl_gpumem_vm_close(struct vm_area_struct *vma)
 	kgsl_mem_entry_put(entry);
 }
 
-static const struct vm_operations_struct kgsl_gpumem_vm_ops = {
+static struct vm_operations_struct kgsl_gpumem_vm_ops = {
 	.open  = kgsl_gpumem_vm_open,
 	.fault = kgsl_gpumem_vm_fault,
 	.close = kgsl_gpumem_vm_close,
@@ -4228,13 +3356,6 @@ get_mmap_entry(struct kgsl_process_private *private,
 		goto err_put;
 	}
 
-	if (entry->memdesc.flags & KGSL_MEMFLAGS_SPARSE_PHYS) {
-		if (len != entry->memdesc.size) {
-			ret = -EINVAL;
-			goto err_put;
-		}
-	}
-
 	if (entry->memdesc.useraddr != 0) {
 		ret = -EBUSY;
 		goto err_put;
@@ -4359,10 +3480,7 @@ static unsigned long _search_range(struct kgsl_process_private *private,
 		}
 
 		/* Check that_gpu_find_svm doesn't put us in a loop */
-		if (gpu >= cpu) {
-			result = -ENOMEM;
-			break;
-		}
+		BUG_ON(gpu >= cpu);
 
 		/* Break if the recommended GPU address is out of range */
 		if (gpu < start) {
@@ -4447,118 +3565,6 @@ static unsigned long _get_svm_area(struct kgsl_process_private *private,
 	return result;
 }
 
-//#ifdef WEAROS_EDIT
-//#Wei.Lv@Wear.Android.Framework.Memory, 2020/05/15, dump process's map and ksgl mem when gpu oom
-static int last_oom_dump_pid;
-static bool kgsl_oom_dump_done;
-static int kgsl_dump_oom_info(int pid)
-{
-	int ret;
-	struct file *fp;
-	char path[64];
-	char *buff, *s_start;
-	int count = 0;
-    int i = 0;
-	mm_segment_t old_fs;
-	loff_t pos;
-
-	buff = (char*)kmalloc(PAGE_SIZE, GFP_KERNEL);
-	if(!buff) {
-		pr_err("[Qcom_debug] kmalloc(PAGE_SIZE, GFP_KERNEL) is failed, exit!\n");
-		ret = -1;
-		goto err_out;
-	}
-
-	// Part 1- dump /proc/$pid/maps
-	sprintf(path, "/proc/%d/maps", pid);
-	fp = filp_open(path, O_RDONLY, 0);
-	if (IS_ERR(fp)) {
-		pr_err("[Qcom_debug] Open %s failed, error %ld\n", path,(unsigned long)fp);
-		ret = -1;
-		goto err_out;
-	}
-	pr_err("[Qcom_debug] Dumping %s\n", path);
-	pr_err("[Qcom_debug] startaddr-endaddr flags offset major:minor size node\n");
-	do {
-		struct mm_struct *mm = current->mm;
-		old_fs = get_fs();
-		set_fs(KERNEL_DS);
-		pos = fp->f_pos;
-		up_write(&mm->mmap_sem);
-		count = vfs_read(fp, buff, PAGE_SIZE-1, &pos);
-		ret = down_write_killable(&mm->mmap_sem);
-		set_fs(old_fs);
-
-		if(count < 0) {
-			ret = -1;
-			pr_err("[Qcom_debug] read file %s failed!\n", path);
-			goto err_out;
-		}
-		fp->f_pos = pos;
-
-		s_start = buff;
-		for(i = 0; i < count; i++) {
-			if(buff[i] == '\n'){
-				buff[i] = '\0';
-				pr_err("[Qcom_debug] %s\n",s_start);
-				s_start = buff + i + 1;
-			}
-		}
-	} while (count > 0);
-	filp_close(fp, NULL);
-	// Part 1- end
-
-	// Part 2- dump /d/kgsl/proc/$pid/mem
-	sprintf(path,"/d/kgsl/proc/%d/mem", pid);
-	fp = filp_open(path, O_RDONLY, 0);
-	if (IS_ERR(fp)) {
-		pr_err("[Qcom_debug] Open %s failed, error %ld, try again\n", path, (unsigned long)fp);
-
-		sprintf(path,"/sys/kernel/debug/kgsl/proc/%d/mem", pid);
-		fp = filp_open(path, O_RDONLY, 0);
-		if (IS_ERR(fp)) {
-			pr_err("[Qcom_debug] Open %s failed, error %ld\n", path, (unsigned long)fp);
-			ret = -1;
-			goto err_out;
-		}
-	}
-	pr_err("[Qcom_debug] Dumping %s\n", path);
-	do {
-		//struct mm_struct *mm = current->mm;
-		old_fs = get_fs();
-		set_fs(KERNEL_DS);
-		pos = fp->f_pos;
-		//up_write(&mm->mmap_sem);
-		count = vfs_read(fp, buff, PAGE_SIZE-1, &pos);
-		//ret = down_write_killable(&mm->mmap_sem);
-		set_fs(old_fs);
-
-		if(count < 0) {
-			ret = -1;
-			pr_err("[Qcom_debug] read file %s failed!\n", path);
-			goto err_out;
-		}
-		fp->f_pos = pos;
-
-		s_start = buff;
-		for(i = 0; i < count; i++) {
-			if(buff[i] == '\n'){
-				buff[i] = '\0';
-				pr_err("[Qcom_debug] %s\n",s_start);
-				s_start = buff + i + 1;
-			}
-		}
-	} while (count > 0);
-	filp_close(fp, NULL);
-	// Part 2 - end
-
-	ret = 0;
-err_out:
-	kfree(buff);
-	return ret;
-}
-//#endif /* WEAROS_EDIT */
-
 static unsigned long
 kgsl_get_unmapped_area(struct file *file, unsigned long addr,
 			unsigned long len, unsigned long pgoff,
@@ -4587,25 +3593,17 @@ kgsl_get_unmapped_area(struct file *file, unsigned long addr,
 	if (!kgsl_memdesc_use_cpu_map(&entry->memdesc)) {
 		val = get_unmapped_area(NULL, addr, len, 0, flags);
 		if (IS_ERR_VALUE(val))
-			KGSL_DRV_ERR_RATELIMIT(device,
+			KGSL_MEM_ERR(device,
 				"get_unmapped_area: pid %d addr %lx pgoff %lx len %ld failed error %d\n",
 				private->pid, addr, pgoff, len, (int) val);
 	} else {
-		val = _get_svm_area(private, entry, addr, len, flags);
-		if (IS_ERR_VALUE(val))
-			KGSL_DRV_ERR_RATELIMIT(device,
-				"_get_svm_area: pid %d mmap_base %lx addr %lx pgoff %lx len %ld failed error %d\n",
-				private->pid, current->mm->mmap_base, addr,
-				pgoff, len, (int) val);
-	}
-    //#ifdef WEAROS_EDIT
-    //#Wei.Lv@Wear.Android.Framework.Memory, 2020/05/15, dump process's map and ksgl mem when gpu oom
-	if (IS_ERR_VALUE(val) && (!kgsl_oom_dump_done || (private->pid != last_oom_dump_pid))) {
-		int ret = kgsl_dump_oom_info(private->pid);
-		kgsl_oom_dump_done = !ret;
-		last_oom_dump_pid = private->pid;
+		 val = _get_svm_area(private, entry, addr, len, flags);
+		 if (IS_ERR_VALUE(val))
+			KGSL_MEM_ERR(device,
+				"_get_svm_area: pid %d addr %lx pgoff %lx len %ld failed error %d\n",
+				private->pid, addr, pgoff, len, (int) val);
 	}
-    //#endif /* WEAROS_EDIT */
+
 put:
 	kgsl_mem_entry_put(entry);
 	return val;
@@ -4648,8 +3646,8 @@ static int kgsl_mmap(struct file *file, struct vm_area_struct *vma)
 		break;
 	case KGSL_CACHEMODE_WRITETHROUGH:
 		vma->vm_page_prot = pgprot_writethroughcache(vma->vm_page_prot);
-		if (pgprot_val(vma->vm_page_prot) ==
-			pgprot_val(pgprot_writebackcache(vma->vm_page_prot)))
+		if (vma->vm_page_prot ==
+			pgprot_writebackcache(vma->vm_page_prot))
 			WARN_ONCE(1, "WRITETHROUGH is deprecated for arm64");
 		break;
 	case KGSL_CACHEMODE_WRITEBACK:
@@ -4670,10 +3668,9 @@ static int kgsl_mmap(struct file *file, struct vm_area_struct *vma)
 		struct kgsl_memdesc *m = &entry->memdesc;
 
 		for (i = 0; i < m->page_count; i++) {
-			struct page *page = m->pages[i];
-
-			vm_insert_page(vma, addr, page);
-			addr += PAGE_SIZE;
+				struct page *page = m->pages[i];
+				vm_insert_page(vma, addr, page);
+				addr += PAGE_SIZE;
 		}
 	}
 
@@ -4807,13 +3804,28 @@ int kgsl_device_platform_probe(struct kgsl_device *device)
 	/* Initialize logging first, so that failures below actually print. */
 	kgsl_device_debugfs_init(device);
 
-	/* Disable the sparse ioctl invocation as they are not used */
-	device->flags &= ~KGSL_FLAG_SPARSE;
-
 	status = kgsl_pwrctrl_init(device);
 	if (status)
 		goto error;
 
+	/* Get starting physical address of device registers */
+	res = platform_get_resource_byname(device->pdev, IORESOURCE_MEM,
+					   device->iomemname);
+	if (res == NULL) {
+		KGSL_DRV_ERR(device, "platform_get_resource_byname failed\n");
+		status = -EINVAL;
+		goto error_pwrctrl_close;
+	}
+	if (res->start == 0 || resource_size(res) == 0) {
+		KGSL_DRV_ERR(device, "dev %d invalid register region\n",
+			device->id);
+		status = -EINVAL;
+		goto error_pwrctrl_close;
+	}
+
+	device->reg_phys = res->start;
+	device->reg_len = resource_size(res);
+
 	/*
 	 * Check if a shadermemname is defined, and then get shader memory
 	 * details including shader memory starting physical address
@@ -4882,7 +3894,6 @@ int kgsl_device_platform_probe(struct kgsl_device *device)
 		device->id, device->reg_phys, device->reg_len);
 
 	rwlock_init(&device->context_lock);
-	spin_lock_init(&device->submit_lock);
 
 	setup_timer(&device->idle_timer, kgsl_timer, (unsigned long) device);
 
@@ -4895,14 +3906,14 @@ int kgsl_device_platform_probe(struct kgsl_device *device)
 	if (status)
 		goto error_close_mmu;
 
-	/* Initialize the memory pools */
-	kgsl_init_page_pools(device->pdev);
-
 	status = kgsl_allocate_global(device, &device->memstore,
-		KGSL_MEMSTORE_SIZE, 0, KGSL_MEMDESC_CONTIG, "memstore");
+		KGSL_MEMSTORE_SIZE, 0, 0);
 
-	if (status != 0)
+	if (status != 0) {
+		KGSL_DRV_ERR(device, "kgsl_allocate_global failed %d\n",
+				status);
 		goto error_close_mmu;
+	}
 
 	/*
 	 * The default request type PM_QOS_REQ_ALL_CORES is
@@ -4938,10 +3949,10 @@ int kgsl_device_platform_probe(struct kgsl_device *device)
 				PM_QOS_DEFAULT_VALUE);
 	}
 
-	device->events_wq = alloc_workqueue("kgsl-events",
-		WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
 
-	/* Initialize the snapshot engine */
+	device->events_wq = create_singlethread_workqueue("kgsl-events");
+
+	/* Initalize the snapshot engine */
 	kgsl_device_snapshot_init(device);
 
 	/* Initialize common sysfs entries */
@@ -4954,7 +3965,6 @@ int kgsl_device_platform_probe(struct kgsl_device *device)
 error_pwrctrl_close:
 	kgsl_pwrctrl_close(device);
 error:
-	kgsl_device_debugfs_close(device);
 	_unregister_device(device);
 	return status;
 }
@@ -4966,8 +3976,6 @@ void kgsl_device_platform_remove(struct kgsl_device *device)
 
 	kgsl_device_snapshot_close(device);
 
-	kgsl_exit_page_pools();
-
 	kgsl_pwrctrl_uninit_sysfs(device);
 
 	pm_qos_remove_request(&device->pwrctrl.pm_qos_req_dma);
@@ -4982,7 +3990,6 @@ void kgsl_device_platform_remove(struct kgsl_device *device)
 
 	kgsl_pwrctrl_close(device);
 
-	kgsl_device_debugfs_close(device);
 	_unregister_device(device);
 }
 EXPORT_SYMBOL(kgsl_device_platform_remove);
@@ -4990,6 +3997,7 @@ EXPORT_SYMBOL(kgsl_device_platform_remove);
 static void kgsl_core_exit(void)
 {
 	kgsl_events_exit();
+	kgsl_cffdump_destroy();
 	kgsl_core_debugfs_close();
 
 	/*
@@ -5008,7 +4016,7 @@ static void kgsl_core_exit(void)
 		kgsl_driver.class = NULL;
 	}
 
-	kgsl_drawobjs_cache_exit();
+	kgsl_cmdbatch_exit();
 
 	kgsl_memfree_exit();
 	unregister_chrdev_region(kgsl_driver.major, KGSL_DEVICE_MAX);
@@ -5036,8 +4044,8 @@ static int __init kgsl_core_init(void)
 		       KGSL_DEVICE_MAX);
 
 	if (result) {
-		KGSL_CORE_ERR("kgsl: cdev_add() failed, dev_num= %d, result= %d\n",
-			kgsl_driver.major, result);
+		KGSL_CORE_ERR("kgsl: cdev_add() failed, dev_num= %d,"
+			     " result= %d\n", kgsl_driver.major, result);
 		goto err;
 	}
 
@@ -5049,10 +4057,8 @@ static int __init kgsl_core_init(void)
 		goto err;
 	}
 
-	/*
-	 * Make a virtual device for managing core related things
-	 * in sysfs
-	 */
+	/* Make a virtual device for managing core related things
+	   in sysfs */
 	kgsl_driver.virtdev.class = kgsl_driver.class;
 	dev_set_name(&kgsl_driver.virtdev, "kgsl");
 	result = device_register(&kgsl_driver.virtdev);
@@ -5074,16 +4080,15 @@ static int __init kgsl_core_init(void)
 	kgsl_core_debugfs_init();
 
 	kgsl_sharedmem_init_sysfs();
+	kgsl_cffdump_init();
 
 	INIT_LIST_HEAD(&kgsl_driver.process_list);
 
 	INIT_LIST_HEAD(&kgsl_driver.pagetable_list);
 
-	kgsl_driver.workqueue = alloc_workqueue("kgsl-workqueue",
-		WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
-
-	kgsl_driver.mem_workqueue = alloc_workqueue("kgsl-mementry",
-		WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	kgsl_driver.workqueue = create_singlethread_workqueue("kgsl-workqueue");
+	kgsl_driver.mem_workqueue =
+		create_singlethread_workqueue("kgsl-mementry");
 
 	kthread_init_worker(&kgsl_driver.worker);
 
@@ -5099,7 +4104,7 @@ static int __init kgsl_core_init(void)
 
 	kgsl_events_init();
 
-	result = kgsl_drawobjs_cache_init();
+	result = kgsl_cmdbatch_init();
 	if (result)
 		goto err;
 
@@ -5115,5 +4120,6 @@ static int __init kgsl_core_init(void)
 module_init(kgsl_core_init);
 module_exit(kgsl_core_exit);
 
+MODULE_AUTHOR("Qualcomm Innovation Center, Inc.");
 MODULE_DESCRIPTION("MSM GPU driver");
 MODULE_LICENSE("GPL");
diff --git a/drivers/gpu/msm/kgsl.h b/drivers/gpu/msm/kgsl.h
index b9f5017fac77..dfa56b68763b 100644
--- a/drivers/gpu/msm/kgsl.h
+++ b/drivers/gpu/msm/kgsl.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008-2019, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2008-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -27,78 +27,23 @@
 #include <linux/uaccess.h>
 #include <linux/kthread.h>
 #include <asm/cacheflush.h>
-#include <linux/compat.h>
-
-/*
- * --- kgsl drawobj flags ---
- * These flags are same as --- drawobj flags ---
- * but renamed to reflect that cmdbatch is renamed to drawobj.
- */
-#define KGSL_DRAWOBJ_MEMLIST           KGSL_CMDBATCH_MEMLIST
-#define KGSL_DRAWOBJ_MARKER            KGSL_CMDBATCH_MARKER
-#define KGSL_DRAWOBJ_SUBMIT_IB_LIST    KGSL_CMDBATCH_SUBMIT_IB_LIST
-#define KGSL_DRAWOBJ_CTX_SWITCH        KGSL_CMDBATCH_CTX_SWITCH
-#define KGSL_DRAWOBJ_PROFILING         KGSL_CMDBATCH_PROFILING
-#define KGSL_DRAWOBJ_PROFILING_KTIME   KGSL_CMDBATCH_PROFILING_KTIME
-#define KGSL_DRAWOBJ_END_OF_FRAME      KGSL_CMDBATCH_END_OF_FRAME
-#define KGSL_DRAWOBJ_SYNC              KGSL_CMDBATCH_SYNC
-#define KGSL_DRAWOBJ_PWR_CONSTRAINT    KGSL_CMDBATCH_PWR_CONSTRAINT
-#define KGSL_DRAWOBJ_SPARSE            KGSL_CMDBATCH_SPARSE
-
-#define kgsl_drawobj_profiling_buffer kgsl_cmdbatch_profiling_buffer
-
 
 /* The number of memstore arrays limits the number of contexts allowed.
  * If more contexts are needed, update multiple for MEMSTORE_SIZE
  */
-#define KGSL_MEMSTORE_SIZE	((int)(PAGE_SIZE * 8))
+#define KGSL_MEMSTORE_SIZE	((int)(PAGE_SIZE * 2))
 #define KGSL_MEMSTORE_GLOBAL	(0)
 #define KGSL_PRIORITY_MAX_RB_LEVELS 4
 #define KGSL_MEMSTORE_MAX	(KGSL_MEMSTORE_SIZE / \
 	sizeof(struct kgsl_devmemstore) - 1 - KGSL_PRIORITY_MAX_RB_LEVELS)
-#define KGSL_MAX_CONTEXTS_PER_PROC 200
-
-#define MEMSTORE_RB_OFFSET(rb, field)	\
-	KGSL_MEMSTORE_OFFSET(((rb)->id + KGSL_MEMSTORE_MAX), field)
-
-#define MEMSTORE_ID_GPU_ADDR(dev, iter, field) \
-	((dev)->memstore.gpuaddr + KGSL_MEMSTORE_OFFSET(iter, field))
-
-#define MEMSTORE_RB_GPU_ADDR(dev, rb, field)	\
-	((dev)->memstore.gpuaddr + \
-	 KGSL_MEMSTORE_OFFSET(((rb)->id + KGSL_MEMSTORE_MAX), field))
-
-/*
- * SCRATCH MEMORY: The scratch memory is one page worth of data that
- * is mapped into the GPU. This allows for some 'shared' data between
- * the GPU and CPU. For example, it will be used by the GPU to write
- * each updated RPTR for each RB.
- *
- * Used Data:
- * Offset: Length(bytes): What
- * 0x0: 4 * KGSL_PRIORITY_MAX_RB_LEVELS: RB0 RPTR
- * 0x10: 8 * KGSL_PRIORITY_MAX_RB_LEVELS: RB0 CTXT RESTORE ADDR
- */
-
-/* Shadow global helpers */
-#define SCRATCH_RPTR_OFFSET(id) ((id) * sizeof(unsigned int))
-#define SCRATCH_RPTR_GPU_ADDR(dev, id) \
-	((dev)->scratch.gpuaddr + SCRATCH_RPTR_OFFSET(id))
-
-#define SCRATCH_PREEMPTION_CTXT_RESTORE_ADDR_OFFSET(id) \
-	(SCRATCH_RPTR_OFFSET(KGSL_PRIORITY_MAX_RB_LEVELS) + \
-	((id) * sizeof(uint64_t)))
-#define SCRATCH_PREEMPTION_CTXT_RESTORE_GPU_ADDR(dev, id) \
-	((dev)->scratch.gpuaddr + \
-	SCRATCH_PREEMPTION_CTXT_RESTORE_ADDR_OFFSET(id))
 
 /* Timestamp window used to detect rollovers (half of integer range) */
 #define KGSL_TIMESTAMP_WINDOW 0x80000000
 
-/*
- * A macro for memory statistics - add the new size to the stat and if
- * the statisic is greater then _max, set _max
- */
+/* A macro for memory statistics - add the new size to the stat and if
+   the statisic is greater then _max, set _max
+*/
+
 static inline void KGSL_STATS_ADD(uint64_t size, atomic_long_t *stat,
 		atomic_long_t *max)
 {
@@ -110,7 +55,6 @@ static inline void KGSL_STATS_ADD(uint64_t size, atomic_long_t *stat,
 
 #define KGSL_MAX_NUMIBS 100000
 #define KGSL_MAX_SYNCPOINTS 32
-#define KGSL_MAX_SPARSE 1000
 
 struct kgsl_device;
 struct kgsl_context;
@@ -195,10 +139,6 @@ struct kgsl_memdesc_ops {
 #define KGSL_MEMDESC_PRIVILEGED BIT(6)
 /* The memdesc is TZ locked content protection */
 #define KGSL_MEMDESC_TZ_LOCKED BIT(7)
-/* The memdesc is allocated through contiguous memory */
-#define KGSL_MEMDESC_CONTIG BIT(8)
-/* For global buffers, randomly assign an address from the region */
-#define KGSL_MEMDESC_RANDOM BIT(9)
 
 /**
  * struct kgsl_memdesc - GPU memory object descriptor
@@ -218,7 +158,6 @@ struct kgsl_memdesc_ops {
  * @attrs: dma attributes for this memory
  * @pages: An array of pointers to allocated pages
  * @page_count: Total number of pages allocated
- * @cur_bindings: Number of sparse pages actively bound
  */
 struct kgsl_memdesc {
 	struct kgsl_pagetable *pagetable;
@@ -237,7 +176,6 @@ struct kgsl_memdesc {
 	unsigned long attrs;
 	struct page **pages;
 	unsigned int page_count;
-	unsigned int cur_bindings;
 };
 
 /*
@@ -271,8 +209,6 @@ struct kgsl_memdesc {
  * @dev_priv: back pointer to the device file that created this entry.
  * @metadata: String containing user specified metadata for the entry
  * @work: Work struct used to schedule a kgsl_mem_entry_put in atomic contexts
- * @bind_lock: Lock for sparse memory bindings
- * @bind_tree: RB Tree for sparse memory bindings
  */
 struct kgsl_mem_entry {
 	struct kref refcount;
@@ -284,8 +220,6 @@ struct kgsl_mem_entry {
 	int pending_free;
 	char metadata[KGSL_GPUOBJ_ALLOC_METADATA_MAX + 1];
 	struct work_struct work;
-	spinlock_t bind_lock;
-	struct rb_root bind_tree;
 };
 
 struct kgsl_device_private;
@@ -355,24 +289,6 @@ struct kgsl_protected_registers {
 	int range;
 };
 
-/**
- * struct sparse_bind_object - Bind metadata
- * @node: Node for the rb tree
- * @p_memdesc: Physical memdesc bound to
- * @v_off: Offset of bind in the virtual entry
- * @p_off: Offset of bind in the physical memdesc
- * @size: Size of the bind
- * @flags: Flags for the bind
- */
-struct sparse_bind_object {
-	struct rb_node node;
-	struct kgsl_memdesc *p_memdesc;
-	uint64_t v_off;
-	uint64_t p_off;
-	uint64_t size;
-	uint64_t flags;
-};
-
 long kgsl_ioctl_device_getproperty(struct kgsl_device_private *dev_priv,
 					  unsigned int cmd, void *data);
 long kgsl_ioctl_device_setproperty(struct kgsl_device_private *dev_priv,
@@ -412,8 +328,14 @@ long kgsl_ioctl_gpumem_alloc_id(struct kgsl_device_private *dev_priv,
 					unsigned int cmd, void *data);
 long kgsl_ioctl_gpumem_get_info(struct kgsl_device_private *dev_priv,
 					unsigned int cmd, void *data);
+long kgsl_ioctl_cff_syncmem(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data);
+long kgsl_ioctl_cff_user_event(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data);
 long kgsl_ioctl_timestamp_event(struct kgsl_device_private *dev_priv,
 					unsigned int cmd, void *data);
+long kgsl_ioctl_cff_sync_gpuobj(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data);
 long kgsl_ioctl_gpuobj_alloc(struct kgsl_device_private *dev_priv,
 					unsigned int cmd, void *data);
 long kgsl_ioctl_gpuobj_free(struct kgsl_device_private *dev_priv,
@@ -429,26 +351,8 @@ long kgsl_ioctl_gpu_command(struct kgsl_device_private *dev_priv,
 long kgsl_ioctl_gpuobj_set_info(struct kgsl_device_private *dev_priv,
 				unsigned int cmd, void *data);
 
-long kgsl_ioctl_sparse_phys_alloc(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_sparse_phys_free(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_sparse_virt_alloc(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_sparse_virt_free(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_sparse_bind(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_sparse_unbind(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_gpu_sparse_command(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-
 void kgsl_mem_entry_destroy(struct kref *kref);
 
-void kgsl_get_egl_counts(struct kgsl_mem_entry *entry,
-			int *egl_surface_count, int *egl_image_count);
-
 struct kgsl_mem_entry * __must_check
 kgsl_sharedmem_find(struct kgsl_process_private *private, uint64_t gpuaddr);
 
@@ -460,10 +364,6 @@ extern const struct dev_pm_ops kgsl_pm_ops;
 int kgsl_suspend_driver(struct platform_device *pdev, pm_message_t state);
 int kgsl_resume_driver(struct platform_device *pdev);
 
-struct kgsl_mem_entry *gpumem_alloc_entry(struct kgsl_device_private *dev_priv,
-				uint64_t size, uint64_t flags);
-long gpumem_free_entry(struct kgsl_mem_entry *entry);
-
 static inline int kgsl_gpuaddr_in_memdesc(const struct kgsl_memdesc *memdesc,
 				uint64_t gpuaddr, uint64_t size)
 {
@@ -552,6 +452,21 @@ kgsl_mem_entry_put(struct kgsl_mem_entry *entry)
 		kref_put(&entry->refcount, kgsl_mem_entry_destroy);
 }
 
+/**
+ * kgsl_mem_entry_put_deferred() - Schedule a task to put the memory entry
+ * @entry: Mem entry to put
+ *
+ * This function is for atomic contexts where a normal kgsl_mem_entry_put()
+ * would result in the memory entry getting destroyed and possibly taking
+ * mutexes along the way.  Schedule the work to happen outside of the atomic
+ * context.
+ */
+static inline void kgsl_mem_entry_put_deferred(struct kgsl_mem_entry *entry)
+{
+	if (entry != NULL)
+		queue_work(kgsl_driver.mem_workqueue, &entry->work);
+}
+
 /*
  * kgsl_addr_range_overlap() - Checks if 2 ranges overlap
  * @gpuaddr1: Start of first address range
@@ -616,24 +531,4 @@ static inline void __user *to_user_ptr(uint64_t address)
 	return (void __user *)(uintptr_t)address;
 }
 
-static inline void kgsl_gpu_sysfs_add_link(struct kobject *dst,
-			struct kobject *src, const char *src_name,
-			const char *dst_name)
-{
-	struct kernfs_node *old;
-
-	if (dst == NULL || src == NULL)
-		return;
-
-	old = sysfs_get_dirent(src->sd, src_name);
-	if (IS_ERR_OR_NULL(old))
-		return;
-
-	kernfs_create_link(dst->sd, dst_name, old);
-}
-
-static inline bool kgsl_is_compat_task(void)
-{
-	return (BITS_PER_LONG == 32) || is_compat_task();
-}
 #endif /* __KGSL_H */
diff --git a/drivers/gpu/msm/kgsl_cffdump.c b/drivers/gpu/msm/kgsl_cffdump.c
new file mode 100644
index 000000000000..67e3d02edb86
--- /dev/null
+++ b/drivers/gpu/msm/kgsl_cffdump.c
@@ -0,0 +1,740 @@
+/* Copyright (c) 2010-2016, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+/* #define DEBUG */
+#define ALIGN_CPU
+
+#include <linux/spinlock.h>
+#include <linux/debugfs.h>
+#include <linux/relay.h>
+#include <linux/slab.h>
+#include <linux/time.h>
+#include <linux/sched.h>
+
+#include "kgsl.h"
+#include "kgsl_cffdump.h"
+#include "kgsl_debugfs.h"
+#include "kgsl_log.h"
+#include "kgsl_sharedmem.h"
+#include "adreno_pm4types.h"
+#include "adreno.h"
+#include "adreno_cp_parser.h"
+
+static struct rchan	*chan;
+static struct dentry	*dir;
+static int		suspended;
+static size_t		dropped;
+static size_t		subbuf_size = 256*1024;
+static size_t		n_subbufs = 64;
+
+/* forward declarations */
+static void destroy_channel(void);
+static struct rchan *create_channel(unsigned subbuf_size, unsigned n_subbufs);
+
+static spinlock_t cffdump_lock;
+static ulong serial_nr;
+static ulong total_bytes;
+static ulong total_syncmem;
+static long last_sec;
+
+/* Some simulators have start address of gmem at this offset */
+#define KGSL_CFF_GMEM_OFFSET	0x100000
+
+#define MEMBUF_SIZE	64
+
+#define CFF_OP_WRITE_REG        0x00000002
+struct cff_op_write_reg {
+	unsigned char op;
+	uint addr;
+	uint value;
+} __packed;
+
+#define CFF_OP_POLL_REG         0x00000004
+struct cff_op_poll_reg {
+	unsigned char op;
+	uint addr;
+	uint value;
+	uint mask;
+} __packed;
+
+#define CFF_OP_WAIT_IRQ         0x00000005
+struct cff_op_wait_irq {
+	unsigned char op;
+} __packed;
+
+#define CFF_OP_RMW              0x0000000a
+
+struct cff_op_write_mem {
+	unsigned char op;
+	uint addr;
+	uint value;
+} __packed;
+
+#define CFF_OP_WRITE_MEMBUF     0x0000000c
+struct cff_op_write_membuf {
+	unsigned char op;
+	uint addr;
+	ushort count;
+	uint buffer[MEMBUF_SIZE];
+} __packed;
+
+#define CFF_OP_MEMORY_BASE	0x0000000d
+struct cff_op_memory_base {
+	unsigned char op;
+	uint base;
+	uint size;
+	uint gmemsize;
+} __packed;
+
+#define CFF_OP_HANG		0x0000000e
+struct cff_op_hang {
+	unsigned char op;
+} __packed;
+
+#define CFF_OP_EOF              0xffffffff
+struct cff_op_eof {
+	unsigned char op;
+} __packed;
+
+#define CFF_OP_VERIFY_MEM_FILE  0x00000007
+#define CFF_OP_WRITE_SURFACE_PARAMS 0x00000011
+struct cff_op_user_event {
+	unsigned char op;
+	unsigned int op1;
+	unsigned int op2;
+	unsigned int op3;
+	unsigned int op4;
+	unsigned int op5;
+} __packed;
+
+
+static void b64_encodeblock(unsigned char in[3], unsigned char out[4], int len)
+{
+	static const char tob64[] = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmno"
+		"pqrstuvwxyz0123456789+/";
+
+	out[0] = tob64[in[0] >> 2];
+	out[1] = tob64[((in[0] & 0x03) << 4) | ((in[1] & 0xf0) >> 4)];
+	out[2] = (unsigned char) (len > 1 ? tob64[((in[1] & 0x0f) << 2)
+		| ((in[2] & 0xc0) >> 6)] : '=');
+	out[3] = (unsigned char) (len > 2 ? tob64[in[2] & 0x3f] : '=');
+}
+
+static void b64_encode(const unsigned char *in_buf, int in_size,
+	unsigned char *out_buf, int out_bufsize, int *out_size)
+{
+	unsigned char in[3], out[4];
+	int i, len;
+
+	*out_size = 0;
+	while (in_size > 0) {
+		len = 0;
+		for (i = 0; i < 3; ++i) {
+			if (in_size-- > 0) {
+				in[i] = *in_buf++;
+				++len;
+			} else
+				in[i] = 0;
+		}
+		if (len) {
+			b64_encodeblock(in, out, len);
+			if (out_bufsize < 4) {
+				pr_warn("kgsl: cffdump: %s: out of buffer\n",
+					__func__);
+				return;
+			}
+			for (i = 0; i < 4; ++i)
+				*out_buf++ = out[i];
+			*out_size += 4;
+			out_bufsize -= 4;
+		}
+	}
+}
+
+#define KLOG_TMPBUF_SIZE (1024)
+static void klog_printk(const char *fmt, ...)
+{
+	/* per-cpu klog formatting temporary buffer */
+	static char klog_buf[NR_CPUS][KLOG_TMPBUF_SIZE];
+
+	va_list args;
+	int len;
+	char *cbuf;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	cbuf = klog_buf[smp_processor_id()];
+	va_start(args, fmt);
+	len = vsnprintf(cbuf, KLOG_TMPBUF_SIZE, fmt, args);
+	total_bytes += len;
+	va_end(args);
+	relay_write(chan, cbuf, len);
+	local_irq_restore(flags);
+}
+
+static struct cff_op_write_membuf cff_op_write_membuf;
+static void cffdump_membuf(int id, unsigned char *out_buf, int out_bufsize)
+{
+	void *data;
+	int len, out_size;
+	struct cff_op_write_mem cff_op_write_mem;
+
+	uint addr = cff_op_write_membuf.addr
+		- sizeof(uint)*cff_op_write_membuf.count;
+
+	if (!cff_op_write_membuf.count) {
+		pr_warn("kgsl: cffdump: membuf: count == 0, skipping");
+		return;
+	}
+
+	if (cff_op_write_membuf.count != 1) {
+		cff_op_write_membuf.op = CFF_OP_WRITE_MEMBUF;
+		cff_op_write_membuf.addr = addr;
+		len = sizeof(cff_op_write_membuf) -
+			sizeof(uint)*(MEMBUF_SIZE - cff_op_write_membuf.count);
+		data = &cff_op_write_membuf;
+	} else {
+		cff_op_write_mem.op = CFF_OP_WRITE_MEM;
+		cff_op_write_mem.addr = addr;
+		cff_op_write_mem.value = cff_op_write_membuf.buffer[0];
+		data = &cff_op_write_mem;
+		len = sizeof(cff_op_write_mem);
+	}
+	b64_encode(data, len, out_buf, out_bufsize, &out_size);
+	out_buf[out_size] = 0;
+	klog_printk("%ld:%d;%s\n", ++serial_nr, id, out_buf);
+	cff_op_write_membuf.count = 0;
+	cff_op_write_membuf.addr = 0;
+}
+
+void kgsl_cffdump_printline(int id, uint opcode, uint op1, uint op2,
+	uint op3, uint op4, uint op5)
+{
+	struct cff_op_write_reg cff_op_write_reg;
+	struct cff_op_poll_reg cff_op_poll_reg;
+	struct cff_op_wait_irq cff_op_wait_irq;
+	struct cff_op_memory_base cff_op_memory_base;
+	struct cff_op_hang cff_op_hang;
+	struct cff_op_eof cff_op_eof;
+	struct cff_op_user_event cff_op_user_event;
+	unsigned char out_buf[sizeof(cff_op_write_membuf)/3*4 + 16];
+	void *data;
+	int len = 0, out_size;
+	long cur_secs;
+
+	spin_lock(&cffdump_lock);
+	if (opcode == CFF_OP_WRITE_MEM) {
+		if ((cff_op_write_membuf.addr != op1 &&
+			cff_op_write_membuf.count)
+			|| (cff_op_write_membuf.count == MEMBUF_SIZE))
+			cffdump_membuf(id, out_buf, sizeof(out_buf));
+
+		cff_op_write_membuf.buffer[cff_op_write_membuf.count++] = op2;
+		cff_op_write_membuf.addr = op1 + sizeof(uint);
+		spin_unlock(&cffdump_lock);
+		return;
+	} else if (cff_op_write_membuf.count)
+		cffdump_membuf(id, out_buf, sizeof(out_buf));
+	spin_unlock(&cffdump_lock);
+
+	switch (opcode) {
+	case CFF_OP_WRITE_REG:
+		cff_op_write_reg.op = opcode;
+		cff_op_write_reg.addr = op1;
+		cff_op_write_reg.value = op2;
+		data = &cff_op_write_reg;
+		len = sizeof(cff_op_write_reg);
+		break;
+
+	case CFF_OP_POLL_REG:
+		cff_op_poll_reg.op = opcode;
+		cff_op_poll_reg.addr = op1;
+		cff_op_poll_reg.value = op2;
+		cff_op_poll_reg.mask = op3;
+		data = &cff_op_poll_reg;
+		len = sizeof(cff_op_poll_reg);
+		break;
+
+	case CFF_OP_WAIT_IRQ:
+		cff_op_wait_irq.op = opcode;
+		data = &cff_op_wait_irq;
+		len = sizeof(cff_op_wait_irq);
+		break;
+
+	case CFF_OP_MEMORY_BASE:
+		cff_op_memory_base.op = opcode;
+		cff_op_memory_base.base = op1;
+		cff_op_memory_base.size = op2;
+		cff_op_memory_base.gmemsize = op3;
+		data = &cff_op_memory_base;
+		len = sizeof(cff_op_memory_base);
+		break;
+
+	case CFF_OP_HANG:
+		cff_op_hang.op = opcode;
+		data = &cff_op_hang;
+		len = sizeof(cff_op_hang);
+		break;
+
+	case CFF_OP_EOF:
+		cff_op_eof.op = opcode;
+		data = &cff_op_eof;
+		len = sizeof(cff_op_eof);
+		break;
+
+	case CFF_OP_WRITE_SURFACE_PARAMS:
+	case CFF_OP_VERIFY_MEM_FILE:
+		cff_op_user_event.op = opcode;
+		cff_op_user_event.op1 = op1;
+		cff_op_user_event.op2 = op2;
+		cff_op_user_event.op3 = op3;
+		cff_op_user_event.op4 = op4;
+		cff_op_user_event.op5 = op5;
+		data = &cff_op_user_event;
+		len = sizeof(cff_op_user_event);
+		break;
+	}
+
+	if (len) {
+		b64_encode(data, len, out_buf, sizeof(out_buf), &out_size);
+		out_buf[out_size] = 0;
+		klog_printk("%ld:%d;%s\n", ++serial_nr, id, out_buf);
+	} else
+		pr_warn("kgsl: cffdump: unhandled opcode: %d\n", opcode);
+
+	cur_secs = get_seconds();
+	if ((cur_secs - last_sec) > 10 || (last_sec - cur_secs) > 10) {
+		pr_info("kgsl: cffdump: total [bytes:%lu kB, syncmem:%lu kB], "
+			"seq#: %lu\n", total_bytes/1024, total_syncmem/1024,
+			serial_nr);
+		last_sec = cur_secs;
+	}
+}
+EXPORT_SYMBOL(kgsl_cffdump_printline);
+
+void kgsl_cffdump_init()
+{
+	struct dentry *debugfs_dir = kgsl_get_debugfs_dir();
+
+#ifdef ALIGN_CPU
+	cpumask_t mask;
+
+	cpumask_clear(&mask);
+	cpumask_set_cpu(0, &mask);
+	sched_setaffinity(0, &mask);
+#endif
+	if (!debugfs_dir || IS_ERR(debugfs_dir)) {
+		KGSL_CORE_ERR("Debugfs directory is bad\n");
+		return;
+	}
+
+	spin_lock_init(&cffdump_lock);
+
+	dir = debugfs_create_dir("cff", debugfs_dir);
+	if (!dir) {
+		KGSL_CORE_ERR("debugfs_create_dir failed\n");
+		return;
+	}
+
+	chan = create_channel(subbuf_size, n_subbufs);
+}
+
+void kgsl_cffdump_destroy()
+{
+	if (chan)
+		relay_flush(chan);
+	destroy_channel();
+	if (dir)
+		debugfs_remove(dir);
+}
+
+void kgsl_cffdump_open(struct kgsl_device *device)
+{
+	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+	if (!device->cff_dump_enable)
+		return;
+
+	/* Set the maximum possible address range */
+	kgsl_cffdump_memory_base(device,
+				adreno_dev->gmem_size + KGSL_CFF_GMEM_OFFSET,
+				0xFFFFFFFF -
+				(adreno_dev->gmem_size + KGSL_CFF_GMEM_OFFSET),
+				adreno_dev->gmem_size);
+}
+
+void kgsl_cffdump_memory_base(struct kgsl_device *device, unsigned int base,
+			      unsigned int range, unsigned gmemsize)
+{
+	if (!device->cff_dump_enable)
+		return;
+	kgsl_cffdump_printline(device->id, CFF_OP_MEMORY_BASE, base,
+			range, gmemsize, 0, 0);
+}
+
+void kgsl_cffdump_hang(struct kgsl_device *device)
+{
+	if (!device->cff_dump_enable)
+		return;
+	kgsl_cffdump_printline(device->id, CFF_OP_HANG, 0, 0, 0, 0, 0);
+}
+
+void kgsl_cffdump_close(struct kgsl_device *device)
+{
+	if (!device->cff_dump_enable)
+		return;
+	kgsl_cffdump_printline(device->id, CFF_OP_EOF, 0, 0, 0, 0, 0);
+}
+
+void kgsl_cffdump_user_event(struct kgsl_device *device,
+		unsigned int cff_opcode, unsigned int op1,
+		unsigned int op2, unsigned int op3,
+		unsigned int op4, unsigned int op5)
+{
+	if (!device->cff_dump_enable)
+		return;
+	kgsl_cffdump_printline(-1, cff_opcode, op1, op2, op3, op4, op5);
+}
+
+
+
+void kgsl_cffdump_memcpy(struct kgsl_device *device,
+		uint64_t gpuaddr, unsigned int *ptr, uint64_t sizebytes)
+{
+	int i;
+
+	if (!device || !device->cff_dump_enable)
+		return;
+
+	for (i = 0; i < ALIGN(sizebytes, 4) / 4; gpuaddr += 4, ptr++, i++)
+		kgsl_cffdump_write(device, gpuaddr, *ptr);
+}
+
+void kgsl_cffdump_syncmem(struct kgsl_device *device,
+		struct kgsl_mem_entry *entry, uint64_t offset,
+		uint64_t sizebytes, bool clean_cache)
+{
+	void *src;
+
+	if (!device || device->cff_dump_enable || !entry)
+		return;
+
+	if (sizebytes == 0)
+		return;
+
+	if ((offset >= entry->memdesc.size) ||
+		(entry->memdesc.size - len) > offset)
+		return;
+
+	total_syncmem += sizebytes;
+
+	src = kgsl_memdesc_map(&entry->memdesc);
+	if (src == NULL) {
+		KGSL_CORE_ERR(
+			"cffdump: no kernel mapping for GPU address 0x%llX\n",
+			gpuaddr);
+		return;
+	}
+
+	if (clean_cache) {
+		/* Makes sure that the region is freshly fetched */
+		mb();
+
+		kgsl_cache_range_op(entry->memdesc,
+			offset, sizebytes, KGSL_CACHE_OP_INV);
+	}
+
+	kgsl_cffdump_memcpy(device, entry->memdesc.gpuaddr + offset,
+			src + offset, sizebytes);
+
+	kgsl_memdesc_unmap(&entry->memdesc);
+}
+
+void kgsl_cffdump_memset(struct kgsl_device *device,
+		uint64_t gpuaddr, unsigned char ch, uint64_t sizebytes)
+{
+	int i;
+
+	if (!device || !device->cff_dump_enable)
+		return;
+
+	/* Expand the input char into a dword and output it */
+	for (i = 0; i < ALIGN(sizebytes, 4) / 4; gpuaddr += 4, i++)
+		kgsl_cffdump_write(device, gpuaddr,
+			(ch << 24) | (ch << 16) | (ch << 8) | ch);
+}
+
+void kgsl_cffdump_regwrite(struct kgsl_device *device, uint addr,
+	uint value)
+{
+	if (!device->cff_dump_enable)
+		return;
+
+	kgsl_cffdump_printline(device->id, CFF_OP_WRITE_REG, addr, value,
+			0, 0, 0);
+}
+
+void kgsl_cffdump_regpoll(struct kgsl_device *device, uint addr,
+	uint value, uint mask)
+{
+	if (!device->cff_dump_enable)
+		return;
+
+	kgsl_cffdump_printline(device->id, CFF_OP_POLL_REG, addr, value,
+			mask, 0, 0);
+}
+
+void kgsl_cffdump_slavewrite(struct kgsl_device *device, uint addr, uint value)
+{
+	if (!device->cff_dump_enable)
+		return;
+
+	kgsl_cffdump_printline(-1, CFF_OP_WRITE_REG, addr, value, 0, 0, 0);
+}
+
+int kgsl_cffdump_waitirq(struct kgsl_device *device)
+{
+	if (!device->cff_dump_enable)
+		return 0;
+
+	kgsl_cffdump_printline(-1, CFF_OP_WAIT_IRQ, 0, 0, 0, 0, 0);
+
+	return 1;
+}
+EXPORT_SYMBOL(kgsl_cffdump_waitirq);
+
+static int subbuf_start_handler(struct rchan_buf *buf,
+	void *subbuf, void *prev_subbuf, size_t prev_padding)
+{
+	if (relay_buf_full(buf)) {
+		if (!suspended) {
+			suspended = 1;
+			pr_warn("kgsl: cffdump: relay: cpu %d buffer full!!!\n",
+				smp_processor_id());
+		}
+		dropped++;
+		return 0;
+	} else if (suspended) {
+		suspended = 0;
+		pr_warn("kgsl: cffdump: relay: cpu %d buffer no longer full.\n",
+			smp_processor_id());
+	}
+
+	subbuf_start_reserve(buf, 0);
+	return 1;
+}
+
+static struct dentry *create_buf_file_handler(const char *filename,
+	struct dentry *parent, unsigned short mode, struct rchan_buf *buf,
+	int *is_global)
+{
+	return debugfs_create_file(filename, mode, parent, buf,
+				       &relay_file_operations);
+}
+
+/*
+ * file_remove() default callback.  Removes relay file in debugfs.
+ */
+static int remove_buf_file_handler(struct dentry *dentry)
+{
+	pr_info("kgsl: cffdump: %s()\n", __func__);
+	debugfs_remove(dentry);
+	return 0;
+}
+
+/*
+ * relay callbacks
+ */
+static struct rchan_callbacks relay_callbacks = {
+	.subbuf_start = subbuf_start_handler,
+	.create_buf_file = create_buf_file_handler,
+	.remove_buf_file = remove_buf_file_handler,
+};
+
+/**
+ *	create_channel - creates channel /debug/klog/cpuXXX
+ *
+ *	Creates channel along with associated produced/consumed control files
+ *
+ *	Returns channel on success, NULL otherwise
+ */
+static struct rchan *create_channel(unsigned subbuf_size, unsigned n_subbufs)
+{
+	struct rchan *chan;
+
+	chan = relay_open("cpu", dir, subbuf_size,
+			  n_subbufs, &relay_callbacks, NULL);
+	if (!chan) {
+		KGSL_CORE_ERR("relay_open failed\n");
+		return NULL;
+	}
+
+	suspended = 0;
+	dropped = 0;
+
+	return chan;
+}
+
+/**
+ *	destroy_channel - destroys channel /debug/kgsl/cff/cpuXXX
+ *
+ *	Destroys channel along with associated produced/consumed control files
+ */
+static void destroy_channel(void)
+{
+	pr_info("kgsl: cffdump: relay: destroy_channel\n");
+	if (chan) {
+		relay_close(chan);
+		chan = NULL;
+	}
+}
+
+int kgsl_cff_dump_enable_set(void *data, u64 val)
+{
+	int ret = 0;
+	struct kgsl_device *device = (struct kgsl_device *)data;
+	int i;
+
+	mutex_lock(&kgsl_driver.devlock);
+	if (val) {
+		/* Check if CFF is on for some other device already */
+		for (i = 0; i < KGSL_DEVICE_MAX; i++) {
+			if (kgsl_driver.devp[i]) {
+				struct kgsl_device *device_temp =
+						kgsl_driver.devp[i];
+				if (device_temp->cff_dump_enable &&
+					device != device_temp) {
+					KGSL_CORE_ERR(
+					"CFF is on for another device %d\n",
+					device_temp->id);
+					ret = -EINVAL;
+					goto done;
+				}
+			}
+		}
+		if (!device->cff_dump_enable) {
+			device->cff_dump_enable = 1;
+			/*
+			 * force device to slumber so that we ensure that the
+			 * start opcode in CFF is present
+			 */
+			mutex_lock(&device->mutex);
+			ret = kgsl_pwrctrl_change_state(device,
+				KGSL_STATE_SUSPEND);
+			ret |= kgsl_pwrctrl_change_state(device,
+				KGSL_STATE_SLUMBER);
+			if (ret)
+				device->cff_dump_enable = 0;
+			mutex_unlock(&device->mutex);
+		}
+	} else if (device->cff_dump_enable && !val) {
+		device->cff_dump_enable = 0;
+	}
+done:
+	mutex_unlock(&kgsl_driver.devlock);
+	return ret;
+}
+EXPORT_SYMBOL(kgsl_cff_dump_enable_set);
+
+int kgsl_cff_dump_enable_get(void *data, u64 *val)
+{
+	struct kgsl_device *device = (struct kgsl_device *)data;
+	*val = device->cff_dump_enable;
+	return 0;
+}
+EXPORT_SYMBOL(kgsl_cff_dump_enable_get);
+
+/*
+ * kgsl_cffdump_capture_adreno_ib_cff() - Capture CFF for an IB
+ * @device: Device for which CFF is to be captured
+ * @ptbase: The pagetable in which the IB is mapped
+ * @gpuaddr: Address of IB
+ * @dwords: Size of the IB
+ *
+ * Dumps the CFF format of the IB including all objects in it like, IB2,
+ * shaders, etc.
+ *
+ * Returns 0 on success else error code
+ */
+static int kgsl_cffdump_capture_adreno_ib_cff(struct kgsl_device *device,
+				struct kgsl_process_private *process,
+				uint64_t gpuaddr, uint64_t dwords)
+{
+	int ret;
+	struct adreno_ib_object_list *ib_obj_list;
+	struct adreno_ib_object *ib_obj;
+	int i;
+
+	if (!device->cff_dump_enable)
+		return 0;
+
+	ret = adreno_ib_create_object_list(device, process, gpuaddr, dwords,
+		&ib_obj_list);
+
+	if (ret) {
+		KGSL_DRV_ERR(device,
+		"Fail to create object list for IB 0x%016llX, size(dwords) 0x%llX\n",
+		gpuaddr, dwords);
+		return ret;
+	}
+
+	for (i = 0; i < ib_obj_list->num_objs; i++) {
+		ib_obj = &(ib_obj_list->obj_list[i]);
+		kgsl_cffdump_syncmem(device, ib_obj->entry, 0, ib_obj->size,
+			false);
+	}
+	adreno_ib_destroy_obj_list(ib_obj_list);
+	return 0;
+}
+
+/*
+ * kgsl_cffdump_capture_ib_desc() - Capture CFF for a list of IB's
+ * @device: Device for which CFF is to be captured
+ * @context: The context under which the IB list executes on device
+ * @ibdesc: The IB list
+ * @numibs: Number of IB's in ibdesc
+ *
+ * Returns 0 on success else error code
+ */
+int kgsl_cffdump_capture_ib_desc(struct kgsl_device *device,
+				struct kgsl_context *context,
+				struct kgsl_cmdbatch *cmdbatch)
+{
+	int ret = 0;
+	struct kgsl_memobj_node *ib;
+
+	if (!device->cff_dump_enable)
+		return 0;
+	/* Dump CFF for IB and all objects in it */
+	list_for_each_entry(ib, &cmdbatch->cmdlist, node) {
+		ret = kgsl_cffdump_capture_adreno_ib_cff(
+			device, context->proc_priv, ib->gpuaddr,
+			ib->size >> 2);
+		if (ret) {
+			KGSL_DRV_ERR(device,
+			"Fail cff capture, IB 0x%016llX, size 0x%llX\n",
+			ib->gpuaddr, ib->size);
+			break;
+		}
+	}
+	return ret;
+}
+EXPORT_SYMBOL(kgsl_cffdump_capture_ib_desc);
+
+DEFINE_SIMPLE_ATTRIBUTE(kgsl_cff_dump_enable_fops, kgsl_cff_dump_enable_get,
+			kgsl_cff_dump_enable_set, "%llu\n");
+
+void kgsl_cffdump_debugfs_create(struct kgsl_device *device)
+{
+	debugfs_create_file("cff_dump", 0644, device->d_debugfs, device,
+			    &kgsl_cff_dump_enable_fops);
+}
diff --git a/drivers/gpu/msm/kgsl_cffdump.h b/drivers/gpu/msm/kgsl_cffdump.h
new file mode 100644
index 000000000000..5eb04e7ea500
--- /dev/null
+++ b/drivers/gpu/msm/kgsl_cffdump.h
@@ -0,0 +1,183 @@
+/* Copyright (c) 2010-2011,2013-2015, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef __KGSL_CFFDUMP_H
+#define __KGSL_CFFDUMP_H
+
+#include <linux/types.h>
+#include "kgsl_device.h"
+
+extern unsigned int kgsl_cff_dump_enable;
+
+static inline bool kgsl_cffdump_flags_no_memzero(void) { return true; }
+
+struct kgsl_device_private;
+
+#ifdef CONFIG_MSM_KGSL_CFF_DUMP
+
+#define CFF_OP_WRITE_MEM        0x0000000b
+
+void kgsl_cffdump_init(void);
+void kgsl_cffdump_destroy(void);
+void kgsl_cffdump_open(struct kgsl_device *device);
+void kgsl_cffdump_close(struct kgsl_device *device);
+void kgsl_cffdump_memcpy(struct kgsl_device *device, uint64_t gpuaddr,
+		unsigned int *ptr, uint64_t sizebytes);
+void kgsl_cffdump_syncmem(struct kgsl_device *, struct kgsl_mem_entry *,
+	uint64_t offset, uint64_t sizebytes, bool clean_cache);
+void kgsl_cffdump_memset(struct kgsl_device *device, uint64_t addr,
+			unsigned char value, size_t sizebytes);
+void kgsl_cffdump_regwrite(struct kgsl_device *device, uint addr,
+	uint value);
+void kgsl_cffdump_regpoll(struct kgsl_device *device, uint addr,
+	uint value, uint mask);
+bool kgsl_cffdump_parse_ibs(struct kgsl_device_private *dev_priv,
+	const struct kgsl_memdesc *memdesc, uint64_t gpuaddr,
+	uint64_t sizedwords, bool check_only);
+void kgsl_cffdump_user_event(struct kgsl_device *device,
+		unsigned int cff_opcode, unsigned int op1,
+		unsigned int op2, unsigned int op3,
+		unsigned int op4, unsigned int op5);
+
+void kgsl_cffdump_memory_base(struct kgsl_device *device, unsigned int base,
+			      unsigned int range, unsigned int gmemsize);
+
+void kgsl_cffdump_hang(struct kgsl_device *device);
+void kgsl_cffdump_debugfs_create(struct kgsl_device *device);
+int kgsl_cff_dump_enable_set(void *data, u64 val);
+int kgsl_cff_dump_enable_get(void *data, u64 *val);
+int kgsl_cffdump_capture_ib_desc(struct kgsl_device *device,
+				struct kgsl_context *context,
+				struct kgsl_cmdbatch *cmdbatch);
+
+void kgsl_cffdump_printline(int id, uint opcode, uint op1, uint op2,
+	uint op3, uint op4, uint op5);
+
+static inline void kgsl_cffdump_write(struct kgsl_device *device,
+		uint64_t gpuaddr, unsigned int value)
+{
+	if (!device || !device->cff_dump_enable)
+		return;
+
+	kgsl_cffdump_printline(-1, CFF_OP_WRITE_MEM, gpuaddr, value, 0, 0, 0);
+}
+
+#else
+
+static inline void kgsl_cffdump_init(void)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_destroy(void)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_open(struct kgsl_device *device)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_close(struct kgsl_device *device)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_write(struct kgsl_device *device,
+		uint64_t gpuaddr, unsigned int value)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_memcpy(struct kgsl_device *device,
+		uint64_t gupaddr, unsigned int *ptr, uint64_t sizebytes)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_syncmem(struct kgsl_device *device,
+		struct kgsl_mem_entry *entry, uint64_t offset,
+		uint64_t sizebytes, bool clean_cache)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_memset(struct kgsl_device *device,
+		uint64_t addr, unsigned char ch, size_t sizebytes)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_regwrite(struct kgsl_device *device, uint addr,
+					 uint value)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_regpoll(struct kgsl_device *device, uint addr,
+		uint value, uint mask)
+{
+	return;
+}
+
+static inline bool kgsl_cffdump_parse_ibs(struct kgsl_device_private *dev_priv,
+	const struct kgsl_memdesc *memdesc, uint64_t gpuaddr,
+	uint64_t sizedwords, bool check_only)
+{
+	return false;
+}
+
+static inline void kgsl_cffdump_memory_base(struct kgsl_device *device,
+		unsigned int base, unsigned int range, unsigned int gmemsize)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_hang(struct kgsl_device *device)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_debugfs_create(struct kgsl_device *device)
+{
+	return;
+}
+
+static inline void kgsl_cffdump_user_event(struct kgsl_device *device,
+		unsigned int cff_opcode, unsigned int op1,
+		unsigned int op2, unsigned int op3,
+		unsigned int op4, unsigned int op5)
+{
+	return;
+}
+
+static inline int kgsl_cffdump_capture_ib_desc(struct kgsl_device *device,
+				struct kgsl_context *context,
+				struct kgsl_cmdbatch *cmdbatch)
+{
+	return 0;
+}
+
+static inline int kgsl_cff_dump_enable_set(void *data, u64 val)
+{
+	return -ENODEV;
+}
+
+static inline int kgsl_cff_dump_enable_get(void *data, u64 *val)
+{
+	return -ENODEV;
+}
+
+#endif /* CONFIG_MSM_KGSL_CFF_DUMP */
+#endif /* __KGSL_CFFDUMP_H */
diff --git a/drivers/gpu/msm/kgsl_cmdbatch.c b/drivers/gpu/msm/kgsl_cmdbatch.c
new file mode 100644
index 000000000000..10de4891fb4a
--- /dev/null
+++ b/drivers/gpu/msm/kgsl_cmdbatch.c
@@ -0,0 +1,985 @@
+/* Copyright (c) 2008-2016, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+/*
+ * KGSL command batch management
+ * A command batch is a single submission from userland.  The cmdbatch
+ * encapsulates everything about the submission : command buffers, flags and
+ * sync points.
+ *
+ * Sync points are events that need to expire before the
+ * cmdbatch can be queued to the hardware. All synpoints are contained in an
+ * array of kgsl_cmdbatch_sync_event structs in the command batch. There can be
+ * multiple types of events both internal ones (GPU events) and external
+ * triggers. As the events expire bits are cleared in a pending bitmap stored
+ * in the command batch. The GPU will submit the command as soon as the bitmap
+ * goes to zero indicating no more pending events.
+ */
+
+#include <linux/uaccess.h>
+#include <linux/list.h>
+#include <linux/compat.h>
+
+#include "kgsl.h"
+#include "kgsl_device.h"
+#include "kgsl_cmdbatch.h"
+#include "kgsl_sync.h"
+#include "kgsl_trace.h"
+#include "kgsl_compat.h"
+
+/*
+ * Define an kmem cache for the memobj structures since we allocate and free
+ * them so frequently
+ */
+static struct kmem_cache *memobjs_cache;
+
+/**
+ * kgsl_cmdbatch_put() - Decrement the refcount for a command batch object
+ * @cmdbatch: Pointer to the command batch object
+ */
+static inline void kgsl_cmdbatch_put(struct kgsl_cmdbatch *cmdbatch)
+{
+	if (cmdbatch)
+		kref_put(&cmdbatch->refcount, kgsl_cmdbatch_destroy_object);
+}
+
+void kgsl_dump_syncpoints(struct kgsl_device *device,
+	struct kgsl_cmdbatch *cmdbatch)
+{
+	struct kgsl_cmdbatch_sync_event *event;
+	unsigned int i;
+	unsigned long flags;
+
+	for (i = 0; i < cmdbatch->numsyncs; i++) {
+		event = &cmdbatch->synclist[i];
+
+		if (!kgsl_cmdbatch_event_pending(cmdbatch, i))
+			continue;
+
+		switch (event->type) {
+		case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP: {
+			unsigned int retired;
+
+			 kgsl_readtimestamp(event->device,
+				event->context, KGSL_TIMESTAMP_RETIRED,
+				&retired);
+
+			dev_err(device->dev,
+				"  [timestamp] context %d timestamp %d (retired %d)\n",
+				event->context->id, event->timestamp,
+				retired);
+			break;
+		}
+		case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
+			spin_lock_irqsave(&event->handle_lock, flags);
+
+			if (event->handle)
+				dev_err(device->dev, "  fence: [%pK] %s\n",
+					event->handle->fence,
+					event->handle->name);
+			else
+				dev_err(device->dev, "  fence: invalid\n");
+
+			spin_unlock_irqrestore(&event->handle_lock, flags);
+			break;
+		}
+	}
+}
+
+static void _kgsl_cmdbatch_timer(unsigned long data)
+{
+	struct kgsl_device *device;
+	struct kgsl_cmdbatch *cmdbatch = (struct kgsl_cmdbatch *) data;
+	struct kgsl_cmdbatch_sync_event *event;
+	unsigned int i;
+	unsigned long flags;
+
+	if (cmdbatch == NULL || cmdbatch->context == NULL)
+		return;
+
+	device = cmdbatch->context->device;
+
+	dev_err(device->dev,
+		"kgsl: possible gpu syncpoint deadlock for context %d timestamp %d\n",
+		cmdbatch->context->id, cmdbatch->timestamp);
+
+	set_bit(CMDBATCH_FLAG_FENCE_LOG, &cmdbatch->priv);
+	kgsl_context_dump(cmdbatch->context);
+	clear_bit(CMDBATCH_FLAG_FENCE_LOG, &cmdbatch->priv);
+
+	dev_err(device->dev, "      pending events:\n");
+
+	for (i = 0; i < cmdbatch->numsyncs; i++) {
+		event = &cmdbatch->synclist[i];
+
+		if (!kgsl_cmdbatch_event_pending(cmdbatch, i))
+			continue;
+
+		switch (event->type) {
+		case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP:
+			dev_err(device->dev, "       [%d] TIMESTAMP %d:%d\n",
+				i, event->context->id, event->timestamp);
+			break;
+		case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
+			spin_lock_irqsave(&event->handle_lock, flags);
+
+			if (event->handle != NULL) {
+				dev_err(device->dev, "       [%d] FENCE %s\n",
+				i, event->handle->fence ?
+					event->handle->fence->name : "NULL");
+				kgsl_sync_fence_log(event->handle->fence);
+			}
+
+			spin_unlock_irqrestore(&event->handle_lock, flags);
+			break;
+		}
+	}
+
+	dev_err(device->dev, "--gpu syncpoint deadlock print end--\n");
+}
+
+/**
+ * kgsl_cmdbatch_destroy_object() - Destroy a cmdbatch object
+ * @kref: Pointer to the kref structure for this object
+ *
+ * Actually destroy a command batch object.  Called from kgsl_cmdbatch_put
+ */
+void kgsl_cmdbatch_destroy_object(struct kref *kref)
+{
+	struct kgsl_cmdbatch *cmdbatch = container_of(kref,
+		struct kgsl_cmdbatch, refcount);
+
+	kgsl_context_put(cmdbatch->context);
+
+	kfree(cmdbatch->synclist);
+	kfree(cmdbatch);
+}
+EXPORT_SYMBOL(kgsl_cmdbatch_destroy_object);
+
+/*
+ * a generic function to retire a pending sync event and (possibly)
+ * kick the dispatcher
+ */
+static void kgsl_cmdbatch_sync_expire(struct kgsl_device *device,
+	struct kgsl_cmdbatch_sync_event *event)
+{
+	/*
+	 * Clear the event from the pending mask - if it is already clear, then
+	 * leave without doing anything useful
+	 */
+	if (!test_and_clear_bit(event->id, &event->cmdbatch->pending))
+		return;
+
+	/*
+	 * If no more pending events, delete the timer and schedule the command
+	 * for dispatch
+	 */
+	if (!kgsl_cmdbatch_events_pending(event->cmdbatch)) {
+		del_timer_sync(&event->cmdbatch->timer);
+
+		if (device->ftbl->drawctxt_sched)
+			device->ftbl->drawctxt_sched(device,
+				event->cmdbatch->context);
+	}
+}
+
+/*
+ * This function is called by the GPU event when the sync event timestamp
+ * expires
+ */
+static void kgsl_cmdbatch_sync_func(struct kgsl_device *device,
+		struct kgsl_event_group *group, void *priv, int result)
+{
+	struct kgsl_cmdbatch_sync_event *event = priv;
+
+	trace_syncpoint_timestamp_expire(event->cmdbatch,
+		event->context, event->timestamp);
+
+	kgsl_cmdbatch_sync_expire(device, event);
+	kgsl_context_put(event->context);
+	kgsl_cmdbatch_put(event->cmdbatch);
+}
+
+static inline void _free_memobj_list(struct list_head *list)
+{
+	struct kgsl_memobj_node *mem, *tmpmem;
+
+	/* Free the cmd mem here */
+	list_for_each_entry_safe(mem, tmpmem, list, node) {
+		list_del_init(&mem->node);
+		kmem_cache_free(memobjs_cache, mem);
+	}
+}
+
+/**
+ * kgsl_cmdbatch_destroy() - Destroy a cmdbatch structure
+ * @cmdbatch: Pointer to the command batch object to destroy
+ *
+ * Start the process of destroying a command batch.  Cancel any pending events
+ * and decrement the refcount.  Asynchronous events can still signal after
+ * kgsl_cmdbatch_destroy has returned.
+ */
+void kgsl_cmdbatch_destroy(struct kgsl_cmdbatch *cmdbatch)
+{
+	unsigned int i;
+	unsigned long pending, flags;
+
+	if (IS_ERR_OR_NULL(cmdbatch))
+		return;
+
+	/* Zap the canary timer */
+	del_timer_sync(&cmdbatch->timer);
+
+	/*
+	 * Copy off the pending list and clear all pending events - this will
+	 * render any subsequent asynchronous callback harmless
+	 */
+	bitmap_copy(&pending, &cmdbatch->pending, KGSL_MAX_SYNCPOINTS);
+	bitmap_zero(&cmdbatch->pending, KGSL_MAX_SYNCPOINTS);
+
+	/*
+	 * Clear all pending events - this will render any subsequent async
+	 * callbacks harmless
+	 */
+
+	for (i = 0; i < cmdbatch->numsyncs; i++) {
+		struct kgsl_cmdbatch_sync_event *event = &cmdbatch->synclist[i];
+
+		/* Don't do anything if the event has already expired */
+		if (!test_bit(i, &pending))
+			continue;
+
+		switch (event->type) {
+		case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP:
+			kgsl_cancel_event(cmdbatch->device,
+				&event->context->events, event->timestamp,
+				kgsl_cmdbatch_sync_func, event);
+			break;
+		case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
+			spin_lock_irqsave(&event->handle_lock, flags);
+
+			if (kgsl_sync_fence_async_cancel(event->handle)) {
+				event->handle = NULL;
+				spin_unlock_irqrestore(
+						&event->handle_lock, flags);
+				kgsl_cmdbatch_put(cmdbatch);
+			} else {
+				spin_unlock_irqrestore(
+						&event->handle_lock, flags);
+			}
+			break;
+		}
+	}
+
+	/*
+	 * Release the the refcount on the mem entry associated with the
+	 * cmdbatch profiling buffer
+	 */
+	if (cmdbatch->flags & KGSL_CMDBATCH_PROFILING)
+		kgsl_mem_entry_put(cmdbatch->profiling_buf_entry);
+
+	/* Destroy the cmdlist we created */
+	_free_memobj_list(&cmdbatch->cmdlist);
+
+	/* Destroy the memlist we created */
+	_free_memobj_list(&cmdbatch->memlist);
+
+	/*
+	 * If we cancelled an event, there's a good chance that the context is
+	 * on a dispatcher queue, so schedule to get it removed.
+	 */
+	if (!bitmap_empty(&pending, KGSL_MAX_SYNCPOINTS) &&
+		cmdbatch->device->ftbl->drawctxt_sched)
+		cmdbatch->device->ftbl->drawctxt_sched(cmdbatch->device,
+							cmdbatch->context);
+
+	kgsl_cmdbatch_put(cmdbatch);
+}
+EXPORT_SYMBOL(kgsl_cmdbatch_destroy);
+
+/*
+ * A callback that gets registered with kgsl_sync_fence_async_wait and is fired
+ * when a fence is expired
+ */
+static void kgsl_cmdbatch_sync_fence_func(void *priv)
+{
+	unsigned long flags;
+	struct kgsl_cmdbatch_sync_event *event = priv;
+
+	kgsl_cmdbatch_sync_expire(event->device, event);
+
+	trace_syncpoint_fence_expire(event->cmdbatch,
+		event->handle ? event->handle->name : "unknown");
+
+	spin_lock_irqsave(&event->handle_lock, flags);
+
+	/*
+	 * Setting the event->handle to NULL here make sure that
+	 * other function does not dereference a invalid pointer.
+	 */
+	event->handle = NULL;
+
+	spin_unlock_irqrestore(&event->handle_lock, flags);
+
+	kgsl_cmdbatch_put(event->cmdbatch);
+}
+
+/* kgsl_cmdbatch_add_sync_fence() - Add a new sync fence syncpoint
+ * @device: KGSL device
+ * @cmdbatch: KGSL cmdbatch to add the sync point to
+ * @priv: Private sructure passed by the user
+ *
+ * Add a new fence sync syncpoint to the cmdbatch.
+ */
+static int kgsl_cmdbatch_add_sync_fence(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void *priv)
+{
+	struct kgsl_cmd_syncpoint_fence *sync = priv;
+	struct kgsl_cmdbatch_sync_event *event;
+	unsigned int id;
+	unsigned long flags;
+
+	kref_get(&cmdbatch->refcount);
+
+	id = cmdbatch->numsyncs++;
+
+	event = &cmdbatch->synclist[id];
+
+	event->id = id;
+	event->type = KGSL_CMD_SYNCPOINT_TYPE_FENCE;
+	event->cmdbatch = cmdbatch;
+	event->device = device;
+	event->context = NULL;
+
+	spin_lock_init(&event->handle_lock);
+	set_bit(event->id, &cmdbatch->pending);
+
+	spin_lock_irqsave(&event->handle_lock, flags);
+
+	event->handle = kgsl_sync_fence_async_wait(sync->fd,
+		kgsl_cmdbatch_sync_fence_func, event);
+
+	if (IS_ERR_OR_NULL(event->handle)) {
+		int ret = PTR_ERR(event->handle);
+
+		event->handle = NULL;
+		spin_unlock_irqrestore(&event->handle_lock, flags);
+
+		clear_bit(event->id, &cmdbatch->pending);
+		kgsl_cmdbatch_put(cmdbatch);
+
+		/*
+		 * If ret == 0 the fence was already signaled - print a trace
+		 * message so we can track that
+		 */
+		if (ret == 0)
+			trace_syncpoint_fence_expire(cmdbatch, "signaled");
+
+		return ret;
+	} else {
+		spin_unlock_irqrestore(&event->handle_lock, flags);
+	}
+
+	trace_syncpoint_fence(cmdbatch, event->handle->name);
+
+	return 0;
+}
+
+/* kgsl_cmdbatch_add_sync_timestamp() - Add a new sync point for a cmdbatch
+ * @device: KGSL device
+ * @cmdbatch: KGSL cmdbatch to add the sync point to
+ * @priv: Private sructure passed by the user
+ *
+ * Add a new sync point timestamp event to the cmdbatch.
+ */
+static int kgsl_cmdbatch_add_sync_timestamp(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void *priv)
+{
+	struct kgsl_cmd_syncpoint_timestamp *sync = priv;
+	struct kgsl_context *context = kgsl_context_get(cmdbatch->device,
+		sync->context_id);
+	struct kgsl_cmdbatch_sync_event *event;
+	int ret = -EINVAL;
+	unsigned int id;
+
+	if (context == NULL)
+		return -EINVAL;
+
+	/*
+	 * We allow somebody to create a sync point on their own context.
+	 * This has the effect of delaying a command from submitting until the
+	 * dependent command has cleared.  That said we obviously can't let them
+	 * create a sync point on a future timestamp.
+	 */
+
+	if (context == cmdbatch->context) {
+		unsigned int queued;
+		kgsl_readtimestamp(device, context, KGSL_TIMESTAMP_QUEUED,
+			&queued);
+
+		if (timestamp_cmp(sync->timestamp, queued) > 0) {
+			KGSL_DRV_ERR(device,
+			"Cannot create syncpoint for future timestamp %d (current %d)\n",
+				sync->timestamp, queued);
+			goto done;
+		}
+	}
+
+	kref_get(&cmdbatch->refcount);
+
+	id = cmdbatch->numsyncs++;
+
+	event = &cmdbatch->synclist[id];
+	event->id = id;
+
+	event->type = KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP;
+	event->cmdbatch = cmdbatch;
+	event->context = context;
+	event->timestamp = sync->timestamp;
+	event->device = device;
+
+	set_bit(event->id, &cmdbatch->pending);
+
+	ret = kgsl_add_event(device, &context->events, sync->timestamp,
+		kgsl_cmdbatch_sync_func, event);
+
+	if (ret) {
+		clear_bit(event->id, &cmdbatch->pending);
+		kgsl_cmdbatch_put(cmdbatch);
+	} else {
+		trace_syncpoint_timestamp(cmdbatch, context, sync->timestamp);
+	}
+
+done:
+	if (ret)
+		kgsl_context_put(context);
+
+	return ret;
+}
+
+/**
+ * kgsl_cmdbatch_add_sync() - Add a sync point to a command batch
+ * @device: Pointer to the KGSL device struct for the GPU
+ * @cmdbatch: Pointer to the cmdbatch
+ * @sync: Pointer to the user-specified struct defining the syncpoint
+ *
+ * Create a new sync point in the cmdbatch based on the user specified
+ * parameters
+ */
+int kgsl_cmdbatch_add_sync(struct kgsl_device *device,
+	struct kgsl_cmdbatch *cmdbatch,
+	struct kgsl_cmd_syncpoint *sync)
+{
+	void *priv;
+	int ret, psize;
+	int (*func)(struct kgsl_device *device, struct kgsl_cmdbatch *cmdbatch,
+			void *priv);
+
+	switch (sync->type) {
+	case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP:
+		psize = sizeof(struct kgsl_cmd_syncpoint_timestamp);
+		func = kgsl_cmdbatch_add_sync_timestamp;
+		break;
+	case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
+		psize = sizeof(struct kgsl_cmd_syncpoint_fence);
+		func = kgsl_cmdbatch_add_sync_fence;
+		break;
+	default:
+		KGSL_DRV_ERR(device,
+			"bad syncpoint type ctxt %d type 0x%x size %zu\n",
+			cmdbatch->context->id, sync->type, sync->size);
+		return -EINVAL;
+	}
+
+	if (sync->size != psize) {
+		KGSL_DRV_ERR(device,
+			"bad syncpoint size ctxt %d type 0x%x size %zu\n",
+			cmdbatch->context->id, sync->type, sync->size);
+		return -EINVAL;
+	}
+
+	priv = kzalloc(sync->size, GFP_KERNEL);
+	if (priv == NULL)
+		return -ENOMEM;
+
+	if (copy_from_user(priv, sync->priv, sync->size)) {
+		kfree(priv);
+		return -EFAULT;
+	}
+
+	ret = func(device, cmdbatch, priv);
+	kfree(priv);
+
+	return ret;
+}
+
+static void add_profiling_buffer(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, uint64_t gpuaddr, uint64_t size,
+		unsigned int id, uint64_t offset)
+{
+	struct kgsl_mem_entry *entry;
+
+	if (!(cmdbatch->flags & KGSL_CMDBATCH_PROFILING))
+		return;
+
+	/* Only the first buffer entry counts - ignore the rest */
+	if (cmdbatch->profiling_buf_entry != NULL)
+		return;
+
+	if (id != 0)
+		entry = kgsl_sharedmem_find_id(cmdbatch->context->proc_priv,
+				id);
+	else
+		entry = kgsl_sharedmem_find(cmdbatch->context->proc_priv,
+			gpuaddr);
+
+	if (entry != NULL) {
+		if (!kgsl_gpuaddr_in_memdesc(&entry->memdesc, gpuaddr, size)) {
+			kgsl_mem_entry_put(entry);
+			entry = NULL;
+		}
+	}
+
+	if (entry == NULL) {
+		KGSL_DRV_ERR(device,
+			"ignore bad profile buffer ctxt %d id %d offset %lld gpuaddr %llx size %lld\n",
+			cmdbatch->context->id, id, offset, gpuaddr, size);
+		return;
+	}
+
+	cmdbatch->profiling_buf_entry = entry;
+
+	if (id != 0)
+		cmdbatch->profiling_buffer_gpuaddr =
+			entry->memdesc.gpuaddr + offset;
+	else
+		cmdbatch->profiling_buffer_gpuaddr = gpuaddr;
+}
+
+/**
+ * kgsl_cmdbatch_add_ibdesc() - Add a legacy ibdesc to a command batch
+ * @cmdbatch: Pointer to the cmdbatch
+ * @ibdesc: Pointer to the user-specified struct defining the memory or IB
+ *
+ * Create a new memory entry in the cmdbatch based on the user specified
+ * parameters
+ */
+int kgsl_cmdbatch_add_ibdesc(struct kgsl_device *device,
+	struct kgsl_cmdbatch *cmdbatch, struct kgsl_ibdesc *ibdesc)
+{
+	uint64_t gpuaddr = (uint64_t) ibdesc->gpuaddr;
+	uint64_t size = (uint64_t) ibdesc->sizedwords << 2;
+	struct kgsl_memobj_node *mem;
+
+	/* sanitize the ibdesc ctrl flags */
+	ibdesc->ctrl &= KGSL_IBDESC_MEMLIST | KGSL_IBDESC_PROFILING_BUFFER;
+
+	if (cmdbatch->flags & KGSL_CMDBATCH_MEMLIST &&
+			ibdesc->ctrl & KGSL_IBDESC_MEMLIST) {
+		if (ibdesc->ctrl & KGSL_IBDESC_PROFILING_BUFFER) {
+			add_profiling_buffer(device, cmdbatch,
+					gpuaddr, size, 0, 0);
+			return 0;
+		}
+	}
+
+	if (cmdbatch->flags & (KGSL_CMDBATCH_SYNC | KGSL_CMDBATCH_MARKER))
+		return 0;
+
+	mem = kmem_cache_alloc(memobjs_cache, GFP_KERNEL);
+	if (mem == NULL)
+		return -ENOMEM;
+
+	mem->gpuaddr = gpuaddr;
+	mem->size = size;
+	mem->priv = 0;
+	mem->id = 0;
+	mem->offset = 0;
+	mem->flags = 0;
+
+	if (cmdbatch->flags & KGSL_CMDBATCH_MEMLIST &&
+			ibdesc->ctrl & KGSL_IBDESC_MEMLIST) {
+		/* add to the memlist */
+		list_add_tail(&mem->node, &cmdbatch->memlist);
+	} else {
+		/* set the preamble flag if directed to */
+		if (cmdbatch->context->flags & KGSL_CONTEXT_PREAMBLE &&
+			list_empty(&cmdbatch->cmdlist))
+			mem->flags = KGSL_CMDLIST_CTXTSWITCH_PREAMBLE;
+
+		/* add to the cmd list */
+		list_add_tail(&mem->node, &cmdbatch->cmdlist);
+	}
+
+	return 0;
+}
+
+/**
+ * kgsl_cmdbatch_create() - Create a new cmdbatch structure
+ * @device: Pointer to a KGSL device struct
+ * @context: Pointer to a KGSL context struct
+ * @flags: Flags for the cmdbatch
+ *
+ * Allocate an new cmdbatch structure
+ */
+struct kgsl_cmdbatch *kgsl_cmdbatch_create(struct kgsl_device *device,
+		struct kgsl_context *context, unsigned int flags)
+{
+	struct kgsl_cmdbatch *cmdbatch = kzalloc(sizeof(*cmdbatch), GFP_KERNEL);
+	if (cmdbatch == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	/*
+	 * Increase the reference count on the context so it doesn't disappear
+	 * during the lifetime of this command batch
+	 */
+
+	if (!_kgsl_context_get(context)) {
+		kfree(cmdbatch);
+		return ERR_PTR(-ENOENT);
+	}
+
+	kref_init(&cmdbatch->refcount);
+	INIT_LIST_HEAD(&cmdbatch->cmdlist);
+	INIT_LIST_HEAD(&cmdbatch->memlist);
+
+	cmdbatch->device = device;
+	cmdbatch->context = context;
+	/* sanitize our flags for cmdbatches */
+	cmdbatch->flags = flags & (KGSL_CMDBATCH_CTX_SWITCH
+				| KGSL_CMDBATCH_MARKER
+				| KGSL_CMDBATCH_END_OF_FRAME
+				| KGSL_CMDBATCH_SYNC
+				| KGSL_CMDBATCH_PWR_CONSTRAINT
+				| KGSL_CMDBATCH_MEMLIST
+				| KGSL_CMDBATCH_PROFILING
+				| KGSL_CMDBATCH_PROFILING_KTIME);
+
+	/* Add a timer to help debug sync deadlocks */
+	setup_timer(&cmdbatch->timer, _kgsl_cmdbatch_timer,
+		(unsigned long) cmdbatch);
+
+	return cmdbatch;
+}
+
+#ifdef CONFIG_COMPAT
+static int add_ibdesc_list_compat(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count)
+{
+	int i, ret = 0;
+	struct kgsl_ibdesc_compat ibdesc32;
+	struct kgsl_ibdesc ibdesc;
+
+	for (i = 0; i < count; i++) {
+		memset(&ibdesc32, 0, sizeof(ibdesc32));
+
+		if (copy_from_user(&ibdesc32, ptr, sizeof(ibdesc32))) {
+			ret = -EFAULT;
+			break;
+		}
+
+		ibdesc.gpuaddr = (unsigned long) ibdesc32.gpuaddr;
+		ibdesc.sizedwords = (size_t) ibdesc32.sizedwords;
+		ibdesc.ctrl = (unsigned int) ibdesc32.ctrl;
+
+		ret = kgsl_cmdbatch_add_ibdesc(device, cmdbatch, &ibdesc);
+		if (ret)
+			break;
+
+		ptr += sizeof(ibdesc32);
+	}
+
+	return ret;
+}
+
+static int add_syncpoints_compat(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count)
+{
+	struct kgsl_cmd_syncpoint_compat sync32;
+	struct kgsl_cmd_syncpoint sync;
+	int i, ret = 0;
+
+	for (i = 0; i < count; i++) {
+		memset(&sync32, 0, sizeof(sync32));
+
+		if (copy_from_user(&sync32, ptr, sizeof(sync32))) {
+			ret = -EFAULT;
+			break;
+		}
+
+		sync.type = sync32.type;
+		sync.priv = compat_ptr(sync32.priv);
+		sync.size = (size_t) sync32.size;
+
+		ret = kgsl_cmdbatch_add_sync(device, cmdbatch, &sync);
+		if (ret)
+			break;
+
+		ptr += sizeof(sync32);
+	}
+
+	return ret;
+}
+#else
+static int add_ibdesc_list_compat(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count)
+{
+	return -EINVAL;
+}
+
+static int add_syncpoints_compat(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count)
+{
+	return -EINVAL;
+}
+#endif
+
+int kgsl_cmdbatch_add_ibdesc_list(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count)
+{
+	struct kgsl_ibdesc ibdesc;
+	int i, ret;
+
+	if (is_compat_task())
+		return add_ibdesc_list_compat(device, cmdbatch, ptr, count);
+
+	for (i = 0; i < count; i++) {
+		memset(&ibdesc, 0, sizeof(ibdesc));
+
+		if (copy_from_user(&ibdesc, ptr, sizeof(ibdesc)))
+			return -EFAULT;
+
+		ret = kgsl_cmdbatch_add_ibdesc(device, cmdbatch, &ibdesc);
+		if (ret)
+			return ret;
+
+		ptr += sizeof(ibdesc);
+	}
+
+	return 0;
+}
+
+int kgsl_cmdbatch_add_syncpoints(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count)
+{
+	struct kgsl_cmd_syncpoint sync;
+	int i, ret;
+
+	if (count == 0)
+		return 0;
+
+	if (count > KGSL_MAX_SYNCPOINTS)
+		return -EINVAL;
+
+	cmdbatch->synclist = kcalloc(count,
+		sizeof(struct kgsl_cmdbatch_sync_event), GFP_KERNEL);
+
+	if (cmdbatch->synclist == NULL)
+		return -ENOMEM;
+
+	if (is_compat_task())
+		return add_syncpoints_compat(device, cmdbatch, ptr, count);
+
+	for (i = 0; i < count; i++) {
+		memset(&sync, 0, sizeof(sync));
+
+		if (copy_from_user(&sync, ptr, sizeof(sync)))
+			return -EFAULT;
+
+		ret = kgsl_cmdbatch_add_sync(device, cmdbatch, &sync);
+		if (ret)
+			return ret;
+
+		ptr += sizeof(sync);
+	}
+
+	return 0;
+}
+
+static int kgsl_cmdbatch_add_object(struct list_head *head,
+		struct kgsl_command_object *obj)
+{
+	struct kgsl_memobj_node *mem;
+
+	mem = kmem_cache_alloc(memobjs_cache, GFP_KERNEL);
+	if (mem == NULL)
+		return -ENOMEM;
+
+	mem->gpuaddr = obj->gpuaddr;
+	mem->size = obj->size;
+	mem->id = obj->id;
+	mem->offset = obj->offset;
+	mem->flags = obj->flags;
+	mem->priv = 0;
+
+	list_add_tail(&mem->node, head);
+	return 0;
+}
+
+#define CMDLIST_FLAGS \
+	(KGSL_CMDLIST_IB | \
+	 KGSL_CMDLIST_CTXTSWITCH_PREAMBLE | \
+	 KGSL_CMDLIST_IB_PREAMBLE)
+
+int kgsl_cmdbatch_add_cmdlist(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr,
+		unsigned int size, unsigned int count)
+{
+	struct kgsl_command_object obj;
+	int i, ret = 0;
+
+	/* Return early if nothing going on */
+	if (count == 0 && ptr == NULL && size == 0)
+		return 0;
+
+	/* Sanity check inputs */
+	if (count == 0 || ptr == NULL || size == 0)
+		return -EINVAL;
+
+	/* Ignore all if SYNC or MARKER is specified */
+	if (cmdbatch->flags & (KGSL_CMDBATCH_SYNC | KGSL_CMDBATCH_MARKER))
+		return 0;
+
+	for (i = 0; i < count; i++) {
+		memset(&obj, 0, sizeof(obj));
+
+		ret = _copy_from_user(&obj, ptr, sizeof(obj), size);
+		if (ret)
+			return ret;
+
+		/* Sanity check the flags */
+		if (!(obj.flags & CMDLIST_FLAGS)) {
+			KGSL_DRV_ERR(device,
+				"invalid cmdobj ctxt %d flags %d id %d offset %lld addr %lld size %lld\n",
+				cmdbatch->context->id, obj.flags, obj.id,
+				obj.offset, obj.gpuaddr, obj.size);
+			return -EINVAL;
+		}
+
+		ret = kgsl_cmdbatch_add_object(&cmdbatch->cmdlist, &obj);
+		if (ret)
+			return ret;
+
+		ptr += sizeof(obj);
+	}
+
+	return 0;
+}
+
+int kgsl_cmdbatch_add_memlist(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr,
+		unsigned int size, unsigned int count)
+{
+	struct kgsl_command_object obj;
+	int i, ret = 0;
+
+	/* Return early if nothing going on */
+	if (count == 0 && ptr == NULL && size == 0)
+		return 0;
+
+	/* Sanity check inputs */
+	if (count == 0 || ptr == NULL || size == 0)
+		return -EINVAL;
+
+	for (i = 0; i < count; i++) {
+		memset(&obj, 0, sizeof(obj));
+
+		ret = _copy_from_user(&obj, ptr, sizeof(obj), size);
+		if (ret)
+			return ret;
+
+		if (!(obj.flags & KGSL_OBJLIST_MEMOBJ)) {
+			KGSL_DRV_ERR(device,
+				"invalid memobj ctxt %d flags %d id %d offset %lld addr %lld size %lld\n",
+				cmdbatch->context->id, obj.flags, obj.id,
+				obj.offset, obj.gpuaddr, obj.size);
+			return -EINVAL;
+		}
+
+		if (obj.flags & KGSL_OBJLIST_PROFILE)
+			add_profiling_buffer(device, cmdbatch, obj.gpuaddr,
+				obj.size, obj.id, obj.offset);
+		else {
+			ret = kgsl_cmdbatch_add_object(&cmdbatch->memlist,
+				&obj);
+			if (ret)
+				return ret;
+		}
+
+		ptr += sizeof(obj);
+	}
+
+	return 0;
+}
+
+int kgsl_cmdbatch_add_synclist(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr,
+		unsigned int size, unsigned int count)
+{
+	struct kgsl_command_syncpoint syncpoint;
+	struct kgsl_cmd_syncpoint sync;
+	int i, ret = 0;
+
+	/* Return early if nothing going on */
+	if (count == 0 && ptr == NULL && size == 0)
+		return 0;
+
+	/* Sanity check inputs */
+	if (count == 0 || ptr == NULL || size == 0)
+		return -EINVAL;
+
+	if (count > KGSL_MAX_SYNCPOINTS)
+		return -EINVAL;
+
+	cmdbatch->synclist = kcalloc(count,
+		sizeof(struct kgsl_cmdbatch_sync_event), GFP_KERNEL);
+
+	if (cmdbatch->synclist == NULL)
+		return -ENOMEM;
+
+	for (i = 0; i < count; i++) {
+		memset(&syncpoint, 0, sizeof(syncpoint));
+
+		ret = _copy_from_user(&syncpoint, ptr, sizeof(syncpoint), size);
+		if (ret)
+			return ret;
+
+		sync.type = syncpoint.type;
+		sync.priv = to_user_ptr(syncpoint.priv);
+		sync.size = syncpoint.size;
+
+		ret = kgsl_cmdbatch_add_sync(device, cmdbatch, &sync);
+		if (ret)
+			return ret;
+
+		ptr += sizeof(syncpoint);
+	}
+
+	return 0;
+}
+
+void kgsl_cmdbatch_exit(void)
+{
+	if (memobjs_cache != NULL)
+		kmem_cache_destroy(memobjs_cache);
+}
+
+int kgsl_cmdbatch_init(void)
+{
+	memobjs_cache = KMEM_CACHE(kgsl_memobj_node, 0);
+	if (memobjs_cache == NULL) {
+		KGSL_CORE_ERR("failed to create memobjs_cache");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/msm/kgsl_cmdbatch.h b/drivers/gpu/msm/kgsl_cmdbatch.h
new file mode 100644
index 000000000000..fad95426dfe0
--- /dev/null
+++ b/drivers/gpu/msm/kgsl_cmdbatch.h
@@ -0,0 +1,172 @@
+/* Copyright (c) 2008-2015, 2017 The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __KGSL_CMDBATCH_H
+#define __KGSL_CMDBATCH_H
+
+#define KGSL_CMDBATCH_FLAGS \
+	{ KGSL_CMDBATCH_MARKER, "MARKER" }, \
+	{ KGSL_CMDBATCH_CTX_SWITCH, "CTX_SWITCH" }, \
+	{ KGSL_CMDBATCH_SYNC, "SYNC" }, \
+	{ KGSL_CMDBATCH_END_OF_FRAME, "EOF" }, \
+	{ KGSL_CMDBATCH_PWR_CONSTRAINT, "PWR_CONSTRAINT" }, \
+	{ KGSL_CMDBATCH_SUBMIT_IB_LIST, "IB_LIST" }
+
+/**
+ * struct kgsl_cmdbatch - KGSl command descriptor
+ * @device: KGSL GPU device that the command was created for
+ * @context: KGSL context that created the command
+ * @timestamp: Timestamp assigned to the command
+ * @flags: flags
+ * @priv: Internal flags
+ * @fault_policy: Internal policy describing how to handle this command in case
+ * of a fault
+ * @fault_recovery: recovery actions actually tried for this batch
+ * @expires: Point in time when the cmdbatch is considered to be hung
+ * @refcount: kref structure to maintain the reference count
+ * @cmdlist: List of IBs to issue
+ * @memlist: List of all memory used in this command batch
+ * @synclist: Array of context/timestamp tuples to wait for before issuing
+ * @numsyncs: Number of sync entries in the array
+ * @pending: Bitmask of sync events that are active
+ * @timer: a timer used to track possible sync timeouts for this cmdbatch
+ * @marker_timestamp: For markers, the timestamp of the last "real" command that
+ * was queued
+ * @profiling_buf_entry: Mem entry containing the profiling buffer
+ * @profiling_buffer_gpuaddr: GPU virt address of the profile buffer added here
+ * for easy access
+ * @profile_index: Index to store the start/stop ticks in the kernel profiling
+ * buffer
+ * @submit_ticks: Variable to hold ticks at the time of cmdbatch submit.
+ * @global_ts: The ringbuffer timestamp corresponding to this cmdbatch
+ * @timeout_jiffies: For a syncpoint cmdbatch the jiffies at which the
+ * timer will expire
+ * This structure defines an atomic batch of command buffers issued from
+ * userspace.
+ */
+struct kgsl_cmdbatch {
+	struct kgsl_device *device;
+	struct kgsl_context *context;
+	uint32_t timestamp;
+	uint32_t flags;
+	unsigned long priv;
+	unsigned long fault_policy;
+	unsigned long fault_recovery;
+	unsigned long expires;
+	struct kref refcount;
+	struct list_head cmdlist;
+	struct list_head memlist;
+	struct kgsl_cmdbatch_sync_event *synclist;
+	unsigned int numsyncs;
+	unsigned long pending;
+	struct timer_list timer;
+	unsigned int marker_timestamp;
+	struct kgsl_mem_entry *profiling_buf_entry;
+	uint64_t profiling_buffer_gpuaddr;
+	unsigned int profile_index;
+	uint64_t submit_ticks;
+	unsigned int global_ts;
+	unsigned long timeout_jiffies;
+};
+
+/**
+ * struct kgsl_cmdbatch_sync_event
+ * @id: identifer (positiion within the pending bitmap)
+ * @type: Syncpoint type
+ * @cmdbatch: Pointer to the cmdbatch that owns the sync event
+ * @context: Pointer to the KGSL context that owns the cmdbatch
+ * @timestamp: Pending timestamp for the event
+ * @handle: Pointer to a sync fence handle
+ * @handle_lock: Spin lock to protect handle
+ * @device: Pointer to the KGSL device
+ */
+struct kgsl_cmdbatch_sync_event {
+	unsigned int id;
+	int type;
+	struct kgsl_cmdbatch *cmdbatch;
+	struct kgsl_context *context;
+	unsigned int timestamp;
+	struct kgsl_sync_fence_waiter *handle;
+	spinlock_t handle_lock;
+	struct kgsl_device *device;
+};
+
+/**
+ * enum kgsl_cmdbatch_priv - Internal cmdbatch flags
+ * @CMDBATCH_FLAG_SKIP - skip the entire command batch
+ * @CMDBATCH_FLAG_FORCE_PREAMBLE - Force the preamble on for the cmdbatch
+ * @CMDBATCH_FLAG_WFI - Force wait-for-idle for the submission
+ * @CMDBATCH_FLAG_PROFILE - store the start / retire ticks for the command batch
+ * in the profiling buffer
+ * @CMDBATCH_FLAG_FENCE_LOG - Set if the cmdbatch is dumping fence logs via the
+ * cmdbatch timer - this is used to avoid recursion
+ */
+
+enum kgsl_cmdbatch_priv {
+	CMDBATCH_FLAG_SKIP = 0,
+	CMDBATCH_FLAG_FORCE_PREAMBLE,
+	CMDBATCH_FLAG_WFI,
+	CMDBATCH_FLAG_PROFILE,
+	CMDBATCH_FLAG_FENCE_LOG,
+};
+
+
+int kgsl_cmdbatch_add_memobj(struct kgsl_cmdbatch *cmdbatch,
+		struct kgsl_ibdesc *ibdesc);
+
+int kgsl_cmdbatch_add_sync(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch,
+		struct kgsl_cmd_syncpoint *sync);
+
+struct kgsl_cmdbatch *kgsl_cmdbatch_create(struct kgsl_device *device,
+		struct kgsl_context *context, unsigned int flags);
+int kgsl_cmdbatch_add_ibdesc(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, struct kgsl_ibdesc *ibdesc);
+int kgsl_cmdbatch_add_ibdesc_list(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count);
+int kgsl_cmdbatch_add_syncpoints(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr, int count);
+int kgsl_cmdbatch_add_cmdlist(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr,
+		unsigned int size, unsigned int count);
+int kgsl_cmdbatch_add_memlist(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr,
+		unsigned int size, unsigned int count);
+int kgsl_cmdbatch_add_synclist(struct kgsl_device *device,
+		struct kgsl_cmdbatch *cmdbatch, void __user *ptr,
+		unsigned int size, unsigned int count);
+
+int kgsl_cmdbatch_init(void);
+void kgsl_cmdbatch_exit(void);
+
+void kgsl_dump_syncpoints(struct kgsl_device *device,
+	struct kgsl_cmdbatch *cmdbatch);
+
+void kgsl_cmdbatch_destroy(struct kgsl_cmdbatch *cmdbatch);
+
+void kgsl_cmdbatch_destroy_object(struct kref *kref);
+
+static inline bool kgsl_cmdbatch_events_pending(struct kgsl_cmdbatch *cmdbatch)
+{
+	return !bitmap_empty(&cmdbatch->pending, KGSL_MAX_SYNCPOINTS);
+}
+
+static inline bool kgsl_cmdbatch_event_pending(struct kgsl_cmdbatch *cmdbatch,
+		unsigned int bit)
+{
+	if (bit >= KGSL_MAX_SYNCPOINTS)
+		return false;
+
+	return test_bit(bit, &cmdbatch->pending);
+}
+
+#endif /* __KGSL_CMDBATCH_H */
diff --git a/drivers/gpu/msm/kgsl_compat.c b/drivers/gpu/msm/kgsl_compat.c
index 1c89ed5c2e04..248c78b7e5c4 100644
--- a/drivers/gpu/msm/kgsl_compat.c
+++ b/drivers/gpu/msm/kgsl_compat.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -274,6 +274,18 @@ kgsl_ioctl_gpumem_get_info_compat(struct kgsl_device_private *dev_priv,
 	return result;
 }
 
+static long kgsl_ioctl_cff_syncmem_compat(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data)
+{
+	struct kgsl_cff_syncmem_compat *param32 = data;
+	struct kgsl_cff_syncmem param;
+
+	param.gpuaddr = (unsigned long)param32->gpuaddr;
+	param.len = (size_t)param32->len;
+
+	return kgsl_ioctl_cff_syncmem(dev_priv, cmd, &param);
+}
+
 static long kgsl_ioctl_timestamp_event_compat(struct kgsl_device_private
 				*dev_priv, unsigned int cmd, void *data)
 {
@@ -318,6 +330,10 @@ static const struct kgsl_ioctl kgsl_compat_ioctl_funcs[] = {
 			kgsl_ioctl_sharedmem_flush_cache_compat),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUMEM_ALLOC_COMPAT,
 			kgsl_ioctl_gpumem_alloc_compat),
+	KGSL_IOCTL_FUNC(IOCTL_KGSL_CFF_SYNCMEM_COMPAT,
+			kgsl_ioctl_cff_syncmem_compat),
+	KGSL_IOCTL_FUNC(IOCTL_KGSL_CFF_USER_EVENT,
+			kgsl_ioctl_cff_user_event),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_TIMESTAMP_EVENT_COMPAT,
 			kgsl_ioctl_timestamp_event_compat),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_SETPROPERTY_COMPAT,
@@ -340,6 +356,8 @@ static const struct kgsl_ioctl kgsl_compat_ioctl_funcs[] = {
 			kgsl_ioctl_syncsource_create_fence),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_SYNCSOURCE_SIGNAL_FENCE,
 			kgsl_ioctl_syncsource_signal_fence),
+	KGSL_IOCTL_FUNC(IOCTL_KGSL_CFF_SYNC_GPUOBJ,
+			kgsl_ioctl_cff_sync_gpuobj),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUOBJ_ALLOC,
 			kgsl_ioctl_gpuobj_alloc),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUOBJ_FREE,
@@ -354,18 +372,6 @@ static const struct kgsl_ioctl kgsl_compat_ioctl_funcs[] = {
 			kgsl_ioctl_gpu_command),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUOBJ_SET_INFO,
 			kgsl_ioctl_gpuobj_set_info),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_PHYS_ALLOC,
-			kgsl_ioctl_sparse_phys_alloc),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_PHYS_FREE,
-			kgsl_ioctl_sparse_phys_free),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_VIRT_ALLOC,
-			kgsl_ioctl_sparse_virt_alloc),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_VIRT_FREE,
-			kgsl_ioctl_sparse_virt_free),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_BIND,
-			kgsl_ioctl_sparse_bind),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPU_SPARSE_COMMAND,
-			kgsl_ioctl_gpu_sparse_command),
 };
 
 long kgsl_compat_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
diff --git a/drivers/gpu/msm/kgsl_compat.h b/drivers/gpu/msm/kgsl_compat.h
index 621b232af3db..b7a1eb174baf 100644
--- a/drivers/gpu/msm/kgsl_compat.h
+++ b/drivers/gpu/msm/kgsl_compat.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2013-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2013-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -92,6 +92,16 @@ struct kgsl_ringbuffer_issueibcmds_compat {
 #define IOCTL_KGSL_RINGBUFFER_ISSUEIBCMDS_COMPAT \
 	_IOWR(KGSL_IOC_TYPE, 0x10, struct kgsl_ringbuffer_issueibcmds_compat)
 
+struct kgsl_cmdstream_freememontimestamp_compat {
+	compat_ulong_t gpuaddr;
+	unsigned int type;
+	unsigned int timestamp;
+};
+
+#define IOCTL_KGSL_CMDSTREAM_FREEMEMONTIMESTAMP_COMPAT \
+	_IOW(KGSL_IOC_TYPE, 0x12, \
+	struct kgsl_cmdstream_freememontimestamp_compat)
+
 struct kgsl_cmdstream_freememontimestamp_ctxtid_compat {
 	unsigned int context_id;
 	compat_ulong_t gpuaddr;
@@ -236,8 +246,8 @@ static inline compat_size_t sizet_to_compat(size_t size)
 	return (compat_size_t)size;
 }
 
-int kgsl_drawobj_create_compat(struct kgsl_device *device, unsigned int flags,
-			struct kgsl_drawobj *drawobj, void __user *cmdlist,
+int kgsl_cmdbatch_create_compat(struct kgsl_device *device, unsigned int flags,
+			struct kgsl_cmdbatch *cmdbatch, void __user *cmdlist,
 			unsigned int numcmds, void __user *synclist,
 			unsigned int numsyncs);
 
@@ -245,18 +255,18 @@ long kgsl_compat_ioctl(struct file *filep, unsigned int cmd,
 			unsigned long arg);
 
 #else
-static inline int kgsl_drawobj_create_compat(struct kgsl_device *device,
-			unsigned int flags, struct kgsl_drawobj *drawobj,
+static inline int kgsl_cmdbatch_create_compat(struct kgsl_device *device,
+			unsigned int flags, struct kgsl_cmdbatch *cmdbatch,
 			void __user *cmdlist, unsigned int numcmds,
 			void __user *synclist, unsigned int numsyncs)
 {
-	return -EINVAL;
+	BUG();
 }
 
 static inline long kgsl_compat_ioctl(struct file *filep, unsigned int cmd,
 			unsigned long arg)
 {
-	return -EINVAL;
+	BUG();
 }
 
 #endif /* CONFIG_COMPAT */
diff --git a/drivers/gpu/msm/kgsl_debugfs.c b/drivers/gpu/msm/kgsl_debugfs.c
index 834706a973d2..8a71e3f8ce13 100644
--- a/drivers/gpu/msm/kgsl_debugfs.c
+++ b/drivers/gpu/msm/kgsl_debugfs.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2008-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2008-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -27,7 +27,7 @@ static struct dentry *proc_d_debugfs;
 
 static inline int kgsl_log_set(unsigned int *log_val, void *data, u64 val)
 {
-	*log_val = min_t(unsigned int, val, KGSL_LOG_LEVEL_MAX);
+	*log_val = min((unsigned int)val, (unsigned int)KGSL_LOG_LEVEL_MAX);
 	return 0;
 }
 
@@ -44,7 +44,7 @@ static int __log ## _get(void *data, u64 *val)	        \
 	return 0;                                       \
 }                                                       \
 DEFINE_SIMPLE_ATTRIBUTE(__log ## _fops,                 \
-__log ## _get, __log ## _set, "%llu\n")                 \
+__log ## _get, __log ## _set, "%llu\n");                \
 
 KGSL_DEBUGFS_LOG(drv_log);
 KGSL_DEBUGFS_LOG(cmd_log);
@@ -87,11 +87,6 @@ void kgsl_device_debugfs_init(struct kgsl_device *device)
 				&pwr_log_fops);
 }
 
-void kgsl_device_debugfs_close(struct kgsl_device *device)
-{
-	debugfs_remove_recursive(device->d_debugfs);
-}
-
 struct type_entry {
 	int type;
 	const char *str;
@@ -102,7 +97,6 @@ static const struct type_entry memtypes[] = { KGSL_MEM_TYPES };
 static const char *memtype_str(int memtype)
 {
 	int i;
-
 	for (i = 0; i < ARRAY_SIZE(memtypes); i++)
 		if (memtypes[i].type == memtype)
 			return memtypes[i].str;
@@ -112,7 +106,6 @@ static const char *memtype_str(int memtype)
 static char get_alignflag(const struct kgsl_memdesc *m)
 {
 	int align = kgsl_memdesc_get_align(m);
-
 	if (align >= ilog2(SZ_1M))
 		return 'L';
 	else if (align >= ilog2(SZ_64K))
@@ -128,23 +121,17 @@ static char get_cacheflag(const struct kgsl_memdesc *m)
 		[KGSL_CACHEMODE_WRITEBACK] = 'b',
 		[KGSL_CACHEMODE_WRITETHROUGH] = 't',
 	};
-
 	return table[kgsl_memdesc_get_cachemode(m)];
 }
 
 
-static int print_mem_entry(void *data, void *ptr)
+static int print_mem_entry(int id, void *ptr, void *data)
 {
 	struct seq_file *s = data;
 	struct kgsl_mem_entry *entry = ptr;
-	char flags[10];
+	char flags[8];
 	char usage[16];
 	struct kgsl_memdesc *m = &entry->memdesc;
-	unsigned int usermem_type = kgsl_memdesc_usermem_type(m);
-	int egl_surface_count = 0, egl_image_count = 0;
-
-	if (m->flags & KGSL_MEMFLAGS_SPARSE_VIRT)
-		return 0;
 
 	flags[0] = kgsl_memdesc_is_global(m) ?  'g' : '-';
 	flags[1] = '-';
@@ -153,23 +140,16 @@ static int print_mem_entry(void *data, void *ptr)
 	flags[4] = get_cacheflag(m);
 	flags[5] = kgsl_memdesc_use_cpu_map(m) ? 'p' : '-';
 	flags[6] = (m->useraddr) ? 'Y' : 'N';
-	flags[7] = kgsl_memdesc_is_secured(m) ?  's' : '-';
-	flags[8] = m->flags & KGSL_MEMFLAGS_SPARSE_PHYS ? 'P' : '-';
-	flags[9] = '\0';
+	flags[7] = '\0';
 
 	kgsl_get_memory_usage(usage, sizeof(usage), m->flags);
 
-	if (usermem_type == KGSL_MEM_ENTRY_ION)
-		kgsl_get_egl_counts(entry, &egl_surface_count,
-						&egl_image_count);
-
-	seq_printf(s, "%pK %pK %16llu %5d %9s %10s %16s %5d %16llu %6d %6d",
+	seq_printf(s, "%pK %pK %16llu %5d %8s %10s %16s %5d %16llu",
 			(uint64_t *)(uintptr_t) m->gpuaddr,
 			(unsigned long *) m->useraddr,
 			m->size, entry->id, flags,
-			memtype_str(usermem_type),
-			usage, (m->sgt ? m->sgt->nents : 0), m->mapsize,
-			egl_surface_count, egl_image_count);
+			memtype_str(kgsl_memdesc_usermem_type(m)),
+			usage, (m->sgt ? m->sgt->nents : 0), m->mapsize);
 
 	if (entry->metadata[0] != 0)
 		seq_printf(s, " %s", entry->metadata);
@@ -179,84 +159,25 @@ static int print_mem_entry(void *data, void *ptr)
 	return 0;
 }
 
-static struct kgsl_mem_entry *process_mem_seq_find(struct seq_file *s,
-						void *ptr, loff_t pos)
+static int process_mem_print(struct seq_file *s, void *unused)
 {
-	struct kgsl_mem_entry *entry = ptr;
 	struct kgsl_process_private *private = s->private;
-	int id = 0;
 
-	loff_t temp_pos = 1;
-
-	if (entry != SEQ_START_TOKEN)
-		id = entry->id + 1;
+	seq_printf(s, "%16s %16s %16s %5s %8s %10s %16s %5s %16s\n",
+		   "gpuaddr", "useraddr", "size", "id", "flags", "type",
+		   "usage", "sglen", "mapsize");
 
 	spin_lock(&private->mem_lock);
-	for (entry = idr_get_next(&private->mem_idr, &id); entry;
-		id++, entry = idr_get_next(&private->mem_idr, &id),
-							temp_pos++) {
-		if (temp_pos == pos && kgsl_mem_entry_get(entry)) {
-			spin_unlock(&private->mem_lock);
-			goto found;
-		}
-	}
+	idr_for_each(&private->mem_idr, print_mem_entry, s);
 	spin_unlock(&private->mem_lock);
 
-	entry = NULL;
-found:
-	if (ptr != SEQ_START_TOKEN)
-		kgsl_mem_entry_put(ptr);
-
-	return entry;
-}
-
-static void *process_mem_seq_start(struct seq_file *s, loff_t *pos)
-{
-	loff_t seq_file_offset = *pos;
-
-	if (seq_file_offset == 0)
-		return SEQ_START_TOKEN;
-	else
-		return process_mem_seq_find(s, SEQ_START_TOKEN,
-						seq_file_offset);
-}
-
-static void process_mem_seq_stop(struct seq_file *s, void *ptr)
-{
-	if (ptr && ptr != SEQ_START_TOKEN)
-		kgsl_mem_entry_put(ptr);
-}
-
-static void *process_mem_seq_next(struct seq_file *s, void *ptr,
-							loff_t *pos)
-{
-	++*pos;
-	return process_mem_seq_find(s, ptr, 1);
-}
-
-static int process_mem_seq_show(struct seq_file *s, void *ptr)
-{
-	if (ptr == SEQ_START_TOKEN) {
-		seq_printf(s, "%16s %16s %16s %5s %9s %10s %16s %5s %16s %6s %6s\n",
-			"gpuaddr", "useraddr", "size", "id", "flags", "type",
-			"usage", "sglen", "mapsize", "eglsrf", "eglimg");
-		return 0;
-	} else
-		return print_mem_entry(s, ptr);
+	return 0;
 }
 
-static const struct seq_operations process_mem_seq_fops = {
-	.start = process_mem_seq_start,
-	.stop = process_mem_seq_stop,
-	.next = process_mem_seq_next,
-	.show = process_mem_seq_show,
-};
-
 static int process_mem_open(struct inode *inode, struct file *file)
 {
 	int ret;
 	pid_t pid = (pid_t) (unsigned long) inode->i_private;
-	struct seq_file *s = NULL;
 	struct kgsl_process_private *private = NULL;
 
 	private = kgsl_process_private_find(pid);
@@ -264,13 +185,9 @@ static int process_mem_open(struct inode *inode, struct file *file)
 	if (!private)
 		return -ENODEV;
 
-	ret = seq_open(file, &process_mem_seq_fops);
+	ret = single_open(file, process_mem_print, private);
 	if (ret)
 		kgsl_process_private_put(private);
-	else {
-		s = file->private_data;
-		s->private = private;
-	}
 
 	return ret;
 }
@@ -283,7 +200,7 @@ static int process_mem_release(struct inode *inode, struct file *file)
 	if (private)
 		kgsl_process_private_put(private);
 
-	return seq_release(inode, file);
+	return single_release(inode, file);
 }
 
 static const struct file_operations process_mem_fops = {
@@ -293,95 +210,6 @@ static const struct file_operations process_mem_fops = {
 	.release = process_mem_release,
 };
 
-static int print_sparse_mem_entry(int id, void *ptr, void *data)
-{
-	struct seq_file *s = data;
-	struct kgsl_mem_entry *entry = ptr;
-	struct kgsl_memdesc *m = &entry->memdesc;
-	struct rb_node *node;
-
-	if (!(m->flags & KGSL_MEMFLAGS_SPARSE_VIRT))
-		return 0;
-
-	spin_lock(&entry->bind_lock);
-	node = rb_first(&entry->bind_tree);
-
-	while (node != NULL) {
-		struct sparse_bind_object *obj = rb_entry(node,
-				struct sparse_bind_object, node);
-		seq_printf(s, "%5d %16llx %16llx %16llx %16llx\n",
-				entry->id, entry->memdesc.gpuaddr,
-				obj->v_off, obj->size, obj->p_off);
-		node = rb_next(node);
-	}
-	spin_unlock(&entry->bind_lock);
-
-	seq_putc(s, '\n');
-
-	return 0;
-}
-
-static int process_sparse_mem_print(struct seq_file *s, void *unused)
-{
-	struct kgsl_process_private *private = s->private;
-
-	seq_printf(s, "%5s %16s %16s %16s %16s\n",
-		   "v_id", "gpuaddr", "v_offset", "v_size", "p_offset");
-
-	spin_lock(&private->mem_lock);
-	idr_for_each(&private->mem_idr, print_sparse_mem_entry, s);
-	spin_unlock(&private->mem_lock);
-
-	return 0;
-}
-
-static int process_sparse_mem_open(struct inode *inode, struct file *file)
-{
-	int ret;
-	pid_t pid = (pid_t) (unsigned long) inode->i_private;
-	struct kgsl_process_private *private = NULL;
-
-	private = kgsl_process_private_find(pid);
-
-	if (!private)
-		return -ENODEV;
-
-	ret = single_open(file, process_sparse_mem_print, private);
-	if (ret)
-		kgsl_process_private_put(private);
-
-	return ret;
-}
-
-static const struct file_operations process_sparse_mem_fops = {
-	.open = process_sparse_mem_open,
-	.read = seq_read,
-	.llseek = seq_lseek,
-	.release = process_mem_release,
-};
-
-static int globals_print(struct seq_file *s, void *unused)
-{
-	kgsl_print_global_pt_entries(s);
-	return 0;
-}
-
-static int globals_open(struct inode *inode, struct file *file)
-{
-	return single_open(file, globals_print, NULL);
-}
-
-static int globals_release(struct inode *inode, struct file *file)
-{
-	return single_release(inode, file);
-}
-
-static const struct file_operations global_fops = {
-	.open = globals_open,
-	.read = seq_read,
-	.llseek = seq_lseek,
-	.release = globals_release,
-};
 
 /**
  * kgsl_process_init_debugfs() - Initialize debugfs for a process
@@ -422,15 +250,6 @@ void kgsl_process_init_debugfs(struct kgsl_process_private *private)
 	if (IS_ERR_OR_NULL(dentry))
 		WARN((dentry == NULL),
 			"Unable to create 'mem' file for %s\n", name);
-
-	dentry = debugfs_create_file("sparse_mem", 0444, private->debug_root,
-		(void *) ((unsigned long) private->pid),
-		&process_sparse_mem_fops);
-
-	if (IS_ERR_OR_NULL(dentry))
-		WARN((dentry == NULL),
-			"Unable to create 'sparse_mem' file for %s\n", name);
-
 }
 
 void kgsl_core_debugfs_init(void)
@@ -439,9 +258,6 @@ void kgsl_core_debugfs_init(void)
 
 	kgsl_debugfs_dir = debugfs_create_dir("kgsl", NULL);
 
-	debugfs_create_file("globals", 0444, kgsl_debugfs_dir, NULL,
-		&global_fops);
-
 	debug_dir = debugfs_create_dir("debug", kgsl_debugfs_dir);
 
 	debugfs_create_file("strict_memory", 0644, debug_dir, NULL,
diff --git a/drivers/gpu/msm/kgsl_debugfs.h b/drivers/gpu/msm/kgsl_debugfs.h
index 7c9ab8f925a2..34875954bb8b 100644
--- a/drivers/gpu/msm/kgsl_debugfs.h
+++ b/drivers/gpu/msm/kgsl_debugfs.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2008-2011,2013,2015,2017 The Linux Foundation.
+/* Copyright (c) 2002,2008-2011,2013,2015 The Linux Foundation.
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
@@ -23,7 +23,6 @@ void kgsl_core_debugfs_init(void);
 void kgsl_core_debugfs_close(void);
 
 void kgsl_device_debugfs_init(struct kgsl_device *device);
-void kgsl_device_debugfs_close(struct kgsl_device *device);
 
 extern struct dentry *kgsl_debugfs_dir;
 static inline struct dentry *kgsl_get_debugfs_dir(void)
@@ -31,11 +30,10 @@ static inline struct dentry *kgsl_get_debugfs_dir(void)
 	return kgsl_debugfs_dir;
 }
 
-void kgsl_process_init_debugfs(struct kgsl_process_private *priv);
+void kgsl_process_init_debugfs(struct kgsl_process_private *);
 #else
 static inline void kgsl_core_debugfs_init(void) { }
 static inline void kgsl_device_debugfs_init(struct kgsl_device *device) { }
-static inline void kgsl_device_debugfs_close(struct kgsl_device *device) { }
 static inline void kgsl_core_debugfs_close(void) { }
 static inline struct dentry *kgsl_get_debugfs_dir(void) { return NULL; }
 static inline void kgsl_process_init_debugfs(struct kgsl_process_private *priv)
diff --git a/drivers/gpu/msm/kgsl_device.h b/drivers/gpu/msm/kgsl_device.h
index 01c38d0b6b30..6284d024a3ad 100644
--- a/drivers/gpu/msm/kgsl_device.h
+++ b/drivers/gpu/msm/kgsl_device.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2019, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -25,33 +25,39 @@
 #include "kgsl_pwrscale.h"
 #include "kgsl_snapshot.h"
 #include "kgsl_sharedmem.h"
-#include "kgsl_drawobj.h"
-#include "kgsl_gmu.h"
+#include "kgsl_cmdbatch.h"
+
+#include <linux/sync.h>
+
+#define KGSL_TIMEOUT_NONE           0
+#define KGSL_TIMEOUT_DEFAULT        0xFFFFFFFF
+#define KGSL_TIMEOUT_PART           50 /* 50 msec */
 
 #define KGSL_IOCTL_FUNC(_cmd, _func) \
 	[_IOC_NR((_cmd))] = \
 		{ .cmd = (_cmd), .func = (_func) }
 
-/*
- * KGSL device state is initialized to INIT when platform_probe		*
- * successfully initialized the device.  Once a device has been opened	*
+/* KGSL device state is initialized to INIT when platform_probe		*
+ * sucessfully initialized the device.  Once a device has been opened	*
  * (started) it becomes active.  NAP implies that only low latency	*
  * resources (for now clocks on some platforms) are off.  SLEEP implies	*
  * that the KGSL module believes a device is idle (has been inactive	*
  * past its timer) and all system resources are released.  SUSPEND is	*
- * requested by the kernel and will be enforced upon all open devices.	*
- * RESET indicates that GPU or GMU hang happens. KGSL is handling	*
- * snapshot or recover GPU from hang.					*
- */
+ * requested by the kernel and will be enforced upon all open devices.	*/
 
 #define KGSL_STATE_NONE		0x00000000
 #define KGSL_STATE_INIT		0x00000001
 #define KGSL_STATE_ACTIVE	0x00000002
 #define KGSL_STATE_NAP		0x00000004
+#define KGSL_STATE_SLEEP	0x00000008
 #define KGSL_STATE_SUSPEND	0x00000010
 #define KGSL_STATE_AWARE	0x00000020
 #define KGSL_STATE_SLUMBER	0x00000080
-#define KGSL_STATE_RESET	0x00000100
+#define KGSL_STATE_DEEP_NAP	0x00000100
+
+#define KGSL_GRAPHICS_MEMORY_LOW_WATERMARK  0x1000000
+
+#define KGSL_IS_PAGE_ALIGNED(addr) (!((addr) & (~PAGE_MASK)))
 
 /**
  * enum kgsl_event_results - result codes passed to an event callback when the
@@ -66,7 +72,6 @@ enum kgsl_event_results {
 };
 
 #define KGSL_FLAG_WAKE_ON_TOUCH BIT(0)
-#define KGSL_FLAG_SPARSE        BIT(1)
 
 /*
  * "list" of event types for ftrace symbolic magic
@@ -77,21 +82,15 @@ enum kgsl_event_results {
 	{ KGSL_EVENT_CANCELLED, "cancelled" }
 
 #define KGSL_CONTEXT_FLAGS \
-	{ KGSL_CONTEXT_NO_GMEM_ALLOC, "NO_GMEM_ALLOC" }, \
+	{ KGSL_CONTEXT_NO_GMEM_ALLOC , "NO_GMEM_ALLOC" }, \
 	{ KGSL_CONTEXT_PREAMBLE, "PREAMBLE" }, \
 	{ KGSL_CONTEXT_TRASH_STATE, "TRASH_STATE" }, \
 	{ KGSL_CONTEXT_CTX_SWITCH, "CTX_SWITCH" }, \
 	{ KGSL_CONTEXT_PER_CONTEXT_TS, "PER_CONTEXT_TS" }, \
 	{ KGSL_CONTEXT_USER_GENERATED_TS, "USER_TS" }, \
 	{ KGSL_CONTEXT_NO_FAULT_TOLERANCE, "NO_FT" }, \
-	{ KGSL_CONTEXT_INVALIDATE_ON_FAULT, "INVALIDATE_ON_FAULT" }, \
 	{ KGSL_CONTEXT_PWR_CONSTRAINT, "PWR" }, \
-	{ KGSL_CONTEXT_SAVE_GMEM, "SAVE_GMEM" }, \
-	{ KGSL_CONTEXT_IFH_NOP, "IFH_NOP" }, \
-	{ KGSL_CONTEXT_SECURE, "SECURE" }, \
-	{ KGSL_CONTEXT_NO_SNAPSHOT, "NO_SNAPSHOT" }, \
-	{ KGSL_CONTEXT_SPARSE, "SPARSE" }
-
+	{ KGSL_CONTEXT_SAVE_GMEM, "SAVE_GMEM" }
 
 #define KGSL_CONTEXT_TYPES \
 	{ KGSL_CONTEXT_TYPE_ANY, "ANY" }, \
@@ -105,7 +104,6 @@ enum kgsl_event_results {
 
 /* Allocate 600K for the snapshot static region*/
 #define KGSL_SNAPSHOT_MEMSIZE (600 * 1024)
-#define MAX_L3_LEVELS	3
 
 struct kgsl_device;
 struct platform_device;
@@ -117,67 +115,57 @@ struct kgsl_snapshot;
 
 struct kgsl_functable {
 	/* Mandatory functions - these functions must be implemented
-	 * by the client device.  The driver will not check for a NULL
-	 * pointer before calling the hook.
+	   by the client device.  The driver will not check for a NULL
+	   pointer before calling the hook.
 	 */
-	void (*regread)(struct kgsl_device *device,
+	void (*regread) (struct kgsl_device *device,
 		unsigned int offsetwords, unsigned int *value);
-	void (*regwrite)(struct kgsl_device *device,
+	void (*regwrite) (struct kgsl_device *device,
 		unsigned int offsetwords, unsigned int value);
-	int (*idle)(struct kgsl_device *device);
-	bool (*isidle)(struct kgsl_device *device);
-	int (*suspend_context)(struct kgsl_device *device);
-	int (*init)(struct kgsl_device *device);
-	int (*start)(struct kgsl_device *device, int priority);
-	int (*stop)(struct kgsl_device *device);
-	void (*gmu_regread)(struct kgsl_device *device,
-		unsigned int offsetwords, unsigned int *value);
-	void (*gmu_regwrite)(struct kgsl_device *device,
-		unsigned int offsetwords, unsigned int value);
-	int (*getproperty)(struct kgsl_device *device,
+	int (*idle) (struct kgsl_device *device);
+	bool (*isidle) (struct kgsl_device *device);
+	int (*suspend_context) (struct kgsl_device *device);
+	int (*init) (struct kgsl_device *device);
+	int (*start) (struct kgsl_device *device, int priority);
+	int (*stop) (struct kgsl_device *device);
+	int (*getproperty) (struct kgsl_device *device,
 		unsigned int type, void __user *value,
 		size_t sizebytes);
-	int (*getproperty_compat)(struct kgsl_device *device,
+	int (*getproperty_compat) (struct kgsl_device *device,
 		unsigned int type, void __user *value,
 		size_t sizebytes);
-	int (*waittimestamp)(struct kgsl_device *device,
+	int (*waittimestamp) (struct kgsl_device *device,
 		struct kgsl_context *context, unsigned int timestamp,
 		unsigned int msecs);
-	int (*readtimestamp)(struct kgsl_device *device, void *priv,
+	int (*readtimestamp) (struct kgsl_device *device, void *priv,
 		enum kgsl_timestamp_type type, unsigned int *timestamp);
-	int (*queue_cmds)(struct kgsl_device_private *dev_priv,
-		struct kgsl_context *context, struct kgsl_drawobj *drawobj[],
-		uint32_t count, uint32_t *timestamp);
+	int (*issueibcmds) (struct kgsl_device_private *dev_priv,
+		struct kgsl_context *context, struct kgsl_cmdbatch *cmdbatch,
+		uint32_t *timestamps);
 	void (*power_stats)(struct kgsl_device *device,
 		struct kgsl_power_stats *stats);
 	unsigned int (*gpuid)(struct kgsl_device *device, unsigned int *chipid);
 	void (*snapshot)(struct kgsl_device *device,
 		struct kgsl_snapshot *snapshot, struct kgsl_context *context);
-	void (*snapshot_gmu)(struct kgsl_device *device,
-		struct kgsl_snapshot *snapshot);
 	irqreturn_t (*irq_handler)(struct kgsl_device *device);
 	int (*drain)(struct kgsl_device *device);
-	struct kgsl_device_private * (*device_private_create)(void);
-	void (*device_private_destroy)(struct kgsl_device_private *dev_priv);
-	/*
-	 * Optional functions - these functions are not mandatory.  The
-	 * driver will check that the function pointer is not NULL before
-	 * calling the hook
-	 */
-	struct kgsl_context *(*drawctxt_create)(struct kgsl_device_private *,
+	/* Optional functions - these functions are not mandatory.  The
+	   driver will check that the function pointer is not NULL before
+	   calling the hook */
+	struct kgsl_context *(*drawctxt_create) (struct kgsl_device_private *,
 						uint32_t *flags);
 	void (*drawctxt_detach)(struct kgsl_context *context);
-	void (*drawctxt_destroy)(struct kgsl_context *context);
-	void (*drawctxt_dump)(struct kgsl_device *device,
+	void (*drawctxt_destroy) (struct kgsl_context *context);
+	void (*drawctxt_dump) (struct kgsl_device *device,
 		struct kgsl_context *context);
-	long (*ioctl)(struct kgsl_device_private *dev_priv,
+	long (*ioctl) (struct kgsl_device_private *dev_priv,
 		unsigned int cmd, unsigned long arg);
-	long (*compat_ioctl)(struct kgsl_device_private *dev_priv,
+	long (*compat_ioctl) (struct kgsl_device_private *dev_priv,
 		unsigned int cmd, unsigned long arg);
-	int (*setproperty)(struct kgsl_device_private *dev_priv,
+	int (*setproperty) (struct kgsl_device_private *dev_priv,
 		unsigned int type, void __user *value,
 		unsigned int sizebytes);
-	int (*setproperty_compat)(struct kgsl_device_private *dev_priv,
+	int (*setproperty_compat) (struct kgsl_device_private *dev_priv,
 		unsigned int type, void __user *value,
 		unsigned int sizebytes);
 	void (*drawctxt_sched)(struct kgsl_device *device,
@@ -189,13 +177,6 @@ struct kgsl_functable {
 	void (*pwrlevel_change_settings)(struct kgsl_device *device,
 		unsigned int prelevel, unsigned int postlevel, bool post);
 	void (*regulator_disable_poll)(struct kgsl_device *device);
-	void (*clk_set_options)(struct kgsl_device *device,
-		const char *name, struct clk *clk, bool on);
-	void (*gpu_model)(struct kgsl_device *device, char *str,
-		size_t bufsz);
-	void (*stop_fault_timer)(struct kgsl_device *device);
-	void (*dispatcher_halt)(struct kgsl_device *device);
-	void (*dispatcher_unhalt)(struct kgsl_device *device);
 };
 
 struct kgsl_ioctl {
@@ -213,7 +194,7 @@ long kgsl_ioctl_helper(struct file *filep, unsigned int cmd, unsigned long arg,
 
 /**
  * struct kgsl_memobj_node - Memory object descriptor
- * @node: Local list node for the object
+ * @node: Local list node for the cmdbatch
  * @id: GPU memory ID for the object
  * offset: Offset within the object
  * @gpuaddr: GPU address for the object
@@ -230,18 +211,6 @@ struct kgsl_memobj_node {
 	unsigned long priv;
 };
 
-/**
- * struct kgsl_sparseobj_node - Sparse object descriptor
- * @node: Local list node for the sparse cmdbatch
- * @virt_id: Virtual ID to bind/unbind
- * @obj:  struct kgsl_sparse_binding_object
- */
-struct kgsl_sparseobj_node {
-	struct list_head node;
-	unsigned int virt_id;
-	struct kgsl_sparse_binding_object obj;
-};
-
 struct kgsl_device {
 	struct device *dev;
 	const char *name;
@@ -268,25 +237,18 @@ struct kgsl_device {
 	/* GPU shader memory size */
 	unsigned int shader_mem_len;
 	struct kgsl_memdesc memstore;
-	struct kgsl_memdesc scratch;
 	const char *iomemname;
 	const char *shadermemname;
 
 	struct kgsl_mmu mmu;
-	struct gmu_device gmu;
 	struct completion hwaccess_gate;
-	struct completion halt_gate;
+	struct completion cmdbatch_gate;
 	const struct kgsl_functable *ftbl;
 	struct work_struct idle_check_ws;
 	struct timer_list idle_timer;
 	struct kgsl_pwrctrl pwrctrl;
 	int open_count;
 
-	/* For GPU inline submission */
-	uint32_t submit_now;
-	spinlock_t submit_lock;
-	bool slumber;
-
 	struct mutex mutex;
 	uint32_t state;
 	uint32_t requested_state;
@@ -308,14 +270,6 @@ struct kgsl_device {
 	struct kgsl_snapshot *snapshot;
 
 	u32 snapshot_faultcount;	/* Total number of faults since boot */
-	bool force_panic;		/* Force panic after snapshot dump */
-	bool prioritize_unrecoverable;	/* Overwrite with new GMU snapshots */
-
-	/* Use CP Crash dumper to get GPU snapshot*/
-	bool snapshot_crashdumper;
-	/* Use HOST side register reads to get GPU snapshot*/
-	bool snapshot_legacy;
-
 	struct kobject snapshot_kobj;
 
 	struct kobject ppd_kobj;
@@ -327,18 +281,13 @@ struct kgsl_device {
 	int mem_log;
 	int pwr_log;
 	struct kgsl_pwrscale pwrscale;
+	struct work_struct event_work;
 
-	int reset_counter; /* Track how many GPU core resets have occurred */
+	int reset_counter; /* Track how many GPU core resets have occured */
+	int cff_dump_enable;
 	struct workqueue_struct *events_wq;
 
 	struct device *busmondev; /* pseudo dev for GPU BW voting governor */
-
-	/* Number of active contexts seen globally for this device */
-	int active_context_count;
-	struct kobject *gpu_sysfs_kobj;
-	struct clk *l3_clk;
-	unsigned int l3_freq[MAX_L3_LEVELS];
-	unsigned int num_l3_pwrlevels;
 };
 
 #define KGSL_MMU_DEVICE(_mmu) \
@@ -346,9 +295,11 @@ struct kgsl_device {
 
 #define KGSL_DEVICE_COMMON_INIT(_dev) \
 	.hwaccess_gate = COMPLETION_INITIALIZER((_dev).hwaccess_gate),\
-	.halt_gate = COMPLETION_INITIALIZER((_dev).halt_gate),\
+	.cmdbatch_gate = COMPLETION_INITIALIZER((_dev).cmdbatch_gate),\
 	.idle_check_ws = __WORK_INITIALIZER((_dev).idle_check_ws,\
 			kgsl_idle_check),\
+	.event_work  = __WORK_INITIALIZER((_dev).event_work,\
+			kgsl_process_events),\
 	.context_idr = IDR_INIT((_dev).context_idr),\
 	.wait_queue = __WAIT_QUEUE_HEAD_INITIALIZER((_dev).wait_queue),\
 	.active_cnt_wq = __WAIT_QUEUE_HEAD_INITIALIZER((_dev).active_cnt_wq),\
@@ -360,7 +311,6 @@ struct kgsl_device {
 
 /**
  * enum bits for struct kgsl_context.priv
- * @KGSL_CONTEXT_PRIV_SUBMITTED - The context has submitted commands to gpu.
  * @KGSL_CONTEXT_PRIV_DETACHED  - The context has been destroyed by userspace
  *	and is no longer using the gpu.
  * @KGSL_CONTEXT_PRIV_INVALID - The context has been destroyed by the kernel
@@ -370,8 +320,7 @@ struct kgsl_device {
  *	reserved for devices specific use.
  */
 enum kgsl_context_priv {
-	KGSL_CONTEXT_PRIV_SUBMITTED = 0,
-	KGSL_CONTEXT_PRIV_DETACHED,
+	KGSL_CONTEXT_PRIV_DETACHED = 0,
 	KGSL_CONTEXT_PRIV_INVALID,
 	KGSL_CONTEXT_PRIV_PAGEFAULT,
 	KGSL_CONTEXT_PRIV_DEVICE_SPECIFIC = 16,
@@ -390,18 +339,20 @@ struct kgsl_process_private;
  * @proc_priv: pointer to process private, the process that allocated the
  * context
  * @priv: in-kernel context flags, use KGSL_CONTEXT_* values
- * @reset_status: status indication whether a gpu reset occurred and whether
+ * @reset_status: status indication whether a gpu reset occured and whether
  * this context was responsible for causing it
+ * @wait_on_invalid_ts: flag indicating if this context has tried to wait on a
+ * bad timestamp
  * @timeline: sync timeline used to create fences that can be signaled when a
  * sync_pt timestamp expires
  * @events: A kgsl_event_group for this context - contains the list of GPU
  * events
+ * @pagefault_ts: global timestamp of the pagefault, if KGSL_CONTEXT_PAGEFAULT
+ * is set.
  * @flags: flags from userspace controlling the behavior of this context
  * @pwr_constraint: power constraint from userspace for this context
  * @fault_count: number of times gpu hanged in last _context_throttle_time ms
  * @fault_time: time of the first gpu hang in last _context_throttle_time ms
- * @user_ctxt_record: memory descriptor used by CP to save/restore VPC data
- * across preemption
  */
 struct kgsl_context {
 	struct kref refcount;
@@ -413,14 +364,14 @@ struct kgsl_context {
 	unsigned long priv;
 	struct kgsl_device *device;
 	unsigned int reset_status;
-	struct kgsl_sync_timeline *ktimeline;
+	bool wait_on_invalid_ts;
+	struct sync_timeline *timeline;
 	struct kgsl_event_group events;
+	unsigned int pagefault_ts;
 	unsigned int flags;
 	struct kgsl_pwr_constraint pwr_constraint;
-	struct kgsl_pwr_constraint l3_pwr_constraint;
 	unsigned int fault_count;
 	unsigned long fault_time;
-	struct kgsl_mem_entry *user_ctxt_record;
 };
 
 #define _context_comm(_c) \
@@ -449,12 +400,9 @@ struct kgsl_context {
  * @kobj: Pointer to a kobj for the sysfs directory for this process
  * @debug_root: Pointer to the debugfs root for this process
  * @stats: Memory allocation statistics for this process
- * @gpumem_mapped: KGSL memory mapped in the process address space
  * @syncsource_idr: sync sources created by this process
  * @syncsource_lock: Spinlock to protect the syncsource idr
  * @fd_count: Counter for the number of FDs for this process
- * @ctxt_count: Count for the number of contexts for this process
- * @ctxt_count_lock: Spinlock to protect ctxt_count
  */
 struct kgsl_process_private {
 	unsigned long priv;
@@ -471,12 +419,9 @@ struct kgsl_process_private {
 		uint64_t cur;
 		uint64_t max;
 	} stats[KGSL_MEM_ENTRY_MAX];
-	uint64_t gpumem_mapped;
 	struct idr syncsource_idr;
 	spinlock_t syncsource_lock;
 	int fd_count;
-	atomic_t ctxt_count;
-	spinlock_t ctxt_count_lock;
 };
 
 /**
@@ -494,12 +439,6 @@ struct kgsl_device_private {
 
 /**
  * struct kgsl_snapshot - details for a specific snapshot instance
- * @ib1base: Active IB1 base address at the time of fault
- * @ib2base: Active IB2 base address at the time of fault
- * @ib1size: Number of DWORDS pending in IB1 at the time of fault
- * @ib2size: Number of DWORDS pending in IB2 at the time of fault
- * @ib1dumped: Active IB1 dump status to sansphot binary
- * @ib2dumped: Active IB2 dump status to sansphot binary
  * @start: Pointer to the start of the static snapshot region
  * @size: Size of the current snapshot instance
  * @ptr: Pointer to the next block of memory to write to during snapshotting
@@ -512,18 +451,9 @@ struct kgsl_device_private {
  * @work: worker to dump the frozen memory
  * @dump_gate: completion gate signaled by worker when it is finished.
  * @process: the process that caused the hang, if known.
- * @sysfs_read: Count of current reads via sysfs
- * @first_read: True until the snapshot read is started
- * @gmu_fault: Snapshot collected when GMU fault happened
- * @recovered: True if GPU was recovered after previous snapshot
+ * @sysfs_read: An atomic for concurrent snapshot reads via syfs.
  */
 struct kgsl_snapshot {
-	uint64_t ib1base;
-	uint64_t ib2base;
-	unsigned int ib1size;
-	unsigned int ib2size;
-	bool ib1dumped;
-	bool ib2dumped;
 	u8 *start;
 	size_t size;
 	u8 *ptr;
@@ -536,10 +466,7 @@ struct kgsl_snapshot {
 	struct work_struct work;
 	struct completion dump_gate;
 	struct kgsl_process_private *process;
-	unsigned int sysfs_read;
-	bool first_read;
-	bool gmu_fault;
-	bool recovered;
+	atomic_t sysfs_read;
 };
 
 /**
@@ -570,67 +497,18 @@ static inline void kgsl_process_add_stats(struct kgsl_process_private *priv,
 		priv->stats[type].max = priv->stats[type].cur;
 }
 
-static inline bool kgsl_is_register_offset(struct kgsl_device *device,
-				unsigned int offsetwords)
-{
-	return ((offsetwords * sizeof(uint32_t)) < device->reg_len);
-}
-
-static inline bool kgsl_is_gmu_offset(struct kgsl_device *device,
-				unsigned int offsetwords)
-{
-	struct gmu_device *gmu = &device->gmu;
-
-	return (gmu->pdev &&
-		(offsetwords >= gmu->gmu2gpu_offset) &&
-		((offsetwords - gmu->gmu2gpu_offset) * sizeof(uint32_t) <
-			gmu->reg_len));
-}
-
 static inline void kgsl_regread(struct kgsl_device *device,
 				unsigned int offsetwords,
 				unsigned int *value)
 {
-	if (kgsl_is_register_offset(device, offsetwords))
-		device->ftbl->regread(device, offsetwords, value);
-	else if (device->ftbl->gmu_regread &&
-			kgsl_is_gmu_offset(device, offsetwords))
-		device->ftbl->gmu_regread(device, offsetwords, value);
-	else {
-		WARN(1, "Out of bounds register read: 0x%x\n", offsetwords);
-		*value = 0;
-	}
+	device->ftbl->regread(device, offsetwords, value);
 }
 
 static inline void kgsl_regwrite(struct kgsl_device *device,
 				 unsigned int offsetwords,
 				 unsigned int value)
 {
-	if (kgsl_is_register_offset(device, offsetwords))
-		device->ftbl->regwrite(device, offsetwords, value);
-	else if (device->ftbl->gmu_regwrite &&
-			kgsl_is_gmu_offset(device, offsetwords))
-		device->ftbl->gmu_regwrite(device, offsetwords, value);
-	else
-		WARN(1, "Out of bounds register write: 0x%x\n", offsetwords);
-}
-
-static inline void kgsl_gmu_regread(struct kgsl_device *device,
-				unsigned int offsetwords,
-				unsigned int *value)
-{
-	if (device->ftbl->gmu_regread)
-		device->ftbl->gmu_regread(device, offsetwords, value);
-	else
-		*value = 0;
-}
-
-static inline void kgsl_gmu_regwrite(struct kgsl_device *device,
-				 unsigned int offsetwords,
-				 unsigned int value)
-{
-	if (device->ftbl->gmu_regwrite)
-		device->ftbl->gmu_regwrite(device, offsetwords, value);
+	device->ftbl->regwrite(device, offsetwords, value);
 }
 
 static inline void kgsl_regrmw(struct kgsl_device *device,
@@ -639,20 +517,9 @@ static inline void kgsl_regrmw(struct kgsl_device *device,
 {
 	unsigned int val = 0;
 
-	kgsl_regread(device, offsetwords, &val);
-	val &= ~mask;
-	kgsl_regwrite(device, offsetwords, val | bits);
-}
-
-static inline void kgsl_gmu_regrmw(struct kgsl_device *device,
-		unsigned int offsetwords,
-		unsigned int mask, unsigned int bits)
-{
-	unsigned int val = 0;
-
-	kgsl_gmu_regread(device, offsetwords, &val);
+	device->ftbl->regread(device, offsetwords, &val);
 	val &= ~mask;
-	kgsl_gmu_regwrite(device, offsetwords, val | bits);
+	device->ftbl->regwrite(device, offsetwords, val | bits);
 }
 
 static inline int kgsl_idle(struct kgsl_device *device)
@@ -670,7 +537,6 @@ static inline int kgsl_create_device_sysfs_files(struct device *root,
 	const struct device_attribute **list)
 {
 	int ret = 0, i;
-
 	for (i = 0; list[i] != NULL; i++)
 		ret |= device_create_file(root, list[i]);
 	return ret;
@@ -680,7 +546,6 @@ static inline void kgsl_remove_device_sysfs_files(struct device *root,
 	const struct device_attribute **list)
 {
 	int i;
-
 	for (i = 0; list[i] != NULL; i++)
 		device_remove_file(root, list[i]);
 }
@@ -699,14 +564,9 @@ static inline struct kgsl_device *kgsl_device_from_dev(struct device *dev)
 
 static inline int kgsl_state_is_awake(struct kgsl_device *device)
 {
-	struct gmu_device *gmu = &device->gmu;
-
 	if (device->state == KGSL_STATE_ACTIVE ||
 		device->state == KGSL_STATE_AWARE)
 		return true;
-	else if (kgsl_gmu_isenabled(device) &&
-			test_bit(GMU_CLK_ON, &gmu->flags))
-		return true;
 	else
 		return false;
 }
@@ -725,14 +585,13 @@ const char *kgsl_pwrstate_to_str(unsigned int state);
 
 int kgsl_device_snapshot_init(struct kgsl_device *device);
 void kgsl_device_snapshot(struct kgsl_device *device,
-			struct kgsl_context *context, bool gmu_fault);
+			struct kgsl_context *context);
 void kgsl_device_snapshot_close(struct kgsl_device *device);
+void kgsl_snapshot_save_frozen_objs(struct work_struct *work);
 
 void kgsl_events_init(void);
 void kgsl_events_exit(void);
 
-void kgsl_context_detach(struct kgsl_context *context);
-
 void kgsl_del_event_group(struct kgsl_event_group *group);
 
 void kgsl_add_event_group(struct kgsl_event_group *group,
@@ -755,12 +614,12 @@ void kgsl_process_event_group(struct kgsl_device *device,
 	struct kgsl_event_group *group);
 void kgsl_flush_event_group(struct kgsl_device *device,
 		struct kgsl_event_group *group);
-void kgsl_process_event_groups(struct kgsl_device *device);
+void kgsl_process_events(struct work_struct *work);
 
 void kgsl_context_destroy(struct kref *kref);
 
-int kgsl_context_init(struct kgsl_device_private *dev_priv,
-		struct kgsl_context *context);
+int kgsl_context_init(struct kgsl_device_private *, struct kgsl_context
+		*context);
 
 void kgsl_context_dump(struct kgsl_context *context);
 
@@ -773,10 +632,7 @@ long kgsl_ioctl_copy_in(unsigned int kernel_cmd, unsigned int user_cmd,
 		unsigned long arg, unsigned char *ptr);
 
 long kgsl_ioctl_copy_out(unsigned int kernel_cmd, unsigned int user_cmd,
-		unsigned long arg, unsigned char *ptr);
-
-void kgsl_sparse_bind(struct kgsl_process_private *private,
-		struct kgsl_drawobj_sparse *sparse);
+		unsigned long, unsigned char *ptr);
 
 /**
  * kgsl_context_put() - Release context reference count
@@ -855,13 +711,13 @@ static inline struct kgsl_context *kgsl_context_get(struct kgsl_device *device,
 }
 
 /**
- * _kgsl_context_get() - lightweight function to just increment the ref count
- * @context: Pointer to the KGSL context
- *
- * Get a reference to the specified KGSL context structure. This is a
- * lightweight way to just increase the refcount on a known context rather than
- * walking through kgsl_context_get and searching the iterator
- */
+* _kgsl_context_get() - lightweight function to just increment the ref count
+* @context: Pointer to the KGSL context
+*
+* Get a reference to the specified KGSL context structure. This is a
+* lightweight way to just increase the refcount on a known context rather than
+* walking through kgsl_context_get and searching the iterator
+*/
 static inline int _kgsl_context_get(struct kgsl_context *context)
 {
 	int ret = 0;
@@ -901,17 +757,16 @@ static inline struct kgsl_context *kgsl_context_get_owner(
 }
 
 /**
- * kgsl_process_private_get() - increment the refcount on a
- * kgsl_process_private struct
- * @process: Pointer to the KGSL process_private
- *
- * Returns 0 if the structure is invalid and a reference count could not be
- * obtained, nonzero otherwise.
- */
+* kgsl_process_private_get() - increment the refcount on a kgsl_process_private
+*   struct
+* @process: Pointer to the KGSL process_private
+*
+* Returns 0 if the structure is invalid and a reference count could not be
+* obtained, nonzero otherwise.
+*/
 static inline int kgsl_process_private_get(struct kgsl_process_private *process)
 {
 	int ret = 0;
-
 	if (process != NULL)
 		ret = kref_get_unless_zero(&process->refcount);
 	return ret;
diff --git a/drivers/gpu/msm/kgsl_drawobj.c b/drivers/gpu/msm/kgsl_drawobj.c
deleted file mode 100644
index 6cd25e69ef51..000000000000
--- a/drivers/gpu/msm/kgsl_drawobj.c
+++ /dev/null
@@ -1,1154 +0,0 @@
-/* Copyright (c) 2016-2017, 2019, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-/*
- * KGSL drawobj management
- * A drawobj is a single submission from userland.  The drawobj
- * encapsulates everything about the submission : command buffers, flags and
- * sync points.
- *
- * Sync points are events that need to expire before the
- * drawobj can be queued to the hardware. All synpoints are contained in an
- * array of kgsl_drawobj_sync_event structs in the drawobj. There can be
- * multiple types of events both internal ones (GPU events) and external
- * triggers. As the events expire bits are cleared in a pending bitmap stored
- * in the drawobj. The GPU will submit the command as soon as the bitmap
- * goes to zero indicating no more pending events.
- */
-
-#include <linux/uaccess.h>
-#include <linux/list.h>
-#include <linux/compat.h>
-
-#include "kgsl.h"
-#include "kgsl_device.h"
-#include "kgsl_drawobj.h"
-#include "kgsl_sync.h"
-#include "kgsl_trace.h"
-#include "kgsl_compat.h"
-
-/*
- * Define an kmem cache for the memobj & sparseobj structures since we
- * allocate and free them so frequently
- */
-static struct kmem_cache *memobjs_cache;
-static struct kmem_cache *sparseobjs_cache;
-
-
-void kgsl_drawobj_destroy_object(struct kref *kref)
-{
-	struct kgsl_drawobj *drawobj = container_of(kref,
-		struct kgsl_drawobj, refcount);
-	struct kgsl_drawobj_sync *syncobj;
-
-	kgsl_context_put(drawobj->context);
-
-	switch (drawobj->type) {
-	case SYNCOBJ_TYPE:
-		syncobj = SYNCOBJ(drawobj);
-		kfree(syncobj->synclist);
-		kfree(syncobj);
-		break;
-	case CMDOBJ_TYPE:
-	case MARKEROBJ_TYPE:
-		kfree(CMDOBJ(drawobj));
-		break;
-	case SPARSEOBJ_TYPE:
-		kfree(SPARSEOBJ(drawobj));
-		break;
-	}
-}
-
-void kgsl_dump_syncpoints(struct kgsl_device *device,
-	struct kgsl_drawobj_sync *syncobj)
-{
-	struct kgsl_drawobj_sync_event *event;
-	unsigned int i;
-
-	for (i = 0; i < syncobj->numsyncs; i++) {
-		event = &syncobj->synclist[i];
-
-		if (!kgsl_drawobj_event_pending(syncobj, i))
-			continue;
-
-		switch (event->type) {
-		case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP: {
-			unsigned int retired;
-
-			 kgsl_readtimestamp(event->device,
-				event->context, KGSL_TIMESTAMP_RETIRED,
-				&retired);
-
-			dev_err(device->dev,
-				"  [timestamp] context %d timestamp %d (retired %d)\n",
-				event->context->id, event->timestamp,
-				retired);
-			break;
-		}
-		case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
-			dev_err(device->dev, "  fence: %s\n",
-					event->fence_name);
-			break;
-		}
-	}
-}
-
-static void syncobj_timer(unsigned long data)
-{
-	struct kgsl_device *device;
-	struct kgsl_drawobj_sync *syncobj = (struct kgsl_drawobj_sync *) data;
-	struct kgsl_drawobj *drawobj;
-	struct kgsl_drawobj_sync_event *event;
-	unsigned int i;
-
-	if (syncobj == NULL)
-		return;
-
-	drawobj = DRAWOBJ(syncobj);
-
-	if (!kref_get_unless_zero(&drawobj->refcount))
-		return;
-
-	if (drawobj->context == NULL) {
-		kgsl_drawobj_put(drawobj);
-		return;
-	}
-
-	device = drawobj->context->device;
-
-	dev_err(device->dev,
-		"kgsl: possible gpu syncpoint deadlock for context %d timestamp %d\n",
-		drawobj->context->id, drawobj->timestamp);
-
-	set_bit(ADRENO_CONTEXT_FENCE_LOG, &drawobj->context->priv);
-	kgsl_context_dump(drawobj->context);
-	clear_bit(ADRENO_CONTEXT_FENCE_LOG, &drawobj->context->priv);
-
-	dev_err(device->dev, "      pending events:\n");
-
-	for (i = 0; i < syncobj->numsyncs; i++) {
-		event = &syncobj->synclist[i];
-
-		if (!kgsl_drawobj_event_pending(syncobj, i))
-			continue;
-
-		switch (event->type) {
-		case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP:
-			dev_err(device->dev, "       [%d] TIMESTAMP %d:%d\n",
-				i, event->context->id, event->timestamp);
-			break;
-		case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
-			dev_err(device->dev, "       [%d] FENCE %s\n",
-					i, event->fence_name);
-			break;
-		}
-	}
-
-	kgsl_drawobj_put(drawobj);
-	dev_err(device->dev, "--gpu syncpoint deadlock print end--\n");
-}
-
-/*
- * a generic function to retire a pending sync event and (possibly) kick the
- * dispatcher.
- * Returns false if the event was already marked for cancellation in another
- * thread. This function should return true if this thread is responsible for
- * freeing up the memory, and the event will not be cancelled.
- */
-static bool drawobj_sync_expire(struct kgsl_device *device,
-	struct kgsl_drawobj_sync_event *event)
-{
-	struct kgsl_drawobj_sync *syncobj = event->syncobj;
-	/*
-	 * Clear the event from the pending mask - if it is already clear, then
-	 * leave without doing anything useful
-	 */
-	if (!test_and_clear_bit(event->id, &syncobj->pending))
-		return false;
-
-	/*
-	 * If no more pending events, delete the timer and schedule the command
-	 * for dispatch
-	 */
-	if (!kgsl_drawobj_events_pending(event->syncobj)) {
-		del_timer_sync(&syncobj->timer);
-
-		if (device->ftbl->drawctxt_sched)
-			device->ftbl->drawctxt_sched(device,
-				event->syncobj->base.context);
-	}
-	return true;
-}
-
-/*
- * This function is called by the GPU event when the sync event timestamp
- * expires
- */
-static void drawobj_sync_func(struct kgsl_device *device,
-		struct kgsl_event_group *group, void *priv, int result)
-{
-	struct kgsl_drawobj_sync_event *event = priv;
-
-	trace_syncpoint_timestamp_expire(event->syncobj,
-		event->context, event->timestamp);
-
-	/*
-	 * Put down the context ref count only if
-	 * this thread successfully clears the pending bit mask.
-	 */
-	if (drawobj_sync_expire(device, event))
-		kgsl_context_put(event->context);
-
-	kgsl_drawobj_put(&event->syncobj->base);
-}
-
-static inline void memobj_list_free(struct list_head *list)
-{
-	struct kgsl_memobj_node *mem, *tmpmem;
-
-	/* Free the cmd mem here */
-	list_for_each_entry_safe(mem, tmpmem, list, node) {
-		list_del_init(&mem->node);
-		kmem_cache_free(memobjs_cache, mem);
-	}
-}
-
-static void drawobj_destroy_sparse(struct kgsl_drawobj *drawobj)
-{
-	struct kgsl_sparseobj_node *mem, *tmpmem;
-	struct list_head *list = &SPARSEOBJ(drawobj)->sparselist;
-
-	/* Free the sparse mem here */
-	list_for_each_entry_safe(mem, tmpmem, list, node) {
-		list_del_init(&mem->node);
-		kmem_cache_free(sparseobjs_cache, mem);
-	}
-}
-
-static void drawobj_destroy_sync(struct kgsl_drawobj *drawobj)
-{
-	struct kgsl_drawobj_sync *syncobj = SYNCOBJ(drawobj);
-	unsigned int i;
-
-	/* Zap the canary timer */
-	del_timer_sync(&syncobj->timer);
-
-	/*
-	 * Clear all pending events - this will render any subsequent async
-	 * callbacks harmless
-	 */
-	for (i = 0; i < syncobj->numsyncs; i++) {
-		struct kgsl_drawobj_sync_event *event = &syncobj->synclist[i];
-
-		/*
-		 * Don't do anything if the event has already expired.
-		 * If this thread clears the pending bit mask then it is
-		 * responsible for doing context put.
-		 */
-		if (!test_and_clear_bit(i, &syncobj->pending))
-			continue;
-
-		switch (event->type) {
-		case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP:
-			kgsl_cancel_event(drawobj->device,
-				&event->context->events, event->timestamp,
-				drawobj_sync_func, event);
-			/*
-			 * Do context put here to make sure the context is alive
-			 * till this thread cancels kgsl event.
-			 */
-			kgsl_context_put(event->context);
-			break;
-		case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
-			kgsl_sync_fence_async_cancel(event->handle);
-			kgsl_drawobj_put(drawobj);
-			break;
-		}
-	}
-
-	/*
-	 * If we cancelled an event, there's a good chance that the context is
-	 * on a dispatcher queue, so schedule to get it removed.
-	 */
-	if (!bitmap_empty(&syncobj->pending, KGSL_MAX_SYNCPOINTS) &&
-		drawobj->device->ftbl->drawctxt_sched)
-		drawobj->device->ftbl->drawctxt_sched(drawobj->device,
-							drawobj->context);
-
-}
-
-static void drawobj_destroy_cmd(struct kgsl_drawobj *drawobj)
-{
-	struct kgsl_drawobj_cmd *cmdobj = CMDOBJ(drawobj);
-
-	/*
-	 * Release the refcount on the mem entry associated with the
-	 * ib profiling buffer
-	 */
-	if (cmdobj->base.flags & KGSL_DRAWOBJ_PROFILING)
-		kgsl_mem_entry_put(cmdobj->profiling_buf_entry);
-
-	/* Destroy the cmdlist we created */
-	memobj_list_free(&cmdobj->cmdlist);
-
-	/* Destroy the memlist we created */
-	memobj_list_free(&cmdobj->memlist);
-}
-
-/**
- * kgsl_drawobj_destroy() - Destroy a kgsl object structure
- * @obj: Pointer to the kgsl object to destroy
- *
- * Start the process of destroying a command batch.  Cancel any pending events
- * and decrement the refcount.  Asynchronous events can still signal after
- * kgsl_drawobj_destroy has returned.
- */
-void kgsl_drawobj_destroy(struct kgsl_drawobj *drawobj)
-{
-	if (!drawobj)
-		return;
-
-	if (drawobj->type & SYNCOBJ_TYPE)
-		drawobj_destroy_sync(drawobj);
-	else if (drawobj->type & (CMDOBJ_TYPE | MARKEROBJ_TYPE))
-		drawobj_destroy_cmd(drawobj);
-	else if (drawobj->type == SPARSEOBJ_TYPE)
-		drawobj_destroy_sparse(drawobj);
-	else
-		return;
-
-	kgsl_drawobj_put(drawobj);
-}
-EXPORT_SYMBOL(kgsl_drawobj_destroy);
-
-static bool drawobj_sync_fence_func(void *priv)
-{
-	struct kgsl_drawobj_sync_event *event = priv;
-
-	trace_syncpoint_fence_expire(event->syncobj, event->fence_name);
-
-	/*
-	 * Only call kgsl_drawobj_put() if it's not marked for cancellation
-	 * in another thread.
-	 */
-	if (drawobj_sync_expire(event->device, event)) {
-		kgsl_drawobj_put(&event->syncobj->base);
-		return true;
-	}
-	return false;
-}
-
-/* drawobj_add_sync_fence() - Add a new sync fence syncpoint
- * @device: KGSL device
- * @syncobj: KGSL sync obj to add the sync point to
- * @priv: Private structure passed by the user
- *
- * Add a new fence sync syncpoint to the sync obj.
- */
-static int drawobj_add_sync_fence(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void *priv)
-{
-	struct kgsl_cmd_syncpoint_fence *sync = priv;
-	struct kgsl_drawobj *drawobj = DRAWOBJ(syncobj);
-	struct kgsl_drawobj_sync_event *event;
-	unsigned int id;
-
-	kref_get(&drawobj->refcount);
-
-	id = syncobj->numsyncs++;
-
-	event = &syncobj->synclist[id];
-
-	event->id = id;
-	event->type = KGSL_CMD_SYNCPOINT_TYPE_FENCE;
-	event->syncobj = syncobj;
-	event->device = device;
-	event->context = NULL;
-
-	set_bit(event->id, &syncobj->pending);
-
-	event->handle = kgsl_sync_fence_async_wait(sync->fd,
-				drawobj_sync_fence_func, event,
-				event->fence_name, sizeof(event->fence_name));
-
-	if (IS_ERR_OR_NULL(event->handle)) {
-		int ret = PTR_ERR(event->handle);
-
-		clear_bit(event->id, &syncobj->pending);
-		event->handle = NULL;
-
-		kgsl_drawobj_put(drawobj);
-
-		/*
-		 * If ret == 0 the fence was already signaled - print a trace
-		 * message so we can track that
-		 */
-		if (ret == 0)
-			trace_syncpoint_fence_expire(syncobj, "signaled");
-
-		return ret;
-	}
-
-	trace_syncpoint_fence(syncobj, event->fence_name);
-
-	return 0;
-}
-
-/* drawobj_add_sync_timestamp() - Add a new sync point for a sync obj
- * @device: KGSL device
- * @syncobj: KGSL sync obj to add the sync point to
- * @priv: Private structure passed by the user
- *
- * Add a new sync point timestamp event to the sync obj.
- */
-static int drawobj_add_sync_timestamp(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void *priv)
-{
-	struct kgsl_cmd_syncpoint_timestamp *sync = priv;
-	struct kgsl_drawobj *drawobj = DRAWOBJ(syncobj);
-	struct kgsl_context *context = kgsl_context_get(device,
-		sync->context_id);
-	struct kgsl_drawobj_sync_event *event;
-	int ret = -EINVAL;
-	unsigned int id;
-
-	if (context == NULL)
-		return -EINVAL;
-
-	/*
-	 * We allow somebody to create a sync point on their own context.
-	 * This has the effect of delaying a command from submitting until the
-	 * dependent command has cleared.  That said we obviously can't let them
-	 * create a sync point on a future timestamp.
-	 */
-
-	if (context == drawobj->context) {
-		unsigned int queued;
-
-		kgsl_readtimestamp(device, context, KGSL_TIMESTAMP_QUEUED,
-			&queued);
-
-		if (timestamp_cmp(sync->timestamp, queued) > 0) {
-			KGSL_DRV_ERR(device,
-			"Cannot create syncpoint for future timestamp %d (current %d)\n",
-				sync->timestamp, queued);
-			goto done;
-		}
-	}
-
-	kref_get(&drawobj->refcount);
-
-	id = syncobj->numsyncs++;
-
-	event = &syncobj->synclist[id];
-	event->id = id;
-
-	event->type = KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP;
-	event->syncobj = syncobj;
-	event->context = context;
-	event->timestamp = sync->timestamp;
-	event->device = device;
-
-	set_bit(event->id, &syncobj->pending);
-
-	ret = kgsl_add_event(device, &context->events, sync->timestamp,
-		drawobj_sync_func, event);
-
-	if (ret) {
-		clear_bit(event->id, &syncobj->pending);
-		kgsl_drawobj_put(drawobj);
-	} else {
-		trace_syncpoint_timestamp(syncobj, context, sync->timestamp);
-	}
-
-done:
-	if (ret)
-		kgsl_context_put(context);
-
-	return ret;
-}
-
-/**
- * kgsl_drawobj_sync_add_sync() - Add a sync point to a command
- * batch
- * @device: Pointer to the KGSL device struct for the GPU
- * @syncobj: Pointer to the sync obj
- * @sync: Pointer to the user-specified struct defining the syncpoint
- *
- * Create a new sync point in the sync obj based on the
- * user specified parameters
- */
-int kgsl_drawobj_sync_add_sync(struct kgsl_device *device,
-	struct kgsl_drawobj_sync *syncobj,
-	struct kgsl_cmd_syncpoint *sync)
-{
-	void *priv;
-	int ret, psize;
-	struct kgsl_drawobj *drawobj = DRAWOBJ(syncobj);
-	int (*func)(struct kgsl_device *device,
-			struct kgsl_drawobj_sync *syncobj,
-			void *priv);
-
-	switch (sync->type) {
-	case KGSL_CMD_SYNCPOINT_TYPE_TIMESTAMP:
-		psize = sizeof(struct kgsl_cmd_syncpoint_timestamp);
-		func = drawobj_add_sync_timestamp;
-		break;
-	case KGSL_CMD_SYNCPOINT_TYPE_FENCE:
-		psize = sizeof(struct kgsl_cmd_syncpoint_fence);
-		func = drawobj_add_sync_fence;
-		break;
-	default:
-		KGSL_DRV_ERR(device,
-			"bad syncpoint type ctxt %d type 0x%x size %zu\n",
-			drawobj->context->id, sync->type, sync->size);
-		return -EINVAL;
-	}
-
-	if (sync->size != psize) {
-		KGSL_DRV_ERR(device,
-			"bad syncpoint size ctxt %d type 0x%x size %zu\n",
-			drawobj->context->id, sync->type, sync->size);
-		return -EINVAL;
-	}
-
-	priv = kzalloc(sync->size, GFP_KERNEL);
-	if (priv == NULL)
-		return -ENOMEM;
-
-	if (copy_from_user(priv, sync->priv, sync->size)) {
-		kfree(priv);
-		return -EFAULT;
-	}
-
-	ret = func(device, syncobj, priv);
-	kfree(priv);
-
-	return ret;
-}
-
-static void add_profiling_buffer(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj,
-		uint64_t gpuaddr, uint64_t size,
-		unsigned int id, uint64_t offset)
-{
-	struct kgsl_mem_entry *entry;
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-
-	if (!(drawobj->flags & KGSL_DRAWOBJ_PROFILING))
-		return;
-
-	/* Only the first buffer entry counts - ignore the rest */
-	if (cmdobj->profiling_buf_entry != NULL)
-		return;
-
-	if (id != 0)
-		entry = kgsl_sharedmem_find_id(drawobj->context->proc_priv,
-				id);
-	else
-		entry = kgsl_sharedmem_find(drawobj->context->proc_priv,
-			gpuaddr);
-
-	if (entry != NULL) {
-		if (!kgsl_gpuaddr_in_memdesc(&entry->memdesc, gpuaddr, size)) {
-			kgsl_mem_entry_put(entry);
-			entry = NULL;
-		}
-	}
-
-	if (entry == NULL) {
-		KGSL_DRV_ERR(device,
-			"ignore bad profile buffer ctxt %d id %d offset %lld gpuaddr %llx size %lld\n",
-			drawobj->context->id, id, offset, gpuaddr, size);
-		return;
-	}
-
-
-	if (!id) {
-		cmdobj->profiling_buffer_gpuaddr = gpuaddr;
-	} else {
-		u64 off = offset + sizeof(struct kgsl_drawobj_profiling_buffer);
-
-		/*
-		 * Make sure there is enough room in the object to store the
-		 * entire profiling buffer object
-		 */
-		if (off < offset || off >= entry->memdesc.size) {
-			dev_err(device->dev,
-				"ignore invalid profile offset ctxt %d id %d offset %lld gpuaddr %llx size %lld\n",
-			drawobj->context->id, id, offset, gpuaddr, size);
-			kgsl_mem_entry_put(entry);
-			return;
-		}
-
-		cmdobj->profiling_buffer_gpuaddr =
-			entry->memdesc.gpuaddr + offset;
-	}
-
-	cmdobj->profiling_buf_entry = entry;
-}
-
-/**
- * kgsl_drawobj_cmd_add_ibdesc() - Add a legacy ibdesc to a command
- * batch
- * @cmdobj: Pointer to the ib
- * @ibdesc: Pointer to the user-specified struct defining the memory or IB
- *
- * Create a new memory entry in the ib based on the
- * user specified parameters
- */
-int kgsl_drawobj_cmd_add_ibdesc(struct kgsl_device *device,
-	struct kgsl_drawobj_cmd *cmdobj, struct kgsl_ibdesc *ibdesc)
-{
-	uint64_t gpuaddr = (uint64_t) ibdesc->gpuaddr;
-	uint64_t size = (uint64_t) ibdesc->sizedwords << 2;
-	struct kgsl_memobj_node *mem;
-	struct kgsl_drawobj *drawobj = DRAWOBJ(cmdobj);
-
-	/* sanitize the ibdesc ctrl flags */
-	ibdesc->ctrl &= KGSL_IBDESC_MEMLIST | KGSL_IBDESC_PROFILING_BUFFER;
-
-	if (drawobj->flags & KGSL_DRAWOBJ_MEMLIST &&
-			ibdesc->ctrl & KGSL_IBDESC_MEMLIST) {
-		if (ibdesc->ctrl & KGSL_IBDESC_PROFILING_BUFFER) {
-			add_profiling_buffer(device, cmdobj,
-					gpuaddr, size, 0, 0);
-			return 0;
-		}
-	}
-
-	/* Ignore if SYNC or MARKER is specified */
-	if (drawobj->type & (SYNCOBJ_TYPE | MARKEROBJ_TYPE))
-		return 0;
-
-	mem = kmem_cache_alloc(memobjs_cache, GFP_KERNEL);
-	if (mem == NULL)
-		return -ENOMEM;
-
-	mem->gpuaddr = gpuaddr;
-	mem->size = size;
-	mem->priv = 0;
-	mem->id = 0;
-	mem->offset = 0;
-	mem->flags = 0;
-
-	if (drawobj->flags & KGSL_DRAWOBJ_MEMLIST &&
-			ibdesc->ctrl & KGSL_IBDESC_MEMLIST)
-		/* add to the memlist */
-		list_add_tail(&mem->node, &cmdobj->memlist);
-	else {
-		/* set the preamble flag if directed to */
-		if (drawobj->context->flags & KGSL_CONTEXT_PREAMBLE &&
-				list_empty(&cmdobj->cmdlist))
-			mem->flags = KGSL_CMDLIST_CTXTSWITCH_PREAMBLE;
-
-		/* add to the cmd list */
-		list_add_tail(&mem->node, &cmdobj->cmdlist);
-	}
-
-	return 0;
-}
-
-static void *_drawobj_create(struct kgsl_device *device,
-	struct kgsl_context *context, unsigned int size,
-	unsigned int type)
-{
-	void *obj = kzalloc(size, GFP_KERNEL);
-	struct kgsl_drawobj *drawobj;
-
-	if (obj == NULL)
-		return ERR_PTR(-ENOMEM);
-
-	/*
-	 * Increase the reference count on the context so it doesn't disappear
-	 * during the lifetime of this object
-	 */
-	if (!_kgsl_context_get(context)) {
-		kfree(obj);
-		return ERR_PTR(-ENOENT);
-	}
-
-	drawobj = obj;
-
-	kref_init(&drawobj->refcount);
-
-	drawobj->device = device;
-	drawobj->context = context;
-	drawobj->type = type;
-
-	return obj;
-}
-
-/**
- * kgsl_drawobj_sparse_create() - Create a new sparse obj structure
- * @device: Pointer to a KGSL device struct
- * @context: Pointer to a KGSL context struct
- * @flags: Flags for the sparse obj
- *
- * Allocate an new kgsl_drawobj_sparse structure
- */
-struct kgsl_drawobj_sparse *kgsl_drawobj_sparse_create(
-		struct kgsl_device *device,
-		struct kgsl_context *context, unsigned int flags)
-{
-	struct kgsl_drawobj_sparse *sparseobj = _drawobj_create(device,
-		context, sizeof(*sparseobj), SPARSEOBJ_TYPE);
-
-	if (!IS_ERR(sparseobj))
-		INIT_LIST_HEAD(&sparseobj->sparselist);
-
-	return sparseobj;
-}
-
-/**
- * kgsl_drawobj_sync_create() - Create a new sync obj
- * structure
- * @device: Pointer to a KGSL device struct
- * @context: Pointer to a KGSL context struct
- *
- * Allocate an new kgsl_drawobj_sync structure
- */
-struct kgsl_drawobj_sync *kgsl_drawobj_sync_create(struct kgsl_device *device,
-		struct kgsl_context *context)
-{
-	struct kgsl_drawobj_sync *syncobj = _drawobj_create(device,
-		context, sizeof(*syncobj), SYNCOBJ_TYPE);
-
-	/* Add a timer to help debug sync deadlocks */
-	if (!IS_ERR(syncobj))
-		setup_timer(&syncobj->timer, syncobj_timer,
-				(unsigned long) syncobj);
-
-	return syncobj;
-}
-
-/**
- * kgsl_drawobj_cmd_create() - Create a new command obj
- * structure
- * @device: Pointer to a KGSL device struct
- * @context: Pointer to a KGSL context struct
- * @flags: Flags for the command obj
- * @type: type of cmdobj MARKER/CMD
- *
- * Allocate a new kgsl_drawobj_cmd structure
- */
-struct kgsl_drawobj_cmd *kgsl_drawobj_cmd_create(struct kgsl_device *device,
-		struct kgsl_context *context, unsigned int flags,
-		unsigned int type)
-{
-	struct kgsl_drawobj_cmd *cmdobj = _drawobj_create(device,
-		context, sizeof(*cmdobj),
-		(type & (CMDOBJ_TYPE | MARKEROBJ_TYPE)));
-
-	if (!IS_ERR(cmdobj)) {
-		/* sanitize our flags for drawobj's */
-		cmdobj->base.flags = flags & (KGSL_DRAWOBJ_CTX_SWITCH
-				| KGSL_DRAWOBJ_MARKER
-				| KGSL_DRAWOBJ_END_OF_FRAME
-				| KGSL_DRAWOBJ_PWR_CONSTRAINT
-				| KGSL_DRAWOBJ_MEMLIST
-				| KGSL_DRAWOBJ_PROFILING
-				| KGSL_DRAWOBJ_PROFILING_KTIME);
-
-		INIT_LIST_HEAD(&cmdobj->cmdlist);
-		INIT_LIST_HEAD(&cmdobj->memlist);
-	}
-
-	return cmdobj;
-}
-
-#ifdef CONFIG_COMPAT
-static int add_ibdesc_list_compat(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr, int count)
-{
-	int i, ret = 0;
-	struct kgsl_ibdesc_compat ibdesc32;
-	struct kgsl_ibdesc ibdesc;
-
-	for (i = 0; i < count; i++) {
-		memset(&ibdesc32, 0, sizeof(ibdesc32));
-
-		if (copy_from_user(&ibdesc32, ptr, sizeof(ibdesc32))) {
-			ret = -EFAULT;
-			break;
-		}
-
-		ibdesc.gpuaddr = (unsigned long) ibdesc32.gpuaddr;
-		ibdesc.sizedwords = (size_t) ibdesc32.sizedwords;
-		ibdesc.ctrl = (unsigned int) ibdesc32.ctrl;
-
-		ret = kgsl_drawobj_cmd_add_ibdesc(device, cmdobj, &ibdesc);
-		if (ret)
-			break;
-
-		ptr += sizeof(ibdesc32);
-	}
-
-	return ret;
-}
-
-static int add_syncpoints_compat(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void __user *ptr, int count)
-{
-	struct kgsl_cmd_syncpoint_compat sync32;
-	struct kgsl_cmd_syncpoint sync;
-	int i, ret = 0;
-
-	for (i = 0; i < count; i++) {
-		memset(&sync32, 0, sizeof(sync32));
-
-		if (copy_from_user(&sync32, ptr, sizeof(sync32))) {
-			ret = -EFAULT;
-			break;
-		}
-
-		sync.type = sync32.type;
-		sync.priv = compat_ptr(sync32.priv);
-		sync.size = (size_t) sync32.size;
-
-		ret = kgsl_drawobj_sync_add_sync(device, syncobj, &sync);
-		if (ret)
-			break;
-
-		ptr += sizeof(sync32);
-	}
-
-	return ret;
-}
-#else
-static int add_ibdesc_list_compat(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr, int count)
-{
-	return -EINVAL;
-}
-
-static int add_syncpoints_compat(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void __user *ptr, int count)
-{
-	return -EINVAL;
-}
-#endif
-
-/* Returns:
- *   -EINVAL: Bad data
- *   0: All data fields are empty (nothing to do)
- *   1: All list information is valid
- */
-static int _verify_input_list(unsigned int count, void __user *ptr,
-		unsigned int size)
-{
-	/* Return early if nothing going on */
-	if (count == 0 && ptr == NULL && size == 0)
-		return 0;
-
-	/* Sanity check inputs */
-	if (count == 0 || ptr == NULL || size == 0)
-		return -EINVAL;
-
-	return 1;
-}
-
-int kgsl_drawobj_cmd_add_ibdesc_list(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr, int count)
-{
-	struct kgsl_ibdesc ibdesc;
-	struct kgsl_drawobj *baseobj = DRAWOBJ(cmdobj);
-	int i, ret;
-
-	/* Ignore everything if this is a MARKER */
-	if (baseobj->type & MARKEROBJ_TYPE)
-		return 0;
-
-	ret = _verify_input_list(count, ptr, sizeof(ibdesc));
-	if (ret <= 0)
-		return -EINVAL;
-
-	if (is_compat_task())
-		return add_ibdesc_list_compat(device, cmdobj, ptr, count);
-
-	for (i = 0; i < count; i++) {
-		memset(&ibdesc, 0, sizeof(ibdesc));
-
-		if (copy_from_user(&ibdesc, ptr, sizeof(ibdesc)))
-			return -EFAULT;
-
-		ret = kgsl_drawobj_cmd_add_ibdesc(device, cmdobj, &ibdesc);
-		if (ret)
-			return ret;
-
-		ptr += sizeof(ibdesc);
-	}
-
-	return 0;
-}
-
-int kgsl_drawobj_sync_add_syncpoints(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void __user *ptr, int count)
-{
-	struct kgsl_cmd_syncpoint sync;
-	int i, ret;
-
-	if (count == 0)
-		return 0;
-
-	syncobj->synclist = kcalloc(count,
-		sizeof(struct kgsl_drawobj_sync_event), GFP_KERNEL);
-
-	if (syncobj->synclist == NULL)
-		return -ENOMEM;
-
-	if (is_compat_task())
-		return add_syncpoints_compat(device, syncobj, ptr, count);
-
-	for (i = 0; i < count; i++) {
-		memset(&sync, 0, sizeof(sync));
-
-		if (copy_from_user(&sync, ptr, sizeof(sync)))
-			return -EFAULT;
-
-		ret = kgsl_drawobj_sync_add_sync(device, syncobj, &sync);
-		if (ret)
-			return ret;
-
-		ptr += sizeof(sync);
-	}
-
-	return 0;
-}
-
-static int kgsl_drawobj_add_memobject(struct list_head *head,
-		struct kgsl_command_object *obj)
-{
-	struct kgsl_memobj_node *mem;
-
-	mem = kmem_cache_alloc(memobjs_cache, GFP_KERNEL);
-	if (mem == NULL)
-		return -ENOMEM;
-
-	mem->gpuaddr = obj->gpuaddr;
-	mem->size = obj->size;
-	mem->id = obj->id;
-	mem->offset = obj->offset;
-	mem->flags = obj->flags;
-	mem->priv = 0;
-
-	list_add_tail(&mem->node, head);
-	return 0;
-}
-
-static int kgsl_drawobj_add_sparseobject(struct list_head *head,
-		struct kgsl_sparse_binding_object *obj, unsigned int virt_id)
-{
-	struct kgsl_sparseobj_node *mem;
-
-	mem = kmem_cache_alloc(sparseobjs_cache, GFP_KERNEL);
-	if (mem == NULL)
-		return -ENOMEM;
-
-	mem->virt_id = virt_id;
-	mem->obj.id = obj->id;
-	mem->obj.virtoffset = obj->virtoffset;
-	mem->obj.physoffset = obj->physoffset;
-	mem->obj.size = obj->size;
-	mem->obj.flags = obj->flags;
-
-	list_add_tail(&mem->node, head);
-	return 0;
-}
-
-int kgsl_drawobj_sparse_add_sparselist(struct kgsl_device *device,
-		struct kgsl_drawobj_sparse *sparseobj, unsigned int id,
-		void __user *ptr, unsigned int size, unsigned int count)
-{
-	struct kgsl_sparse_binding_object obj;
-	int i, ret = 0;
-
-	ret = _verify_input_list(count, ptr, size);
-	if (ret <= 0)
-		return ret;
-
-	for (i = 0; i < count; i++) {
-		memset(&obj, 0, sizeof(obj));
-
-		ret = _copy_from_user(&obj, ptr, sizeof(obj), size);
-		if (ret)
-			return ret;
-
-		if (!(obj.flags & (KGSL_SPARSE_BIND | KGSL_SPARSE_UNBIND)))
-			return -EINVAL;
-
-		ret = kgsl_drawobj_add_sparseobject(&sparseobj->sparselist,
-			&obj, id);
-		if (ret)
-			return ret;
-
-		ptr += sizeof(obj);
-	}
-
-	sparseobj->size = size;
-	sparseobj->count = count;
-
-	return 0;
-}
-
-
-#define CMDLIST_FLAGS \
-	(KGSL_CMDLIST_IB | \
-	 KGSL_CMDLIST_CTXTSWITCH_PREAMBLE | \
-	 KGSL_CMDLIST_IB_PREAMBLE)
-
-/* This can only accept MARKEROBJ_TYPE and CMDOBJ_TYPE */
-int kgsl_drawobj_cmd_add_cmdlist(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr,
-		unsigned int size, unsigned int count)
-{
-	struct kgsl_command_object obj;
-	struct kgsl_drawobj *baseobj = DRAWOBJ(cmdobj);
-	int i, ret;
-
-	/* Ignore everything if this is a MARKER */
-	if (baseobj->type & MARKEROBJ_TYPE)
-		return 0;
-
-	ret = _verify_input_list(count, ptr, size);
-	if (ret <= 0)
-		return ret;
-
-	for (i = 0; i < count; i++) {
-		memset(&obj, 0, sizeof(obj));
-
-		ret = _copy_from_user(&obj, ptr, sizeof(obj), size);
-		if (ret)
-			return ret;
-
-		/* Sanity check the flags */
-		if (!(obj.flags & CMDLIST_FLAGS)) {
-			KGSL_DRV_ERR(device,
-				"invalid cmdobj ctxt %d flags %d id %d offset %lld addr %lld size %lld\n",
-				baseobj->context->id, obj.flags, obj.id,
-				obj.offset, obj.gpuaddr, obj.size);
-			return -EINVAL;
-		}
-
-		ret = kgsl_drawobj_add_memobject(&cmdobj->cmdlist, &obj);
-		if (ret)
-			return ret;
-
-		ptr += sizeof(obj);
-	}
-
-	return 0;
-}
-
-int kgsl_drawobj_cmd_add_memlist(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr,
-		unsigned int size, unsigned int count)
-{
-	struct kgsl_command_object obj;
-	struct kgsl_drawobj *baseobj = DRAWOBJ(cmdobj);
-	int i, ret;
-
-	/* Ignore everything if this is a MARKER */
-	if (baseobj->type & MARKEROBJ_TYPE)
-		return 0;
-
-	ret = _verify_input_list(count, ptr, size);
-	if (ret <= 0)
-		return ret;
-
-	for (i = 0; i < count; i++) {
-		memset(&obj, 0, sizeof(obj));
-
-		ret = _copy_from_user(&obj, ptr, sizeof(obj), size);
-		if (ret)
-			return ret;
-
-		if (!(obj.flags & KGSL_OBJLIST_MEMOBJ)) {
-			KGSL_DRV_ERR(device,
-				"invalid memobj ctxt %d flags %d id %d offset %lld addr %lld size %lld\n",
-				DRAWOBJ(cmdobj)->context->id, obj.flags,
-				obj.id, obj.offset, obj.gpuaddr, obj.size);
-			return -EINVAL;
-		}
-
-		if (obj.flags & KGSL_OBJLIST_PROFILE)
-			add_profiling_buffer(device, cmdobj, obj.gpuaddr,
-				obj.size, obj.id, obj.offset);
-		else {
-			ret = kgsl_drawobj_add_memobject(&cmdobj->memlist,
-				&obj);
-			if (ret)
-				return ret;
-		}
-
-		ptr += sizeof(obj);
-	}
-
-	return 0;
-}
-
-int kgsl_drawobj_sync_add_synclist(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void __user *ptr,
-		unsigned int size, unsigned int count)
-{
-	struct kgsl_command_syncpoint syncpoint;
-	struct kgsl_cmd_syncpoint sync;
-	int i, ret;
-
-	/* If creating a sync and the data is not there or wrong then error */
-	ret = _verify_input_list(count, ptr, size);
-	if (ret <= 0)
-		return -EINVAL;
-
-	syncobj->synclist = kcalloc(count,
-		sizeof(struct kgsl_drawobj_sync_event), GFP_KERNEL);
-
-	if (syncobj->synclist == NULL)
-		return -ENOMEM;
-
-	for (i = 0; i < count; i++) {
-		memset(&syncpoint, 0, sizeof(syncpoint));
-
-		ret = _copy_from_user(&syncpoint, ptr, sizeof(syncpoint), size);
-		if (ret)
-			return ret;
-
-		sync.type = syncpoint.type;
-		sync.priv = to_user_ptr(syncpoint.priv);
-		sync.size = syncpoint.size;
-
-		ret = kgsl_drawobj_sync_add_sync(device, syncobj, &sync);
-		if (ret)
-			return ret;
-
-		ptr += sizeof(syncpoint);
-	}
-
-	return 0;
-}
-
-void kgsl_drawobjs_cache_exit(void)
-{
-	kmem_cache_destroy(memobjs_cache);
-	kmem_cache_destroy(sparseobjs_cache);
-}
-
-int kgsl_drawobjs_cache_init(void)
-{
-	memobjs_cache = KMEM_CACHE(kgsl_memobj_node, 0);
-	sparseobjs_cache = KMEM_CACHE(kgsl_sparseobj_node, 0);
-
-	if (!memobjs_cache || !sparseobjs_cache)
-		return -ENOMEM;
-
-	return 0;
-}
diff --git a/drivers/gpu/msm/kgsl_drawobj.h b/drivers/gpu/msm/kgsl_drawobj.h
deleted file mode 100644
index 06eef7fdd884..000000000000
--- a/drivers/gpu/msm/kgsl_drawobj.h
+++ /dev/null
@@ -1,236 +0,0 @@
-/* Copyright (c) 2016-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- */
-
-#ifndef __KGSL_DRAWOBJ_H
-#define __KGSL_DRAWOBJ_H
-
-#define DRAWOBJ(obj) (&obj->base)
-#define SYNCOBJ(obj) \
-	container_of(obj, struct kgsl_drawobj_sync, base)
-#define CMDOBJ(obj) \
-	container_of(obj, struct kgsl_drawobj_cmd, base)
-#define SPARSEOBJ(obj) \
-	container_of(obj, struct kgsl_drawobj_sparse, base)
-
-#define CMDOBJ_TYPE     BIT(0)
-#define MARKEROBJ_TYPE  BIT(1)
-#define SYNCOBJ_TYPE    BIT(2)
-#define SPARSEOBJ_TYPE  BIT(3)
-
-/**
- * struct kgsl_drawobj - KGSL drawobj descriptor
- * @device: KGSL GPU device that the command was created for
- * @context: KGSL context that created the command
- * @type: Object type
- * @timestamp: Timestamp assigned to the command
- * @flags: flags
- * @refcount: kref structure to maintain the reference count
- */
-struct kgsl_drawobj {
-	struct kgsl_device *device;
-	struct kgsl_context *context;
-	uint32_t type;
-	uint32_t timestamp;
-	unsigned long flags;
-	struct kref refcount;
-};
-
-/**
- * struct kgsl_drawobj_cmd - KGSL command obj, This covers marker
- * cmds also since markers are special form of cmds that do not
- * need their cmds to be executed.
- * @base: Base kgsl_drawobj, this needs to be the first entry
- * @priv: Internal flags
- * @global_ts: The ringbuffer timestamp corresponding to this
- *    command obj
- * @fault_policy: Internal policy describing how to handle this command in case
- * of a fault
- * @fault_recovery: recovery actions actually tried for this batch
- *     be hung
- * @refcount: kref structure to maintain the reference count
- * @cmdlist: List of IBs to issue
- * @memlist: List of all memory used in this command batch
- * @marker_timestamp: For markers, the timestamp of the last "real" command that
- * was queued
- * @profiling_buf_entry: Mem entry containing the profiling buffer
- * @profiling_buffer_gpuaddr: GPU virt address of the profile buffer added here
- * for easy access
- * @profile_index: Index to store the start/stop ticks in the kernel profiling
- * buffer
- * @submit_ticks: Variable to hold ticks at the time of
- *     command obj submit.
-
- */
-struct kgsl_drawobj_cmd {
-	struct kgsl_drawobj base;
-	unsigned long priv;
-	unsigned int global_ts;
-	unsigned long fault_policy;
-	unsigned long fault_recovery;
-	struct list_head cmdlist;
-	struct list_head memlist;
-	unsigned int marker_timestamp;
-	struct kgsl_mem_entry *profiling_buf_entry;
-	uint64_t profiling_buffer_gpuaddr;
-	unsigned int profile_index;
-	uint64_t submit_ticks;
-};
-
-/**
- * struct kgsl_drawobj_sync - KGSL sync object
- * @base: Base kgsl_drawobj, this needs to be the first entry
- * @synclist: Array of context/timestamp tuples to wait for before issuing
- * @numsyncs: Number of sync entries in the array
- * @pending: Bitmask of sync events that are active
- * @timer: a timer used to track possible sync timeouts for this
- *         sync obj
- * @timeout_jiffies: For a sync obj the jiffies at
- * which the timer will expire
- */
-struct kgsl_drawobj_sync {
-	struct kgsl_drawobj base;
-	struct kgsl_drawobj_sync_event *synclist;
-	unsigned int numsyncs;
-	unsigned long pending;
-	struct timer_list timer;
-	unsigned long timeout_jiffies;
-};
-
-#define KGSL_FENCE_NAME_LEN 74
-
-/**
- * struct kgsl_drawobj_sync_event
- * @id: identifer (positiion within the pending bitmap)
- * @type: Syncpoint type
- * @syncobj: Pointer to the syncobj that owns the sync event
- * @context: KGSL context for whose timestamp we want to
- *           register this event
- * @timestamp: Pending timestamp for the event
- * @handle: Pointer to a sync fence handle
- * @fence_name: A fence name string to describe the fence
- * @device: Pointer to the KGSL device
- */
-struct kgsl_drawobj_sync_event {
-	unsigned int id;
-	int type;
-	struct kgsl_drawobj_sync *syncobj;
-	struct kgsl_context *context;
-	unsigned int timestamp;
-	struct kgsl_sync_fence_cb *handle;
-	char fence_name[KGSL_FENCE_NAME_LEN];
-	struct kgsl_device *device;
-};
-
-/**
- * struct kgsl_drawobj_sparse - KGSl sparse obj descriptor
- * @base: Base kgsl_obj, this needs to be the first entry
- * @id: virtual id of the bind/unbind
- * @sparselist: list of binds/unbinds
- * @size: Size of kgsl_sparse_bind_object
- * @count: Number of elements in list
- */
-struct kgsl_drawobj_sparse {
-	struct kgsl_drawobj base;
-	unsigned int id;
-	struct list_head sparselist;
-	unsigned int size;
-	unsigned int count;
-};
-
-#define KGSL_DRAWOBJ_FLAGS \
-	{ KGSL_DRAWOBJ_MARKER, "MARKER" }, \
-	{ KGSL_DRAWOBJ_CTX_SWITCH, "CTX_SWITCH" }, \
-	{ KGSL_DRAWOBJ_SYNC, "SYNC" }, \
-	{ KGSL_DRAWOBJ_END_OF_FRAME, "EOF" }, \
-	{ KGSL_DRAWOBJ_PWR_CONSTRAINT, "PWR_CONSTRAINT" }, \
-	{ KGSL_DRAWOBJ_SUBMIT_IB_LIST, "IB_LIST" }
-
-/**
- * enum kgsl_drawobj_cmd_priv - Internal command obj flags
- * @CMDOBJ_SKIP - skip the entire command obj
- * @CMDOBJ_FORCE_PREAMBLE - Force the preamble on for
- *           command obj
- * @CMDOBJ_WFI - Force wait-for-idle for the submission
- * @CMDOBJ_PROFILE - store the start / retire ticks for
- * the command obj in the profiling buffer
- */
-enum kgsl_drawobj_cmd_priv {
-	CMDOBJ_SKIP = 0,
-	CMDOBJ_FORCE_PREAMBLE,
-	CMDOBJ_WFI,
-	CMDOBJ_PROFILE,
-};
-
-struct kgsl_drawobj_cmd *kgsl_drawobj_cmd_create(struct kgsl_device *device,
-		struct kgsl_context *context, unsigned int flags,
-		unsigned int type);
-int kgsl_drawobj_cmd_add_ibdesc(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, struct kgsl_ibdesc *ibdesc);
-int kgsl_drawobj_cmd_add_ibdesc_list(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr, int count);
-int kgsl_drawobj_cmd_add_cmdlist(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr,
-		unsigned int size, unsigned int count);
-int kgsl_drawobj_cmd_add_memlist(struct kgsl_device *device,
-		struct kgsl_drawobj_cmd *cmdobj, void __user *ptr,
-		unsigned int size, unsigned int count);
-
-struct kgsl_drawobj_sync *kgsl_drawobj_sync_create(struct kgsl_device *device,
-		struct kgsl_context *context);
-int kgsl_drawobj_sync_add_syncpoints(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void __user *ptr,
-		int count);
-int kgsl_drawobj_sync_add_synclist(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj, void __user *ptr,
-		unsigned int size, unsigned int count);
-int kgsl_drawobj_sync_add_sync(struct kgsl_device *device,
-		struct kgsl_drawobj_sync *syncobj,
-		struct kgsl_cmd_syncpoint *sync);
-struct kgsl_drawobj_sparse *kgsl_drawobj_sparse_create(
-		struct kgsl_device *device,
-		struct kgsl_context *context, unsigned int flags);
-int kgsl_drawobj_sparse_add_sparselist(struct kgsl_device *device,
-		struct kgsl_drawobj_sparse *sparseobj, unsigned int id,
-		void __user *ptr, unsigned int size, unsigned int count);
-
-int kgsl_drawobjs_cache_init(void);
-void kgsl_drawobjs_cache_exit(void);
-
-void kgsl_dump_syncpoints(struct kgsl_device *device,
-	struct kgsl_drawobj_sync *syncobj);
-
-void kgsl_drawobj_destroy(struct kgsl_drawobj *drawobj);
-
-void kgsl_drawobj_destroy_object(struct kref *kref);
-
-static inline bool kgsl_drawobj_events_pending(
-		struct kgsl_drawobj_sync *syncobj)
-{
-	return !bitmap_empty(&syncobj->pending, KGSL_MAX_SYNCPOINTS);
-}
-
-static inline bool kgsl_drawobj_event_pending(
-		struct kgsl_drawobj_sync *syncobj, unsigned int bit)
-{
-	if (bit >= KGSL_MAX_SYNCPOINTS)
-		return false;
-
-	return test_bit(bit, &syncobj->pending);
-}
-
-static inline void kgsl_drawobj_put(struct kgsl_drawobj *drawobj)
-{
-	if (drawobj)
-		kref_put(&drawobj->refcount, kgsl_drawobj_destroy_object);
-}
-
-#endif /* __KGSL_DRAWOBJ_H */
diff --git a/drivers/gpu/msm/kgsl_events.c b/drivers/gpu/msm/kgsl_events.c
index 759a96601170..aa2c82bc68b9 100644
--- a/drivers/gpu/msm/kgsl_events.c
+++ b/drivers/gpu/msm/kgsl_events.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2011-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2011-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -56,23 +56,6 @@ static void _kgsl_event_worker(struct kthread_work *work)
 	kmem_cache_free(events_cache, event);
 }
 
-/* return true if the group needs to be processed */
-static bool _do_process_group(unsigned int processed, unsigned int cur)
-{
-	if (processed == cur)
-		return false;
-
-	/*
-	 * This ensures that the timestamp didn't slip back accidently, maybe
-	 * due to a memory barrier issue. This is highly unlikely but we've
-	 * been burned here in the past.
-	 */
-	if ((cur < processed) && ((processed - cur) < KGSL_TIMESTAMP_WINDOW))
-		return false;
-
-	return true;
-}
-
 static void _process_event_group(struct kgsl_device *device,
 		struct kgsl_event_group *group, bool flush)
 {
@@ -89,17 +72,19 @@ static void _process_event_group(struct kgsl_device *device,
 	 * Sanity check to be sure that we we aren't racing with the context
 	 * getting destroyed
 	 */
-	if (context != NULL && !_kgsl_context_get(context)) {
-		WARN_ON(1);
-		return;
-	}
+	if (context != NULL && !_kgsl_context_get(context))
+		BUG();
 
 	spin_lock(&group->lock);
 
 	group->readtimestamp(device, group->priv, KGSL_TIMESTAMP_RETIRED,
 		&timestamp);
 
-	if (!flush && _do_process_group(group->processed, timestamp) == false)
+	/*
+	 * If no timestamps have been retired since the last time we were here
+	 * then we can avoid going through this loop
+	 */
+	if (!flush && timestamp_cmp(timestamp, group->processed) <= 0)
 		goto out;
 
 	list_for_each_entry_safe(event, tmp, &group->events, node) {
@@ -197,7 +182,6 @@ void kgsl_cancel_event(struct kgsl_device *device,
 		kgsl_event_func func, void *priv)
 {
 	struct kgsl_event *event, *tmp;
-
 	spin_lock(&group->lock);
 
 	list_for_each_entry_safe(event, tmp, &group->events, node) {
@@ -224,7 +208,6 @@ bool kgsl_event_pending(struct kgsl_device *device,
 {
 	struct kgsl_event *event;
 	bool result = false;
-
 	spin_lock(&group->lock);
 	list_for_each_entry(event, &group->events, node) {
 		if (timestamp == event->timestamp && func == event->func &&
@@ -318,16 +301,22 @@ EXPORT_SYMBOL(kgsl_add_event);
 static DEFINE_RWLOCK(group_lock);
 static LIST_HEAD(group_list);
 
-void kgsl_process_event_groups(struct kgsl_device *device)
+/**
+ * kgsl_process_events() - Work queue for processing new timestamp events
+ * @work: Pointer to a work_struct
+ */
+void kgsl_process_events(struct work_struct *work)
 {
 	struct kgsl_event_group *group;
+	struct kgsl_device *device = container_of(work, struct kgsl_device,
+		event_work);
 
 	read_lock(&group_lock);
 	list_for_each_entry(group, &group_list, group)
 		_process_event_group(device, group, false);
 	read_unlock(&group_lock);
 }
-EXPORT_SYMBOL(kgsl_process_event_groups);
+EXPORT_SYMBOL(kgsl_process_events);
 
 /**
  * kgsl_del_event_group() - Remove a GPU event group
@@ -433,7 +422,8 @@ static const struct file_operations events_fops = {
  */
 void kgsl_events_exit(void)
 {
-	kmem_cache_destroy(events_cache);
+	if (events_cache)
+		kmem_cache_destroy(events_cache);
 
 	debugfs_remove(events_dentry);
 }
@@ -444,7 +434,6 @@ void kgsl_events_exit(void)
 void __init kgsl_events_init(void)
 {
 	struct dentry *debugfs_dir = kgsl_get_debugfs_dir();
-
 	events_cache = KMEM_CACHE(kgsl_event, 0);
 
 	events_dentry = debugfs_create_file("events", 0444, debugfs_dir, NULL,
diff --git a/drivers/gpu/msm/kgsl_gmu.c b/drivers/gpu/msm/kgsl_gmu.c
deleted file mode 100644
index 6e3196469bd1..000000000000
--- a/drivers/gpu/msm/kgsl_gmu.c
+++ /dev/null
@@ -1,1695 +0,0 @@
-/* Copyright (c) 2017-2018, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-#include <linux/module.h>
-#include <linux/types.h>
-#include <linux/device.h>
-#include <linux/iommu.h>
-#include <linux/of_platform.h>
-#include <linux/msm-bus.h>
-#include <linux/msm-bus-board.h>
-#include <linux/pm_opp.h>
-#include <linux/io.h>
-#include <soc/qcom/cmd-db.h>
-
-#include "kgsl_device.h"
-#include "kgsl_gmu.h"
-#include "kgsl_hfi.h"
-#include "a6xx_reg.h"
-#include "adreno.h"
-
-#undef MODULE_PARAM_PREFIX
-#define MODULE_PARAM_PREFIX "kgsl_gmu."
-
-static bool nogmu;
-module_param(nogmu, bool, 0444);
-MODULE_PARM_DESC(nogmu, "Disable the GMU");
-
-#define GMU_CONTEXT_USER		0
-#define GMU_CONTEXT_KERNEL		1
-#define GMU_KERNEL_ENTRIES		8
-
-enum gmu_iommu_mem_type {
-	GMU_CACHED_CODE,
-	GMU_CACHED_DATA,
-	GMU_NONCACHED_KERNEL,
-	GMU_NONCACHED_USER
-};
-
-/*
- * GMU virtual memory mapping definitions
- */
-struct gmu_vma {
-	unsigned int noncached_ustart;
-	unsigned int noncached_usize;
-	unsigned int noncached_kstart;
-	unsigned int noncached_ksize;
-	unsigned int cached_dstart;
-	unsigned int cached_dsize;
-	unsigned int cached_cstart;
-	unsigned int cached_csize;
-	unsigned int image_start;
-};
-
-struct gmu_iommu_context {
-	const char *name;
-	struct device *dev;
-	struct iommu_domain *domain;
-};
-
-#define HFIMEM_SIZE SZ_16K
-
-#define DUMPMEM_SIZE SZ_16K
-
-/* Define target specific GMU VMA configurations */
-static const struct gmu_vma vma = {
-	/* Noncached user segment */
-	0x80000000, SZ_1G,
-	/* Noncached kernel segment */
-	0x60000000, SZ_512M,
-	/* Cached data segment */
-	0x44000, (SZ_256K-SZ_16K),
-	/* Cached code segment */
-	0x0, (SZ_256K-SZ_16K),
-	/* FW image */
-	0x0,
-};
-
-struct gmu_iommu_context gmu_ctx[] = {
-	[GMU_CONTEXT_USER] = { .name = "gmu_user" },
-	[GMU_CONTEXT_KERNEL] = { .name = "gmu_kernel" }
-};
-
-/*
- * There are a few static memory buffers that are allocated and mapped at boot
- * time for GMU to function. The buffers are permanent (not freed) after
- * GPU boot. The size of the buffers are constant and not expected to change.
- *
- * We define an array and a simple allocator to keep track of the currently
- * active SMMU entries of GMU kernel mode context. Each entry is assigned
- * a unique address inside GMU kernel mode address range. The addresses
- * are assigned sequentially and aligned to 1MB each.
- *
- */
-static struct gmu_memdesc gmu_kmem_entries[GMU_KERNEL_ENTRIES];
-static unsigned long gmu_kmem_bitmap;
-
-/*
- * kgsl_gmu_isenabled() - Check if there is a GMU and it is enabled
- * @device: Pointer to the KGSL device that owns the GMU
- *
- * Check if a GMU has been found and successfully probed. Also
- * check that the feature flag to use a GMU is enabled. Returns
- * true if both of these conditions are met, otherwise false.
- */
-bool kgsl_gmu_isenabled(struct kgsl_device *device)
-{
-	struct gmu_device *gmu = &device->gmu;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-
-	if (!nogmu && gmu->pdev &&
-		ADRENO_FEATURE(adreno_dev, ADRENO_GPMU))
-		return true;
-	return false;
-}
-
-static int _gmu_iommu_fault_handler(struct device *dev,
-		unsigned long addr, int flags, const char *name)
-{
-	char *fault_type = "unknown";
-
-	if (flags & IOMMU_FAULT_TRANSLATION)
-		fault_type = "translation";
-	else if (flags & IOMMU_FAULT_PERMISSION)
-		fault_type = "permission";
-	else if (flags & IOMMU_FAULT_EXTERNAL)
-		fault_type = "external";
-	else if (flags & IOMMU_FAULT_TRANSACTION_STALLED)
-		fault_type = "transaction stalled";
-
-	dev_err(dev, "GMU fault addr = %lX, context=%s (%s %s fault)\n",
-			addr, name,
-			(flags & IOMMU_FAULT_WRITE) ? "write" : "read",
-			fault_type);
-
-	return 0;
-}
-
-static int gmu_kernel_fault_handler(struct iommu_domain *domain,
-		struct device *dev, unsigned long addr, int flags, void *token)
-{
-	return _gmu_iommu_fault_handler(dev, addr, flags, "gmu_kernel");
-}
-
-static int gmu_user_fault_handler(struct iommu_domain *domain,
-		struct device *dev, unsigned long addr, int flags, void *token)
-{
-	return _gmu_iommu_fault_handler(dev, addr, flags, "gmu_user");
-}
-
-static void free_gmu_mem(struct gmu_device *gmu,
-		struct gmu_memdesc *md)
-{
-	/* Free GMU image memory */
-	if (md->hostptr)
-		dma_free_attrs(&gmu->pdev->dev, (size_t) md->size,
-				(void *)md->hostptr, md->physaddr, 0);
-	memset(md, 0, sizeof(*md));
-}
-
-static int alloc_and_map(struct gmu_device *gmu, unsigned int ctx_id,
-		struct gmu_memdesc *md, unsigned int attrs)
-{
-	int ret;
-	struct iommu_domain *domain;
-
-	domain = gmu_ctx[ctx_id].domain;
-
-	md->hostptr = dma_alloc_attrs(&gmu->pdev->dev, (size_t) md->size,
-		&md->physaddr, GFP_KERNEL, 0);
-
-	if (md->hostptr == NULL)
-		return -ENOMEM;
-
-	ret = iommu_map(domain, md->gmuaddr,
-			md->physaddr, md->size,
-			attrs);
-
-	if (ret) {
-		dev_err(&gmu->pdev->dev,
-				"gmu map err: gaddr=0x%016llX, paddr=0x%pa\n",
-				md->gmuaddr, &(md->physaddr));
-		free_gmu_mem(gmu, md);
-	}
-
-	return ret;
-}
-
-/*
- * allocate_gmu_image() - allocates & maps memory for FW image, the size
- * shall come from the loaded f/w file. Firmware image size shall be
- * less than code cache size. Otherwise, FW may experience performance issue.
- * @gmu: Pointer to GMU device
- * @size: Requested allocation size
- */
-int allocate_gmu_image(struct gmu_device *gmu, unsigned int size)
-{
-	struct gmu_memdesc *md = &gmu->fw_image;
-
-	if (size > vma.cached_csize) {
-		dev_err(&gmu->pdev->dev,
-			"GMU firmware size too big: %d\n", size);
-		return -EINVAL;
-	}
-
-	md->size = size;
-	md->gmuaddr = vma.image_start;
-	md->attr = GMU_CACHED_CODE;
-
-	return alloc_and_map(gmu, GMU_CONTEXT_KERNEL, md, IOMMU_READ);
-}
-
-/*
- * allocate_gmu_kmem() - allocates and maps GMU kernel shared memory
- * @gmu: Pointer to GMU device
- * @size: Requested size
- * @attrs: IOMMU mapping attributes
- */
-static struct gmu_memdesc *allocate_gmu_kmem(struct gmu_device *gmu,
-		unsigned int size, unsigned int attrs)
-{
-	struct gmu_memdesc *md;
-	int ret, entry_idx = find_first_zero_bit(
-			&gmu_kmem_bitmap, GMU_KERNEL_ENTRIES);
-
-	size = PAGE_ALIGN(size);
-
-	if (size > SZ_1M || size == 0) {
-		dev_err(&gmu->pdev->dev,
-			"Requested %d bytes of GMU kernel memory, max=1MB\n",
-			size);
-		return ERR_PTR(-EINVAL);
-	}
-
-	if (entry_idx >= GMU_KERNEL_ENTRIES) {
-		dev_err(&gmu->pdev->dev,
-				"Ran out of GMU kernel mempool slots\n");
-		return ERR_PTR(-EINVAL);
-	}
-
-	/* Allocate GMU virtual memory */
-	md = &gmu_kmem_entries[entry_idx];
-	md->gmuaddr = vma.noncached_kstart + (entry_idx * SZ_1M);
-	set_bit(entry_idx, &gmu_kmem_bitmap);
-	md->attr = GMU_NONCACHED_KERNEL;
-	md->size = size;
-
-	ret = alloc_and_map(gmu, GMU_CONTEXT_KERNEL, md, attrs);
-
-	if (ret) {
-		clear_bit(entry_idx, &gmu_kmem_bitmap);
-		md->gmuaddr = 0;
-		return ERR_PTR(ret);
-	}
-
-	return md;
-}
-
-static int gmu_iommu_cb_probe(struct gmu_device *gmu,
-		struct gmu_iommu_context *ctx,
-		struct device_node *node)
-{
-	struct platform_device *pdev = of_find_device_by_node(node);
-	struct device *dev;
-	int ret;
-
-	dev = &pdev->dev;
-
-	ctx->dev = dev;
-	ctx->domain = iommu_domain_alloc(&platform_bus_type);
-	if (ctx->domain == NULL) {
-		dev_err(&gmu->pdev->dev, "gmu iommu fail to alloc %s domain\n",
-			ctx->name);
-		return -ENODEV;
-	}
-
-	ret = iommu_attach_device(ctx->domain, dev);
-	if (ret) {
-		dev_err(&gmu->pdev->dev, "gmu iommu fail to attach %s device\n",
-			ctx->name);
-		iommu_domain_free(ctx->domain);
-	}
-
-	return ret;
-}
-
-static struct {
-	const char *compatible;
-	int index;
-	iommu_fault_handler_t hdlr;
-} cbs[] = {
-	{ "qcom,smmu-gmu-user-cb",
-		GMU_CONTEXT_USER,
-		gmu_user_fault_handler,
-	},
-	{ "qcom,smmu-gmu-kernel-cb",
-		GMU_CONTEXT_KERNEL,
-		gmu_kernel_fault_handler,
-	},
-};
-
-/*
- * gmu_iommu_init() - probe IOMMU context banks used by GMU
- * and attach GMU device
- * @gmu: Pointer to GMU device
- * @node: Pointer to GMU device node
- */
-static int gmu_iommu_init(struct gmu_device *gmu, struct device_node *node)
-{
-	struct device_node *child;
-	struct gmu_iommu_context *ctx = NULL;
-	int ret, i;
-
-	of_platform_populate(node, NULL, NULL, &gmu->pdev->dev);
-
-	for (i = 0; i < ARRAY_SIZE(cbs); i++) {
-		child = of_find_compatible_node(node, NULL, cbs[i].compatible);
-		if (child) {
-			ctx = &gmu_ctx[cbs[i].index];
-			ret = gmu_iommu_cb_probe(gmu, ctx, child);
-			if (ret)
-				return ret;
-			iommu_set_fault_handler(ctx->domain,
-					cbs[i].hdlr, ctx);
-			}
-		}
-
-	for (i = 0; i < ARRAY_SIZE(gmu_ctx); i++) {
-		if (gmu_ctx[i].domain == NULL) {
-			dev_err(&gmu->pdev->dev,
-				"Missing GMU %s context bank node\n",
-				gmu_ctx[i].name);
-			return -EINVAL;
-		}
-	}
-
-	return 0;
-}
-
-/*
- * gmu_kmem_close() - free all kernel memory allocated for GMU and detach GMU
- * from IOMMU context banks.
- * @gmu: Pointer to GMU device
- */
-static void gmu_kmem_close(struct gmu_device *gmu)
-{
-	int i;
-	struct gmu_memdesc *md = &gmu->fw_image;
-	struct gmu_iommu_context *ctx = &gmu_ctx[GMU_CONTEXT_KERNEL];
-
-	/* Free GMU image memory */
-	free_gmu_mem(gmu, md);
-
-	/* Unmap image memory */
-	iommu_unmap(ctx->domain,
-			gmu->fw_image.gmuaddr,
-			gmu->fw_image.size);
-
-
-	gmu->hfi_mem = NULL;
-	gmu->dump_mem = NULL;
-
-	/* Unmap all memories in GMU kernel memory pool */
-	for (i = 0; i < GMU_KERNEL_ENTRIES; i++) {
-		struct gmu_memdesc *memptr = &gmu_kmem_entries[i];
-
-		if (memptr->gmuaddr)
-			iommu_unmap(ctx->domain, memptr->gmuaddr, memptr->size);
-	}
-
-	/* Free GMU shared kernel memory */
-	for (i = 0; i < GMU_KERNEL_ENTRIES; i++) {
-		md = &gmu_kmem_entries[i];
-		free_gmu_mem(gmu, md);
-		clear_bit(i, &gmu_kmem_bitmap);
-	}
-
-	/* Detach the device from SMMU context bank */
-	iommu_detach_device(ctx->domain, ctx->dev);
-
-	/* free kernel mem context */
-	iommu_domain_free(ctx->domain);
-}
-
-static void gmu_memory_close(struct gmu_device *gmu)
-{
-	gmu_kmem_close(gmu);
-	/* Free user memory context */
-	iommu_domain_free(gmu_ctx[GMU_CONTEXT_USER].domain);
-
-}
-
-/*
- * gmu_memory_probe() - probe GMU IOMMU context banks and allocate memory
- * to share with GMU in kernel mode.
- * @gmu: Pointer to GMU device
- * @node: Pointer to GMU device node
- */
-static int gmu_memory_probe(struct gmu_device *gmu, struct device_node *node)
-{
-	int ret;
-
-	ret = gmu_iommu_init(gmu, node);
-	if (ret)
-		return ret;
-
-	/* Allocates & maps memory for HFI */
-	gmu->hfi_mem = allocate_gmu_kmem(gmu, HFIMEM_SIZE,
-				(IOMMU_READ | IOMMU_WRITE));
-	if (IS_ERR(gmu->hfi_mem)) {
-		ret = PTR_ERR(gmu->hfi_mem);
-		goto err_ret;
-	}
-
-	/* Allocates & maps GMU crash dump memory */
-	gmu->dump_mem = allocate_gmu_kmem(gmu, DUMPMEM_SIZE,
-				(IOMMU_READ | IOMMU_WRITE));
-	if (IS_ERR(gmu->dump_mem)) {
-		ret = PTR_ERR(gmu->dump_mem);
-		goto err_ret;
-	}
-
-	return 0;
-err_ret:
-	gmu_memory_close(gmu);
-	return ret;
-}
-
-/*
- * gmu_dcvs_set() - request GMU to change GPU frequency and/or bandwidth.
- * @gmu: Pointer to GMU device
- * @gpu_pwrlevel: index to GPU DCVS table used by KGSL
- * @bus_level: index to GPU bus table used by KGSL
- *
- * The function converts GPU power level and bus level index used by KGSL
- * to index being used by GMU/RPMh.
- */
-int gmu_dcvs_set(struct gmu_device *gmu,
-		unsigned int gpu_pwrlevel, unsigned int bus_level)
-{
-	struct kgsl_device *device = container_of(gmu, struct kgsl_device, gmu);
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	int perf_idx = INVALID_DCVS_IDX, bw_idx = INVALID_DCVS_IDX;
-	int ret;
-
-	if (gpu_pwrlevel < gmu->num_gpupwrlevels - 1)
-		perf_idx = gmu->num_gpupwrlevels - gpu_pwrlevel - 1;
-
-	if (bus_level < gmu->num_bwlevels && bus_level > 0)
-		bw_idx = bus_level;
-
-	if ((perf_idx == INVALID_DCVS_IDX) &&
-		(bw_idx == INVALID_DCVS_IDX))
-		return -EINVAL;
-
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_HFI_USE_REG)) {
-		ret = gpudev->rpmh_gpu_pwrctrl(adreno_dev,
-			GMU_DCVS_NOHFI, perf_idx, bw_idx);
-
-		if (ret) {
-			dev_err_ratelimited(&gmu->pdev->dev,
-				"Failed to set GPU perf idx %d, bw idx %d\n",
-				perf_idx, bw_idx);
-
-			adreno_set_gpu_fault(adreno_dev, ADRENO_GMU_FAULT);
-			adreno_dispatcher_schedule(device);
-		}
-
-		return ret;
-	}
-
-	return hfi_send_dcvs_vote(gmu, perf_idx, bw_idx, ACK_NONBLOCK);
-}
-
-struct rpmh_arc_vals {
-	unsigned int num;
-	uint16_t val[MAX_GX_LEVELS];
-};
-
-static const char gfx_res_id[] = "gfx.lvl";
-static const char cx_res_id[] = "cx.lvl";
-static const char mx_res_id[] = "mx.lvl";
-
-enum rpmh_vote_type {
-	GPU_ARC_VOTE = 0,
-	GMU_ARC_VOTE,
-	INVALID_ARC_VOTE,
-};
-
-static const char debug_strs[][8] = {
-	[GPU_ARC_VOTE] = "gpu",
-	[GMU_ARC_VOTE] = "gmu",
-};
-
-/*
- * rpmh_arc_cmds() - query RPMh command database for GX/CX/MX rail
- * VLVL tables. The index of table will be used by GMU to vote rail
- * voltage.
- *
- * @gmu: Pointer to GMU device
- * @arc: Pointer to RPMh rail controller (ARC) voltage table
- * @res_id: Pointer to 8 char array that contains rail name
- */
-static int rpmh_arc_cmds(struct gmu_device *gmu,
-		struct rpmh_arc_vals *arc, const char *res_id)
-{
-	unsigned int len;
-
-	len = cmd_db_get_aux_data_len(res_id);
-	if (len == 0)
-		return -EINVAL;
-
-	if (len > (MAX_GX_LEVELS << 1)) {
-		dev_err(&gmu->pdev->dev,
-			"gfx cmddb size %d larger than alloc buf %d of %s\n",
-			len, (MAX_GX_LEVELS << 1), res_id);
-		return -EINVAL;
-	}
-
-	cmd_db_get_aux_data(res_id, (uint8_t *)arc->val, len);
-
-	/*
-	 * cmd_db_get_aux_data() gives us a zero-padded table of
-	 * size len that contains the arc values. To determine the
-	 * number of arc values, we loop through the table and count
-	 * them until we get to the end of the buffer or hit the
-	 * zero padding.
-	 */
-	for (arc->num = 1; arc->num < (len >> 1); arc->num++) {
-		if (arc->val[arc->num - 1] >= arc->val[arc->num])
-			break;
-	}
-
-	return 0;
-}
-
-/*
- * setup_volt_dependency_tbl() - set up GX->MX or CX->MX rail voltage
- * dependencies. Second rail voltage shall be equal to or higher than
- * primary rail voltage. VLVL table index was used by RPMh for PMIC
- * voltage setting.
- * @votes: Pointer to a ARC vote descriptor
- * @pri_rail: Pointer to primary power rail VLVL table
- * @sec_rail: Pointer to second/dependent power rail VLVL table
- * @vlvl: Pointer to VLVL table being used by GPU or GMU driver, a subset
- *	of pri_rail VLVL table
- * @num_entries: Valid number of entries in table pointed by "vlvl" parameter
- */
-static int setup_volt_dependency_tbl(struct arc_vote_desc *votes,
-		struct rpmh_arc_vals *pri_rail, struct rpmh_arc_vals *sec_rail,
-		unsigned int *vlvl, unsigned int num_entries)
-{
-	int i, j, k;
-	uint16_t cur_vlvl;
-	bool found_match;
-
-	/* i tracks current KGSL GPU frequency table entry
-	 * j tracks second rail voltage table entry
-	 * k tracks primary rail voltage table entry
-	 */
-	for (i = 0; i < num_entries; i++) {
-		found_match = false;
-
-		/* Look for a primary rail voltage that matches a VLVL level */
-		for (k = 0; k < pri_rail->num; k++) {
-			if (pri_rail->val[k] == vlvl[i]) {
-				votes[i].pri_idx = k;
-				votes[i].vlvl = vlvl[i];
-				cur_vlvl = vlvl[i];
-				found_match = true;
-				break;
-			}
-		}
-
-		/* If we did not find a matching VLVL level then abort */
-		if (!found_match)
-			return -EINVAL;
-
-		/*
-		 * Look for a secondary rail index whose VLVL value
-		 * is greater than or equal to the VLVL value of the
-		 * corresponding index of the primary rail
-		 */
-		for (j = 0; j < sec_rail->num; j++) {
-			if (sec_rail->val[j] >= cur_vlvl ||
-					j + 1 == sec_rail->num) {
-				votes[i].sec_idx = j;
-				break;
-			}
-		}
-	}
-	return 0;
-}
-
-/*
- * rpmh_arc_votes_init() - initialized RPMh votes needed for rails voltage
- * scaling by GMU.
- * @gmu: Pointer to GMU device
- * @pri_rail: Pointer to primary power rail VLVL table
- * @sec_rail: Pointer to second/dependent power rail VLVL table
- *	of pri_rail VLVL table
- * @type: the type of the primary rail, GPU or GMU
- */
-static int rpmh_arc_votes_init(struct gmu_device *gmu,
-		struct rpmh_arc_vals *pri_rail,
-		struct rpmh_arc_vals *sec_rail,
-		unsigned int type)
-{
-	struct device *dev;
-	struct kgsl_device *device = container_of(gmu, struct kgsl_device, gmu);
-	unsigned int num_freqs;
-	struct arc_vote_desc *votes;
-	unsigned int vlvl_tbl[MAX_GX_LEVELS];
-	unsigned int *freq_tbl;
-	int i, ret;
-	struct dev_pm_opp *opp;
-
-	if (type == GPU_ARC_VOTE) {
-		num_freqs = gmu->num_gpupwrlevels;
-		votes = gmu->rpmh_votes.gx_votes;
-		freq_tbl = gmu->gpu_freqs;
-		dev = &device->pdev->dev;
-	} else if (type == GMU_ARC_VOTE) {
-		num_freqs = gmu->num_gmupwrlevels;
-		votes = gmu->rpmh_votes.cx_votes;
-		freq_tbl = gmu->gmu_freqs;
-		dev = &gmu->pdev->dev;
-	} else {
-		return -EINVAL;
-	}
-
-	if (num_freqs > pri_rail->num) {
-		dev_err(&gmu->pdev->dev,
-			"%s defined more DCVS levels than RPMh can support\n",
-			debug_strs[type]);
-		return -EINVAL;
-	}
-
-	memset(vlvl_tbl, 0, sizeof(vlvl_tbl));
-	for (i = 0; i < num_freqs; i++) {
-		/* Hardcode VLVL for 0 because it is not registered in OPP */
-		if (freq_tbl[i] == 0) {
-			vlvl_tbl[i] = 0;
-			continue;
-		}
-
-		/* Otherwise get the value from the OPP API */
-		opp = dev_pm_opp_find_freq_exact(dev, freq_tbl[i], true);
-		if (IS_ERR(opp)) {
-			dev_err(&gmu->pdev->dev,
-				"Failed to find opp freq %d of %s\n",
-				freq_tbl[i], debug_strs[type]);
-			return PTR_ERR(opp);
-		}
-
-		/* Values from OPP framework are offset by 1 */
-		vlvl_tbl[i] = dev_pm_opp_get_voltage(opp) - 1;
-	}
-
-	ret = setup_volt_dependency_tbl(votes,
-			pri_rail, sec_rail, vlvl_tbl, num_freqs);
-
-	if (ret)
-		dev_err(&gmu->pdev->dev, "%s rail volt failed to match DT freqs\n",
-				debug_strs[type]);
-
-	return ret;
-}
-
-/*
- * build_rpmh_bw_votes() - build TCS commands to vote for bandwidth.
- * Each command sets frequency of a node along path to DDR or CNOC.
- * @rpmh_vote: Pointer to RPMh vote needed by GMU to set BW via RPMh
- * @num_usecases: Number of BW use cases (or BW levels)
- * @handle: Provided by bus driver. It contains TCS command sets for
- * all BW use cases of a bus client.
- */
-static void build_rpmh_bw_votes(struct gmu_bw_votes *rpmh_vote,
-		unsigned int num_usecases, struct msm_bus_tcs_handle handle)
-{
-	struct msm_bus_tcs_usecase *tmp;
-	int i, j;
-
-	for (i = 0; i < num_usecases; i++) {
-		tmp = &handle.usecases[i];
-		for (j = 0; j < tmp->num_cmds; j++) {
-			if (!i) {
-			/*
-			 * Wait bitmask and TCS command addresses are
-			 * same for all bw use cases. To save data volume
-			 * exchanged between driver and GMU, only
-			 * transfer bitmasks and TCS command addresses
-			 * of first set of bw use case
-			 */
-				rpmh_vote->cmds_per_bw_vote = tmp->num_cmds;
-				rpmh_vote->cmds_wait_bitmask =
-						tmp->cmds[j].complete ?
-						rpmh_vote->cmds_wait_bitmask
-						| BIT(i)
-						: rpmh_vote->cmds_wait_bitmask
-						& (~BIT(i));
-				rpmh_vote->cmd_addrs[j] = tmp->cmds[j].addr;
-			}
-			rpmh_vote->cmd_data[i][j] = tmp->cmds[j].data;
-		}
-	}
-}
-
-/*
- * gmu_bus_vote_init - initialized RPMh votes needed for bw scaling by GMU.
- * @gmu: Pointer to GMU device
- * @pwr: Pointer to KGSL power controller
- */
-static int gmu_bus_vote_init(struct gmu_device *gmu, struct kgsl_pwrctrl *pwr)
-{
-	struct msm_bus_tcs_usecase *usecases;
-	struct msm_bus_tcs_handle hdl;
-	struct rpmh_votes_t *votes = &gmu->rpmh_votes;
-	int ret;
-
-	usecases  = kcalloc(gmu->num_bwlevels, sizeof(*usecases), GFP_KERNEL);
-	if (!usecases)
-		return -ENOMEM;
-
-	hdl.num_usecases = gmu->num_bwlevels;
-	hdl.usecases = usecases;
-
-	/*
-	 * Query TCS command set for each use case defined in GPU b/w table
-	 */
-	ret = msm_bus_scale_query_tcs_cmd_all(&hdl, gmu->pcl);
-	if (ret)
-		return ret;
-
-	build_rpmh_bw_votes(&votes->ddr_votes, gmu->num_bwlevels, hdl);
-
-	/*
-	 *Query CNOC TCS command set for each use case defined in cnoc bw table
-	 */
-	ret = msm_bus_scale_query_tcs_cmd_all(&hdl, gmu->ccl);
-	if (ret)
-		return ret;
-
-	build_rpmh_bw_votes(&votes->cnoc_votes, gmu->num_cnocbwlevels, hdl);
-
-	kfree(usecases);
-
-	return 0;
-}
-
-static int gmu_rpmh_init(struct gmu_device *gmu, struct kgsl_pwrctrl *pwr)
-{
-	struct rpmh_arc_vals gfx_arc, cx_arc, mx_arc;
-	int ret;
-
-	/* Populate BW vote table */
-	ret = gmu_bus_vote_init(gmu, pwr);
-	if (ret)
-		return ret;
-
-	/* Populate GPU and GMU frequency vote table */
-	ret = rpmh_arc_cmds(gmu, &gfx_arc, gfx_res_id);
-	if (ret)
-		return ret;
-
-	ret = rpmh_arc_cmds(gmu, &cx_arc, cx_res_id);
-	if (ret)
-		return ret;
-
-	ret = rpmh_arc_cmds(gmu, &mx_arc, mx_res_id);
-	if (ret)
-		return ret;
-
-	ret = rpmh_arc_votes_init(gmu, &gfx_arc, &mx_arc, GPU_ARC_VOTE);
-	if (ret)
-		return ret;
-
-	return rpmh_arc_votes_init(gmu, &cx_arc, &mx_arc, GMU_ARC_VOTE);
-}
-
-static irqreturn_t gmu_irq_handler(int irq, void *data)
-{
-	struct gmu_device *gmu = data;
-	struct kgsl_device *device = container_of(gmu, struct kgsl_device, gmu);
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	unsigned int status = 0;
-
-	adreno_read_gmureg(ADRENO_DEVICE(device),
-			ADRENO_REG_GMU_AO_HOST_INTERRUPT_STATUS, &status);
-	adreno_write_gmureg(ADRENO_DEVICE(device),
-			ADRENO_REG_GMU_AO_HOST_INTERRUPT_CLR, status);
-
-	/* Ignore GMU_INT_RSCC_COMP and GMU_INT_DBD WAKEUP interrupts */
-	if (status & GMU_INT_WDOG_BITE) {
-		dev_err_ratelimited(&gmu->pdev->dev,
-				"GMU watchdog expired interrupt received\n");
-		adreno_set_gpu_fault(adreno_dev, ADRENO_GMU_FAULT);
-		adreno_dispatcher_schedule(device);
-	}
-	if (status & GMU_INT_HOST_AHB_BUS_ERR)
-		dev_err_ratelimited(&gmu->pdev->dev,
-				"AHB bus error interrupt received\n");
-	if (status & GMU_INT_FENCE_ERR) {
-		unsigned int fence_status;
-
-		adreno_read_gmureg(ADRENO_DEVICE(device),
-			ADRENO_REG_GMU_AHB_FENCE_STATUS, &fence_status);
-		dev_err_ratelimited(&gmu->pdev->dev,
-			"FENCE error interrupt received %x\n", fence_status);
-	}
-
-	if (status & ~GMU_AO_INT_MASK)
-		dev_err_ratelimited(&gmu->pdev->dev,
-				"Unhandled GMU interrupts 0x%lx\n",
-				status & ~GMU_AO_INT_MASK);
-
-	return IRQ_HANDLED;
-}
-
-static irqreturn_t hfi_irq_handler(int irq, void *data)
-{
-	struct kgsl_hfi *hfi = data;
-	struct gmu_device *gmu = container_of(hfi, struct gmu_device, hfi);
-	struct kgsl_device *device = container_of(gmu, struct kgsl_device, gmu);
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	unsigned int status = 0;
-
-	adreno_read_gmureg(ADRENO_DEVICE(device),
-			ADRENO_REG_GMU_GMU2HOST_INTR_INFO, &status);
-	adreno_write_gmureg(ADRENO_DEVICE(device),
-			ADRENO_REG_GMU_GMU2HOST_INTR_CLR, status);
-
-	if (status & HFI_IRQ_MSGQ_MASK)
-		tasklet_hi_schedule(&hfi->tasklet);
-	if (status & HFI_IRQ_CM3_FAULT_MASK) {
-		dev_err_ratelimited(&gmu->pdev->dev,
-				"GMU CM3 fault interrupt received\n");
-		adreno_set_gpu_fault(adreno_dev, ADRENO_GMU_FAULT);
-		adreno_dispatcher_schedule(device);
-	}
-	if (status & ~HFI_IRQ_MASK)
-		dev_err_ratelimited(&gmu->pdev->dev,
-				"Unhandled HFI interrupts 0x%lx\n",
-				status & ~HFI_IRQ_MASK);
-
-	return IRQ_HANDLED;
-}
-
-static int gmu_pwrlevel_probe(struct gmu_device *gmu, struct device_node *node)
-{
-	struct device_node *pwrlevel_node, *child;
-
-	pwrlevel_node = of_find_node_by_name(node, "qcom,gmu-pwrlevels");
-
-	if (pwrlevel_node == NULL) {
-		dev_err(&gmu->pdev->dev, "Unable to find 'qcom,gmu-pwrlevels'\n");
-		return -EINVAL;
-	}
-
-	gmu->num_gmupwrlevels = 0;
-
-	for_each_child_of_node(pwrlevel_node, child) {
-		unsigned int index;
-
-		if (of_property_read_u32(child, "reg", &index))
-			return -EINVAL;
-
-		if (index >= MAX_CX_LEVELS) {
-			dev_err(&gmu->pdev->dev, "gmu pwrlevel %d is out of range\n",
-				index);
-			continue;
-		}
-
-		if (index >= gmu->num_gmupwrlevels)
-			gmu->num_gmupwrlevels = index + 1;
-
-		if (of_property_read_u32(child, "qcom,gmu-freq",
-					&gmu->gmu_freqs[index]))
-			return -EINVAL;
-	}
-
-	return 0;
-}
-
-static int gmu_reg_probe(struct gmu_device *gmu, const char *name, bool is_gmu)
-{
-	struct resource *res;
-
-	res = platform_get_resource_byname(gmu->pdev, IORESOURCE_MEM, name);
-	if (res == NULL) {
-		dev_err(&gmu->pdev->dev,
-				"platform_get_resource %s failed\n", name);
-		return -EINVAL;
-	}
-
-	if (res->start == 0 || resource_size(res) == 0) {
-		dev_err(&gmu->pdev->dev,
-				"dev %d %s invalid register region\n",
-				gmu->pdev->dev.id, name);
-		return -EINVAL;
-	}
-
-	if (is_gmu) {
-		gmu->reg_phys = res->start;
-		gmu->reg_len = resource_size(res);
-		gmu->reg_virt = devm_ioremap(&gmu->pdev->dev, res->start,
-				resource_size(res));
-
-		if (gmu->reg_virt == NULL) {
-			dev_err(&gmu->pdev->dev, "GMU regs ioremap failed\n");
-			return -ENODEV;
-		}
-
-	} else {
-		gmu->pdc_reg_virt = devm_ioremap(&gmu->pdev->dev, res->start,
-				resource_size(res));
-		if (gmu->pdc_reg_virt == NULL) {
-			dev_err(&gmu->pdev->dev, "PDC regs ioremap failed\n");
-			return -ENODEV;
-		}
-	}
-
-	return 0;
-}
-
-static int gmu_clocks_probe(struct gmu_device *gmu, struct device_node *node)
-{
-	const char *cname;
-	struct property *prop;
-	struct clk *c;
-	int i = 0;
-
-	of_property_for_each_string(node, "clock-names", prop, cname) {
-		c = devm_clk_get(&gmu->pdev->dev, cname);
-
-		if (IS_ERR(c)) {
-			dev_err(&gmu->pdev->dev,
-				"dt: Couldn't get GMU clock: %s\n", cname);
-			return PTR_ERR(c);
-		}
-
-		if (i >= MAX_GMU_CLKS) {
-			dev_err(&gmu->pdev->dev,
-				"dt: too many GMU clocks defined\n");
-			return -EINVAL;
-		}
-
-		gmu->clks[i++] = c;
-	}
-
-	return 0;
-}
-
-static int gmu_gpu_bw_probe(struct gmu_device *gmu)
-{
-	struct kgsl_device *device = container_of(gmu, struct kgsl_device, gmu);
-	struct msm_bus_scale_pdata *bus_scale_table;
-
-	bus_scale_table = msm_bus_cl_get_pdata(device->pdev);
-	if (bus_scale_table == NULL) {
-		dev_err(&gmu->pdev->dev, "dt: cannot get bus table\n");
-		return -ENODEV;
-	}
-
-	gmu->num_bwlevels = bus_scale_table->num_usecases;
-	gmu->pcl = msm_bus_scale_register_client(bus_scale_table);
-	if (!gmu->pcl) {
-		dev_err(&gmu->pdev->dev, "dt: cannot register bus client\n");
-		return -ENODEV;
-	}
-
-	return 0;
-}
-
-static int gmu_cnoc_bw_probe(struct gmu_device *gmu)
-{
-	struct msm_bus_scale_pdata *cnoc_table;
-
-	cnoc_table = msm_bus_cl_get_pdata(gmu->pdev);
-	if (cnoc_table == NULL) {
-		dev_err(&gmu->pdev->dev, "dt: cannot get cnoc table\n");
-		return -ENODEV;
-	}
-
-	gmu->num_cnocbwlevels = cnoc_table->num_usecases;
-	gmu->ccl = msm_bus_scale_register_client(cnoc_table);
-	if (!gmu->ccl) {
-		dev_err(&gmu->pdev->dev, "dt: cannot register cnoc client\n");
-		return -ENODEV;
-	}
-
-	return 0;
-}
-
-static int gmu_regulators_probe(struct gmu_device *gmu,
-		struct device_node *node)
-{
-	const char *name;
-	struct property *prop;
-	struct device *dev = &gmu->pdev->dev;
-	int ret = 0;
-
-	of_property_for_each_string(node, "regulator-names", prop, name) {
-		if (!strcmp(name, "vddcx")) {
-			gmu->cx_gdsc = devm_regulator_get(dev, name);
-			if (IS_ERR(gmu->cx_gdsc)) {
-				ret = PTR_ERR(gmu->cx_gdsc);
-				dev_err(dev, "dt: GMU couldn't get CX gdsc\n");
-				gmu->cx_gdsc = NULL;
-				return ret;
-			}
-		} else if (!strcmp(name, "vdd")) {
-			gmu->gx_gdsc = devm_regulator_get(dev, name);
-			if (IS_ERR(gmu->gx_gdsc)) {
-				ret = PTR_ERR(gmu->gx_gdsc);
-				dev_err(dev, "dt: GMU couldn't get GX gdsc\n");
-				gmu->gx_gdsc = NULL;
-				return ret;
-			}
-		} else {
-			dev_err(dev, "dt: Unknown GMU regulator: %s\n", name);
-			return -ENODEV;
-		}
-	}
-
-	return 0;
-}
-
-static int gmu_irq_probe(struct gmu_device *gmu)
-{
-	int ret;
-	struct kgsl_hfi *hfi = &gmu->hfi;
-
-	hfi->hfi_interrupt_num = platform_get_irq_byname(gmu->pdev,
-			"kgsl_hfi_irq");
-	ret = devm_request_irq(&gmu->pdev->dev,
-			hfi->hfi_interrupt_num,
-			hfi_irq_handler, IRQF_TRIGGER_HIGH,
-			"HFI", hfi);
-	if (ret) {
-		dev_err(&gmu->pdev->dev, "request_irq(%d) failed: %d\n",
-				hfi->hfi_interrupt_num, ret);
-		return ret;
-	}
-
-	gmu->gmu_interrupt_num = platform_get_irq_byname(gmu->pdev,
-			"kgsl_gmu_irq");
-	ret = devm_request_irq(&gmu->pdev->dev,
-			gmu->gmu_interrupt_num,
-			gmu_irq_handler, IRQF_TRIGGER_HIGH,
-			"GMU", gmu);
-	if (ret)
-		dev_err(&gmu->pdev->dev, "request_irq(%d) failed: %d\n",
-				gmu->gmu_interrupt_num, ret);
-
-	return ret;
-}
-
-static void gmu_irq_enable(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-	struct kgsl_hfi *hfi = &gmu->hfi;
-
-	/* Clear any pending IRQs before unmasking on GMU */
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_GMU2HOST_INTR_CLR,
-			0xFFFFFFFF);
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_AO_HOST_INTERRUPT_CLR,
-			0xFFFFFFFF);
-
-	/* Unmask needed IRQs on GMU */
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_GMU2HOST_INTR_MASK,
-			(unsigned int) ~HFI_IRQ_MASK);
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_AO_HOST_INTERRUPT_MASK,
-			(unsigned int) ~GMU_AO_INT_MASK);
-
-	/* Enable all IRQs on host */
-	enable_irq(hfi->hfi_interrupt_num);
-	enable_irq(gmu->gmu_interrupt_num);
-}
-
-static void gmu_irq_disable(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-	struct kgsl_hfi *hfi = &gmu->hfi;
-
-	/* Disable all IRQs on host */
-	disable_irq(gmu->gmu_interrupt_num);
-	disable_irq(hfi->hfi_interrupt_num);
-
-	/* Mask all IRQs on GMU */
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_AO_HOST_INTERRUPT_MASK,
-			0xFFFFFFFF);
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_GMU2HOST_INTR_MASK,
-			0xFFFFFFFF);
-
-	/* Clear any pending IRQs before disabling */
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_AO_HOST_INTERRUPT_CLR,
-			0xFFFFFFFF);
-	adreno_write_gmureg(adreno_dev, ADRENO_REG_GMU_GMU2HOST_INTR_CLR,
-			0xFFFFFFFF);
-}
-
-/* Do not access any GMU registers in GMU probe function */
-int gmu_probe(struct kgsl_device *device)
-{
-	struct device_node *node;
-	struct gmu_device *gmu = &device->gmu;
-	struct gmu_memdesc *mem_addr = NULL;
-	struct kgsl_hfi *hfi = &gmu->hfi;
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	int i = 0, ret = -ENXIO;
-
-	node = of_find_compatible_node(device->pdev->dev.of_node,
-			NULL, "qcom,gpu-gmu");
-
-	if (node == NULL)
-		return ret;
-
-	device->gmu.pdev = of_find_device_by_node(node);
-
-	/* Set up GMU regulators */
-	ret = gmu_regulators_probe(gmu, node);
-	if (ret)
-		goto error;
-
-	/* Set up GMU clocks */
-	ret = gmu_clocks_probe(gmu, node);
-	if (ret)
-		goto error;
-
-	/* Set up GMU IOMMU and shared memory with GMU */
-	ret = gmu_memory_probe(&device->gmu, node);
-	if (ret)
-		goto error;
-	mem_addr = gmu->hfi_mem;
-
-	/* Map and reserve GMU CSRs registers */
-	ret = gmu_reg_probe(gmu, "kgsl_gmu_reg", true);
-	if (ret)
-		goto error;
-
-	ret = gmu_reg_probe(gmu, "kgsl_gmu_pdc_reg", false);
-	if (ret)
-		goto error;
-
-	gmu->gmu2gpu_offset = (gmu->reg_phys - device->reg_phys) >> 2;
-
-	/* Initialize HFI and GMU interrupts */
-	ret = gmu_irq_probe(gmu);
-	if (ret)
-		goto error;
-
-	/* Don't enable GMU interrupts until GMU started */
-	/* We cannot use gmu_irq_disable because it writes registers */
-	disable_irq(gmu->gmu_interrupt_num);
-	disable_irq(hfi->hfi_interrupt_num);
-
-	tasklet_init(&hfi->tasklet, hfi_receiver, (unsigned long)gmu);
-	INIT_LIST_HEAD(&hfi->msglist);
-	spin_lock_init(&hfi->msglock);
-
-	/* Retrieves GMU/GPU power level configurations*/
-	ret = gmu_pwrlevel_probe(gmu, node);
-	if (ret)
-		goto error;
-
-	gmu->num_gpupwrlevels = pwr->num_pwrlevels;
-
-	for (i = 0; i < gmu->num_gpupwrlevels; i++) {
-		int j = gmu->num_gpupwrlevels - 1 - i;
-
-		gmu->gpu_freqs[i] = pwr->pwrlevels[j].gpu_freq;
-	}
-
-	/* Initializes GPU b/w levels configuration */
-	ret = gmu_gpu_bw_probe(gmu);
-	if (ret)
-		goto error;
-
-	/* Initialize GMU CNOC b/w levels configuration */
-	ret = gmu_cnoc_bw_probe(gmu);
-	if (ret)
-		goto error;
-
-	/* Populates RPMh configurations */
-	ret = gmu_rpmh_init(gmu, pwr);
-	if (ret)
-		goto error;
-
-	hfi_init(&gmu->hfi, mem_addr, HFI_QUEUE_SIZE);
-
-	/* Set up GMU idle states */
-	if (ADRENO_FEATURE(adreno_dev, ADRENO_MIN_VOLT))
-		gmu->idle_level = GPU_HW_MIN_VOLT;
-	else if (ADRENO_FEATURE(adreno_dev, ADRENO_HW_NAP))
-		gmu->idle_level = GPU_HW_NAP;
-	else if (ADRENO_FEATURE(adreno_dev, ADRENO_IFPC))
-		gmu->idle_level = GPU_HW_IFPC;
-	else if (ADRENO_FEATURE(adreno_dev, ADRENO_SPTP_PC))
-		gmu->idle_level = GPU_HW_SPTP_PC;
-	else
-		gmu->idle_level = GPU_HW_ACTIVE;
-
-	/* disable LM during boot time */
-	clear_bit(ADRENO_LM_CTRL, &adreno_dev->pwrctrl_flag);
-	return 0;
-
-error:
-	gmu_remove(device);
-	return ret;
-}
-
-
-
-static int gmu_enable_clks(struct gmu_device *gmu)
-{
-	int ret, j = 0;
-
-	if (IS_ERR_OR_NULL(gmu->clks[0]))
-		return -EINVAL;
-
-	ret = clk_set_rate(gmu->clks[0], gmu->gmu_freqs[DEFAULT_GMU_FREQ_IDX]);
-	if (ret) {
-		dev_err(&gmu->pdev->dev, "fail to set default GMU clk freq %d\n",
-				gmu->gmu_freqs[DEFAULT_GMU_FREQ_IDX]);
-		return ret;
-	}
-
-	while ((j < MAX_GMU_CLKS) && gmu->clks[j]) {
-		ret = clk_prepare_enable(gmu->clks[j]);
-		if (ret) {
-			dev_err(&gmu->pdev->dev,
-					"fail to enable gpucc clk idx %d\n",
-					j);
-			return ret;
-		}
-		j++;
-	}
-
-	set_bit(GMU_CLK_ON, &gmu->flags);
-	return 0;
-}
-
-static int gmu_disable_clks(struct gmu_device *gmu)
-{
-	int j = 0;
-
-	if (IS_ERR_OR_NULL(gmu->clks[0]))
-		return 0;
-
-	while ((j < MAX_GMU_CLKS) && gmu->clks[j]) {
-		clk_disable_unprepare(gmu->clks[j]);
-		j++;
-	}
-
-	clear_bit(GMU_CLK_ON, &gmu->flags);
-	return 0;
-
-}
-
-static int gmu_enable_gdsc(struct gmu_device *gmu)
-{
-	int ret;
-
-	if (IS_ERR_OR_NULL(gmu->cx_gdsc))
-		return 0;
-
-	ret = regulator_enable(gmu->cx_gdsc);
-	if (ret)
-		dev_err(&gmu->pdev->dev,
-			"Failed to enable GMU CX gdsc, error %d\n", ret);
-
-	return ret;
-}
-
-#define CX_GDSC_TIMEOUT	5000	/* ms */
-static int gmu_disable_gdsc(struct gmu_device *gmu)
-{
-	int ret;
-	unsigned long t;
-
-	if (IS_ERR_OR_NULL(gmu->cx_gdsc))
-		return 0;
-
-	ret = regulator_disable(gmu->cx_gdsc);
-	if (ret) {
-		dev_err(&gmu->pdev->dev,
-			"Failed to disable GMU CX gdsc, error %d\n", ret);
-		return ret;
-	}
-
-	/*
-	 * After GX GDSC is off, CX GDSC must be off
-	 * Voting off alone from GPU driver cannot
-	 * Guarantee CX GDSC off. Polling with 5s
-	 * timeout to ensure
-	 */
-	t = jiffies + msecs_to_jiffies(CX_GDSC_TIMEOUT);
-	do {
-		if (!regulator_is_enabled(gmu->cx_gdsc))
-			return 0;
-		usleep_range(10, 100);
-
-	} while (!(time_after(jiffies, t)));
-
-	if (!regulator_is_enabled(gmu->cx_gdsc))
-		return 0;
-
-	dev_err(&gmu->pdev->dev, "GMU CX gdsc off timeout");
-	return -ETIMEDOUT;
-}
-
-int gmu_suspend(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct gmu_device *gmu = &device->gmu;
-
-	if (!test_bit(GMU_CLK_ON, &gmu->flags))
-		return 0;
-
-	/* Pending message in all queues are abandoned */
-	hfi_stop(gmu);
-	clear_bit(GMU_HFI_ON, &gmu->flags);
-	gmu_irq_disable(device);
-
-	if (gpudev->rpmh_gpu_pwrctrl(adreno_dev, GMU_SUSPEND, 0, 0))
-		return -EINVAL;
-
-	gmu_disable_clks(gmu);
-	gmu_disable_gdsc(gmu);
-	dev_err(&gmu->pdev->dev, "Suspended GMU\n");
-	return 0;
-}
-
-void gmu_snapshot(struct kgsl_device *device)
-{
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct gmu_device *gmu = &device->gmu;
-
-	/* Mask so there's no interrupt caused by NMI */
-	adreno_write_gmureg(adreno_dev,
-			ADRENO_REG_GMU_GMU2HOST_INTR_MASK, 0xFFFFFFFF);
-
-	/* Make sure the interrupt is masked before causing it */
-	wmb();
-	adreno_write_gmureg(adreno_dev,
-		ADRENO_REG_GMU_NMI_CONTROL_STATUS, 0);
-	adreno_write_gmureg(adreno_dev,
-		ADRENO_REG_GMU_CM3_CFG, (1 << 9));
-
-	/* Wait for the NMI to be handled */
-	wmb();
-	udelay(100);
-	kgsl_device_snapshot(device, NULL, true);
-
-	adreno_write_gmureg(adreno_dev,
-			ADRENO_REG_GMU_GMU2HOST_INTR_CLR, 0xFFFFFFFF);
-	adreno_write_gmureg(adreno_dev,
-			ADRENO_REG_GMU_GMU2HOST_INTR_MASK,
-			(unsigned int) ~HFI_IRQ_MASK);
-
-	gmu->fault_count++;
-}
-
-static void gmu_change_gpu_pwrlevel(struct kgsl_device *device,
-	unsigned int new_level) {
-
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	unsigned int old_level = pwr->active_pwrlevel;
-
-	/*
-	 * Update the level according to any thermal,
-	 * max/min, or power constraints.
-	 */
-	new_level = kgsl_pwrctrl_adjust_pwrlevel(device, new_level);
-
-	/*
-	 * If thermal cycling is required and the new level hits the
-	 * thermal limit, kick off the cycling.
-	 */
-	kgsl_pwrctrl_set_thermal_cycle(device, new_level);
-
-	pwr->active_pwrlevel = new_level;
-	pwr->previous_pwrlevel = old_level;
-
-	/* Request adjusted DCVS level */
-	kgsl_clk_set_rate(device, pwr->active_pwrlevel);
-}
-
-/* To be called to power on both GPU and GMU */
-int gmu_start(struct kgsl_device *device)
-{
-	int ret = 0;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	struct gmu_device *gmu = &device->gmu;
-	unsigned int boot_state = GMU_WARM_BOOT;
-
-	switch (device->state) {
-	case KGSL_STATE_INIT:
-	case KGSL_STATE_SUSPEND:
-		WARN_ON(test_bit(GMU_CLK_ON, &gmu->flags));
-		gmu_enable_gdsc(gmu);
-		gmu_enable_clks(gmu);
-		gmu_irq_enable(device);
-
-		/* Vote for 300MHz DDR for GMU to init */
-		ret = msm_bus_scale_client_update_request(gmu->pcl,
-				pwr->pwrlevels[pwr->default_pwrlevel].bus_freq);
-		if (ret)
-			dev_err(&gmu->pdev->dev,
-				"Failed to allocate gmu b/w: %d\n", ret);
-
-		ret = gpudev->rpmh_gpu_pwrctrl(adreno_dev, GMU_FW_START,
-				GMU_COLD_BOOT, 0);
-		if (ret)
-			goto error_gmu;
-
-		ret = hfi_start(gmu, GMU_COLD_BOOT);
-		if (ret)
-			goto error_gmu;
-
-		/* Request default DCVS level */
-		gmu_change_gpu_pwrlevel(device, pwr->default_pwrlevel);
-		msm_bus_scale_client_update_request(gmu->pcl, 0);
-		break;
-
-	case KGSL_STATE_SLUMBER:
-		WARN_ON(test_bit(GMU_CLK_ON, &gmu->flags));
-		gmu_enable_gdsc(gmu);
-		gmu_enable_clks(gmu);
-		gmu_irq_enable(device);
-
-		/*
-		 * If unrecovered is set that means last
-		 * wakeup from SLUMBER state failed. Use GMU
-		 * and HFI boot state as COLD as this is a
-		 * boot after RESET.
-		 */
-		if (gmu->unrecovered)
-			boot_state = GMU_COLD_BOOT;
-
-		ret = gpudev->rpmh_gpu_pwrctrl(adreno_dev, GMU_FW_START,
-				boot_state, 0);
-		if (ret)
-			goto error_gmu;
-
-		ret = hfi_start(gmu, boot_state);
-		if (ret)
-			goto error_gmu;
-
-		gmu_change_gpu_pwrlevel(device, pwr->default_pwrlevel);
-		break;
-
-	case KGSL_STATE_RESET:
-		if (test_bit(ADRENO_DEVICE_HARD_RESET, &adreno_dev->priv) ||
-			test_bit(GMU_FAULT, &gmu->flags)) {
-			gmu_suspend(device);
-			gmu_enable_gdsc(gmu);
-			gmu_enable_clks(gmu);
-			gmu_irq_enable(device);
-
-			ret = gpudev->rpmh_gpu_pwrctrl(
-				adreno_dev, GMU_FW_START, GMU_COLD_BOOT, 0);
-			if (ret)
-				goto error_gmu;
-
-
-			ret = hfi_start(gmu, GMU_COLD_BOOT);
-			if (ret)
-				goto error_gmu;
-
-			/* Send DCVS level prior to reset*/
-			gmu_change_gpu_pwrlevel(device,
-				pwr->default_pwrlevel);
-		} else {
-			/* GMU fast boot */
-			hfi_stop(gmu);
-
-			ret = gpudev->rpmh_gpu_pwrctrl(adreno_dev, GMU_FW_START,
-					GMU_COLD_BOOT, 0);
-			if (ret)
-				goto error_gmu;
-
-			ret = hfi_start(gmu, GMU_WARM_BOOT);
-			if (ret)
-				goto error_gmu;
-		}
-		break;
-	default:
-		break;
-	}
-
-	/* Clear unrecovered as GMU start is successful */
-	gmu->unrecovered = false;
-	return ret;
-
-error_gmu:
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_HFI_USE_REG))
-		gpudev->oob_clear(adreno_dev,
-				OOB_BOOT_SLUMBER_CLEAR_MASK);
-	gmu_snapshot(device);
-	return ret;
-}
-
-/* Caller shall ensure GPU is ready for SLUMBER */
-void gmu_stop(struct kgsl_device *device)
-{
-	struct gmu_device *gmu = &device->gmu;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	int ret = 0;
-
-	if (!test_bit(GMU_CLK_ON, &gmu->flags))
-		return;
-
-	/* Wait for the lowest idle level we requested */
-	if (gpudev->wait_for_lowest_idle &&
-			gpudev->wait_for_lowest_idle(adreno_dev))
-		goto error;
-
-	ret = gpudev->rpmh_gpu_pwrctrl(adreno_dev, GMU_NOTIFY_SLUMBER, 0, 0);
-	if (ret)
-		goto error;
-
-	if (gpudev->wait_for_gmu_idle &&
-			gpudev->wait_for_gmu_idle(adreno_dev))
-		goto error;
-
-	/* Pending message in all queues are abandoned */
-	hfi_stop(gmu);
-	clear_bit(GMU_HFI_ON, &gmu->flags);
-	gmu_irq_disable(device);
-
-	gpudev->rpmh_gpu_pwrctrl(adreno_dev, GMU_FW_STOP, 0, 0);
-	gmu_disable_clks(gmu);
-	gmu_disable_gdsc(gmu);
-
-	msm_bus_scale_client_update_request(gmu->pcl, 0);
-	return;
-
-error:
-	/*
-	 * The power controller will change state to SLUMBER anyway
-	 * Set GMU_FAULT flag to indicate to power contrller
-	 * that hang recovery is needed to power on GPU
-	 */
-	set_bit(GMU_FAULT, &gmu->flags);
-	dev_err(&gmu->pdev->dev, "Failed to stop GMU\n");
-	gmu_snapshot(device);
-}
-
-void gmu_remove(struct kgsl_device *device)
-{
-	struct gmu_device *gmu = &device->gmu;
-	struct kgsl_hfi *hfi = &gmu->hfi;
-	int i = 0;
-
-	if (!device->gmu.pdev)
-		return;
-
-	tasklet_kill(&hfi->tasklet);
-
-	gmu_stop(device);
-	gmu_irq_disable(device);
-
-	while ((i < MAX_GMU_CLKS) && gmu->clks[i]) {
-		gmu->clks[i] = NULL;
-		i++;
-	}
-
-	if (gmu->gmu_interrupt_num) {
-		devm_free_irq(&gmu->pdev->dev,
-				gmu->gmu_interrupt_num, gmu);
-		gmu->gmu_interrupt_num = 0;
-	}
-
-	if (hfi->hfi_interrupt_num) {
-		devm_free_irq(&gmu->pdev->dev,
-				hfi->hfi_interrupt_num, hfi);
-		hfi->hfi_interrupt_num = 0;
-	}
-
-	if (gmu->ccl) {
-		msm_bus_scale_unregister_client(gmu->ccl);
-		gmu->ccl = 0;
-	}
-
-	if (gmu->pcl) {
-		msm_bus_scale_unregister_client(gmu->pcl);
-		gmu->pcl = 0;
-	}
-
-	if (gmu->pdc_reg_virt) {
-		devm_iounmap(&gmu->pdev->dev, gmu->pdc_reg_virt);
-		gmu->pdc_reg_virt = NULL;
-	}
-
-	if (gmu->reg_virt) {
-		devm_iounmap(&gmu->pdev->dev, gmu->reg_virt);
-		gmu->reg_virt = NULL;
-	}
-
-	if (gmu->hfi_mem || gmu->dump_mem)
-		gmu_memory_close(&device->gmu);
-
-	for (i = 0; i < MAX_GMU_CLKS; i++) {
-		if (gmu->clks[i]) {
-			devm_clk_put(&gmu->pdev->dev, gmu->clks[i]);
-			gmu->clks[i] = NULL;
-		}
-	}
-
-	if (gmu->gx_gdsc) {
-		devm_regulator_put(gmu->gx_gdsc);
-		gmu->gx_gdsc = NULL;
-	}
-
-	if (gmu->cx_gdsc) {
-		devm_regulator_put(gmu->cx_gdsc);
-		gmu->cx_gdsc = NULL;
-	}
-
-	device->gmu.pdev = NULL;
-}
-
-/*
- * adreno_gmu_fenced_write() - Check if there is a GMU and it is enabled
- * @adreno_dev: Pointer to the Adreno device device that owns the GMU
- * @offset: 32bit register enum that is to be written
- * @val: The value to be written to the register
- * @fence_mask: The value to poll the fence status register
- *
- * Check the WRITEDROPPED0/1 bit in the FENCE_STATUS regsiter to check if
- * the write to the fenced register went through. If it didn't then we retry
- * the write until it goes through or we time out.
- */
-int adreno_gmu_fenced_write(struct adreno_device *adreno_dev,
-		enum adreno_regs offset, unsigned int val,
-		unsigned int fence_mask)
-{
-	unsigned int status, i;
-	struct adreno_gpudev *gpudev = ADRENO_GPU_DEVICE(adreno_dev);
-	unsigned int reg_offset = gpudev->reg_offsets->offsets[offset];
-
-	adreno_writereg(adreno_dev, offset, val);
-
-	if (!kgsl_gmu_isenabled(KGSL_DEVICE(adreno_dev)))
-		return 0;
-
-	for (i = 0; i < GMU_LONG_WAKEUP_RETRY_LIMIT; i++) {
-		adreno_read_gmureg(adreno_dev, ADRENO_REG_GMU_AHB_FENCE_STATUS,
-			&status);
-
-		/*
-		 * If !writedropped0/1, then the write to fenced register
-		 * was successful
-		 */
-		if (!(status & fence_mask))
-			return 0;
-		/* Wait a small amount of time before trying again */
-		udelay(GMU_WAKEUP_DELAY_US);
-
-		/* Try to write the fenced register again */
-		adreno_writereg(adreno_dev, offset, val);
-
-		if (i == GMU_SHORT_WAKEUP_RETRY_LIMIT)
-			dev_err(adreno_dev->dev.dev,
-				"Waited %d usecs to write fenced register 0x%x. Continuing to wait...\n",
-				(GMU_SHORT_WAKEUP_RETRY_LIMIT *
-				GMU_WAKEUP_DELAY_US),
-				reg_offset);
-	}
-
-	dev_err(adreno_dev->dev.dev,
-		"Timed out waiting %d usecs to write fenced register 0x%x\n",
-		GMU_LONG_WAKEUP_RETRY_LIMIT * GMU_WAKEUP_DELAY_US,
-		reg_offset);
-
-	return -ETIMEDOUT;
-}
diff --git a/drivers/gpu/msm/kgsl_gmu.h b/drivers/gpu/msm/kgsl_gmu.h
deleted file mode 100644
index 069f83581cfe..000000000000
--- a/drivers/gpu/msm/kgsl_gmu.h
+++ /dev/null
@@ -1,270 +0,0 @@
-/* Copyright (c) 2017-2018, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-#ifndef __KGSL_GMU_H
-#define __KGSL_GMU_H
-
-#include "kgsl_hfi.h"
-
-#define FW_VER_MAJOR(ver)		(((ver)>>28) & 0xFF)
-#define FW_VER_MINOR(ver)		(((ver)>>16) & 0xFFF)
-#define FW_VERSION(major, minor)	\
-		(((major) << 28) | (((minor) & 0xFFF) << 16))
-
-#define GMU_INT_WDOG_BITE		BIT(0)
-#define GMU_INT_RSCC_COMP		BIT(1)
-#define GMU_INT_FENCE_ERR		BIT(3)
-#define GMU_INT_DBD_WAKEUP		BIT(4)
-#define GMU_INT_HOST_AHB_BUS_ERR	BIT(5)
-#define GMU_AO_INT_MASK		\
-		(GMU_INT_WDOG_BITE |	\
-		GMU_INT_HOST_AHB_BUS_ERR |	\
-		GMU_INT_FENCE_ERR)
-
-#define MAX_GMUFW_SIZE	0x2000	/* in bytes */
-#define FENCE_RANGE_MASK	((0x1 << 31) | ((0xA << 2) << 18) | (0x8A0))
-
-#define FENCE_STATUS_WRITEDROPPED0_MASK 0x1
-#define FENCE_STATUS_WRITEDROPPED1_MASK 0x2
-
-/* Bitmask for GPU low power mode enabling and hysterisis*/
-#define SPTP_ENABLE_MASK (BIT(2) | BIT(0))
-#define IFPC_ENABLE_MASK (BIT(1) | BIT(0))
-#define HW_NAP_ENABLE_MASK	BIT(0)
-#define MIN_BW_ENABLE_MASK	BIT(12)
-#define MIN_BW_HYST		0xFA0
-
-/* Bitmask for RPMH capability enabling */
-#define RPMH_INTERFACE_ENABLE	BIT(0)
-#define LLC_VOTE_ENABLE			BIT(4)
-#define DDR_VOTE_ENABLE			BIT(8)
-#define MX_VOTE_ENABLE			BIT(9)
-#define CX_VOTE_ENABLE			BIT(10)
-#define GFX_VOTE_ENABLE			BIT(11)
-#define RPMH_ENABLE_MASK	(RPMH_INTERFACE_ENABLE	| \
-				LLC_VOTE_ENABLE		| \
-				DDR_VOTE_ENABLE		| \
-				MX_VOTE_ENABLE		| \
-				CX_VOTE_ENABLE		| \
-				GFX_VOTE_ENABLE)
-
-/* Bitmask for GPU idle status check */
-#define GPUBUSYIGNAHB		BIT(23)
-#define CXGXCPUBUSYIGNAHB	BIT(30)
-
-/* GMU timeouts */
-#define GMU_IDLE_TIMEOUT        100 /* ms */
-
-/* Constants for GMU OOBs */
-#define OOB_BOOT_OPTION         0
-#define OOB_SLUMBER_OPTION      1
-
-/* Bitmasks for GMU OOBs */
-#define OOB_BOOT_SLUMBER_SET_MASK	BIT(22)
-#define OOB_BOOT_SLUMBER_CHECK_MASK	BIT(30)
-#define OOB_BOOT_SLUMBER_CLEAR_MASK	BIT(30)
-#define OOB_DCVS_SET_MASK		BIT(23)
-#define OOB_DCVS_CHECK_MASK		BIT(31)
-#define OOB_DCVS_CLEAR_MASK		BIT(31)
-#define OOB_GPU_SET_MASK		BIT(16)
-#define OOB_GPU_CHECK_MASK		BIT(24)
-#define OOB_GPU_CLEAR_MASK		BIT(24)
-#define OOB_PERFCNTR_SET_MASK		BIT(17)
-#define OOB_PERFCNTR_CHECK_MASK		BIT(25)
-#define OOB_PERFCNTR_CLEAR_MASK		BIT(25)
-#define OOB_PREEMPTION_SET_MASK		BIT(18)
-#define OOB_PREEMPTION_CHECK_MASK	BIT(26)
-#define OOB_PREEMPTION_CLEAR_MASK	BIT(26)
-
-/*
- * Wait time before trying to write the register again.
- * Hopefully the GMU has finished waking up during this delay.
- * This delay must be less than the IFPC main hysteresis or
- * the GMU will start shutting down before we try again.
- */
-#define GMU_WAKEUP_DELAY_US 10
-
-/* Max amount of tries to wake up the GMU. The short retry
- * limit is half of the long retry limit. After the short
- * number of retries, we print an informational message to say
- * exiting IFPC is taking longer than expected. We continue
- * to retry after this until the long retry limit.
- */
-#define GMU_SHORT_WAKEUP_RETRY_LIMIT 100
-#define GMU_LONG_WAKEUP_RETRY_LIMIT 200
-
-/* Bits for the flags field in the gmu structure */
-enum gmu_flags {
-	GMU_BOOT_INIT_DONE = 0,
-	GMU_CLK_ON = 1,
-	GMU_HFI_ON = 2,
-	GMU_FAULT = 3,
-	GMU_DCVS_REPLAY = 4,
-	GMU_RSCC_SLEEP_SEQ_DONE = 5,
-};
-
-/**
- * struct gmu_memdesc - Gmu shared memory object descriptor
- * @hostptr: Kernel virtual address
- * @gmuaddr: GPU virtual address
- * @physaddr: Physical address of the memory object
- * @size: Size of the memory object
- * @attr: memory attributes for this memory
- */
-struct gmu_memdesc {
-	void *hostptr;
-	uint64_t gmuaddr;
-	phys_addr_t physaddr;
-	uint64_t size;
-	uint32_t attr;
-};
-
-struct gmu_bw_votes {
-	uint32_t cmds_wait_bitmask;
-	uint32_t cmds_per_bw_vote;
-	uint32_t cmd_addrs[MAX_BW_CMDS];
-	uint32_t cmd_data[MAX_GX_LEVELS][MAX_BW_CMDS];
-};
-
-struct rpmh_votes_t {
-	struct arc_vote_desc gx_votes[MAX_GX_LEVELS];
-	struct arc_vote_desc cx_votes[MAX_CX_LEVELS];
-	struct gmu_bw_votes ddr_votes;
-	struct gmu_bw_votes cnoc_votes;
-};
-
-#define MAX_GMU_CLKS 6
-#define DEFAULT_GMU_FREQ_IDX 1
-
-/*
- * These are the different ways the GMU can boot. GMU_WARM_BOOT is waking up
- * from slumber. GMU_COLD_BOOT is booting for the first time.
- */
-enum gmu_boot {
-	GMU_WARM_BOOT = 0,
-	GMU_COLD_BOOT = 1,
-};
-
-enum gmu_load_mode {
-	CACHED_LOAD_BOOT,
-	CACHED_BOOT,
-	TCM_BOOT,
-	TCM_LOAD_BOOT,
-	INVALID_LOAD
-};
-
-enum gmu_pwrctrl_mode {
-	GMU_FW_START,
-	GMU_FW_STOP,
-	GMU_SUSPEND,
-	GMU_DCVS_NOHFI,
-	GMU_NOTIFY_SLUMBER,
-	INVALID_POWER_CTRL
-};
-
-enum gpu_idle_level {
-	GPU_HW_ACTIVE = 0x0,
-	GPU_HW_SPTP_PC = 0x2,
-	GPU_HW_IFPC = 0x3,
-	GPU_HW_NAP = 0x4,
-	GPU_HW_MIN_VOLT = 0x5,
-	GPU_HW_MIN_DDR = 0x6,
-	GPU_HW_SLUMBER = 0xF
-};
-
-/**
- * struct gmu_device - GMU device structure
- * @ver: GMU FW version, read from GMU
- * @reg_phys: GMU CSR physical address
- * @reg_virt: GMU CSR virtual address
- * @reg_len: GMU CSR range
- * @gmu2gpu_offset: address difference between GMU register set
- *	and GPU register set, the offset will be used when accessing
- *	gmu registers using offset defined in GPU register space.
- * @pdc_reg_virt: starting kernel virtual address for RPMh PDC registers
- * @gmu_interrupt_num: GMU interrupt number
- * @fw_image: descriptor of GMU memory that has GMU image in it
- * @hfi_mem: pointer to HFI shared memory
- * @dump_mem: pointer to GMU debug dump memory
- * @hfi: HFI controller
- * @lm_config: GPU LM configuration data
- * @lm_dcvs_level: Minimal DCVS level that enable LM. LM disable in
- *		lower levels
- * @bcl_config: Battery Current Limit configuration data
- * @gmu_freqs: GMU frequency table with lowest freq at index 0
- * @gpu_freqs: GPU frequency table with lowest freq at index 0
- * @num_gmupwrlevels: number GMU frequencies in GMU freq table
- * @num_gpupwrlevels: number GPU frequencies in GPU freq table
- * @num_bwlevel: number of GPU BW levels
- * @num_cnocbwlevel: number CNOC BW levels
- * @rpmh_votes: RPMh TCS command set for GPU, GMU voltage and bw scaling
- * @cx_gdsc: CX headswitch that controls power of GMU and
-		subsystem peripherals
- * @gx_gdsc: GX headswitch that controls power of GPU subsystem
- * @clks: GPU subsystem clocks required for GMU functionality
- * @load_mode: GMU FW load/boot mode
- * @flags: GMU power control flags
- * @wakeup_pwrlevel: GPU wake up power/DCVS level in case different
- *		than default power level
- * @pcl: GPU BW scaling client
- * @ccl: CNOC BW scaling client
- * @idle_level: Minimal GPU idle power level
- * @fault_count: GMU fault count
- * @unrecovered: Indicates whether GMU recovery failed or not
- */
-struct gmu_device {
-	unsigned int ver;
-	struct platform_device *pdev;
-	unsigned long reg_phys;
-	void __iomem *reg_virt;
-	unsigned int reg_len;
-	unsigned int gmu2gpu_offset;
-	void __iomem *pdc_reg_virt;
-	unsigned int gmu_interrupt_num;
-	struct gmu_memdesc fw_image;
-	struct gmu_memdesc *hfi_mem;
-	struct gmu_memdesc *dump_mem;
-	struct kgsl_hfi hfi;
-	struct limits_config lm_config;
-	unsigned int lm_dcvs_level;
-	unsigned int bcl_config;
-	unsigned int gmu_freqs[MAX_CX_LEVELS];
-	unsigned int gpu_freqs[MAX_GX_LEVELS];
-	unsigned int num_gmupwrlevels;
-	unsigned int num_gpupwrlevels;
-	unsigned int num_bwlevels;
-	unsigned int num_cnocbwlevels;
-	struct rpmh_votes_t rpmh_votes;
-	struct regulator *cx_gdsc;
-	struct regulator *gx_gdsc;
-	struct clk *clks[MAX_GMU_CLKS];
-	enum gmu_load_mode load_mode;
-	unsigned long flags;
-	unsigned int wakeup_pwrlevel;
-	unsigned int pcl;
-	unsigned int ccl;
-	unsigned int idle_level;
-	unsigned int fault_count;
-	bool unrecovered;
-};
-
-void gmu_snapshot(struct kgsl_device *device);
-bool kgsl_gmu_isenabled(struct kgsl_device *device);
-int gmu_probe(struct kgsl_device *device);
-void gmu_remove(struct kgsl_device *device);
-int allocate_gmu_image(struct gmu_device *gmu, unsigned int size);
-int gmu_start(struct kgsl_device *device);
-void gmu_stop(struct kgsl_device *device);
-int gmu_suspend(struct kgsl_device *device);
-int gmu_dcvs_set(struct gmu_device *gmu, unsigned int gpu_pwrlevel,
-		unsigned int bus_level);
-#endif /* __KGSL_GMU_H */
diff --git a/drivers/gpu/msm/kgsl_hfi.c b/drivers/gpu/msm/kgsl_hfi.c
deleted file mode 100644
index b1e6c8305ca5..000000000000
--- a/drivers/gpu/msm/kgsl_hfi.c
+++ /dev/null
@@ -1,650 +0,0 @@
-/* Copyright (c) 2017-2018, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#include "kgsl_device.h"
-#include "kgsl_gmu.h"
-#include "adreno.h"
-#include "kgsl_trace.h"
-
-/* Size in below functions are in unit of dwords */
-static int hfi_msgq_read(struct gmu_device *gmu,
-		enum hfi_queue_type queue_idx, void *msg,
-		unsigned int max_size)
-{
-	struct gmu_memdesc *mem_addr = gmu->hfi_mem;
-	struct hfi_queue_table *tbl = mem_addr->hostptr;
-	struct hfi_queue_header *hdr = &tbl->qhdr[queue_idx];
-	uint32_t *queue = HOST_QUEUE_START_ADDR(mem_addr, queue_idx);
-	uint32_t *output = msg;
-	struct hfi_msg_hdr *msg_hdr;
-	int i, read, result = 0;
-
-	if (hdr->read_index == hdr->write_index) {
-		hdr->rx_req = 1;
-		return -ENODATA;
-	}
-
-	msg_hdr = (struct hfi_msg_hdr *)&queue[hdr->read_index];
-
-	if (msg_hdr->size > max_size) {
-		dev_err(&gmu->pdev->dev,
-			"Received invalid msg: size=%d dwords, rd idx=%d, id=%d\n",
-			msg_hdr->size, hdr->read_index, msg_hdr->id);
-		return -EMSGSIZE;
-	}
-
-	read = hdr->read_index;
-
-	if (read < hdr->queue_size) {
-		for (i = 0; i < msg_hdr->size; i++) {
-			output[i] = queue[read];
-			read = (read + 1)%hdr->queue_size;
-		}
-		result = msg_hdr->size;
-	} else {
-		/* In case FW messed up */
-		dev_err(&gmu->pdev->dev,
-			"Read index %d greater than queue size %d\n",
-			hdr->read_index, hdr->queue_size);
-		result = -ENODATA;
-	}
-	hdr->read_index = read;
-
-	return result;
-}
-
-/* Size in below functions are in unit of dwords */
-static int hfi_cmdq_write(struct gmu_device *gmu,
-		enum hfi_queue_type queue_idx,
-		struct hfi_msg_hdr *msg)
-{
-	struct kgsl_device *device = container_of(gmu, struct kgsl_device, gmu);
-	struct hfi_queue_table *tbl = gmu->hfi_mem->hostptr;
-	struct hfi_queue_header *hdr = &tbl->qhdr[queue_idx];
-	uint32_t *queue = HOST_QUEUE_START_ADDR(gmu->hfi_mem, queue_idx);
-	uint32_t *input = (uint32_t *) msg;
-	struct kgsl_hfi *hfi = &gmu->hfi;
-	uint32_t i, write, empty_space;
-
-	if (msg->size > HFI_MAX_MSG_SIZE) {
-		dev_err(&gmu->pdev->dev,
-			"Message too big to send: sz=%d, id=%d\n",
-			msg->size, msg->id);
-		return -EINVAL;
-	}
-
-	trace_kgsl_hfi_send(msg->id, msg->size, msg->seqnum);
-
-	mutex_lock(&hfi->cmdq_mutex);
-
-	empty_space = (hdr->write_index >= hdr->read_index) ?
-			(hdr->queue_size - (hdr->write_index - hdr->read_index))
-			: (hdr->read_index - hdr->write_index);
-
-	if (empty_space < msg->size) {
-		dev_err(&gmu->pdev->dev,
-			"Insufficient bufsize %d for msg id=%d of size %d\n",
-			empty_space, msg->id, msg->size);
-
-		hdr->drop_cnt++;
-		mutex_unlock(&hfi->cmdq_mutex);
-		return -ENOSPC;
-	}
-
-	write = hdr->write_index;
-
-	for (i = 0; i < msg->size; i++) {
-		queue[write] = input[i];
-		write = (write + 1) % hdr->queue_size;
-	}
-
-	hdr->write_index = write;
-
-	mutex_unlock(&hfi->cmdq_mutex);
-
-	/*
-	 * Memory barrier to make sure packet and write index are written before
-	 * an interrupt is raised
-	 */
-	wmb();
-
-	/* Send interrupt to GMU to receive the message */
-	adreno_write_gmureg(ADRENO_DEVICE(device),
-		ADRENO_REG_GMU_HOST2GMU_INTR_SET, 0x1);
-
-	return msg->size;
-}
-
-#define QUEUE_HDR_TYPE(id, prio, rtype, stype) \
-	(((id) & 0xFF) | (((prio) & 0xFF) << 8) | \
-	(((rtype) & 0xFF) << 16) | (((stype) & 0xFF) << 24))
-
-
-/* Sizes of the queue and message are in unit of dwords */
-void hfi_init(struct kgsl_hfi *hfi, struct gmu_memdesc *mem_addr,
-		uint32_t queue_sz_bytes)
-{
-	int i;
-	struct hfi_queue_table *tbl;
-	struct hfi_queue_header *hdr;
-	int queue_prio[HFI_QUEUE_MAX] = {
-		HFI_H2F_QPRI_CMD,
-		HFI_F2H_QPRI_MSG,
-		HFI_F2H_QPRI_DEBUG
-	};
-	int queue_ids[HFI_QUEUE_MAX] = {0, 4, 5};
-
-	/* Fill Table Header */
-	tbl = mem_addr->hostptr;
-	tbl->qtbl_hdr.version = 0;
-	tbl->qtbl_hdr.size = sizeof(struct hfi_queue_table) >> 2;
-	tbl->qtbl_hdr.qhdr0_offset = sizeof(struct hfi_queue_table_header) >> 2;
-	tbl->qtbl_hdr.qhdr_size = sizeof(struct hfi_queue_header) >> 2;
-	tbl->qtbl_hdr.num_q = HFI_QUEUE_MAX;
-	tbl->qtbl_hdr.num_active_q = HFI_QUEUE_MAX;
-
-	/* Fill I dividual Queue Headers */
-	for (i = 0; i < HFI_QUEUE_MAX; i++) {
-		hdr = &tbl->qhdr[i];
-		hdr->start_addr = GMU_QUEUE_START_ADDR(mem_addr, i);
-		hdr->type = QUEUE_HDR_TYPE(queue_ids[i], queue_prio[i], 0,  0);
-		hdr->status = 0x1;
-		hdr->queue_size = queue_sz_bytes >> 2; /* convert to dwords */
-		hdr->msg_size = 0;
-		hdr->drop_cnt = 0;
-		hdr->rx_wm = 0x1;
-		hdr->tx_wm = 0x1;
-		hdr->rx_req = 0x1;
-		hdr->tx_req = 0x0;
-		hdr->read_index = 0x0;
-		hdr->write_index = 0x0;
-	}
-
-	mutex_init(&hfi->cmdq_mutex);
-}
-
-static void receive_ack_msg(struct gmu_device *gmu, struct hfi_msg_rsp *rsp)
-{
-	struct kgsl_hfi *hfi = &gmu->hfi;
-	struct pending_msg *msg = NULL, *next;
-	bool in_queue = false;
-
-	trace_kgsl_hfi_receive(rsp->ret_hdr.id,
-		rsp->ret_hdr.size,
-		rsp->ret_hdr.seqnum);
-
-	spin_lock_bh(&hfi->msglock);
-	list_for_each_entry_safe(msg, next, &hfi->msglist, node) {
-		if (msg->msg_id == rsp->ret_hdr.id &&
-				msg->seqnum == rsp->ret_hdr.seqnum) {
-			in_queue = true;
-			break;
-		}
-	}
-
-	if (in_queue == false) {
-		spin_unlock_bh(&hfi->msglock);
-		dev_err(&gmu->pdev->dev,
-				"Cannot find receiver of ack msg with id=%d\n",
-				rsp->ret_hdr.id);
-		return;
-	}
-
-	memcpy(&msg->results, (void *) rsp, rsp->hdr.size << 2);
-	complete(&msg->msg_complete);
-	spin_unlock_bh(&hfi->msglock);
-}
-
-static void receive_err_msg(struct gmu_device *gmu, struct hfi_msg_rsp *rsp)
-{
-	struct hfi_fw_err_msg *err = (struct hfi_fw_err_msg *) rsp;
-
-	dev_err(&gmu->pdev->dev, "FW error with error code %d\n",
-			err->error_code);
-}
-
-static int hfi_send_msg(struct gmu_device *gmu, struct hfi_msg_hdr *msg,
-		unsigned int size, struct pending_msg *ret_msg)
-{
-	int rc = 0;
-	struct kgsl_hfi *hfi = &gmu->hfi;
-
-	msg->seqnum = atomic_inc_return(&hfi->seqnum);
-	if (msg->type != HFI_MSG_CMD) {
-		if (hfi_cmdq_write(gmu, HFI_CMD_QUEUE, msg) != size)
-			rc = -EINVAL;
-		return rc;
-	}
-
-	/* For messages of type HFI_MSG_CMD we must handle the ack */
-	init_completion(&ret_msg->msg_complete);
-	ret_msg->msg_id = msg->id;
-	ret_msg->seqnum = msg->seqnum;
-
-	spin_lock_bh(&hfi->msglock);
-	list_add_tail(&ret_msg->node, &hfi->msglist);
-	spin_unlock_bh(&hfi->msglock);
-
-	if (hfi_cmdq_write(gmu, HFI_CMD_QUEUE, msg) != size) {
-		rc = -EINVAL;
-		goto done;
-	}
-
-	rc = wait_for_completion_timeout(
-			&ret_msg->msg_complete,
-			msecs_to_jiffies(HFI_RSP_TIMEOUT));
-	if (!rc) {
-		dev_err(&gmu->pdev->dev,
-				"Receiving GMU ack %d timed out\n", msg->id);
-		rc = -ETIMEDOUT;
-		goto done;
-	}
-
-	/* If we got here we succeeded */
-	rc = 0;
-done:
-	spin_lock_bh(&hfi->msglock);
-	list_del(&ret_msg->node);
-	spin_unlock_bh(&hfi->msglock);
-	return rc;
-}
-
-static int hfi_send_gmu_init(struct gmu_device *gmu, uint32_t boot_state)
-{
-	struct hfi_gmu_init_cmd init_msg = {
-		.hdr = {
-			.id = H2F_MSG_INIT,
-			.size = sizeof(init_msg) >> 2,
-			.type = HFI_MSG_CMD,
-		},
-		.seg_id = 0,
-		.dbg_buffer_addr = (unsigned int) gmu->dump_mem->gmuaddr,
-		.dbg_buffer_size = (unsigned int) gmu->dump_mem->size,
-		.boot_state = boot_state,
-	};
-
-	struct hfi_msg_rsp *rsp;
-	uint32_t msg_size_dwords = (sizeof(init_msg)) >> 2;
-	int rc = 0;
-	struct pending_msg msg;
-
-	rc = hfi_send_msg(gmu, &init_msg.hdr, msg_size_dwords, &msg);
-	if (rc)
-		return rc;
-
-	rsp = (struct hfi_msg_rsp *) &msg.results;
-	rc = rsp->error;
-	if (!rc)
-		gmu->hfi.gmu_init_done = true;
-	else
-		dev_err(&gmu->pdev->dev,
-			"gmu init message failed with error=%d\n", rc);
-	return rc;
-}
-
-static int hfi_get_fw_version(struct gmu_device *gmu,
-		uint32_t expected_ver, uint32_t *ver)
-{
-	struct hfi_fw_version_cmd fw_ver = {
-		.hdr = {
-			.id = H2F_MSG_FW_VER,
-			.size = sizeof(fw_ver) >> 2,
-			.type = HFI_MSG_CMD
-		},
-		.supported_ver = expected_ver,
-	};
-	struct hfi_msg_rsp *rsp;
-	uint32_t msg_size_dwords = (sizeof(fw_ver)) >> 2;
-	int rc = 0;
-	struct pending_msg msg;
-
-	rc = hfi_send_msg(gmu, &fw_ver.hdr, msg_size_dwords, &msg);
-	if (rc)
-		return rc;
-
-	rsp = (struct hfi_msg_rsp *) &msg.results;
-	rc = rsp->error;
-	if (!rc)
-		*ver = rsp->payload[0];
-	else
-		dev_err(&gmu->pdev->dev,
-			"gmu get fw ver failed with error=%d\n", rc);
-	return rc;
-}
-
-int hfi_send_lmconfig(struct gmu_device *gmu)
-{
-	struct hfi_lmconfig_cmd lmconfig = {
-		.hdr = {
-			.id =  H2F_MSG_LM_CFG,
-			.size = sizeof(lmconfig) >> 2,
-			.type = HFI_MSG_CMD
-		},
-		.limit_conf = gmu->lm_config,
-		.bcl_conf.bcl = gmu->bcl_config
-	};
-	struct hfi_msg_rsp *rsp;
-	uint32_t msg_size_dwords = (sizeof(lmconfig)) >> 2;
-	int rc = 0;
-	struct pending_msg msg;
-
-	if (gmu->lm_dcvs_level > MAX_GX_LEVELS)
-		lmconfig.lm_enable_bitmask = 0;
-	else
-		lmconfig.lm_enable_bitmask =
-			(1 << (gmu->lm_dcvs_level + 1)) - 1;
-
-	rc = hfi_send_msg(gmu, &lmconfig.hdr, msg_size_dwords, &msg);
-	if (rc)
-		return rc;
-
-	rsp = (struct hfi_msg_rsp *) &msg.results;
-	rc = rsp->error;
-	if (rc)
-		dev_err(&gmu->pdev->dev,
-			"gmu send lmconfig failed with error=%d\n", rc);
-	return rc;
-}
-
-static int hfi_send_perftbl(struct gmu_device *gmu)
-{
-	struct hfi_dcvstable_cmd dcvstbl = {
-		.hdr = {
-			.id = H2F_MSG_PERF_TBL,
-			.size = sizeof(dcvstbl) >> 2,
-			.type = HFI_MSG_CMD
-		},
-	};
-	struct hfi_msg_rsp *rsp;
-	struct pending_msg msg;
-	uint32_t msg_size = (sizeof(dcvstbl)) >> 2;
-	int i, rc = 0;
-
-	dcvstbl.gpu_level_num = gmu->num_gpupwrlevels;
-	dcvstbl.gmu_level_num = gmu->num_gmupwrlevels;
-
-	for (i = 0; i < gmu->num_gpupwrlevels; i++) {
-		dcvstbl.gx_votes[i].vote = gmu->rpmh_votes.gx_votes[i];
-		/* Divide by 1000 to convert to kHz */
-		dcvstbl.gx_votes[i].freq = gmu->gpu_freqs[i] / 1000;
-	}
-
-	for (i = 0; i < gmu->num_gmupwrlevels; i++) {
-		dcvstbl.cx_votes[i].vote = gmu->rpmh_votes.cx_votes[i];
-		dcvstbl.cx_votes[i].freq = gmu->gmu_freqs[i] / 1000;
-
-	}
-
-	rc = hfi_send_msg(gmu, &dcvstbl.hdr, msg_size, &msg);
-	if (rc)
-		return rc;
-
-	rsp = (struct hfi_msg_rsp *)&msg.results;
-	rc = rsp->error;
-	if (rc)
-		dev_err(&gmu->pdev->dev,
-			"gmu send perf table failed with error=%d\n", rc);
-	return rc;
-}
-
-static int hfi_send_bwtbl(struct gmu_device *gmu)
-{
-	struct hfi_bwtable_cmd bwtbl = {
-		.hdr = {
-			.id = H2F_MSG_BW_VOTE_TBL,
-			.size = sizeof(bwtbl) >> 2,
-			.type = HFI_MSG_CMD,
-		},
-		.bw_level_num = gmu->num_bwlevels,
-		.cnoc_cmds_num =
-			gmu->rpmh_votes.cnoc_votes.cmds_per_bw_vote,
-		.cnoc_wait_bitmask =
-			gmu->rpmh_votes.cnoc_votes.cmds_wait_bitmask,
-		.ddr_cmds_num = gmu->rpmh_votes.ddr_votes.cmds_per_bw_vote,
-		.ddr_wait_bitmask = gmu->rpmh_votes.ddr_votes.cmds_wait_bitmask,
-	};
-	struct hfi_msg_rsp *rsp;
-	struct pending_msg msg;
-	uint32_t msg_size_dwords = (sizeof(bwtbl)) >> 2;
-	int i, j, rc = 0;
-
-	for (i = 0; i < bwtbl.ddr_cmds_num; i++)
-		bwtbl.ddr_cmd_addrs[i] = gmu->rpmh_votes.ddr_votes.cmd_addrs[i];
-
-	for (i = 0; i < bwtbl.bw_level_num; i++)
-		for (j = 0; j < bwtbl.ddr_cmds_num; j++)
-			bwtbl.ddr_cmd_data[i][j] =
-					gmu->rpmh_votes.
-					ddr_votes.cmd_data[i][j];
-
-	for (i = 0; i < bwtbl.cnoc_cmds_num; i++)
-		bwtbl.cnoc_cmd_addrs[i] =
-				gmu->rpmh_votes.cnoc_votes.cmd_addrs[i];
-
-	for (i = 0; i < MAX_CNOC_LEVELS; i++)
-		for (j = 0; j < bwtbl.cnoc_cmds_num; j++)
-			bwtbl.cnoc_cmd_data[i][j] =
-					gmu->rpmh_votes.cnoc_votes.
-					cmd_data[i][j];
-
-	rc = hfi_send_msg(gmu, &bwtbl.hdr, msg_size_dwords, &msg);
-	if (rc)
-		return rc;
-
-	rsp = (struct hfi_msg_rsp *) &msg.results;
-	rc = rsp->error;
-	if (rc)
-		dev_err(&gmu->pdev->dev,
-			"gmu send bw table failed with error=%d\n", rc);
-	return rc;
-}
-
-static int hfi_send_test(struct gmu_device *gmu)
-{
-	struct hfi_test_cmd test_msg = {
-		.hdr = {
-			.id = H2F_MSG_TEST,
-			.size = sizeof(test_msg) >> 2,
-			.type = HFI_MSG_CMD,
-		},
-	};
-	uint32_t msg_size_dwords = (sizeof(test_msg)) >> 2;
-	struct pending_msg msg;
-
-	return hfi_send_msg(gmu, (struct hfi_msg_hdr *)&test_msg.hdr,
-			msg_size_dwords, &msg);
-}
-
-int hfi_send_dcvs_vote(struct gmu_device *gmu, uint32_t perf_idx,
-		uint32_t bw_idx, enum rpm_ack_type ack_type)
-{
-	struct hfi_dcvs_cmd dcvs_cmd = {
-		.hdr = {
-			.id = H2F_MSG_DCVS_VOTE,
-			.size = sizeof(dcvs_cmd) >> 2,
-			.type = HFI_MSG_CMD,
-		},
-		.ack_type = ack_type,
-		.freq = {
-			.perf_idx = perf_idx,
-			.clkset_opt = OPTION_AT_LEAST,
-		},
-		.bw = {
-			.bw_idx = bw_idx,
-		},
-
-	};
-	struct hfi_msg_rsp *rsp;
-	uint32_t msg_size_dwords = (sizeof(dcvs_cmd)) >> 2;
-	int rc = 0;
-	struct pending_msg msg;
-
-	rc = hfi_send_msg(gmu, &dcvs_cmd.hdr, msg_size_dwords, &msg);
-	if (rc)
-		return rc;
-
-	rsp = (struct hfi_msg_rsp *)&msg.results;
-	rc = rsp->error;
-	if (rc)
-		dev_err(&gmu->pdev->dev,
-			"gmu send dcvs cmd failed with error=%d\n", rc);
-	return rc;
-}
-
-int hfi_notify_slumber(struct gmu_device *gmu,
-		uint32_t init_perf_idx, uint32_t init_bw_idx)
-{
-	struct hfi_prep_slumber_cmd slumber_cmd = {
-		.hdr = {
-			.id = H2F_MSG_PREPARE_SLUMBER,
-			.size = sizeof(slumber_cmd) >> 2,
-			.type = HFI_MSG_CMD,
-		},
-		.init_bw_idx = init_bw_idx,
-		.init_perf_idx = init_perf_idx,
-	};
-	struct hfi_msg_rsp *rsp;
-	uint32_t msg_size_dwords = (sizeof(slumber_cmd)) >> 2;
-	int rc = 0;
-	struct pending_msg msg;
-
-	if (init_perf_idx >= MAX_GX_LEVELS || init_bw_idx >= MAX_GX_LEVELS)
-		return -EINVAL;
-
-	rc = hfi_send_msg(gmu, &slumber_cmd.hdr, msg_size_dwords, &msg);
-	if (rc)
-		return rc;
-
-	rsp = (struct hfi_msg_rsp *) &msg.results;
-	rc = rsp->error;
-	if (rc)
-		dev_err(&gmu->pdev->dev,
-			"gmu send slumber notification failed with error=%d\n",
-			rc);
-	return rc;
-}
-
-void hfi_receiver(unsigned long data)
-{
-	struct gmu_device *gmu;
-	struct hfi_msg_rsp response;
-
-	if (!data)
-		return;
-
-	gmu = (struct gmu_device *)data;
-
-	while (hfi_msgq_read(gmu, HFI_MSG_QUEUE,
-			&response, sizeof(response)) > 0) {
-		if (response.hdr.size > (sizeof(response) >> 2)) {
-			dev_err(&gmu->pdev->dev,
-					"Ack is too large, id=%d, size=%d\n",
-					response.ret_hdr.id,
-					response.hdr.size);
-			continue;
-		}
-
-		switch (response.hdr.id) {
-		case F2H_MSG_ACK:
-			receive_ack_msg(gmu, &response);
-			break;
-		case F2H_MSG_ERR:
-			receive_err_msg(gmu, &response);
-			break;
-		default:
-			dev_err(&gmu->pdev->dev,
-				"Invalid packet with id %d\n", response.hdr.id);
-			break;
-		}
-	};
-}
-
-int hfi_start(struct gmu_device *gmu, uint32_t boot_state)
-{
-	struct kgsl_device *device =
-			container_of(gmu, struct kgsl_device, gmu);
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	struct device *dev = &gmu->pdev->dev;
-	int result;
-	unsigned int ver = 0, major, minor;
-
-	if (test_bit(GMU_HFI_ON, &gmu->flags))
-		return 0;
-
-	result = hfi_send_gmu_init(gmu, boot_state);
-	if (result)
-		return result;
-
-	major = adreno_dev->gpucore->gpmu_major;
-	minor = adreno_dev->gpucore->gpmu_minor;
-	result = hfi_get_fw_version(gmu,
-			FW_VERSION(major, minor), &ver);
-	if (result)
-		dev_err(dev, "Failed to get FW version via HFI\n");
-
-	gmu->ver = ver;
-	if (major != FW_VER_MAJOR(ver))
-		WARN_ONCE(1, "FW version major %d error (expect %d)\n",
-				FW_VER_MAJOR(ver),
-				adreno_dev->gpucore->gpmu_major);
-
-	if (minor > FW_VER_MINOR(ver))
-		WARN_ONCE(1, "FW version minor %d error (expect %d)\n",
-				FW_VER_MINOR(ver),
-				adreno_dev->gpucore->gpmu_minor);
-
-	result = hfi_send_perftbl(gmu);
-	if (result)
-		return result;
-
-	result = hfi_send_bwtbl(gmu);
-	if (result)
-		return result;
-
-	/* Tell the GMU we are sending no more HFIs until the next boot */
-	if (ADRENO_QUIRK(adreno_dev, ADRENO_QUIRK_HFI_USE_REG)) {
-		result = hfi_send_test(gmu);
-		if (result)
-			return result;
-	}
-
-	set_bit(GMU_HFI_ON, &gmu->flags);
-	return 0;
-}
-
-void hfi_stop(struct gmu_device *gmu)
-{
-	struct gmu_memdesc *mem_addr = gmu->hfi_mem;
-	struct hfi_queue_table *tbl = mem_addr->hostptr;
-	struct hfi_queue_header *hdr;
-	unsigned int i;
-
-
-	if (!test_bit(GMU_HFI_ON, &gmu->flags))
-		return;
-
-	/* Flush HFI queues */
-	for (i = 0; i < HFI_QUEUE_MAX; i++) {
-		hdr = &tbl->qhdr[i];
-
-		if (hdr->read_index != hdr->write_index)
-			dev_err(&gmu->pdev->dev,
-			"HFI queue at idx %d is not empty before close: rd=%d,wt=%d",
-				i, hdr->read_index, hdr->write_index);
-
-		hdr->read_index = 0x0;
-		hdr->write_index = 0x0;
-	}
-
-	clear_bit(GMU_HFI_ON, &gmu->flags);
-}
diff --git a/drivers/gpu/msm/kgsl_hfi.h b/drivers/gpu/msm/kgsl_hfi.h
deleted file mode 100644
index b24509d7f533..000000000000
--- a/drivers/gpu/msm/kgsl_hfi.h
+++ /dev/null
@@ -1,364 +0,0 @@
-/* Copyright (c) 2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-#ifndef __KGSL_HFI_H
-#define __KGSL_HFI_H
-
-#include <linux/types.h>
-
-#define HFI_QUEUE_SIZE			SZ_4K		/* bytes */
-#define MAX_RSP_PAYLOAD_SIZE		16		/* dwords */
-#define HFI_MAX_MSG_SIZE		(SZ_1K>>2)	/* dwords */
-
-/* Below section is for all structures related to HFI queues */
-enum hfi_queue_type {
-	HFI_CMD_QUEUE = 0,
-	HFI_MSG_QUEUE,
-	HFI_DBG_QUEUE,
-	HFI_QUEUE_MAX
-};
-
-/* Add 16B guard band between HFI queues */
-#define HFI_QUEUE_OFFSET(i)		\
-		((sizeof(struct hfi_queue_table)) + \
-		((i) * (HFI_QUEUE_SIZE + 16)))
-
-#define HOST_QUEUE_START_ADDR(hfi_mem, i) \
-	((hfi_mem)->hostptr + HFI_QUEUE_OFFSET(i))
-
-#define GMU_QUEUE_START_ADDR(hfi_mem, i) \
-	((hfi_mem)->gmuaddr + HFI_QUEUE_OFFSET(i))
-
-#define HFI_IRQ_MSGQ_MASK		BIT(0)
-#define HFI_IRQ_DBGQ_MASK		BIT(1)
-#define HFI_IRQ_BLOCKED_MSG_MASK	BIT(2)
-#define HFI_IRQ_CM3_FAULT_MASK		BIT(23)
-#define HFI_IRQ_GMU_ERR_MASK		GENMASK(22, 16)
-#define HFI_IRQ_OOB_MASK		GENMASK(31, 24)
-#define HFI_IRQ_MASK			(HFI_IRQ_MSGQ_MASK |\
-					HFI_IRQ_CM3_FAULT_MASK)
-
-/**
- * struct hfi_queue_table_header - HFI queue table structure
- * @version: HFI protocol version
- * @size: queue table size in dwords
- * @qhdr0_offset: first queue header offset (dwords) in this table
- * @qhdr_size: queue header size
- * @num_q: number of queues defined in this table
- * @num_active_q: number of active queues
- */
-struct hfi_queue_table_header {
-	uint32_t version;
-	uint32_t size;
-	uint32_t qhdr0_offset;
-	uint32_t qhdr_size;
-	uint32_t num_q;
-	uint32_t num_active_q;
-};
-
-/**
- * struct hfi_queue_header - HFI queue header structure
- * @status: active: 1; inactive: 0
- * @start_addr: starting address of the queue in GMU VA space
- * @type: queue type encoded the priority, ID and send/recevie types
- * @queue_size: size of the queue
- * @msg_size: size of the message if each message has fixed size.
- *	Otherwise, 0 means variable size of message in the queue.
- * @drop_cnt: count of dropped messages
- * @rx_wm: receiver watermark
- * @tx_wm: sender watermark
- * @rx_req: receiver request
- * @tx_req: sender request
- * @read_index: read index of the queue
- * @write_index: write index of the queue
- */
-struct hfi_queue_header {
-	uint32_t status;
-	uint32_t start_addr;
-	uint32_t type;
-	uint32_t queue_size;
-	uint32_t msg_size;
-	uint32_t drop_cnt;
-	uint32_t rx_wm;
-	uint32_t tx_wm;
-	uint32_t rx_req;
-	uint32_t tx_req;
-	uint32_t read_index;
-	uint32_t write_index;
-};
-
-struct hfi_queue_table {
-	struct hfi_queue_table_header qtbl_hdr;
-	struct hfi_queue_header qhdr[HFI_QUEUE_MAX];
-};
-
-/* HTOF queue priority, 1 is highest priority */
-enum hfi_h2f_qpri {
-	HFI_H2F_QPRI_CMD = 10,
-	HFI_H2F_QPRI_DISPATCH_P0 = 20,
-	HFI_H2F_QPRI_DISPATCH_P1 = 21,
-	HFI_H2F_QPRI_DISPATCH_P2 = 22,
-};
-
-/* FTOH queue priority, 1 is highest priority */
-enum hfi_f2h_qpri {
-	HFI_F2H_QPRI_MSG = 10,
-	HFI_F2H_QPRI_DEBUG = 40,
-};
-
-#define HFI_RSP_TIMEOUT 5000 /* msec */
-#define HFI_H2F_CMD_IRQ_MASK BIT(0)
-
-enum hfi_msg_type {
-	HFI_MSG_CMD = 0,
-	HFI_MSG_POST = 1,
-	HFI_MSG_ACK = 2,
-	HFI_MSG_INVALID = 3
-};
-
-enum hfi_msg_id {
-	H2F_MSG_INIT = 0,
-	H2F_MSG_FW_VER = 1,
-	H2F_MSG_LM_CFG = 2,
-	H2F_MSG_BW_VOTE_TBL = 3,
-	H2F_MSG_PERF_TBL = 4,
-	H2F_MSG_TEST = 5,
-	H2F_MSG_DCVS_VOTE = 30,
-	H2F_MSG_FW_HALT = 31,
-	H2F_MSG_PREPARE_SLUMBER = 33,
-	F2H_MSG_ERR  = 100,
-	F2H_MSG_GMU_CNTR_REGISTER = 101,
-	F2H_MSG_ACK = 126,
-	H2F_MSG_ACK = 127,
-	H2F_MSG_REGISTER_CONTEXT = 128,
-	H2F_MSG_UNREGISTER_CONTEXT = 129,
-	H2F_MSG_ISSUE_IB = 130,
-	H2F_MSG_REGISTER_QUEUE = 131,
-	H2F_MSG_UNREGISTER_QUEUE = 132,
-	H2F_MSG_CLOSE = 133,
-	H2F_REGISTER_CONTEXT_DONE = 134,
-	H2F_UNREG_CONTEXT_DONE = 135,
-	H2F_ISSUE_IB_DONE = 136,
-	H2F_REGISTER_QUEUE_DONE = 137,
-};
-
-#define MAX_GX_LEVELS		16
-#define MAX_CX_LEVELS		4
-#define MAX_CNOC_LEVELS		2
-#define MAX_CNOC_CMDS		6
-#define MAX_BW_CMDS		8
-#define INVALID_DCVS_IDX	0xFF
-
-#if MAX_CNOC_LEVELS > MAX_GX_LEVELS
-#error "CNOC levels cannot exceed GX levels"
-#endif
-
-/**
- * For detail usage of structures defined below,
- * please look up HFI spec.
- */
-
-struct hfi_msg_hdr {
-	uint32_t id: 8;		/* 0~127 power, 128~255 ecp */
-	uint32_t size: 8;	/* unit in dword */
-	uint32_t type: 4;
-	uint32_t seqnum: 12;
-};
-
-struct hfi_msg_rsp {
-	struct hfi_msg_hdr hdr;
-	struct hfi_msg_hdr ret_hdr;
-	uint32_t error;
-	uint32_t payload[MAX_RSP_PAYLOAD_SIZE];
-};
-
-struct hfi_gmu_init_cmd {
-	struct hfi_msg_hdr  hdr;
-	uint32_t seg_id;
-	uint32_t dbg_buffer_addr;
-	uint32_t dbg_buffer_size;
-	uint32_t boot_state;
-};
-
-struct hfi_fw_version_cmd {
-	struct hfi_msg_hdr hdr;
-	uint32_t supported_ver;
-};
-
-struct limits_config {
-	uint32_t lm_type: 4;
-	uint32_t lm_sensor_type: 4;
-	uint32_t throttle_config: 4;
-	uint32_t idle_throttle_en: 4;
-	uint32_t acd_en: 4;
-	uint32_t reserved: 12;
-};
-
-struct bcl_config {
-	uint32_t bcl: 8;
-	uint32_t reserved: 24;
-};
-
-struct hfi_lmconfig_cmd {
-	struct hfi_msg_hdr hdr;
-	struct limits_config limit_conf;
-	struct bcl_config bcl_conf;
-	uint32_t lm_enable_bitmask;
-};
-
-struct hfi_bwtable_cmd {
-	struct hfi_msg_hdr hdr;
-	uint32_t bw_level_num;
-	uint32_t cnoc_cmds_num;
-	uint32_t ddr_cmds_num;
-	uint32_t cnoc_wait_bitmask;
-	uint32_t ddr_wait_bitmask;
-	uint32_t cnoc_cmd_addrs[MAX_CNOC_CMDS];
-	uint32_t cnoc_cmd_data[MAX_CNOC_LEVELS][MAX_CNOC_CMDS];
-	uint32_t ddr_cmd_addrs[MAX_BW_CMDS];
-	uint32_t ddr_cmd_data[MAX_GX_LEVELS][MAX_BW_CMDS];
-};
-
-struct hfi_test_cmd {
-	struct hfi_msg_hdr hdr;
-};
-
-struct arc_vote_desc {
-	/* In case of GPU freq vote, primary is GX, secondary is MX
-	 * in case of GMU freq vote, primary is CX, secondary is MX
-	 */
-	uint32_t pri_idx: 8;
-	uint32_t sec_idx : 8;
-	uint32_t vlvl: 16;
-};
-
-struct opp_desc {
-	struct arc_vote_desc vote;
-	uint32_t freq;
-};
-
-struct hfi_dcvstable_cmd {
-	struct hfi_msg_hdr hdr;
-	uint32_t gpu_level_num;
-	uint32_t gmu_level_num;
-	struct opp_desc gx_votes[MAX_GX_LEVELS];
-	struct opp_desc cx_votes[MAX_CX_LEVELS];
-};
-
-enum fw_clkset_options {
-	OPTION_DEFAULT = 0,
-	OPTION_CLOSEST = 1,
-	OPTION_AT_MOST = 2,
-	OPTION_AT_LEAST = 3,
-	OPTION_INVALID = 4
-};
-
-enum rpm_ack_type {
-	ACK_NONBLOCK = 0,
-	ACK_BLOCK = 1,
-	ACK_INVALID = 2,
-};
-
-struct gpu_dcvs_vote {
-	uint32_t perf_idx : 8;
-	uint32_t reserved : 20;
-	uint32_t clkset_opt : 4;
-};
-
-struct gpu_bw_vote {
-	uint32_t bw_idx : 8;
-/* to support split AB and IB vote */
-	uint32_t reserved : 24;
-};
-
-union gpu_perf_vote {
-	struct gpu_dcvs_vote fvote;
-	struct gpu_bw_vote bvote;
-	uint32_t raw;
-};
-
-struct hfi_dcvs_cmd {
-	struct hfi_msg_hdr hdr;
-	uint32_t ack_type;
-	struct gpu_dcvs_vote freq;
-	struct gpu_bw_vote bw;
-};
-
-struct hfi_prep_slumber_cmd {
-	struct hfi_msg_hdr hdr;
-	uint32_t init_bw_idx;
-	uint32_t init_perf_idx;
-};
-
-struct hfi_fw_err_msg {
-	struct hfi_msg_hdr hdr;
-	uint32_t error_code;
-	uint32_t data_1;
-	uint32_t data_2;
-};
-
-/**
- * struct pending_msg - data structure to track outstanding HFI
- *	command messages
- * @msg_complete: a blocking mechanism for sender to wait for ACK
- * @node: a node in pending message queue
- * @msg_id: the ID of the command message pending for ACK
- * @seqnum: the seqnum of the command message pending for ACK.
- *	together with msg_id, are used to correlate a receiving ACK
- *	to a pending cmd message
- * @results: the payload of received return message (ACK)
- */
-struct pending_msg {
-	struct completion msg_complete;
-	struct list_head node;
-	uint32_t msg_id;
-	uint32_t seqnum;
-	struct hfi_msg_rsp results;
-};
-
-/**
- * struct kgsl_hfi - HFI control structure
- * @hfi_interrupt_num: number of GMU asserted HFI interrupt
- * @msglock: spinlock to protect access to outstanding command message list
- * @cmdq_mutex: mutex to protect command queue access from multiple senders
- * @msglist: outstanding command message list. Each message in the list
- *	is waiting for ACK from GMU
- * @tasklet: the thread handling received messages from GMU
- * @fw_version: FW version number provided by GMU
- * @seqnum: atomic counter that is incremented for each message sent. The
- *	value of the counter is used as sequence number for HFI message
- */
-struct kgsl_hfi {
-	int hfi_interrupt_num;
-	spinlock_t msglock;
-	struct mutex cmdq_mutex;
-	struct list_head msglist;
-	struct tasklet_struct tasklet;
-	uint32_t fw_version;
-	atomic_t seqnum;
-	bool gmu_init_done;
-};
-
-struct gmu_device;
-struct gmu_memdesc;
-
-int hfi_start(struct gmu_device *gmu, uint32_t boot_state);
-void hfi_stop(struct gmu_device *gmu);
-void hfi_receiver(unsigned long data);
-void hfi_init(struct kgsl_hfi *hfi, struct gmu_memdesc *mem_addr,
-		uint32_t queue_sz_bytes);
-int hfi_send_dcvs_vote(struct gmu_device *gmu, uint32_t perf_idx,
-		uint32_t bw_idx, enum rpm_ack_type ack_type);
-int hfi_notify_slumber(struct gmu_device *gmu, uint32_t init_perf_idx,
-		uint32_t init_bw_idx);
-int hfi_send_lmconfig(struct gmu_device *gmu);
-#endif  /* __KGSL_HFI_H */
diff --git a/drivers/gpu/msm/kgsl_ioctl.c b/drivers/gpu/msm/kgsl_ioctl.c
index 9b02e1993a09..0802e94f56ad 100644
--- a/drivers/gpu/msm/kgsl_ioctl.c
+++ b/drivers/gpu/msm/kgsl_ioctl.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2008-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -48,6 +48,10 @@ static const struct kgsl_ioctl kgsl_ioctl_funcs[] = {
 			kgsl_ioctl_sharedmem_flush_cache),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUMEM_ALLOC,
 			kgsl_ioctl_gpumem_alloc),
+	KGSL_IOCTL_FUNC(IOCTL_KGSL_CFF_SYNCMEM,
+			kgsl_ioctl_cff_syncmem),
+	KGSL_IOCTL_FUNC(IOCTL_KGSL_CFF_USER_EVENT,
+			kgsl_ioctl_cff_user_event),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_TIMESTAMP_EVENT,
 			kgsl_ioctl_timestamp_event),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_SETPROPERTY,
@@ -70,6 +74,8 @@ static const struct kgsl_ioctl kgsl_ioctl_funcs[] = {
 			kgsl_ioctl_syncsource_create_fence),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_SYNCSOURCE_SIGNAL_FENCE,
 			kgsl_ioctl_syncsource_signal_fence),
+	KGSL_IOCTL_FUNC(IOCTL_KGSL_CFF_SYNC_GPUOBJ,
+			kgsl_ioctl_cff_sync_gpuobj),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUOBJ_ALLOC,
 			kgsl_ioctl_gpuobj_alloc),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUOBJ_FREE,
@@ -84,18 +90,6 @@ static const struct kgsl_ioctl kgsl_ioctl_funcs[] = {
 			kgsl_ioctl_gpu_command),
 	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPUOBJ_SET_INFO,
 			kgsl_ioctl_gpuobj_set_info),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_PHYS_ALLOC,
-			kgsl_ioctl_sparse_phys_alloc),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_PHYS_FREE,
-			kgsl_ioctl_sparse_phys_free),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_VIRT_ALLOC,
-			kgsl_ioctl_sparse_virt_alloc),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_VIRT_FREE,
-			kgsl_ioctl_sparse_virt_free),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_SPARSE_BIND,
-			kgsl_ioctl_sparse_bind),
-	KGSL_IOCTL_FUNC(IOCTL_KGSL_GPU_SPARSE_COMMAND,
-			kgsl_ioctl_gpu_sparse_command),
 };
 
 long kgsl_ioctl_copy_in(unsigned int kernel_cmd, unsigned int user_cmd,
@@ -136,19 +130,10 @@ long kgsl_ioctl_helper(struct file *filep, unsigned int cmd, unsigned long arg,
 	unsigned int nr = _IOC_NR(cmd);
 	long ret;
 
-	static DEFINE_RATELIMIT_STATE(_rs,
-			DEFAULT_RATELIMIT_INTERVAL,
-			DEFAULT_RATELIMIT_BURST);
-
 	if (nr >= len || cmds[nr].func == NULL)
 		return -ENOIOCTLCMD;
 
-	if (_IOC_SIZE(cmds[nr].cmd) > sizeof(data)) {
-		if (__ratelimit(&_rs))
-			WARN(1, "data too big for ioctl 0x%08X: %d/%zu\n",
-				cmd, _IOC_SIZE(cmds[nr].cmd), sizeof(data));
-		return -EINVAL;
-	}
+	BUG_ON(_IOC_SIZE(cmds[nr].cmd) > sizeof(data));
 
 	if (_IOC_SIZE(cmds[nr].cmd)) {
 		ret = kgsl_ioctl_copy_in(cmds[nr].cmd, cmd, arg, data);
diff --git a/drivers/gpu/msm/kgsl_iommu.c b/drivers/gpu/msm/kgsl_iommu.c
index df21bc806682..a7fab9d44f2b 100644
--- a/drivers/gpu/msm/kgsl_iommu.c
+++ b/drivers/gpu/msm/kgsl_iommu.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2011-2019, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2011-2017, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -20,9 +20,9 @@
 #include <linux/msm_kgsl.h>
 #include <linux/ratelimit.h>
 #include <linux/of_platform.h>
-#include <linux/random.h>
 #include <soc/qcom/scm.h>
 #include <soc/qcom/secure_buffer.h>
+#include <stddef.h>
 #include <linux/compat.h>
 
 #include "kgsl.h"
@@ -33,26 +33,11 @@
 #include "adreno_pm4types.h"
 #include "adreno.h"
 #include "kgsl_trace.h"
+#include "kgsl_cffdump.h"
 #include "kgsl_pwrctrl.h"
 
-#define CP_APERTURE_REG	0
-#define CP_SMMU_APERTURE_ID 0x1B
-
 #define _IOMMU_PRIV(_mmu) (&((_mmu)->priv.iommu))
 
-#define ADDR_IN_GLOBAL(_mmu, _a) \
-	(((_a) >= KGSL_IOMMU_GLOBAL_MEM_BASE(_mmu)) && \
-	 ((_a) < (KGSL_IOMMU_GLOBAL_MEM_BASE(_mmu) + \
-	 KGSL_IOMMU_GLOBAL_MEM_SIZE)))
-
-/*
- * Flag to set SMMU memory attributes required to
- * enable system cache for GPU transactions.
- */
-#ifndef IOMMU_USE_UPSTREAM_HINT
-#define IOMMU_USE_UPSTREAM_HINT 0
-#endif
-
 static struct kgsl_mmu_pt_ops iommu_pt_ops;
 static bool need_iommu_sync;
 
@@ -94,108 +79,67 @@ static struct kmem_cache *addr_entry_cache;
  *
  * Here we define an array and a simple allocator to keep track of the currently
  * active global entries. Each entry is assigned a unique address inside of a
- * MMU implementation specific "global" region. We use a simple bitmap based
- * allocator for the region to allow for both fixed and dynamic addressing.
+ * MMU implementation specific "global" region. The addresses are assigned
+ * sequentially and never re-used to avoid having to go back and reprogram
+ * existing pagetables. The entire list of active entries are mapped and
+ * unmapped into every new pagetable as it is created and destroyed.
+ *
+ * Because there are relatively few entries and they are defined at boot time we
+ * don't need to go over the top to define a dynamic allocation scheme. It will
+ * be less wasteful to pick a static number with a little bit of growth
+ * potential.
  */
 
 #define GLOBAL_PT_ENTRIES 32
 
-struct global_pt_entry {
-	struct kgsl_memdesc *memdesc;
-	char name[32];
-};
-
-#define GLOBAL_MAP_PAGES (KGSL_IOMMU_GLOBAL_MEM_SIZE >> PAGE_SHIFT)
-
-static struct global_pt_entry global_pt_entries[GLOBAL_PT_ENTRIES];
-static DECLARE_BITMAP(global_map, GLOBAL_MAP_PAGES);
-
-static int secure_global_size;
+static struct kgsl_memdesc *global_pt_entries[GLOBAL_PT_ENTRIES];
+static struct kgsl_memdesc *kgsl_global_secure_pt_entry;
 static int global_pt_count;
-static struct kgsl_memdesc gpu_qdss_desc;
-static struct kgsl_memdesc gpu_qtimer_desc;
-
-void kgsl_print_global_pt_entries(struct seq_file *s)
-{
-	int i;
-
-	for (i = 0; i < global_pt_count; i++) {
-		struct kgsl_memdesc *memdesc = global_pt_entries[i].memdesc;
-
-		if (memdesc == NULL)
-			continue;
-
-		seq_printf(s, "0x%pK-0x%pK %16llu %s\n",
-			(uint64_t *)(uintptr_t) memdesc->gpuaddr,
-			(uint64_t *)(uintptr_t) (memdesc->gpuaddr +
-			memdesc->size - 1), memdesc->size,
-			global_pt_entries[i].name);
-	}
-}
+uint64_t global_pt_alloc;
 
 static void kgsl_iommu_unmap_globals(struct kgsl_pagetable *pagetable)
 {
 	unsigned int i;
 
 	for (i = 0; i < global_pt_count; i++) {
-		if (global_pt_entries[i].memdesc != NULL)
-			kgsl_mmu_unmap(pagetable,
-					global_pt_entries[i].memdesc);
+		if (global_pt_entries[i] != NULL)
+			kgsl_mmu_unmap(pagetable, global_pt_entries[i]);
 	}
 }
 
-static int kgsl_iommu_map_globals(struct kgsl_pagetable *pagetable)
+static void kgsl_iommu_map_globals(struct kgsl_pagetable *pagetable)
 {
 	unsigned int i;
 
 	for (i = 0; i < global_pt_count; i++) {
-		if (global_pt_entries[i].memdesc != NULL) {
-			int ret = kgsl_mmu_map(pagetable,
-					global_pt_entries[i].memdesc);
+		if (global_pt_entries[i] != NULL) {
+			int ret = kgsl_mmu_map(pagetable, global_pt_entries[i]);
 
-			if (ret)
-				return ret;
+			BUG_ON(ret);
 		}
 	}
-
-	return 0;
 }
 
-void kgsl_iommu_unmap_global_secure_pt_entry(struct kgsl_device *device,
-				struct kgsl_memdesc *memdesc)
+static void kgsl_iommu_unmap_global_secure_pt_entry(struct kgsl_pagetable
+								*pagetable)
 {
-	if (!kgsl_mmu_is_secured(&device->mmu) || memdesc == NULL)
-		return;
+	struct kgsl_memdesc *entry = kgsl_global_secure_pt_entry;
 
-	/* Check if an empty memdesc got passed in */
-	if ((memdesc->gpuaddr == 0) || (memdesc->size == 0))
-		return;
+	if (entry != NULL)
+		kgsl_mmu_unmap(pagetable, entry);
 
-	if (memdesc->pagetable) {
-		if (memdesc->pagetable->name == KGSL_MMU_SECURE_PT)
-			kgsl_mmu_unmap(memdesc->pagetable, memdesc);
-	}
 }
 
-int kgsl_iommu_map_global_secure_pt_entry(struct kgsl_device *device,
-				struct kgsl_memdesc *entry)
+static void kgsl_map_global_secure_pt_entry(struct kgsl_pagetable *pagetable)
 {
-	int ret = 0;
-
-	if (!kgsl_mmu_is_secured(&device->mmu))
-		return -ENOTSUPP;
+	int ret;
+	struct kgsl_memdesc *entry = kgsl_global_secure_pt_entry;
 
 	if (entry != NULL) {
-		struct kgsl_pagetable *pagetable = device->mmu.securepagetable;
 		entry->pagetable = pagetable;
-		entry->gpuaddr = KGSL_IOMMU_SECURE_BASE(&device->mmu) +
-			secure_global_size;
-
 		ret = kgsl_mmu_map(pagetable, entry);
-		if (ret == 0)
-			secure_global_size += entry->size;
+		BUG_ON(ret);
 	}
-	return ret;
 }
 
 static void kgsl_iommu_remove_global(struct kgsl_mmu *mmu,
@@ -207,155 +151,38 @@ static void kgsl_iommu_remove_global(struct kgsl_mmu *mmu,
 		return;
 
 	for (i = 0; i < global_pt_count; i++) {
-		if (global_pt_entries[i].memdesc == memdesc) {
-			u64 offset = memdesc->gpuaddr -
-				KGSL_IOMMU_GLOBAL_MEM_BASE(mmu);
-
-			bitmap_clear(global_map, offset >> PAGE_SHIFT,
-				kgsl_memdesc_footprint(memdesc) >> PAGE_SHIFT);
-
+		if (global_pt_entries[i] == memdesc) {
 			memdesc->gpuaddr = 0;
 			memdesc->priv &= ~KGSL_MEMDESC_GLOBAL;
-			global_pt_entries[i].memdesc = NULL;
+			global_pt_entries[i] = NULL;
 			return;
 		}
 	}
 }
 
 static void kgsl_iommu_add_global(struct kgsl_mmu *mmu,
-		struct kgsl_memdesc *memdesc, const char *name)
+		struct kgsl_memdesc *memdesc)
 {
-	u32 bit, start = 0;
-	u64 size = kgsl_memdesc_footprint(memdesc);
-
 	if (memdesc->gpuaddr != 0)
 		return;
 
-	if (WARN_ON(global_pt_count >= GLOBAL_PT_ENTRIES))
-		return;
-
-	if (WARN_ON(size > KGSL_IOMMU_GLOBAL_MEM_SIZE))
-		return;
-
-	if (memdesc->priv & KGSL_MEMDESC_RANDOM) {
-		u32 range = GLOBAL_MAP_PAGES - (size >> PAGE_SHIFT);
-
-		start = get_random_int() % range;
-	}
-
-	while (start >= 0) {
-		bit = bitmap_find_next_zero_area(global_map, GLOBAL_MAP_PAGES,
-			start, size >> PAGE_SHIFT, 0);
-
-		if (bit < GLOBAL_MAP_PAGES)
-			break;
-
-		start--;
-	}
-
-	if (WARN_ON(start < 0))
-		return;
-
-	memdesc->gpuaddr =
-		KGSL_IOMMU_GLOBAL_MEM_BASE(mmu) + (bit << PAGE_SHIFT);
-
-	bitmap_set(global_map, bit, size >> PAGE_SHIFT);
+	BUG_ON(global_pt_count >= GLOBAL_PT_ENTRIES);
+	BUG_ON((global_pt_alloc + memdesc->size) >= KGSL_IOMMU_GLOBAL_MEM_SIZE);
 
+	memdesc->gpuaddr = KGSL_IOMMU_GLOBAL_MEM_BASE + global_pt_alloc;
 	memdesc->priv |= KGSL_MEMDESC_GLOBAL;
+	global_pt_alloc += memdesc->size;
 
-	global_pt_entries[global_pt_count].memdesc = memdesc;
-	strlcpy(global_pt_entries[global_pt_count].name, name,
-			sizeof(global_pt_entries[global_pt_count].name));
-	global_pt_count++;
-}
-
-struct kgsl_memdesc *kgsl_iommu_get_qdss_global_entry(void)
-{
-	return &gpu_qdss_desc;
-}
-
-static void kgsl_setup_qdss_desc(struct kgsl_device *device)
-{
-	int result = 0;
-	uint32_t gpu_qdss_entry[2];
-
-	if (!of_find_property(device->pdev->dev.of_node,
-		"qcom,gpu-qdss-stm", NULL))
-		return;
-
-	if (of_property_read_u32_array(device->pdev->dev.of_node,
-				"qcom,gpu-qdss-stm", gpu_qdss_entry, 2)) {
-		KGSL_CORE_ERR("Failed to read gpu qdss dts entry\n");
-		return;
-	}
-
-	kgsl_memdesc_init(device, &gpu_qdss_desc, 0);
-	gpu_qdss_desc.priv = 0;
-	gpu_qdss_desc.physaddr = gpu_qdss_entry[0];
-	gpu_qdss_desc.size = gpu_qdss_entry[1];
-	gpu_qdss_desc.pagetable = NULL;
-	gpu_qdss_desc.ops = NULL;
-	gpu_qdss_desc.hostptr = NULL;
-
-	result = memdesc_sg_dma(&gpu_qdss_desc, gpu_qdss_desc.physaddr,
-			gpu_qdss_desc.size);
-	if (result) {
-		KGSL_CORE_ERR("memdesc_sg_dma failed: %d\n", result);
-		return;
-	}
-
-	kgsl_mmu_add_global(device, &gpu_qdss_desc, "gpu-qdss");
+	global_pt_entries[global_pt_count++] = memdesc;
 }
 
-static inline void kgsl_cleanup_qdss_desc(struct kgsl_mmu *mmu)
+void kgsl_add_global_secure_entry(struct kgsl_device *device,
+					struct kgsl_memdesc *memdesc)
 {
-	kgsl_iommu_remove_global(mmu, &gpu_qdss_desc);
-	kgsl_sharedmem_free(&gpu_qdss_desc);
-}
-
-struct kgsl_memdesc *kgsl_iommu_get_qtimer_global_entry(void)
-{
-	return &gpu_qtimer_desc;
-}
-
-static void kgsl_setup_qtimer_desc(struct kgsl_device *device)
-{
-	int result = 0;
-	uint32_t gpu_qtimer_entry[2];
-
-	if (!of_find_property(device->pdev->dev.of_node,
-		"qcom,gpu-qtimer", NULL))
-		return;
-
-	if (of_property_read_u32_array(device->pdev->dev.of_node,
-				"qcom,gpu-qtimer", gpu_qtimer_entry, 2)) {
-		KGSL_CORE_ERR("Failed to read gpu qtimer dts entry\n");
-		return;
-	}
-
-	kgsl_memdesc_init(device, &gpu_qtimer_desc, 0);
-	gpu_qtimer_desc.priv = 0;
-	gpu_qtimer_desc.physaddr = gpu_qtimer_entry[0];
-	gpu_qtimer_desc.size = gpu_qtimer_entry[1];
-	gpu_qtimer_desc.pagetable = NULL;
-	gpu_qtimer_desc.ops = NULL;
-	gpu_qtimer_desc.hostptr = NULL;
-
-	result = memdesc_sg_dma(&gpu_qtimer_desc, gpu_qtimer_desc.physaddr,
-			gpu_qtimer_desc.size);
-	if (result) {
-		KGSL_CORE_ERR("memdesc_sg_dma failed: %d\n", result);
-		return;
-	}
-
-	kgsl_mmu_add_global(device, &gpu_qtimer_desc, "gpu-qtimer");
+	memdesc->gpuaddr = KGSL_IOMMU_SECURE_BASE;
+	kgsl_global_secure_pt_entry = memdesc;
 }
 
-static inline void kgsl_cleanup_qtimer_desc(struct kgsl_mmu *mmu)
-{
-	kgsl_iommu_remove_global(mmu, &gpu_qtimer_desc);
-	kgsl_sharedmem_free(&gpu_qtimer_desc);
-}
 
 static inline void _iommu_sync_mmu_pc(bool lock)
 {
@@ -393,23 +220,62 @@ static int _attach_pt(struct kgsl_iommu_pt *iommu_pt,
 
 	if (ret == 0)
 		iommu_pt->attached = true;
+	else
+		KGSL_CORE_ERR("iommu_attach_device(%s) failed: %d\n",
+				ctx->name, ret);
 
 	return ret;
 }
 
+static int _lock_if_secure_mmu(struct kgsl_device *device,
+		struct kgsl_memdesc *memdesc, struct kgsl_mmu *mmu)
+{
+	if (!kgsl_memdesc_is_secured(memdesc))
+		return 0;
+
+	if (!kgsl_mmu_is_secured(mmu))
+		return -EINVAL;
+
+	mutex_lock(&device->mutex);
+	if (kgsl_active_count_get(device)) {
+		mutex_unlock(&device->mutex);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void _unlock_if_secure_mmu(struct kgsl_device *device,
+		struct kgsl_memdesc *memdesc, struct kgsl_mmu *mmu)
+{
+	if (!kgsl_memdesc_is_secured(memdesc) || !kgsl_mmu_is_secured(mmu))
+		return;
+
+	kgsl_active_count_put(device);
+	mutex_unlock(&device->mutex);
+}
+
 static int _iommu_map_sync_pc(struct kgsl_pagetable *pt,
+		struct kgsl_memdesc *memdesc,
 		uint64_t gpuaddr, phys_addr_t physaddr,
 		uint64_t size, unsigned int flags)
 {
+	struct kgsl_device *device = KGSL_MMU_DEVICE(pt->mmu);
 	struct kgsl_iommu_pt *iommu_pt = pt->priv;
 	int ret;
 
+	ret = _lock_if_secure_mmu(device, memdesc, pt->mmu);
+	if (ret)
+		return ret;
+
 	_iommu_sync_mmu_pc(true);
 
 	ret = iommu_map(iommu_pt->domain, gpuaddr, physaddr, size, flags);
 
 	_iommu_sync_mmu_pc(false);
 
+	_unlock_if_secure_mmu(device, memdesc, pt->mmu);
+
 	if (ret) {
 		KGSL_CORE_ERR("map err: 0x%016llX, 0x%llx, 0x%x, %d\n",
 			gpuaddr, size, flags, ret);
@@ -420,10 +286,16 @@ static int _iommu_map_sync_pc(struct kgsl_pagetable *pt,
 }
 
 static int _iommu_unmap_sync_pc(struct kgsl_pagetable *pt,
-		uint64_t addr, uint64_t size)
+		struct kgsl_memdesc *memdesc, uint64_t addr, uint64_t size)
 {
+	struct kgsl_device *device = KGSL_MMU_DEVICE(pt->mmu);
 	struct kgsl_iommu_pt *iommu_pt = pt->priv;
-	size_t unmapped = 0;
+	size_t unmapped;
+	int ret;
+
+	ret = _lock_if_secure_mmu(device, memdesc, pt->mmu);
+	if (ret)
+		return ret;
 
 	_iommu_sync_mmu_pc(true);
 
@@ -431,6 +303,8 @@ static int _iommu_unmap_sync_pc(struct kgsl_pagetable *pt,
 
 	_iommu_sync_mmu_pc(false);
 
+	_unlock_if_secure_mmu(device, memdesc, pt->mmu);
+
 	if (unmapped != size) {
 		KGSL_CORE_ERR("unmap err: 0x%016llx, 0x%llx, %zd\n",
 			addr, size, unmapped);
@@ -440,86 +314,32 @@ static int _iommu_unmap_sync_pc(struct kgsl_pagetable *pt,
 	return 0;
 }
 
-static int _iommu_map_sg_offset_sync_pc(struct kgsl_pagetable *pt,
-		uint64_t addr, struct scatterlist *sg, int nents,
-		uint64_t offset, uint64_t size, unsigned int flags)
-{
-	struct kgsl_iommu_pt *iommu_pt = pt->priv;
-	uint64_t offset_tmp = offset;
-	uint64_t size_tmp = size;
-	size_t mapped = 0;
-	unsigned int i;
-	struct scatterlist *s;
-	phys_addr_t physaddr;
-	int ret;
-
-	_iommu_sync_mmu_pc(true);
-
-	for_each_sg(sg, s, nents, i) {
-		/* Iterate until we find the offset */
-		if (offset_tmp >= s->length) {
-			offset_tmp -= s->length;
-			continue;
-		}
-
-		/* How much mapping is needed in this sg? */
-		if (size < s->length - offset_tmp)
-			size_tmp = size;
-		else
-			size_tmp = s->length - offset_tmp;
-
-		/* Get the phys addr for the offset page */
-		if (offset_tmp != 0) {
-			physaddr = page_to_phys(nth_page(sg_page(s),
-					offset_tmp >> PAGE_SHIFT));
-			/* Reset offset_tmp */
-			offset_tmp = 0;
-		} else
-			physaddr = page_to_phys(sg_page(s));
-
-		/* Do the map for this sg */
-		ret = iommu_map(iommu_pt->domain, addr + mapped,
-				physaddr, size_tmp, flags);
-		if (ret)
-			break;
-
-		mapped += size_tmp;
-		size -= size_tmp;
-
-		if (size == 0)
-			break;
-	}
-
-	_iommu_sync_mmu_pc(false);
-
-	if (size != 0) {
-		/* Cleanup on error */
-		_iommu_unmap_sync_pc(pt, addr, mapped);
-		KGSL_CORE_ERR(
-			"map sg offset err: 0x%016llX, %d, %x, %zd\n",
-			addr, nents, flags, mapped);
-		return  -ENODEV;
-	}
-
-	return 0;
-}
-
 static int _iommu_map_sg_sync_pc(struct kgsl_pagetable *pt,
-		uint64_t addr, struct scatterlist *sg, int nents,
+		uint64_t addr, struct kgsl_memdesc *memdesc,
 		unsigned int flags)
 {
+	struct kgsl_device *device = KGSL_MMU_DEVICE(pt->mmu);
 	struct kgsl_iommu_pt *iommu_pt = pt->priv;
 	size_t mapped;
+	int ret;
+
+	ret = _lock_if_secure_mmu(device, memdesc, pt->mmu);
+	if (ret)
+		return ret;
 
 	_iommu_sync_mmu_pc(true);
 
-	mapped = iommu_map_sg(iommu_pt->domain, addr, sg, nents, flags);
+	mapped = iommu_map_sg(iommu_pt->domain, addr, memdesc->sgt->sgl,
+			memdesc->sgt->nents, flags);
 
 	_iommu_sync_mmu_pc(false);
 
+	_unlock_if_secure_mmu(device, memdesc, pt->mmu);
+
 	if (mapped == 0) {
-		KGSL_CORE_ERR("map sg err: 0x%016llX, %d, %x, %zd\n",
-			addr, nents, flags, mapped);
+		KGSL_CORE_ERR("map err: 0x%016llX, %d, %x, %zd\n",
+			addr, memdesc->sgt->nents,
+			flags, mapped);
 		return  -ENODEV;
 	}
 
@@ -534,13 +354,6 @@ static int _iommu_map_sg_sync_pc(struct kgsl_pagetable *pt,
 static struct page *kgsl_guard_page;
 static struct kgsl_memdesc kgsl_secure_guard_page_memdesc;
 
-/*
- * The dummy page is a placeholder/extra page to be used for sparse mappings.
- * This page will be mapped to all virtual sparse bindings that are not
- * physically backed.
- */
-static struct page *kgsl_dummy_page;
-
 /* These functions help find the nearest allocated memory entries on either side
  * of a faulting address. If we know the nearby allocations memory we can
  * get a better determination of what we think should have been located in the
@@ -559,62 +372,8 @@ struct _mem_entry {
 	unsigned int priv;
 	int pending_free;
 	pid_t pid;
-	char name[32];
 };
 
-static void _get_global_entries(uint64_t faultaddr,
-		struct _mem_entry *prev,
-		struct _mem_entry *next)
-{
-	int i;
-	uint64_t prevaddr = 0;
-	struct global_pt_entry *p = NULL;
-
-	uint64_t nextaddr = (uint64_t) -1;
-	struct global_pt_entry *n = NULL;
-
-	for (i = 0; i < global_pt_count; i++) {
-		uint64_t addr;
-
-		if (global_pt_entries[i].memdesc == NULL)
-			continue;
-
-		addr = global_pt_entries[i].memdesc->gpuaddr;
-		if ((addr < faultaddr) && (addr > prevaddr)) {
-			prevaddr = addr;
-			p = &global_pt_entries[i];
-		}
-
-		if ((addr > faultaddr) && (addr < nextaddr)) {
-			nextaddr = addr;
-			n = &global_pt_entries[i];
-		}
-	}
-
-	if (p != NULL) {
-		prev->gpuaddr = p->memdesc->gpuaddr;
-		prev->size = p->memdesc->size;
-		prev->flags = p->memdesc->flags;
-		prev->priv = p->memdesc->priv;
-		prev->pid = 0;
-		strlcpy(prev->name, p->name, sizeof(prev->name));
-	}
-
-	if (n != NULL) {
-		next->gpuaddr = n->memdesc->gpuaddr;
-		next->size = n->memdesc->size;
-		next->flags = n->memdesc->flags;
-		next->priv = n->memdesc->priv;
-		next->pid = 0;
-		strlcpy(next->name, n->name, sizeof(next->name));
-	}
-}
-
-void __kgsl_get_memory_usage(struct _mem_entry *entry)
-{
-	kgsl_get_memory_usage(entry->name, sizeof(entry->name), entry->flags);
-}
-
 static void _get_entries(struct kgsl_process_private *private,
 		uint64_t faultaddr, struct _mem_entry *prev,
 		struct _mem_entry *next)
@@ -649,7 +408,6 @@ static void _get_entries(struct kgsl_process_private *private,
 		prev->priv = p->memdesc.priv;
 		prev->pending_free = p->pending_free;
 		prev->pid = private->pid;
-		__kgsl_get_memory_usage(prev);
 	}
 
 	if (n != NULL) {
@@ -659,7 +417,6 @@ static void _get_entries(struct kgsl_process_private *private,
 		next->priv = n->memdesc.priv;
 		next->pending_free = n->pending_free;
 		next->pid = private->pid;
-		__kgsl_get_memory_usage(next);
 	}
 }
 
@@ -675,9 +432,7 @@ static void _find_mem_entries(struct kgsl_mmu *mmu, uint64_t faultaddr,
 	/* Set the maximum possible size as an initial value */
 	nextentry->gpuaddr = (uint64_t) -1;
 
-	if (ADDR_IN_GLOBAL(mmu, faultaddr)) {
-		_get_global_entries(faultaddr, preventry, nextentry);
-	} else if (context) {
+	if (context) {
 		private = context->proc_priv;
 		spin_lock(&private->mem_lock);
 		_get_entries(private, faultaddr, preventry, nextentry);
@@ -687,13 +442,18 @@ static void _find_mem_entries(struct kgsl_mmu *mmu, uint64_t faultaddr,
 
 static void _print_entry(struct kgsl_device *device, struct _mem_entry *entry)
 {
+	char name[32];
+	memset(name, 0, sizeof(name));
+
+	kgsl_get_memory_usage(name, sizeof(name) - 1, entry->flags);
+
 	KGSL_LOG_DUMP(device,
 		"[%016llX - %016llX] %s %s (pid = %d) (%s)\n",
 		entry->gpuaddr,
 		entry->gpuaddr + entry->size,
 		entry->priv & KGSL_MEMDESC_GUARD_PAGE ? "(+guard)" : "",
 		entry->pending_free ? "(pending free)" : "",
-		entry->pid, entry->name);
+		entry->pid, name);
 }
 
 static void _check_if_freed(struct kgsl_iommu_context *ctx,
@@ -705,7 +465,6 @@ static void _check_if_freed(struct kgsl_iommu_context *ctx,
 	pid_t pid;
 
 	char name[32];
-
 	memset(name, 0, sizeof(name));
 
 	if (kgsl_memfree_find_entry(ptname, &gpuaddr, &size, &flags, &pid)) {
@@ -717,48 +476,6 @@ static void _check_if_freed(struct kgsl_iommu_context *ctx,
 	}
 }
 
-static bool
-kgsl_iommu_uche_overfetch(struct kgsl_process_private *private,
-		uint64_t faultaddr)
-{
-	int id;
-	struct kgsl_mem_entry *entry = NULL;
-
-	spin_lock(&private->mem_lock);
-	idr_for_each_entry(&private->mem_idr, entry, id) {
-		struct kgsl_memdesc *m = &entry->memdesc;
-
-		if ((faultaddr >= (m->gpuaddr + m->size))
-				&& (faultaddr < (m->gpuaddr + m->size + 64))) {
-			spin_unlock(&private->mem_lock);
-			return true;
-		}
-	}
-	spin_unlock(&private->mem_lock);
-	return false;
-}
-
-/*
- * Read pagefaults where the faulting address lies within the first 64 bytes
- * of a page (UCHE line size is 64 bytes) and the fault page is preceded by a
- * valid allocation are considered likely due to UCHE overfetch and suppressed.
- */
-
-static bool kgsl_iommu_suppress_pagefault(uint64_t faultaddr, int write,
-					struct kgsl_context *context)
-{
-	/*
-	 * If there is no context associated with the pagefault then this
-	 * could be a fault on a global buffer. We do not suppress faults
-	 * on global buffers as they are mainly accessed by the CP bypassing
-	 * the UCHE. Also, write pagefaults are never suppressed.
-	 */
-	if (!context || write)
-		return false;
-
-	return kgsl_iommu_uche_overfetch(context->proc_priv, faultaddr);
-}
-
 static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 	struct device *dev, unsigned long addr, int flags, void *token)
 {
@@ -769,13 +486,12 @@ static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 	struct kgsl_iommu_context *ctx;
 	u64 ptbase;
 	u32 contextidr;
-	pid_t pid = 0;
+	pid_t tid = 0;
 	pid_t ptname;
 	struct _mem_entry prev, next;
 	int write;
 	struct kgsl_device *device;
 	struct adreno_device *adreno_dev;
-	struct adreno_gpudev *gpudev;
 	unsigned int no_page_fault_log = 0;
 	unsigned int curr_context_id = 0;
 	struct kgsl_context *context;
@@ -792,7 +508,6 @@ static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 	ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
 	device = KGSL_MMU_DEVICE(mmu);
 	adreno_dev = ADRENO_DEVICE(device);
-	gpudev = ADRENO_GPU_DEVICE(adreno_dev);
 
 	if (pt->name == KGSL_MMU_SECURE_PT)
 		ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_SECURE];
@@ -809,26 +524,10 @@ static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 
 	context = kgsl_context_get(device, curr_context_id);
 
-	write = (flags & IOMMU_FAULT_WRITE) ? 1 : 0;
-	if (flags & IOMMU_FAULT_TRANSLATION)
-		fault_type = "translation";
-	else if (flags & IOMMU_FAULT_PERMISSION)
-		fault_type = "permission";
-	else if (flags & IOMMU_FAULT_EXTERNAL)
-		fault_type = "external";
-	else if (flags & IOMMU_FAULT_TRANSACTION_STALLED)
-		fault_type = "transaction stalled";
-
-	if (kgsl_iommu_suppress_pagefault(addr, write, context)) {
-		iommu->pagefault_suppression_count++;
-		kgsl_context_put(context);
-		return ret;
-	}
-
 	if (context != NULL) {
 		/* save pagefault timestamp for GFT */
 		set_bit(KGSL_CONTEXT_PRIV_PAGEFAULT, &context->priv);
-		pid = context->proc_priv->pid;
+		tid = context->tid;
 	}
 
 	ctx->fault = 1;
@@ -845,18 +544,17 @@ static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 		mutex_unlock(&device->mutex);
 	}
 
+	write = (flags & IOMMU_FAULT_WRITE) ? 1 : 0;
+	if (flags & IOMMU_FAULT_TRANSLATION)
+		fault_type = "translation";
+	else if (flags & IOMMU_FAULT_PERMISSION)
+		fault_type = "permission";
+
 	ptbase = KGSL_IOMMU_GET_CTX_REG_Q(ctx, TTBR0);
 	contextidr = KGSL_IOMMU_GET_CTX_REG(ctx, CONTEXTIDR);
 
 	ptname = MMU_FEATURE(mmu, KGSL_MMU_GLOBAL_PAGETABLE) ?
-		KGSL_MMU_GLOBAL_PT : pid;
-	/*
-	 * Trace needs to be logged before searching the faulting
-	 * address in free list as it takes quite long time in
-	 * search and delays the trace unnecessarily.
-	 */
-	trace_kgsl_mmu_pagefault(ctx->kgsldev, addr,
-			ptname, write ? "write" : "read");
+		KGSL_MMU_GLOBAL_PT : tid;
 
 	if (test_bit(KGSL_FT_PAGEFAULT_LOG_ONE_PER_PAGE,
 		&adreno_dev->ft_pf_policy))
@@ -870,16 +568,6 @@ static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 			ctx->name, ptbase, contextidr,
 			write ? "write" : "read", fault_type);
 
-		if (gpudev->iommu_fault_block) {
-			unsigned int fsynr1;
-
-			fsynr1 = KGSL_IOMMU_GET_CTX_REG(ctx, FSYNR1);
-			KGSL_MEM_CRIT(ctx->kgsldev,
-				"FAULTING BLOCK: %s\n",
-				gpudev->iommu_fault_block(adreno_dev,
-								fsynr1));
-		}
-
 		/* Don't print the debug if this is a permissions fault */
 		if (!(flags & IOMMU_FAULT_PERMISSION)) {
 			_check_if_freed(ctx, addr, ptname);
@@ -903,6 +591,8 @@ static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 		}
 	}
 
+	trace_kgsl_mmu_pagefault(ctx->kgsldev, addr,
+			ptname, write ? "write" : "read");
 
 	/*
 	 * We do not want the h/w to resume fetching data from an iommu
@@ -913,7 +603,6 @@ static int kgsl_iommu_fault_handler(struct iommu_domain *domain,
 		&adreno_dev->ft_pf_policy) &&
 		(flags & IOMMU_FAULT_TRANSACTION_STALLED)) {
 		uint32_t sctlr_val;
-
 		ret = -EBUSY;
 		/*
 		 * Disable context fault interrupts
@@ -943,12 +632,7 @@ static void kgsl_iommu_disable_clk(struct kgsl_mmu *mmu)
 	int j;
 
 	atomic_dec(&iommu->clk_enable_count);
-
-	/*
-	 * Make sure the clk refcounts are good. An unbalance may
-	 * cause the clocks to be off when we need them on.
-	 */
-	WARN_ON(atomic_read(&iommu->clk_enable_count) < 0);
+	BUG_ON(atomic_read(&iommu->clk_enable_count) < 0);
 
 	for (j = (KGSL_IOMMU_MAX_CLKS - 1); j >= 0; j--)
 		if (iommu->clks[j])
@@ -1037,17 +721,15 @@ static void kgsl_iommu_destroy_pagetable(struct kgsl_pagetable *pt)
 	struct kgsl_iommu *iommu;
 	struct kgsl_iommu_context  *ctx;
 
-	/*
-	 * Make sure all allocations are unmapped before destroying
-	 * the pagetable
-	 */
-	WARN_ON(!list_empty(&pt->list));
+	BUG_ON(!list_empty(&pt->list));
 
 	iommu = _IOMMU_PRIV(mmu);
 
-	if (pt->name == KGSL_MMU_SECURE_PT) {
+	if (KGSL_MMU_SECURE_PT == pt->name) {
 		ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_SECURE];
-	} else {
+		kgsl_iommu_unmap_global_secure_pt_entry(pt);
+	}
+	else {
 		ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
 		kgsl_iommu_unmap_globals(pt);
 	}
@@ -1067,23 +749,26 @@ static void setup_64bit_pagetable(struct kgsl_mmu *mmu,
 		struct kgsl_pagetable *pagetable,
 		struct kgsl_iommu_pt *pt)
 {
+	unsigned int secure_global_size = kgsl_global_secure_pt_entry != NULL ?
+					kgsl_global_secure_pt_entry->size : 0;
 	if (mmu->secured && pagetable->name == KGSL_MMU_SECURE_PT) {
-		pt->compat_va_start = KGSL_IOMMU_SECURE_BASE(mmu);
-		pt->compat_va_end = KGSL_IOMMU_SECURE_END(mmu);
-		pt->va_start = KGSL_IOMMU_SECURE_BASE(mmu);
-		pt->va_end = KGSL_IOMMU_SECURE_END(mmu);
+		pt->compat_va_start = KGSL_IOMMU_SECURE_BASE +
+						secure_global_size;
+		pt->compat_va_end = KGSL_IOMMU_SECURE_END;
+		pt->va_start = KGSL_IOMMU_SECURE_BASE + secure_global_size;
+		pt->va_end = KGSL_IOMMU_SECURE_END;
 	} else {
 		pt->compat_va_start = KGSL_IOMMU_SVM_BASE32;
-		pt->compat_va_end = KGSL_IOMMU_SECURE_BASE(mmu);
+		pt->compat_va_end = KGSL_IOMMU_SVM_END32;
 		pt->va_start = KGSL_IOMMU_VA_BASE64;
 		pt->va_end = KGSL_IOMMU_VA_END64;
 	}
 
 	if (pagetable->name != KGSL_MMU_GLOBAL_PT &&
 		pagetable->name != KGSL_MMU_SECURE_PT) {
-		if (kgsl_is_compat_task()) {
+		if ((BITS_PER_LONG == 32) || is_compat_task()) {
 			pt->svm_start = KGSL_IOMMU_SVM_BASE32;
-			pt->svm_end = KGSL_IOMMU_SECURE_BASE(mmu);
+			pt->svm_end = KGSL_IOMMU_SVM_END32;
 		} else {
 			pt->svm_start = KGSL_IOMMU_SVM_BASE64;
 			pt->svm_end = KGSL_IOMMU_SVM_END64;
@@ -1095,21 +780,26 @@ static void setup_32bit_pagetable(struct kgsl_mmu *mmu,
 		struct kgsl_pagetable *pagetable,
 		struct kgsl_iommu_pt *pt)
 {
+	unsigned int secure_global_size = kgsl_global_secure_pt_entry != NULL ?
+					kgsl_global_secure_pt_entry->size : 0;
 	if (mmu->secured) {
 		if (pagetable->name == KGSL_MMU_SECURE_PT) {
-			pt->compat_va_start = KGSL_IOMMU_SECURE_BASE(mmu);
-			pt->compat_va_end = KGSL_IOMMU_SECURE_END(mmu);
-			pt->va_start = KGSL_IOMMU_SECURE_BASE(mmu);
-			pt->va_end = KGSL_IOMMU_SECURE_END(mmu);
+			pt->compat_va_start = KGSL_IOMMU_SECURE_BASE +
+						secure_global_size;
+			pt->compat_va_end = KGSL_IOMMU_SECURE_END;
+			pt->va_start = KGSL_IOMMU_SECURE_BASE +
+						secure_global_size;
+			pt->va_end = KGSL_IOMMU_SECURE_END;
 		} else {
 			pt->va_start = KGSL_IOMMU_SVM_BASE32;
-			pt->va_end = KGSL_IOMMU_SECURE_BASE(mmu);
+			pt->va_end = KGSL_IOMMU_SECURE_BASE +
+						secure_global_size;
 			pt->compat_va_start = pt->va_start;
 			pt->compat_va_end = pt->va_end;
 		}
 	} else {
 		pt->va_start = KGSL_IOMMU_SVM_BASE32;
-		pt->va_end = KGSL_IOMMU_GLOBAL_MEM_BASE(mmu);
+		pt->va_end = KGSL_IOMMU_GLOBAL_MEM_BASE;
 		pt->compat_va_start = pt->va_start;
 		pt->compat_va_end = pt->va_end;
 	}
@@ -1172,45 +862,11 @@ static void _free_pt(struct kgsl_iommu_context *ctx, struct kgsl_pagetable *pt)
 	kfree(iommu_pt);
 }
 
-void _enable_gpuhtw_llc(struct kgsl_mmu *mmu, struct kgsl_iommu_pt *iommu_pt)
-{
-	struct kgsl_device *device = KGSL_MMU_DEVICE(mmu);
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
-	int gpuhtw_llc_enable = 1;
-	int ret;
-
-	/* GPU pagetable walk LLC slice not enabled */
-	if (!adreno_dev->gpuhtw_llc_slice)
-		return;
-
-	/* Domain attribute to enable system cache for GPU pagetable walks */
-	ret = iommu_domain_set_attr(iommu_pt->domain,
-			DOMAIN_ATTR_USE_UPSTREAM_HINT, &gpuhtw_llc_enable);
-	/*
-	 * Warn that the system cache will not be used for GPU
-	 * pagetable walks. This is not a fatal error.
-	 */
-	WARN_ONCE(ret,
-		"System cache not enabled for GPU pagetable walks: %d\n", ret);
-}
-
-static int program_smmu_aperture(unsigned int cb, unsigned int aperture_reg)
-{
-	struct scm_desc desc = {0};
-
-	desc.args[0] = 0xFFFF0000 | ((aperture_reg & 0xff) << 8) | (cb & 0xff);
-	desc.args[1] = 0xFFFFFFFF;
-	desc.args[2] = 0xFFFFFFFF;
-	desc.args[3] = 0xFFFFFFFF;
-	desc.arginfo = SCM_ARGS(4);
-
-	return scm_call2(SCM_SIP_FNID(SCM_SVC_MP, CP_SMMU_APERTURE_ID), &desc);
-}
-
 static int _init_global_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 {
 	int ret = 0;
 	struct kgsl_iommu_pt *iommu_pt = NULL;
+	//int disable_htw = !MMU_FEATURE(mmu, KGSL_MMU_COHERENT_HTW);
 	unsigned int cb_num;
 	struct kgsl_iommu *iommu = _IOMMU_PRIV(mmu);
 	struct kgsl_iommu_context *ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
@@ -1220,6 +876,9 @@ static int _init_global_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 	if (IS_ERR(iommu_pt))
 		return PTR_ERR(iommu_pt);
 
+	//iommu_domain_set_attr(iommu_pt->domain,
+	//			DOMAIN_ATTR_COHERENT_HTW_DISABLE, &disable_htw);
+
 	if (kgsl_mmu_is_perprocess(mmu)) {
 		ret = iommu_domain_set_attr(iommu_pt->domain,
 				DOMAIN_ATTR_PROCID, &pt->name);
@@ -1230,8 +889,6 @@ static int _init_global_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 		}
 	}
 
-	_enable_gpuhtw_llc(mmu, iommu_pt);
-
 	ret = _attach_pt(iommu_pt, ctx);
 	if (ret)
 		goto done;
@@ -1242,21 +899,11 @@ static int _init_global_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 	ret = iommu_domain_get_attr(iommu_pt->domain,
 				DOMAIN_ATTR_CONTEXT_BANK, &cb_num);
 	if (ret) {
-		KGSL_CORE_ERR("get DOMAIN_ATTR_CONTEXT_BANK failed: %d\n",
+		KGSL_CORE_ERR("get DOMAIN_ATTR_PROCID failed: %d\n",
 				ret);
 		goto done;
 	}
 
-	if (!MMU_FEATURE(mmu, KGSL_MMU_GLOBAL_PAGETABLE) &&
-		scm_is_call_available(SCM_SVC_MP, CP_SMMU_APERTURE_ID)) {
-		ret = program_smmu_aperture(cb_num, CP_APERTURE_REG);
-		if (ret) {
-			pr_err("SMMU aperture programming call failed with error %d\n",
-									ret);
-			return ret;
-		}
-	}
-
 	ctx->cb_num = cb_num;
 	ctx->regbase = iommu->regbase + KGSL_IOMMU_CB0_OFFSET
 			+ (cb_num << KGSL_IOMMU_CB_SHIFT);
@@ -1276,7 +923,7 @@ static int _init_global_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 		goto done;
 	}
 
-	ret = kgsl_iommu_map_globals(pt);
+	kgsl_iommu_map_globals(pt);
 
 done:
 	if (ret)
@@ -1290,6 +937,7 @@ static int _init_secure_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 	int ret = 0;
 	struct kgsl_iommu_pt *iommu_pt = NULL;
 	struct kgsl_iommu *iommu = _IOMMU_PRIV(mmu);
+	//int disable_htw = !MMU_FEATURE(mmu, KGSL_MMU_COHERENT_HTW);
 	struct kgsl_iommu_context *ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_SECURE];
 	int secure_vmid = VMID_CP_PIXEL;
 	unsigned int cb_num;
@@ -1307,6 +955,9 @@ static int _init_secure_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 	if (IS_ERR(iommu_pt))
 		return PTR_ERR(iommu_pt);
 
+	//iommu_domain_set_attr(iommu_pt->domain,
+	//			DOMAIN_ATTR_COHERENT_HTW_DISABLE, &disable_htw);
+
 	ret = iommu_domain_set_attr(iommu_pt->domain,
 				    DOMAIN_ATTR_SECURE_VMID, &secure_vmid);
 	if (ret) {
@@ -1314,8 +965,6 @@ static int _init_secure_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 		goto done;
 	}
 
-	_enable_gpuhtw_llc(mmu, iommu_pt);
-
 	ret = _attach_pt(iommu_pt, ctx);
 
 	if (MMU_FEATURE(mmu, KGSL_MMU_HYP_SECURE_ALLOC))
@@ -1334,6 +983,8 @@ static int _init_secure_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 	ctx->regbase = iommu->regbase + KGSL_IOMMU_CB0_OFFSET
 			+ (cb_num << KGSL_IOMMU_CB_SHIFT);
 
+	kgsl_map_global_secure_pt_entry(pt);
+
 done:
 	if (ret)
 		_free_pt(ctx, pt);
@@ -1348,6 +999,7 @@ static int _init_per_process_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 	struct kgsl_iommu_context *ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
 	int dynamic = 1;
 	unsigned int cb_num = ctx->cb_num;
+	//int disable_htw = !MMU_FEATURE(mmu, KGSL_MMU_COHERENT_HTW);
 
 	iommu_pt = _alloc_pt(ctx->dev, mmu, pt);
 
@@ -1374,7 +1026,8 @@ static int _init_per_process_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 		goto done;
 	}
 
-	_enable_gpuhtw_llc(mmu, iommu_pt);
+	//iommu_domain_set_attr(iommu_pt->domain,
+	//			DOMAIN_ATTR_COHERENT_HTW_DISABLE, &disable_htw);
 
 	ret = _attach_pt(iommu_pt, ctx);
 	if (ret)
@@ -1395,7 +1048,7 @@ static int _init_per_process_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 		goto done;
 	}
 
-	ret = kgsl_iommu_map_globals(pt);
+	kgsl_iommu_map_globals(pt);
 
 done:
 	if (ret)
@@ -1495,15 +1148,8 @@ static void kgsl_iommu_close(struct kgsl_mmu *mmu)
 		kgsl_guard_page = NULL;
 	}
 
-	if (kgsl_dummy_page != NULL) {
-		__free_page(kgsl_dummy_page);
-		kgsl_dummy_page = NULL;
-	}
-
 	kgsl_iommu_remove_global(mmu, &iommu->setstate);
 	kgsl_sharedmem_free(&iommu->setstate);
-	kgsl_cleanup_qdss_desc(mmu);
-	kgsl_cleanup_qtimer_desc(mmu);
 }
 
 static int _setstate_alloc(struct kgsl_device *device,
@@ -1511,7 +1157,6 @@ static int _setstate_alloc(struct kgsl_device *device,
 {
 	int ret;
 
-	kgsl_memdesc_init(device, &iommu->setstate, 0);
 	ret = kgsl_sharedmem_alloc_contig(device, &iommu->setstate, PAGE_SIZE);
 
 	if (!ret) {
@@ -1532,6 +1177,7 @@ static int kgsl_iommu_init(struct kgsl_mmu *mmu)
 	int status;
 
 	mmu->features |= KGSL_MMU_PAGED;
+	mmu->features |= KGSL_MMU_NEED_GUARD_PAGE;
 
 	if (ctx->name == NULL) {
 		KGSL_CORE_ERR("dt: gfx3d0_user context bank not found\n");
@@ -1574,22 +1220,17 @@ static int kgsl_iommu_init(struct kgsl_mmu *mmu)
 		}
 	}
 
-	kgsl_iommu_add_global(mmu, &iommu->setstate, "setstate");
-	kgsl_setup_qdss_desc(device);
-	kgsl_setup_qtimer_desc(device);
-
-	if (!mmu->secured)
-		goto done;
-
-	mmu->securepagetable = kgsl_mmu_getpagetable(mmu,
-				KGSL_MMU_SECURE_PT);
-	if (IS_ERR(mmu->securepagetable)) {
-		status = PTR_ERR(mmu->securepagetable);
-		mmu->securepagetable = NULL;
-	} else if (mmu->securepagetable == NULL) {
-		status = -ENOMEM;
+	if (kgsl_guard_page == NULL) {
+		kgsl_guard_page = alloc_page(GFP_KERNEL | __GFP_ZERO |
+				__GFP_HIGHMEM);
+		if (kgsl_guard_page == NULL) {
+			status = -ENOMEM;
+			goto done;
+		}
 	}
 
+	kgsl_iommu_add_global(mmu, &iommu->setstate);
+
 done:
 	if (status)
 		kgsl_iommu_close(mmu);
@@ -1615,8 +1256,6 @@ static int _setup_user_context(struct kgsl_mmu *mmu)
 			ret = PTR_ERR(mmu->defaultpagetable);
 			mmu->defaultpagetable = NULL;
 			return ret;
-		} else if (mmu->defaultpagetable == NULL) {
-			return -ENOMEM;
 		}
 	}
 
@@ -1645,6 +1284,7 @@ static int _setup_user_context(struct kgsl_mmu *mmu)
 	 *    independently of any outstanding fault)
 	 */
 
+	sctlr_val = KGSL_IOMMU_GET_CTX_REG(ctx, SCTLR);
 	if (test_bit(KGSL_FT_PAGEFAULT_GPUHALT_ENABLE,
 				&adreno_dev->ft_pf_policy)) {
 		sctlr_val |= (0x1 << KGSL_IOMMU_SCTLR_CFCFG_SHIFT);
@@ -1671,9 +1311,17 @@ static int _setup_secure_context(struct kgsl_mmu *mmu)
 	if (ctx->dev == NULL || !mmu->secured)
 		return 0;
 
-	if (mmu->securepagetable == NULL)
-		return -ENOMEM;
-
+	if (mmu->securepagetable == NULL) {
+		mmu->securepagetable = kgsl_mmu_getpagetable(mmu,
+						KGSL_MMU_SECURE_PT);
+		if (IS_ERR(mmu->securepagetable)) {
+			ret = PTR_ERR(mmu->securepagetable);
+			mmu->securepagetable = NULL;
+			return ret;
+		} else if (mmu->securepagetable == NULL) {
+			return -ENOMEM;
+		}
+	}
 	iommu_pt = mmu->securepagetable->priv;
 
 	ret = _attach_pt(iommu_pt, ctx);
@@ -1695,55 +1343,37 @@ static int _setup_secure_context(struct kgsl_mmu *mmu)
 	return ret;
 }
 
-static int kgsl_iommu_set_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt);
-
 static int kgsl_iommu_start(struct kgsl_mmu *mmu)
 {
 	int status;
 	struct kgsl_iommu *iommu = _IOMMU_PRIV(mmu);
+	struct kgsl_iommu_context *ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
 
 	status = _setup_user_context(mmu);
 	if (status)
 		return status;
 
 	status = _setup_secure_context(mmu);
-	if (status) {
+	if (status)
 		_detach_context(&iommu->ctx[KGSL_IOMMU_CONTEXT_USER]);
-		return status;
+	else {
+		kgsl_iommu_enable_clk(mmu);
+		KGSL_IOMMU_SET_CTX_REG(ctx, TLBIALL, 1);
+		kgsl_iommu_disable_clk(mmu);
 	}
-
-	/* Make sure the hardware is programmed to the default pagetable */
-	return kgsl_iommu_set_pt(mmu, mmu->defaultpagetable);
+	return status;
 }
 
 static int
-kgsl_iommu_unmap_offset(struct kgsl_pagetable *pt,
-		struct kgsl_memdesc *memdesc, uint64_t addr,
-		uint64_t offset, uint64_t size)
+kgsl_iommu_unmap(struct kgsl_pagetable *pt,
+		struct kgsl_memdesc *memdesc)
 {
-	if (size == 0 || (size + offset) > kgsl_memdesc_footprint(memdesc))
-		return -EINVAL;
-	/*
-	 * All GPU addresses as assigned are page aligned, but some
-	 * functions perturb the gpuaddr with an offset, so apply the
-	 * mask here to make sure we have the right address.
-	 */
+	uint64_t size = memdesc->size;
 
-	addr = PAGE_ALIGN(addr);
-	if (addr == 0)
-		return -EINVAL;
+	if (kgsl_memdesc_has_guard_page(memdesc))
+		size += kgsl_memdesc_guard_page_size(pt->mmu, memdesc);
 
-	return _iommu_unmap_sync_pc(pt, addr + offset, size);
-}
-
-static int
-kgsl_iommu_unmap(struct kgsl_pagetable *pt, struct kgsl_memdesc *memdesc)
-{
-	if (memdesc->size == 0 || memdesc->gpuaddr == 0)
-		return -EINVAL;
-
-	return kgsl_iommu_unmap_offset(pt, memdesc, memdesc->gpuaddr, 0,
-			kgsl_memdesc_footprint(memdesc));
+	return _iommu_unmap_sync_pc(pt, memdesc, memdesc->gpuaddr, size);
 }
 
 /**
@@ -1787,39 +1417,14 @@ static int _iommu_map_guard_page(struct kgsl_pagetable *pt,
 
 		sg = kgsl_secure_guard_page_memdesc.sgt->sgl;
 		physaddr = page_to_phys(sg_page(sg));
-	} else {
-		if (kgsl_guard_page == NULL) {
-			kgsl_guard_page = alloc_page(GFP_KERNEL | __GFP_ZERO |
-					__GFP_NORETRY | __GFP_HIGHMEM);
-			if (kgsl_guard_page == NULL)
-				return -ENOMEM;
-		}
-
+	} else
 		physaddr = page_to_phys(kgsl_guard_page);
-	}
 
-	return _iommu_map_sync_pc(pt, gpuaddr, physaddr,
-			kgsl_memdesc_guard_page_size(memdesc),
+	return _iommu_map_sync_pc(pt, memdesc, gpuaddr, physaddr,
+			kgsl_memdesc_guard_page_size(pt->mmu, memdesc),
 			protflags & ~IOMMU_WRITE);
 }
 
-static unsigned int _get_protection_flags(struct kgsl_memdesc *memdesc)
-{
-	unsigned int flags = IOMMU_READ | IOMMU_WRITE |
-		IOMMU_NOEXEC | IOMMU_USE_UPSTREAM_HINT;
-
-	if (memdesc->flags & KGSL_MEMFLAGS_GPUREADONLY)
-		flags &= ~IOMMU_WRITE;
-
-	if (memdesc->priv & KGSL_MEMDESC_PRIVILEGED)
-		flags |= IOMMU_PRIV;
-
-	if (memdesc->flags & KGSL_MEMFLAGS_IOCOHERENT)
-		flags |= IOMMU_CACHE;
-
-	return flags;
-}
-
 static int
 kgsl_iommu_map(struct kgsl_pagetable *pt,
 			struct kgsl_memdesc *memdesc)
@@ -1827,175 +1432,48 @@ kgsl_iommu_map(struct kgsl_pagetable *pt,
 	int ret;
 	uint64_t addr = memdesc->gpuaddr;
 	uint64_t size = memdesc->size;
-	unsigned int flags = _get_protection_flags(memdesc);
+	unsigned int flags;
 	struct sg_table *sgt = NULL;
 
+	BUG_ON(NULL == pt->priv);
+
+	flags = IOMMU_READ | IOMMU_WRITE | IOMMU_NOEXEC;
+
+	/* Set up the protection for the page(s) */
+	if (memdesc->flags & KGSL_MEMFLAGS_GPUREADONLY)
+		flags &= ~IOMMU_WRITE;
+
+	if (memdesc->priv & KGSL_MEMDESC_PRIVILEGED)
+		flags |= IOMMU_PRIV;
+
 	/*
 	 * For paged memory allocated through kgsl, memdesc->pages is not NULL.
 	 * Allocate sgt here just for its map operation. Contiguous memory
 	 * already has its sgt, so no need to allocate it here.
 	 */
-	if (memdesc->pages != NULL)
+	if (memdesc->pages != NULL) {
 		sgt = kgsl_alloc_sgt_from_pages(memdesc);
-	else
-		sgt = memdesc->sgt;
+		memdesc->sgt = sgt;
+	}
 
 	if (IS_ERR(sgt))
 		return PTR_ERR(sgt);
 
-	ret = _iommu_map_sg_sync_pc(pt, addr, sgt->sgl, sgt->nents, flags);
+	ret = _iommu_map_sg_sync_pc(pt, addr, memdesc, flags);
+
 	if (ret)
 		goto done;
 
 	ret = _iommu_map_guard_page(pt, memdesc, addr + size, flags);
 	if (ret)
-		_iommu_unmap_sync_pc(pt, addr, size);
+		_iommu_unmap_sync_pc(pt, memdesc, addr, size);
 
 done:
-	if (memdesc->pages != NULL)
+	if (memdesc->pages != NULL) {
 		kgsl_free_sgt(sgt);
-
-	return ret;
-}
-
-static int kgsl_iommu_sparse_dummy_map(struct kgsl_pagetable *pt,
-		struct kgsl_memdesc *memdesc, uint64_t offset, uint64_t size)
-{
-	int ret = 0, i;
-	struct page **pages = NULL;
-	struct sg_table sgt;
-	int count = size >> PAGE_SHIFT;
-
-	/* verify the offset is within our range */
-	if (size + offset > memdesc->size)
-		return -EINVAL;
-
-	if (kgsl_dummy_page == NULL) {
-		kgsl_dummy_page = alloc_page(GFP_KERNEL | __GFP_ZERO |
-				__GFP_HIGHMEM);
-		if (kgsl_dummy_page == NULL)
-			return -ENOMEM;
-	}
-
-	pages = kcalloc(count, sizeof(struct page *), GFP_KERNEL);
-	if (pages == NULL)
-		return -ENOMEM;
-
-	for (i = 0; i < count; i++)
-		pages[i] = kgsl_dummy_page;
-
-	ret = sg_alloc_table_from_pages(&sgt, pages, count,
-			0, size, GFP_KERNEL);
-	if (ret == 0) {
-		ret = _iommu_map_sg_sync_pc(pt, memdesc->gpuaddr + offset,
-				sgt.sgl, sgt.nents, IOMMU_READ | IOMMU_NOEXEC);
-		sg_free_table(&sgt);
+		memdesc->sgt = NULL;
 	}
 
-	kfree(pages);
-
-	return ret;
-}
-
-static int _map_to_one_page(struct kgsl_pagetable *pt, uint64_t addr,
-		struct kgsl_memdesc *memdesc, uint64_t physoffset,
-		uint64_t size, unsigned int map_flags)
-{
-	int ret = 0, i;
-	int pg_sz = kgsl_memdesc_get_pagesize(memdesc);
-	int count = size >> PAGE_SHIFT;
-	struct page *page = NULL;
-	struct page **pages = NULL;
-	struct sg_page_iter sg_iter;
-	struct sg_table sgt;
-
-	/* Find our physaddr offset addr */
-	if (memdesc->pages != NULL)
-		page = memdesc->pages[physoffset >> PAGE_SHIFT];
-	else {
-		for_each_sg_page(memdesc->sgt->sgl, &sg_iter,
-				memdesc->sgt->nents, physoffset >> PAGE_SHIFT) {
-			page = sg_page_iter_page(&sg_iter);
-			break;
-		}
-	}
-
-	if (page == NULL)
-		return -EINVAL;
-
-	pages = kcalloc(count, sizeof(struct page *), GFP_KERNEL);
-	if (pages == NULL)
-		return -ENOMEM;
-
-	for (i = 0; i < count; i++) {
-		if (pg_sz != PAGE_SIZE) {
-			struct page *tmp_page = page;
-			int j;
-
-			for (j = 0; j < 16; j++, tmp_page += PAGE_SIZE)
-				pages[i++] = tmp_page;
-		} else
-			pages[i] = page;
-	}
-
-	ret = sg_alloc_table_from_pages(&sgt, pages, count,
-			0, size, GFP_KERNEL);
-	if (ret == 0) {
-		ret = _iommu_map_sg_sync_pc(pt, addr, sgt.sgl,
-				sgt.nents, map_flags);
-		sg_free_table(&sgt);
-	}
-
-	kfree(pages);
-
-	return ret;
-}
-
-static int kgsl_iommu_map_offset(struct kgsl_pagetable *pt,
-		uint64_t virtaddr, uint64_t virtoffset,
-		struct kgsl_memdesc *memdesc, uint64_t physoffset,
-		uint64_t size, uint64_t feature_flag)
-{
-	int pg_sz;
-	unsigned int protflags = _get_protection_flags(memdesc);
-	int ret;
-	struct sg_table *sgt = NULL;
-
-	pg_sz = kgsl_memdesc_get_pagesize(memdesc);
-	if (!IS_ALIGNED(virtaddr | virtoffset | physoffset | size, pg_sz))
-		return -EINVAL;
-
-	if (size == 0)
-		return -EINVAL;
-
-	if (!(feature_flag & KGSL_SPARSE_BIND_MULTIPLE_TO_PHYS) &&
-			size + physoffset > kgsl_memdesc_footprint(memdesc))
-		return -EINVAL;
-
-	/*
-	 * For paged memory allocated through kgsl, memdesc->pages is not NULL.
-	 * Allocate sgt here just for its map operation. Contiguous memory
-	 * already has its sgt, so no need to allocate it here.
-	 */
-	if (memdesc->pages != NULL)
-		sgt = kgsl_alloc_sgt_from_pages(memdesc);
-	else
-		sgt = memdesc->sgt;
-
-	if (IS_ERR(sgt))
-		return PTR_ERR(sgt);
-
-	if (feature_flag & KGSL_SPARSE_BIND_MULTIPLE_TO_PHYS)
-		ret = _map_to_one_page(pt, virtaddr + virtoffset,
-				memdesc, physoffset, size, protflags);
-	else
-		ret = _iommu_map_sg_offset_sync_pc(pt, virtaddr + virtoffset,
-				sgt->sgl, sgt->nents,
-				physoffset, size, protflags);
-
-	if (memdesc->pages != NULL)
-		kgsl_free_sgt(sgt);
-
 	return ret;
 }
 
@@ -2091,15 +1569,23 @@ kgsl_iommu_get_current_ttbr0(struct kgsl_mmu *mmu)
  *
  * Return - void
  */
-static int kgsl_iommu_set_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
+static int kgsl_iommu_set_pt(struct kgsl_mmu *mmu,
+				struct kgsl_pagetable *pt)
 {
+	struct kgsl_device *device = KGSL_MMU_DEVICE(mmu);
 	struct kgsl_iommu *iommu = _IOMMU_PRIV(mmu);
 	struct kgsl_iommu_context *ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
+	int ret = 0;
 	uint64_t ttbr0, temp;
 	unsigned int contextidr;
 	unsigned long wait_for_flush;
 
-	if ((pt != mmu->defaultpagetable) && !kgsl_mmu_is_perprocess(mmu))
+	/*
+	 * If using a global pagetable, we can skip all this
+	 * because the pagetable will be set up by the iommu
+	 * driver and never changed at runtime.
+	 */
+	if (!kgsl_mmu_is_perprocess(mmu))
 		return 0;
 
 	kgsl_iommu_enable_clk(mmu);
@@ -2107,10 +1593,17 @@ static int kgsl_iommu_set_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 	ttbr0 = kgsl_mmu_pagetable_get_ttbr0(pt);
 	contextidr = kgsl_mmu_pagetable_get_contextidr(pt);
 
+	/*
+	 * Taking the liberty to spin idle since this codepath
+	 * is invoked when we can spin safely for it to be idle
+	 */
+	ret = adreno_spin_idle(ADRENO_DEVICE(device), ADRENO_IDLE_TIMEOUT);
+	if (ret)
+		return ret;
+
 	KGSL_IOMMU_SET_CTX_REG_Q(ctx, TTBR0, ttbr0);
 	KGSL_IOMMU_SET_CTX_REG(ctx, CONTEXTIDR, contextidr);
 
-	/* memory barrier before reading TTBR0 register */
 	mb();
 	temp = KGSL_IOMMU_GET_CTX_REG_Q(ctx, TTBR0);
 
@@ -2136,8 +1629,10 @@ static int kgsl_iommu_set_pt(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt)
 		cpu_relax();
 	}
 
+	/* Disable smmu clock */
 	kgsl_iommu_disable_clk(mmu);
-	return 0;
+
+	return ret;
 }
 
 /*
@@ -2155,6 +1650,8 @@ static int kgsl_iommu_set_pf_policy(struct kgsl_mmu *mmu,
 	struct kgsl_iommu_context *ctx = &iommu->ctx[KGSL_IOMMU_CONTEXT_USER];
 	struct kgsl_device *device = KGSL_MMU_DEVICE(mmu);
 	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
+	int ret = 0;
+	unsigned int sctlr_val;
 
 	if ((adreno_dev->ft_pf_policy &
 		BIT(KGSL_FT_PAGEFAULT_GPUHALT_ENABLE)) ==
@@ -2163,7 +1660,10 @@ static int kgsl_iommu_set_pf_policy(struct kgsl_mmu *mmu,
 
 	/* If not attached, policy will be updated during the next attach */
 	if (ctx->default_pt != NULL) {
-		unsigned int sctlr_val;
+		/* Need to idle device before changing options */
+		ret = device->ftbl->idle(device);
+		if (ret)
+			return ret;
 
 		kgsl_iommu_enable_clk(mmu);
 
@@ -2182,7 +1682,7 @@ static int kgsl_iommu_set_pf_policy(struct kgsl_mmu *mmu,
 		kgsl_iommu_disable_clk(mmu);
 	}
 
-	return 0;
+	return ret;
 }
 
 static struct kgsl_protected_registers *
@@ -2228,7 +1728,6 @@ static int _remove_gpuaddr(struct kgsl_pagetable *pagetable,
 		return 0;
 	}
 
-	WARN(1, "Couldn't remove gpuaddr: 0x%llx\n", gpuaddr);
 	return -ENOMEM;
 }
 
@@ -2258,11 +1757,8 @@ static int _insert_gpuaddr(struct kgsl_pagetable *pagetable,
 			node = &parent->rb_left;
 		else if (new->base > this->base)
 			node = &parent->rb_right;
-		else {
-			/* Duplicate entry */
-			WARN(1, "duplicate gpuaddr: 0x%llx\n", gpuaddr);
-			return -EEXIST;
-		}
+		else
+			BUG();
 	}
 
 	rb_link_node(&new->node, parent, node);
@@ -2403,8 +1899,7 @@ static uint64_t kgsl_iommu_find_svm_region(struct kgsl_pagetable *pagetable,
 	uint64_t addr;
 
 	/* Avoid black holes */
-	if (WARN(end <= start, "Bad search range: 0x%llx-0x%llx", start, end))
-		return (uint64_t) -EINVAL;
+	BUG_ON(end <= start);
 
 	spin_lock(&pagetable->lock);
 	addr = _get_unmapped_area_topdown(pagetable,
@@ -2413,6 +1908,10 @@ static uint64_t kgsl_iommu_find_svm_region(struct kgsl_pagetable *pagetable,
 	return addr;
 }
 
+#define ADDR_IN_GLOBAL(_a) \
+	(((_a) >= KGSL_IOMMU_GLOBAL_MEM_BASE) && \
+	 ((_a) < (KGSL_IOMMU_GLOBAL_MEM_BASE + KGSL_IOMMU_GLOBAL_MEM_SIZE)))
+
 static int kgsl_iommu_set_svm_region(struct kgsl_pagetable *pagetable,
 		uint64_t gpuaddr, uint64_t size)
 {
@@ -2421,8 +1920,7 @@ static int kgsl_iommu_set_svm_region(struct kgsl_pagetable *pagetable,
 	struct rb_node *node;
 
 	/* Make sure the requested address doesn't fall in the global range */
-	if (ADDR_IN_GLOBAL(pagetable->mmu, gpuaddr) ||
-			ADDR_IN_GLOBAL(pagetable->mmu, gpuaddr + size))
+	if (ADDR_IN_GLOBAL(gpuaddr) || ADDR_IN_GLOBAL(gpuaddr + size))
 		return -ENOMEM;
 
 	spin_lock(&pagetable->lock);
@@ -2456,17 +1954,18 @@ static int kgsl_iommu_get_gpuaddr(struct kgsl_pagetable *pagetable,
 {
 	struct kgsl_iommu_pt *pt = pagetable->priv;
 	int ret = 0;
-	uint64_t addr, start, end, size;
+	uint64_t addr, start, end;
+	uint64_t size = memdesc->size;
 	unsigned int align;
 
-	if (WARN_ON(kgsl_memdesc_use_cpu_map(memdesc)))
-		return -EINVAL;
+	BUG_ON(kgsl_memdesc_use_cpu_map(memdesc));
 
 	if (memdesc->flags & KGSL_MEMFLAGS_SECURE &&
 			pagetable->name != KGSL_MMU_SECURE_PT)
 		return -EINVAL;
 
-	size = kgsl_memdesc_footprint(memdesc);
+	if (kgsl_memdesc_has_guard_page(memdesc))
+		size += kgsl_memdesc_guard_page_size(pagetable->mmu, memdesc);
 
 	align = 1 << kgsl_memdesc_get_align(memdesc);
 
@@ -2478,13 +1977,6 @@ static int kgsl_iommu_get_gpuaddr(struct kgsl_pagetable *pagetable,
 		end = pt->va_end;
 	}
 
-	/*
-	 * When mapping secure buffers, adjust the start of the va range
-	 * to the end of secure global buffers.
-	 */
-	if (kgsl_memdesc_is_secured(memdesc))
-		start += secure_global_size;
-
 	spin_lock(&pagetable->lock);
 
 	addr = _get_unmapped_area(pagetable, start, end, size, align);
@@ -2512,7 +2004,8 @@ static void kgsl_iommu_put_gpuaddr(struct kgsl_memdesc *memdesc)
 
 	spin_lock(&memdesc->pagetable->lock);
 
-	_remove_gpuaddr(memdesc->pagetable, memdesc->gpuaddr);
+	if (_remove_gpuaddr(memdesc->pagetable, memdesc->gpuaddr))
+		BUG();
 
 	spin_unlock(&memdesc->pagetable->lock);
 }
@@ -2557,7 +2050,6 @@ static const struct {
 } kgsl_iommu_cbs[] = {
 	{ KGSL_IOMMU_CONTEXT_USER, "gfx3d_user", },
 	{ KGSL_IOMMU_CONTEXT_SECURE, "gfx3d_secure" },
-	{ KGSL_IOMMU_CONTEXT_SECURE, "gfx3d_secure_alt" },
 };
 
 static int _kgsl_iommu_cb_probe(struct kgsl_device *device,
@@ -2565,20 +2057,12 @@ static int _kgsl_iommu_cb_probe(struct kgsl_device *device,
 {
 	struct platform_device *pdev = of_find_device_by_node(node);
 	struct kgsl_iommu_context *ctx = NULL;
-	struct adreno_device *adreno_dev = ADRENO_DEVICE(device);
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(kgsl_iommu_cbs); i++) {
 		if (!strcmp(node->name, kgsl_iommu_cbs[i].name)) {
 			int id = kgsl_iommu_cbs[i].id;
 
-			if (ADRENO_QUIRK(adreno_dev,
-				ADRENO_QUIRK_MMU_SECURE_CB_ALT)) {
-				if (!strcmp(node->name, "gfx3d_secure"))
-					continue;
-			} else if (!strcmp(node->name, "gfx3d_secure_alt"))
-				continue;
-
 			ctx = &iommu->ctx[id];
 			ctx->id = id;
 			ctx->cb_num = -1;
@@ -2589,8 +2073,8 @@ static int _kgsl_iommu_cb_probe(struct kgsl_device *device,
 	}
 
 	if (ctx == NULL) {
-		KGSL_CORE_ERR("dt: Unused context label %s\n", node->name);
-		return 0;
+		KGSL_CORE_ERR("dt: Unknown context label %s\n", node->name);
+		return -EINVAL;
 	}
 
 	if (ctx->id == KGSL_IOMMU_CONTEXT_SECURE)
@@ -2617,12 +2101,13 @@ static int _kgsl_iommu_cb_probe(struct kgsl_device *device,
 
 static const struct {
 	char *feature;
-	unsigned long bit;
+	int bit;
 } kgsl_iommu_features[] = {
 	{ "qcom,retention", KGSL_MMU_RETENTION },
 	{ "qcom,global_pt", KGSL_MMU_GLOBAL_PAGETABLE },
 	{ "qcom,hyp_secure_alloc", KGSL_MMU_HYP_SECURE_ALLOC },
 	{ "qcom,force-32bit", KGSL_MMU_FORCE_32BIT },
+	{ "qcom,coherent-htw", KGSL_MMU_COHERENT_HTW },
 };
 
 static int _kgsl_iommu_probe(struct kgsl_device *device,
@@ -2656,7 +2141,7 @@ static int _kgsl_iommu_probe(struct kgsl_device *device,
 		return -EINVAL;
 	}
 	iommu->protect.base = reg_val[0] / sizeof(u32);
-	iommu->protect.range = reg_val[1] / sizeof(u32);
+	iommu->protect.range = ilog2(reg_val[1] / sizeof(u32));
 
 	of_property_for_each_string(node, "clock-names", prop, cname) {
 		struct clk *c = devm_clk_get(&pdev->dev, cname);
@@ -2748,8 +2233,6 @@ struct kgsl_mmu_ops kgsl_iommu_ops = {
 	.mmu_add_global = kgsl_iommu_add_global,
 	.mmu_remove_global = kgsl_iommu_remove_global,
 	.mmu_getpagetable = kgsl_iommu_getpagetable,
-	.mmu_get_qdss_global_entry = kgsl_iommu_get_qdss_global_entry,
-	.mmu_get_qtimer_global_entry = kgsl_iommu_get_qtimer_global_entry,
 	.probe = kgsl_iommu_probe,
 };
 
@@ -2765,7 +2248,4 @@ static struct kgsl_mmu_pt_ops iommu_pt_ops = {
 	.find_svm_region = kgsl_iommu_find_svm_region,
 	.svm_range = kgsl_iommu_svm_range,
 	.addr_in_range = kgsl_iommu_addr_in_range,
-	.mmu_map_offset = kgsl_iommu_map_offset,
-	.mmu_unmap_offset = kgsl_iommu_unmap_offset,
-	.mmu_sparse_dummy_map = kgsl_iommu_sparse_dummy_map,
 };
diff --git a/drivers/gpu/msm/kgsl_iommu.h b/drivers/gpu/msm/kgsl_iommu.h
index 65460f743342..4c10bfa04bfa 100644
--- a/drivers/gpu/msm/kgsl_iommu.h
+++ b/drivers/gpu/msm/kgsl_iommu.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -13,7 +13,7 @@
 #ifndef __KGSL_IOMMU_H
 #define __KGSL_IOMMU_H
 
-#ifdef CONFIG_QCOM_IOMMU
+#ifdef CONFIG_MSM_IOMMU
 #include <linux/qcom_iommu.h>
 #endif
 #include <linux/of.h>
@@ -23,18 +23,13 @@
  * These defines control the address range for allocations that
  * are mapped into all pagetables.
  */
-#define KGSL_IOMMU_GLOBAL_MEM_SIZE	(20 * SZ_1M)
-#define KGSL_IOMMU_GLOBAL_MEM_BASE32	0xf8000000
-#define KGSL_IOMMU_GLOBAL_MEM_BASE64	0xfc000000
-
-#define KGSL_IOMMU_GLOBAL_MEM_BASE(__mmu)	\
-	(MMU_FEATURE(__mmu, KGSL_MMU_64BIT) ? \
-		KGSL_IOMMU_GLOBAL_MEM_BASE64 : KGSL_IOMMU_GLOBAL_MEM_BASE32)
+#define KGSL_IOMMU_GLOBAL_MEM_SIZE	SZ_8M
+#define KGSL_IOMMU_GLOBAL_MEM_BASE	0xf8000000
 
 #define KGSL_IOMMU_SECURE_SIZE SZ_256M
-#define KGSL_IOMMU_SECURE_END(_mmu) KGSL_IOMMU_GLOBAL_MEM_BASE(_mmu)
-#define KGSL_IOMMU_SECURE_BASE(_mmu)	\
-	(KGSL_IOMMU_GLOBAL_MEM_BASE(_mmu) - KGSL_IOMMU_SECURE_SIZE)
+#define KGSL_IOMMU_SECURE_END KGSL_IOMMU_GLOBAL_MEM_BASE
+#define KGSL_IOMMU_SECURE_BASE	\
+	(KGSL_IOMMU_GLOBAL_MEM_BASE - KGSL_IOMMU_SECURE_SIZE)
 
 #define KGSL_IOMMU_SVM_BASE32		0x300000
 #define KGSL_IOMMU_SVM_END32		(0xC0000000 - SZ_16M)
@@ -50,6 +45,20 @@
 #define KGSL_IOMMU_SVM_BASE64		0x700000000ULL
 #define KGSL_IOMMU_SVM_END64		0x800000000ULL
 
+/* Pagetable virtual base */
+#define KGSL_IOMMU_CTX_OFFSET_V1	0x8000
+#define KGSL_IOMMU_CTX_OFFSET_V2	0x9000
+#define KGSL_IOMMU_CTX_OFFSET_V2_A530	0x8000
+#define KGSL_IOMMU_CTX_OFFSET_A405V2	0x8000
+#define KGSL_IOMMU_CTX_SHIFT		12
+
+/* FSYNR1 V0 fields */
+#define KGSL_IOMMU_FSYNR1_AWRITE_MASK		0x00000001
+#define KGSL_IOMMU_FSYNR1_AWRITE_SHIFT		8
+/* FSYNR0 V1 fields */
+#define KGSL_IOMMU_V1_FSYNR0_WNR_MASK		0x00000001
+#define KGSL_IOMMU_V1_FSYNR0_WNR_SHIFT		4
+
 /* TLBSTATUS register fields */
 #define KGSL_IOMMU_CTX_TLBSTATUS_SACTIVE BIT(0)
 
@@ -128,8 +137,6 @@ struct kgsl_iommu_context {
  * @micro_mmu_ctrl: GPU register offset of this glob al register
  * @smmu_info: smmu info used in a5xx preemption
  * @protect: register protection settings for the iommu.
- * @pagefault_suppression_count: Total number of pagefaults
- *				 suppressed since boot.
  */
 struct kgsl_iommu {
 	struct kgsl_iommu_context ctx[KGSL_IOMMU_CONTEXT_MAX];
@@ -143,7 +150,6 @@ struct kgsl_iommu {
 	struct kgsl_memdesc smmu_info;
 	unsigned int version;
 	struct kgsl_protected_registers protect;
-	u32 pagefault_suppression_count;
 };
 
 /*
@@ -187,13 +193,11 @@ struct kgsl_iommu_pt {
 /* Macros to read/write IOMMU registers */
 extern const unsigned int kgsl_iommu_reg_list[KGSL_IOMMU_REG_MAX];
 
-/*
- * Don't use this function directly. Use the macros below to read/write
- * IOMMU registers.
- */
 static inline void __iomem *
 kgsl_iommu_reg(struct kgsl_iommu_context *ctx, enum kgsl_iommu_reg_map reg)
 {
+	BUG_ON(ctx->regbase == NULL);
+	BUG_ON(reg >= KGSL_IOMMU_REG_MAX);
 	return ctx->regbase + kgsl_iommu_reg_list[reg];
 }
 
diff --git a/drivers/gpu/msm/kgsl_log.h b/drivers/gpu/msm/kgsl_log.h
index 4f1241be2069..70480f8e9189 100644
--- a/drivers/gpu/msm/kgsl_log.h
+++ b/drivers/gpu/msm/kgsl_log.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2008-2011,2013-2014,2016-2017 The Linux Foundation.
+/* Copyright (c) 2002,2008-2011,2013-2014 The Linux Foundation.
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
@@ -48,6 +48,9 @@
 		BUG(); \
 	} while (0)
 
+#define KGSL_LOG_POSTMORTEM_WRITE(_dev, fmt, args...) \
+	do { dev_crit(_dev->dev, fmt, ##args); } while (0)
+
 #define KGSL_LOG_DUMP(_dev, fmt, args...)	dev_err(_dev->dev, fmt, ##args)
 
 #define KGSL_DEV_ERR_ONCE(_dev, fmt, args...) \
@@ -67,13 +70,6 @@
 					__func__, ##args);\
 	} while (0)
 
-#define KGSL_LOG_ERR_RATELIMITED(dev, lvl, fmt, args...) \
-	do { \
-		if ((lvl) >= 3) \
-			dev_err_ratelimited(dev, "|%s| " fmt, \
-					__func__, ##args);\
-	} while (0)
-
 #define KGSL_DRV_INFO(_dev, fmt, args...) \
 KGSL_LOG_INFO(_dev->dev, _dev->drv_log, fmt, ##args)
 #define KGSL_DRV_WARN(_dev, fmt, args...) \
@@ -84,11 +80,27 @@ KGSL_LOG_ERR(_dev->dev, _dev->drv_log, fmt, ##args)
 KGSL_LOG_CRIT(_dev->dev, _dev->drv_log, fmt, ##args)
 #define KGSL_DRV_CRIT_RATELIMIT(_dev, fmt, args...) \
 KGSL_LOG_CRIT_RATELIMITED(_dev->dev, _dev->drv_log, fmt, ##args)
-#define KGSL_DRV_ERR_RATELIMIT(_dev, fmt, args...) \
-KGSL_LOG_ERR_RATELIMITED(_dev->dev, _dev->drv_log, fmt, ##args)
 #define KGSL_DRV_FATAL(_dev, fmt, args...) \
 KGSL_LOG_FATAL((_dev)->dev, (_dev)->drv_log, fmt, ##args)
 
+#define KGSL_CMD_INFO(_dev, fmt, args...) \
+KGSL_LOG_INFO(_dev->dev, _dev->cmd_log, fmt, ##args)
+#define KGSL_CMD_WARN(_dev, fmt, args...) \
+KGSL_LOG_WARN(_dev->dev, _dev->cmd_log, fmt, ##args)
+#define KGSL_CMD_ERR(_dev, fmt, args...) \
+KGSL_LOG_ERR(_dev->dev, _dev->cmd_log, fmt, ##args)
+#define KGSL_CMD_CRIT(_dev, fmt, args...) \
+KGSL_LOG_CRIT(_dev->dev, _dev->cmd_log, fmt, ##args)
+
+#define KGSL_CTXT_INFO(_dev, fmt, args...) \
+KGSL_LOG_INFO(_dev->dev, _dev->ctxt_log, fmt, ##args)
+#define KGSL_CTXT_WARN(_dev, fmt, args...) \
+KGSL_LOG_WARN(_dev->dev, _dev->ctxt_log, fmt, ##args)
+#define KGSL_CTXT_ERR(_dev, fmt, args...)  \
+KGSL_LOG_ERR(_dev->dev, _dev->ctxt_log, fmt, ##args)
+#define KGSL_CTXT_CRIT(_dev, fmt, args...) \
+KGSL_LOG_CRIT(_dev->dev, _dev->ctxt_log, fmt, ##args)
+
 #define KGSL_MEM_INFO(_dev, fmt, args...) \
 KGSL_LOG_INFO(_dev->dev, _dev->mem_log, fmt, ##args)
 #define KGSL_MEM_WARN(_dev, fmt, args...) \
@@ -107,12 +119,19 @@ KGSL_LOG_ERR(_dev->dev, _dev->pwr_log, fmt, ##args)
 #define KGSL_PWR_CRIT(_dev, fmt, args...) \
 KGSL_LOG_CRIT(_dev->dev, _dev->pwr_log, fmt, ##args)
 
-/*
- * Core error messages - these are for core KGSL functions that have
- * no device associated with them (such as memory)
- */
+/* Core error messages - these are for core KGSL functions that have
+   no device associated with them (such as memory) */
 
 #define KGSL_CORE_ERR(fmt, args...) \
 pr_err("kgsl: %s: " fmt, __func__, ##args)
 
+#define KGSL_CORE_ERR_ONCE(fmt, args...) \
+({ \
+	static bool kgsl_core_err_once; \
+	if (!kgsl_core_err_once) { \
+		kgsl_core_err_once = true; \
+		pr_err("kgsl: %s: " fmt, __func__, ##args); \
+	} \
+})
+
 #endif /* __KGSL_LOG_H */
diff --git a/drivers/gpu/msm/kgsl_mmu.c b/drivers/gpu/msm/kgsl_mmu.c
index a0fd3ece4e78..a174d91a5f4a 100644
--- a/drivers/gpu/msm/kgsl_mmu.c
+++ b/drivers/gpu/msm/kgsl_mmu.c
@@ -26,17 +26,6 @@
 
 static void pagetable_remove_sysfs_objects(struct kgsl_pagetable *pagetable);
 
-static void _deferred_destroy(struct work_struct *ws)
-{
-	struct kgsl_pagetable *pagetable = container_of(ws,
-					struct kgsl_pagetable, destroy_ws);
-
-	if (PT_OP_VALID(pagetable, mmu_destroy_pagetable))
-		pagetable->pt_ops->mmu_destroy_pagetable(pagetable);
-
-	kfree(pagetable);
-}
-
 static void kgsl_destroy_pagetable(struct kref *kref)
 {
 	struct kgsl_pagetable *pagetable = container_of(kref,
@@ -44,7 +33,10 @@ static void kgsl_destroy_pagetable(struct kref *kref)
 
 	kgsl_mmu_detach_pagetable(pagetable);
 
-	kgsl_schedule_work(&pagetable->destroy_ws);
+	if (PT_OP_VALID(pagetable, mmu_destroy_pagetable))
+		pagetable->pt_ops->mmu_destroy_pagetable(pagetable);
+
+	kfree(pagetable);
 }
 
 static inline void kgsl_put_pagetable(struct kgsl_pagetable *pagetable)
@@ -259,10 +251,12 @@ kgsl_mmu_log_fault_addr(struct kgsl_mmu *mmu, u64 pt_base,
 			if ((addr & ~(PAGE_SIZE-1)) == pt->fault_addr) {
 				ret = 1;
 				break;
+			} else {
+				pt->fault_addr =
+					(addr & ~(PAGE_SIZE-1));
+				ret = 0;
+				break;
 			}
-			pt->fault_addr = (addr & ~(PAGE_SIZE-1));
-			ret = 0;
-			break;
 		}
 	}
 	spin_unlock(&kgsl_driver.ptlock);
@@ -307,7 +301,6 @@ kgsl_mmu_createpagetableobject(struct kgsl_mmu *mmu, unsigned int name)
 	kref_init(&pagetable->refcount);
 
 	spin_lock_init(&pagetable->lock);
-	INIT_WORK(&pagetable->destroy_ws, _deferred_destroy);
 
 	pagetable->mmu = mmu;
 	pagetable->name = name;
@@ -393,34 +386,29 @@ int
 kgsl_mmu_map(struct kgsl_pagetable *pagetable,
 				struct kgsl_memdesc *memdesc)
 {
+	int ret = 0;
 	int size;
 
 	if (!memdesc->gpuaddr)
 		return -EINVAL;
-	if (!(memdesc->flags & (KGSL_MEMFLAGS_SPARSE_VIRT |
-					KGSL_MEMFLAGS_SPARSE_PHYS))) {
-		/* Only global mappings should be mapped multiple times */
-		if (!kgsl_memdesc_is_global(memdesc) &&
-				(KGSL_MEMDESC_MAPPED & memdesc->priv))
-			return -EINVAL;
-	}
+	/* Only global mappings should be mapped multiple times */
+	if (!kgsl_memdesc_is_global(memdesc) &&
+		(KGSL_MEMDESC_MAPPED & memdesc->priv))
+		return -EINVAL;
 
 	size = kgsl_memdesc_footprint(memdesc);
 
-	if (PT_OP_VALID(pagetable, mmu_map)) {
-		int ret;
-
+	if (PT_OP_VALID(pagetable, mmu_map))
 		ret = pagetable->pt_ops->mmu_map(pagetable, memdesc);
-		if (ret)
-			return ret;
 
-		atomic_inc(&pagetable->stats.entries);
-		KGSL_STATS_ADD(size, &pagetable->stats.mapped,
-				&pagetable->stats.max_mapped);
+	if (ret)
+		return ret;
 
-		/* This is needed for non-sparse mappings */
-		memdesc->priv |= KGSL_MEMDESC_MAPPED;
-	}
+	atomic_inc(&pagetable->stats.entries);
+	KGSL_STATS_ADD(size, &pagetable->stats.mapped,
+		&pagetable->stats.max_mapped);
+
+	memdesc->priv |= KGSL_MEMDESC_MAPPED;
 
 	return 0;
 }
@@ -446,7 +434,7 @@ void kgsl_mmu_put_gpuaddr(struct kgsl_memdesc *memdesc)
 	 * Do not free the gpuaddr/size if unmap fails. Because if we
 	 * try to map this range in future, the iommu driver will throw
 	 * a BUG_ON() because it feels we are overwriting a mapping.
-	 */
+	*/
 	if (PT_OP_VALID(pagetable, put_gpuaddr) && (unmap_fail == 0))
 		pagetable->pt_ops->put_gpuaddr(memdesc);
 
@@ -479,96 +467,26 @@ int
 kgsl_mmu_unmap(struct kgsl_pagetable *pagetable,
 		struct kgsl_memdesc *memdesc)
 {
-	int ret = 0;
+	uint64_t size;
 
-	if (memdesc->size == 0)
+	if (memdesc->size == 0 || memdesc->gpuaddr == 0 ||
+		!(KGSL_MEMDESC_MAPPED & memdesc->priv))
 		return -EINVAL;
 
-	if (!(memdesc->flags & (KGSL_MEMFLAGS_SPARSE_VIRT |
-					KGSL_MEMFLAGS_SPARSE_PHYS))) {
-		/* Only global mappings should be mapped multiple times */
-		if (!(KGSL_MEMDESC_MAPPED & memdesc->priv))
-			return -EINVAL;
-	}
-
-	if (PT_OP_VALID(pagetable, mmu_unmap)) {
-		uint64_t size;
-
-		size = kgsl_memdesc_footprint(memdesc);
-
-		ret = pagetable->pt_ops->mmu_unmap(pagetable, memdesc);
-
-		atomic_dec(&pagetable->stats.entries);
-		atomic_long_sub(size, &pagetable->stats.mapped);
-
-		if (!kgsl_memdesc_is_global(memdesc))
-			memdesc->priv &= ~KGSL_MEMDESC_MAPPED;
-	}
-
-	return ret;
-}
-EXPORT_SYMBOL(kgsl_mmu_unmap);
-
-int kgsl_mmu_map_offset(struct kgsl_pagetable *pagetable,
-			uint64_t virtaddr, uint64_t virtoffset,
-			struct kgsl_memdesc *memdesc, uint64_t physoffset,
-			uint64_t size, uint64_t flags)
-{
-	if (PT_OP_VALID(pagetable, mmu_map_offset)) {
-		int ret;
-
-		ret = pagetable->pt_ops->mmu_map_offset(pagetable, virtaddr,
-				virtoffset, memdesc, physoffset, size, flags);
-		if (ret)
-			return ret;
-
-		atomic_inc(&pagetable->stats.entries);
-		KGSL_STATS_ADD(size, &pagetable->stats.mapped,
-				&pagetable->stats.max_mapped);
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL(kgsl_mmu_map_offset);
-
-int kgsl_mmu_unmap_offset(struct kgsl_pagetable *pagetable,
-		struct kgsl_memdesc *memdesc, uint64_t addr, uint64_t offset,
-		uint64_t size)
-{
-	if (PT_OP_VALID(pagetable, mmu_unmap_offset)) {
-		int ret;
-
-		ret = pagetable->pt_ops->mmu_unmap_offset(pagetable, memdesc,
-				addr, offset, size);
-		if (ret)
-			return ret;
-
-		atomic_dec(&pagetable->stats.entries);
-		atomic_long_sub(size, &pagetable->stats.mapped);
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL(kgsl_mmu_unmap_offset);
+	size = kgsl_memdesc_footprint(memdesc);
 
-int kgsl_mmu_sparse_dummy_map(struct kgsl_pagetable *pagetable,
-		struct kgsl_memdesc *memdesc, uint64_t offset, uint64_t size)
-{
-	if (PT_OP_VALID(pagetable, mmu_sparse_dummy_map)) {
-		int ret;
+	if (PT_OP_VALID(pagetable, mmu_unmap))
+		pagetable->pt_ops->mmu_unmap(pagetable, memdesc);
 
-		ret = pagetable->pt_ops->mmu_sparse_dummy_map(pagetable,
-				memdesc, offset, size);
-		if (ret)
-			return ret;
+	atomic_dec(&pagetable->stats.entries);
+	atomic_long_sub(size, &pagetable->stats.mapped);
 
-		atomic_dec(&pagetable->stats.entries);
-		atomic_long_sub(size, &pagetable->stats.mapped);
-	}
+	if (!kgsl_memdesc_is_global(memdesc))
+		memdesc->priv &= ~KGSL_MEMDESC_MAPPED;
 
 	return 0;
 }
-EXPORT_SYMBOL(kgsl_mmu_sparse_dummy_map);
+EXPORT_SYMBOL(kgsl_mmu_unmap);
 
 void kgsl_mmu_remove_global(struct kgsl_device *device,
 		struct kgsl_memdesc *memdesc)
@@ -581,12 +499,12 @@ void kgsl_mmu_remove_global(struct kgsl_device *device,
 EXPORT_SYMBOL(kgsl_mmu_remove_global);
 
 void kgsl_mmu_add_global(struct kgsl_device *device,
-		struct kgsl_memdesc *memdesc, const char *name)
+		struct kgsl_memdesc *memdesc)
 {
 	struct kgsl_mmu *mmu = &device->mmu;
 
 	if (MMU_OP_VALID(mmu, mmu_add_global))
-		mmu->mmu_ops->mmu_add_global(mmu, memdesc, name);
+		mmu->mmu_ops->mmu_add_global(mmu, memdesc);
 }
 EXPORT_SYMBOL(kgsl_mmu_add_global);
 
@@ -615,33 +533,10 @@ bool kgsl_mmu_gpuaddr_in_range(struct kgsl_pagetable *pagetable,
 }
 EXPORT_SYMBOL(kgsl_mmu_gpuaddr_in_range);
 
-struct kgsl_memdesc *kgsl_mmu_get_qdss_global_entry(struct kgsl_device *device)
-{
-	struct kgsl_mmu *mmu = &device->mmu;
-
-	if (MMU_OP_VALID(mmu, mmu_get_qdss_global_entry))
-		return mmu->mmu_ops->mmu_get_qdss_global_entry();
-
-	return NULL;
-}
-EXPORT_SYMBOL(kgsl_mmu_get_qdss_global_entry);
-
-struct kgsl_memdesc *kgsl_mmu_get_qtimer_global_entry(
-		struct kgsl_device *device)
-{
-	struct kgsl_mmu *mmu = &device->mmu;
-
-	if (MMU_OP_VALID(mmu, mmu_get_qtimer_global_entry))
-		return mmu->mmu_ops->mmu_get_qtimer_global_entry();
-
-	return NULL;
-}
-EXPORT_SYMBOL(kgsl_mmu_get_qtimer_global_entry);
-
 /*
- * NOMMU definitions - NOMMU really just means that the MMU is kept in pass
- * through and the GPU directly accesses physical memory. Used in debug mode
- * and when a real MMU isn't up and running yet.
+ * NOMMU defintions - NOMMU really just means that the MMU is kept in pass
+ * through and the GPU directly accesses physical memory. Used in debug mode and
+ * when a real MMU isn't up and running yet.
  */
 
 static bool nommu_gpuaddr_in_range(struct kgsl_pagetable *pagetable,
@@ -675,7 +570,7 @@ static struct kgsl_mmu_pt_ops nommu_pt_ops = {
 };
 
 static void nommu_add_global(struct kgsl_mmu *mmu,
-		struct kgsl_memdesc *memdesc, const char *name)
+		struct kgsl_memdesc *memdesc)
 {
 	memdesc->gpuaddr = (uint64_t) sg_phys(memdesc->sgt->sgl);
 }
diff --git a/drivers/gpu/msm/kgsl_mmu.h b/drivers/gpu/msm/kgsl_mmu.h
index 430a14081d35..11cd12b02390 100644
--- a/drivers/gpu/msm/kgsl_mmu.h
+++ b/drivers/gpu/msm/kgsl_mmu.h
@@ -16,19 +16,11 @@
 #include "kgsl_iommu.h"
 
 /* Identifier for the global page table */
-/*
- * Per process page tables will probably pass in the thread group
- *  as an identifier
- */
+/* Per process page tables will probably pass in the thread group
+   as an identifier */
 #define KGSL_MMU_GLOBAL_PT 0
 #define KGSL_MMU_SECURE_PT 1
 
-#define MMU_DEFAULT_TTBR0(_d) \
-	(kgsl_mmu_pagetable_get_ttbr0((_d)->mmu.defaultpagetable))
-
-#define MMU_DEFAULT_CONTEXTIDR(_d) \
-	(kgsl_mmu_pagetable_get_contextidr((_d)->mmu.defaultpagetable))
-
 struct kgsl_device;
 
 enum kgsl_mmutype {
@@ -42,7 +34,6 @@ struct kgsl_pagetable {
 	struct list_head list;
 	unsigned int name;
 	struct kobject *kobj;
-	struct work_struct destroy_ws;
 
 	struct {
 		atomic_t entries;
@@ -59,11 +50,11 @@ struct kgsl_mmu;
 
 struct kgsl_mmu_ops {
 	int (*probe)(struct kgsl_device *device);
-	int (*mmu_init)(struct kgsl_mmu *mmu);
+	int (*mmu_init) (struct kgsl_mmu *mmu);
 	void (*mmu_close)(struct kgsl_mmu *mmu);
-	int (*mmu_start)(struct kgsl_mmu *mmu);
-	void (*mmu_stop)(struct kgsl_mmu *mmu);
-	int (*mmu_set_pt)(struct kgsl_mmu *mmu, struct kgsl_pagetable *pt);
+	int (*mmu_start) (struct kgsl_mmu *mmu);
+	void (*mmu_stop) (struct kgsl_mmu *mmu);
+	int (*mmu_set_pt) (struct kgsl_mmu *mmu, struct kgsl_pagetable *pt);
 	uint64_t (*mmu_get_current_ttbr0)(struct kgsl_mmu *mmu);
 	void (*mmu_pagefault_resume)(struct kgsl_mmu *mmu);
 	void (*mmu_clear_fsr)(struct kgsl_mmu *mmu);
@@ -78,13 +69,11 @@ struct kgsl_mmu_ops {
 			(struct kgsl_mmu *mmu);
 	int (*mmu_init_pt)(struct kgsl_mmu *mmu, struct kgsl_pagetable *);
 	void (*mmu_add_global)(struct kgsl_mmu *mmu,
-			struct kgsl_memdesc *memdesc, const char *name);
+			struct kgsl_memdesc *memdesc);
 	void (*mmu_remove_global)(struct kgsl_mmu *mmu,
 			struct kgsl_memdesc *memdesc);
 	struct kgsl_pagetable * (*mmu_getpagetable)(struct kgsl_mmu *mmu,
 			unsigned long name);
-	struct kgsl_memdesc* (*mmu_get_qdss_global_entry)(void);
-	struct kgsl_memdesc* (*mmu_get_qtimer_global_entry)(void);
 };
 
 struct kgsl_mmu_pt_ops {
@@ -92,7 +81,7 @@ struct kgsl_mmu_pt_ops {
 			struct kgsl_memdesc *memdesc);
 	int (*mmu_unmap)(struct kgsl_pagetable *pt,
 			struct kgsl_memdesc *memdesc);
-	void (*mmu_destroy_pagetable)(struct kgsl_pagetable *);
+	void (*mmu_destroy_pagetable) (struct kgsl_pagetable *);
 	u64 (*get_ttbr0)(struct kgsl_pagetable *);
 	u32 (*get_contextidr)(struct kgsl_pagetable *);
 	int (*get_gpuaddr)(struct kgsl_pagetable *, struct kgsl_memdesc *);
@@ -103,16 +92,6 @@ struct kgsl_mmu_pt_ops {
 	int (*svm_range)(struct kgsl_pagetable *, uint64_t *, uint64_t *,
 			uint64_t);
 	bool (*addr_in_range)(struct kgsl_pagetable *pagetable, uint64_t);
-	int (*mmu_map_offset)(struct kgsl_pagetable *pt,
-			uint64_t virtaddr, uint64_t virtoffset,
-			struct kgsl_memdesc *memdesc, uint64_t physoffset,
-			uint64_t size, uint64_t flags);
-	int (*mmu_unmap_offset)(struct kgsl_pagetable *pt,
-			struct kgsl_memdesc *memdesc, uint64_t addr,
-			uint64_t offset, uint64_t size);
-	int (*mmu_sparse_dummy_map)(struct kgsl_pagetable *pt,
-			struct kgsl_memdesc *memdesc, uint64_t offset,
-			uint64_t size);
 };
 
 /*
@@ -134,12 +113,12 @@ struct kgsl_mmu_pt_ops {
 #define KGSL_MMU_FORCE_32BIT BIT(5)
 /* 64 bit address is live */
 #define KGSL_MMU_64BIT BIT(6)
+/* MMU can do coherent hardware table walks */
+#define KGSL_MMU_COHERENT_HTW BIT(7)
 /* The MMU supports non-contigious pages */
 #define KGSL_MMU_PAGED BIT(8)
 /* The device requires a guard page */
 #define KGSL_MMU_NEED_GUARD_PAGE BIT(9)
-/* The device supports IO coherency */
-#define KGSL_MMU_IO_COHERENT BIT(10)
 
 /**
  * struct kgsl_mmu - Master definition for KGSL MMU devices
@@ -173,14 +152,11 @@ extern struct kgsl_mmu_ops kgsl_iommu_ops;
 
 int kgsl_mmu_probe(struct kgsl_device *device, char *name);
 int kgsl_mmu_start(struct kgsl_device *device);
-struct kgsl_pagetable *kgsl_mmu_getpagetable_ptbase(struct kgsl_mmu *mmu,
+struct kgsl_pagetable *kgsl_mmu_getpagetable_ptbase(struct kgsl_mmu *,
 						u64 ptbase);
 
-int kgsl_iommu_map_global_secure_pt_entry(struct kgsl_device *device,
-					struct kgsl_memdesc *memdesc);
-void kgsl_iommu_unmap_global_secure_pt_entry(struct kgsl_device *device,
+void kgsl_add_global_secure_entry(struct kgsl_device *device,
 					struct kgsl_memdesc *memdesc);
-void kgsl_print_global_pt_entries(struct seq_file *s);
 void kgsl_mmu_putpagetable(struct kgsl_pagetable *pagetable);
 
 int kgsl_mmu_get_gpuaddr(struct kgsl_pagetable *pagetable,
@@ -204,7 +180,7 @@ int kgsl_mmu_find_region(struct kgsl_pagetable *pagetable,
 		uint64_t *gpuaddr, uint64_t size, unsigned int align);
 
 void kgsl_mmu_add_global(struct kgsl_device *device,
-	struct kgsl_memdesc *memdesc, const char *name);
+	struct kgsl_memdesc *memdesc);
 void kgsl_mmu_remove_global(struct kgsl_device *device,
 		struct kgsl_memdesc *memdesc);
 
@@ -229,22 +205,6 @@ struct kgsl_pagetable *kgsl_get_pagetable(unsigned long name);
 struct kgsl_pagetable *
 kgsl_mmu_createpagetableobject(struct kgsl_mmu *mmu, unsigned int name);
 
-int kgsl_mmu_map_offset(struct kgsl_pagetable *pagetable,
-		uint64_t virtaddr, uint64_t virtoffset,
-		struct kgsl_memdesc *memdesc, uint64_t physoffset,
-		uint64_t size, uint64_t flags);
-int kgsl_mmu_unmap_offset(struct kgsl_pagetable *pagetable,
-		struct kgsl_memdesc *memdesc, uint64_t addr, uint64_t offset,
-		uint64_t size);
-
-struct kgsl_memdesc *kgsl_mmu_get_qdss_global_entry(struct kgsl_device *device);
-
-struct kgsl_memdesc *kgsl_mmu_get_qtimer_global_entry(
-		struct kgsl_device *device);
-
-int kgsl_mmu_sparse_dummy_map(struct kgsl_pagetable *pagetable,
-		struct kgsl_memdesc *memdesc, uint64_t offset, uint64_t size);
-
 /*
  * Static inline functions of MMU that simply call the SMMU specific
  * function using a function pointer. These functions can be thought
@@ -395,7 +355,7 @@ kgsl_mmu_pagetable_get_contextidr(struct kgsl_pagetable *pagetable)
 	return 0;
 }
 
-#ifdef CONFIG_QCOM_IOMMU
+#ifdef CONFIG_MSM_IOMMU
 #include <linux/qcom_iommu.h>
 #ifndef CONFIG_ARM_SMMU
 static inline bool kgsl_mmu_bus_secured(struct device *dev)
diff --git a/drivers/gpu/msm/kgsl_pool.c b/drivers/gpu/msm/kgsl_pool.c
deleted file mode 100644
index 5da8c1dcf5c1..000000000000
--- a/drivers/gpu/msm/kgsl_pool.c
+++ /dev/null
@@ -1,579 +0,0 @@
-/* Copyright (c) 2016-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-
-#include <linux/vmalloc.h>
-#include <asm/cacheflush.h>
-#include <linux/slab.h>
-#include <linux/highmem.h>
-#include <linux/version.h>
-
-#include "kgsl.h"
-#include "kgsl_device.h"
-#include "kgsl_pool.h"
-
-#define KGSL_MAX_POOLS 4
-#define KGSL_MAX_POOL_ORDER 8
-#define KGSL_MAX_RESERVED_PAGES 4096
-
-/**
- * struct kgsl_page_pool - Structure to hold information for the pool
- * @pool_order: Page order describing the size of the page
- * @page_count: Number of pages currently present in the pool
- * @reserved_pages: Number of pages reserved at init for the pool
- * @allocation_allowed: Tells if reserved pool gets exhausted, can we allocate
- * from system memory
- * @list_lock: Spinlock for page list in the pool
- * @page_list: List of pages held/reserved in this pool
- */
-struct kgsl_page_pool {
-	unsigned int pool_order;
-	int page_count;
-	unsigned int reserved_pages;
-	bool allocation_allowed;
-	spinlock_t list_lock;
-	struct list_head page_list;
-};
-
-static struct kgsl_page_pool kgsl_pools[KGSL_MAX_POOLS];
-static int kgsl_num_pools;
-static int kgsl_pool_max_pages;
-
-
-/* Returns KGSL pool corresponding to input page order*/
-static struct kgsl_page_pool *
-_kgsl_get_pool_from_order(unsigned int order)
-{
-	int i;
-
-	for (i = 0; i < kgsl_num_pools; i++) {
-		if (kgsl_pools[i].pool_order == order)
-			return &kgsl_pools[i];
-	}
-
-	return NULL;
-}
-
-/* Map the page into kernel and zero it out */
-static void
-_kgsl_pool_zero_page(struct page *p, unsigned int pool_order)
-{
-	int i;
-
-	for (i = 0; i < (1 << pool_order); i++) {
-		struct page *page = nth_page(p, i);
-		void *addr = kmap_atomic(page);
-
-		memset(addr, 0, PAGE_SIZE);
-		dmac_flush_range(addr, addr + PAGE_SIZE);
-		kunmap_atomic(addr);
-	}
-}
-
-/* Add a page to specified pool */
-static void
-_kgsl_pool_add_page(struct kgsl_page_pool *pool, struct page *p)
-{
-	_kgsl_pool_zero_page(p, pool->pool_order);
-
-	spin_lock(&pool->list_lock);
-	list_add_tail(&p->lru, &pool->page_list);
-	pool->page_count++;
-	spin_unlock(&pool->list_lock);
-}
-
-/* Returns a page from specified pool */
-static struct page *
-_kgsl_pool_get_page(struct kgsl_page_pool *pool)
-{
-	struct page *p = NULL;
-
-	spin_lock(&pool->list_lock);
-	if (pool->page_count) {
-		p = list_first_entry(&pool->page_list, struct page, lru);
-		pool->page_count--;
-		list_del(&p->lru);
-	}
-	spin_unlock(&pool->list_lock);
-
-	return p;
-}
-
-/* Returns the number of pages in specified pool */
-static int
-kgsl_pool_size(struct kgsl_page_pool *kgsl_pool)
-{
-	int size;
-
-	spin_lock(&kgsl_pool->list_lock);
-	size = kgsl_pool->page_count * (1 << kgsl_pool->pool_order);
-	spin_unlock(&kgsl_pool->list_lock);
-
-	return size;
-}
-
-/* Returns the number of pages in all kgsl page pools */
-static int kgsl_pool_size_total(void)
-{
-	int i;
-	int total = 0;
-
-	for (i = 0; i < kgsl_num_pools; i++)
-		total += kgsl_pool_size(&kgsl_pools[i]);
-	return total;
-}
-
-/*
- * This will shrink the specified pool by num_pages or its pool_size,
- * whichever is smaller.
- */
-static unsigned int
-_kgsl_pool_shrink(struct kgsl_page_pool *pool, int num_pages)
-{
-	int j;
-	unsigned int pcount = 0;
-
-	if (pool == NULL || num_pages <= 0)
-		return pcount;
-
-	for (j = 0; j < num_pages >> pool->pool_order; j++) {
-		struct page *page = _kgsl_pool_get_page(pool);
-
-		if (page != NULL) {
-			__free_pages(page, pool->pool_order);
-			pcount += (1 << pool->pool_order);
-		} else {
-			/* Break as this pool is empty */
-			break;
-		}
-	}
-
-	return pcount;
-}
-
-/*
- * This function reduces the total pool size
- * to number of pages specified by target_pages.
- *
- * If target_pages are greater than current pool size
- * nothing needs to be done otherwise remove
- * (current_pool_size - target_pages) pages from pool
- * starting from higher order pool.
- */
-static unsigned long
-kgsl_pool_reduce(unsigned int target_pages, bool exit)
-{
-	int total_pages = 0;
-	int i;
-	int nr_removed;
-	struct kgsl_page_pool *pool;
-	unsigned long pcount = 0;
-
-	total_pages = kgsl_pool_size_total();
-
-	for (i = (kgsl_num_pools - 1); i >= 0; i--) {
-		pool = &kgsl_pools[i];
-
-		/*
-		 * Only reduce the pool sizes for pools which are allowed to
-		 * allocate memory unless we are at close, in which case the
-		 * reserved memory for all pools needs to be freed
-		 */
-		if (!pool->allocation_allowed && !exit)
-			continue;
-
-		total_pages -= pcount;
-
-		nr_removed = total_pages - target_pages;
-		if (nr_removed <= 0)
-			return pcount;
-
-		/* Round up to integral number of pages in this pool */
-		nr_removed = ALIGN(nr_removed, 1 << pool->pool_order);
-
-		/* Remove nr_removed pages from this pool*/
-		pcount += _kgsl_pool_shrink(pool, nr_removed);
-	}
-
-	return pcount;
-}
-
-/**
- * kgsl_pool_free_sgt() - Free scatter-gather list
- * @sgt: pointer of the sg list
- *
- * Free the sg list by collapsing any physical adjacent pages.
- * Pages are added back to the pool, if pool has sufficient space
- * otherwise they are given back to system.
- */
-
-void kgsl_pool_free_sgt(struct sg_table *sgt)
-{
-	int i;
-	struct scatterlist *sg;
-
-	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
-		/*
-		 * sg_alloc_table_from_pages() will collapse any physically
-		 * adjacent pages into a single scatterlist entry. We cannot
-		 * just call __free_pages() on the entire set since we cannot
-		 * ensure that the size is a whole order. Instead, free each
-		 * page or compound page group individually.
-		 */
-		struct page *p = sg_page(sg), *next;
-		unsigned int count;
-		unsigned int j = 0;
-
-		while (j < (sg->length/PAGE_SIZE)) {
-			count = 1 << compound_order(p);
-			next = nth_page(p, count);
-			kgsl_pool_free_page(p);
-
-			p = next;
-			j += count;
-		}
-	}
-}
-
-/**
- * kgsl_pool_free_pages() - Free pages in the pages array
- * @pages: pointer of the pages array
- *
- * Free the pages by collapsing any physical adjacent pages.
- * Pages are added back to the pool, if pool has sufficient space
- * otherwise they are given back to system.
- */
-void kgsl_pool_free_pages(struct page **pages, unsigned int pcount)
-{
-	int i;
-
-	if (pages == NULL || pcount == 0)
-		return;
-
-	for (i = 0; i < pcount;) {
-		/*
-		 * Free each page or compound page group individually.
-		 */
-		struct page *p = pages[i];
-
-		i += 1 << compound_order(p);
-		kgsl_pool_free_page(p);
-	}
-}
-static int kgsl_pool_idx_lookup(unsigned int order)
-{
-	int i;
-
-	for (i = 0; i < kgsl_num_pools; i++)
-		if (order == kgsl_pools[i].pool_order)
-			return i;
-
-	return -ENOMEM;
-}
-
-static int kgsl_pool_get_retry_order(unsigned int order)
-{
-	int i;
-
-	for (i = kgsl_num_pools-1; i > 0; i--)
-		if (order >= kgsl_pools[i].pool_order)
-			return kgsl_pools[i].pool_order;
-
-	return 0;
-}
-
-/**
- * kgsl_pool_alloc_page() - Allocate a page of requested size
- * @page_size: Size of the page to be allocated
- * @pages: pointer to hold list of pages, should be big enough to hold
- * requested page
- * @len: Length of array pages.
- *
- * Return total page count on success and negative value on failure
- */
-int kgsl_pool_alloc_page(int *page_size, struct page **pages,
-			unsigned int pages_len, unsigned int *align)
-{
-	int j;
-	int pcount = 0;
-	struct kgsl_page_pool *pool;
-	struct page *page = NULL;
-	struct page *p = NULL;
-	int order = get_order(*page_size);
-	int pool_idx;
-	size_t size = 0;
-
-	if ((pages == NULL) || pages_len < (*page_size >> PAGE_SHIFT))
-		return -EINVAL;
-
-	/* If the pool is not configured get pages from the system */
-	if (!kgsl_num_pools) {
-		gfp_t gfp_mask = kgsl_gfp_mask(order);
-
-		page = alloc_pages(gfp_mask, order);
-		if (page == NULL) {
-			/* Retry with lower order pages */
-			if (order > 0) {
-				size = PAGE_SIZE << --order;
-				goto eagain;
-
-			} else
-				return -ENOMEM;
-		}
-		_kgsl_pool_zero_page(page, order);
-		goto done;
-	}
-
-	pool = _kgsl_get_pool_from_order(order);
-	if (pool == NULL) {
-		/* Retry with lower order pages */
-		if (order > 0) {
-			size = PAGE_SIZE << kgsl_pool_get_retry_order(order);
-			goto eagain;
-		} else {
-			/*
-			 * Fall back to direct allocation in case
-			 * pool with zero order is not present
-			 */
-			gfp_t gfp_mask = kgsl_gfp_mask(order);
-
-			page = alloc_pages(gfp_mask, order);
-			if (page == NULL)
-				return -ENOMEM;
-			_kgsl_pool_zero_page(page, order);
-			goto done;
-		}
-	}
-
-	pool_idx = kgsl_pool_idx_lookup(order);
-	page = _kgsl_pool_get_page(pool);
-
-	/* Allocate a new page if not allocated from pool */
-	if (page == NULL) {
-		gfp_t gfp_mask = kgsl_gfp_mask(order);
-
-		/* Only allocate non-reserved memory for certain pools */
-		if (!pool->allocation_allowed && pool_idx > 0) {
-			size = PAGE_SIZE <<
-					kgsl_pools[pool_idx-1].pool_order;
-			goto eagain;
-		}
-
-		page = alloc_pages(gfp_mask, order);
-
-		if (!page) {
-			if (pool_idx > 0) {
-				/* Retry with lower order pages */
-				size = PAGE_SIZE <<
-					kgsl_pools[pool_idx-1].pool_order;
-				goto eagain;
-			} else
-				return -ENOMEM;
-		}
-
-		_kgsl_pool_zero_page(page, order);
-	}
-
-done:
-	for (j = 0; j < (*page_size >> PAGE_SHIFT); j++) {
-		p = nth_page(page, j);
-		pages[pcount] = p;
-		pcount++;
-	}
-
-	return pcount;
-
-eagain:
-	*page_size = kgsl_get_page_size(size,
-			ilog2(size));
-	*align = ilog2(*page_size);
-	return -EAGAIN;
-}
-
-void kgsl_pool_free_page(struct page *page)
-{
-	struct kgsl_page_pool *pool;
-	int page_order;
-
-	if (page == NULL)
-		return;
-
-	page_order = compound_order(page);
-
-	if (!kgsl_pool_max_pages ||
-			(kgsl_pool_size_total() < kgsl_pool_max_pages)) {
-		pool = _kgsl_get_pool_from_order(page_order);
-		if (pool != NULL) {
-			_kgsl_pool_add_page(pool, page);
-			return;
-		}
-	}
-
-	/* Give back to system as not added to pool */
-	__free_pages(page, page_order);
-}
-
-/*
- * Return true if the pool of specified page size is supported
- * or no pools are supported otherwise return false.
- */
-bool kgsl_pool_avaialable(int page_size)
-{
-	int i;
-
-	if (!kgsl_num_pools)
-		return true;
-
-	for (i = 0; i < kgsl_num_pools; i++)
-		if (ilog2(page_size >> PAGE_SHIFT) == kgsl_pools[i].pool_order)
-			return true;
-
-	return false;
-}
-
-static void kgsl_pool_reserve_pages(void)
-{
-	int i, j;
-
-	for (i = 0; i < kgsl_num_pools; i++) {
-		struct page *page;
-
-		for (j = 0; j < kgsl_pools[i].reserved_pages; j++) {
-			int order = kgsl_pools[i].pool_order;
-			gfp_t gfp_mask = kgsl_gfp_mask(order);
-
-			page = alloc_pages(gfp_mask, order);
-			if (page != NULL)
-				_kgsl_pool_add_page(&kgsl_pools[i], page);
-		}
-	}
-}
-
-/* Functions for the shrinker */
-
-static unsigned long
-kgsl_pool_shrink_scan_objects(struct shrinker *shrinker,
-					struct shrink_control *sc)
-{
-	/* nr represents number of pages to be removed*/
-	int nr = sc->nr_to_scan;
-	int total_pages = kgsl_pool_size_total();
-
-	/* Target pages represents new  pool size */
-	int target_pages = (nr > total_pages) ? 0 : (total_pages - nr);
-
-	/* Reduce pool size to target_pages */
-	return kgsl_pool_reduce(target_pages, false);
-}
-
-static unsigned long
-kgsl_pool_shrink_count_objects(struct shrinker *shrinker,
-					struct shrink_control *sc)
-{
-	/* Return total pool size as everything in pool can be freed */
-	return kgsl_pool_size_total();
-}
-
-/* Shrinker callback data*/
-static struct shrinker kgsl_pool_shrinker = {
-	.count_objects = kgsl_pool_shrink_count_objects,
-	.scan_objects = kgsl_pool_shrink_scan_objects,
-	.seeks = DEFAULT_SEEKS,
-	.batch = 0,
-};
-
-static void kgsl_pool_config(unsigned int order, unsigned int reserved_pages,
-		bool allocation_allowed)
-{
-#ifdef CONFIG_ALLOC_BUFFERS_IN_4K_CHUNKS
-	if (order > 0) {
-		pr_info("%s: Pool order:%d not supprted.!!\n", __func__, order);
-		return;
-	}
-#endif
-	if ((order > KGSL_MAX_POOL_ORDER) ||
-			(reserved_pages > KGSL_MAX_RESERVED_PAGES))
-		return;
-
-	kgsl_pools[kgsl_num_pools].pool_order = order;
-	kgsl_pools[kgsl_num_pools].reserved_pages = reserved_pages;
-	kgsl_pools[kgsl_num_pools].allocation_allowed = allocation_allowed;
-	spin_lock_init(&kgsl_pools[kgsl_num_pools].list_lock);
-	INIT_LIST_HEAD(&kgsl_pools[kgsl_num_pools].page_list);
-	kgsl_num_pools++;
-}
-
-static void kgsl_of_parse_mempools(struct device_node *node)
-{
-	struct device_node *child;
-	unsigned int page_size, reserved_pages = 0;
-	bool allocation_allowed;
-
-	for_each_child_of_node(node, child) {
-		unsigned int index;
-
-		if (of_property_read_u32(child, "reg", &index))
-			return;
-
-		if (index >= KGSL_MAX_POOLS)
-			continue;
-
-		if (of_property_read_u32(child, "qcom,mempool-page-size",
-					&page_size))
-			return;
-
-		of_property_read_u32(child, "qcom,mempool-reserved",
-				&reserved_pages);
-
-		allocation_allowed = of_property_read_bool(child,
-				"qcom,mempool-allocate");
-
-		kgsl_pool_config(ilog2(page_size >> PAGE_SHIFT), reserved_pages,
-				allocation_allowed);
-	}
-}
-
-static void kgsl_of_get_mempools(struct device_node *parent)
-{
-	struct device_node *node;
-
-	node = of_find_compatible_node(parent, NULL, "qcom,gpu-mempools");
-	if (node != NULL) {
-		/* Get Max pages limit for mempool */
-		of_property_read_u32(node, "qcom,mempool-max-pages",
-				&kgsl_pool_max_pages);
-		kgsl_of_parse_mempools(node);
-	}
-}
-
-void kgsl_init_page_pools(struct platform_device *pdev)
-{
-
-	/* Get GPU mempools data and configure pools */
-	kgsl_of_get_mempools(pdev->dev.of_node);
-
-	/* Reserve the appropriate number of pages for each pool */
-	kgsl_pool_reserve_pages();
-
-	/* Initialize shrinker */
-	register_shrinker(&kgsl_pool_shrinker);
-}
-
-void kgsl_exit_page_pools(void)
-{
-	/* Release all pages in pools, if any.*/
-	kgsl_pool_reduce(0, true);
-
-	/* Unregister shrinker */
-	unregister_shrinker(&kgsl_pool_shrinker);
-}
-
diff --git a/drivers/gpu/msm/kgsl_pool.h b/drivers/gpu/msm/kgsl_pool.h
deleted file mode 100644
index 8091afb1ff11..000000000000
--- a/drivers/gpu/msm/kgsl_pool.h
+++ /dev/null
@@ -1,45 +0,0 @@
-/* Copyright (c) 2016-2017, The Linux Foundation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 and
- * only version 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- */
-#ifndef __KGSL_POOL_H
-#define __KGSL_POOL_H
-
-#include <linux/mm_types.h>
-#include "kgsl_sharedmem.h"
-
-static inline unsigned int
-kgsl_gfp_mask(unsigned int page_order)
-{
-	unsigned int gfp_mask = __GFP_HIGHMEM;
-
-	if (page_order > 0) {
-		gfp_mask |= __GFP_COMP | __GFP_NORETRY | __GFP_NOWARN;
-		gfp_mask &= ~__GFP_RECLAIM;
-	} else
-		gfp_mask |= GFP_KERNEL;
-
-	if (kgsl_sharedmem_get_noretry() == true)
-		gfp_mask |= __GFP_NORETRY | __GFP_NOWARN;
-
-	return gfp_mask;
-}
-
-void kgsl_pool_free_sgt(struct sg_table *sgt);
-void kgsl_pool_free_pages(struct page **pages, unsigned int page_count);
-void kgsl_init_page_pools(struct platform_device *pdev);
-void kgsl_exit_page_pools(void);
-int kgsl_pool_alloc_page(int *page_size, struct page **pages,
-			unsigned int pages_len, unsigned int *align);
-void kgsl_pool_free_page(struct page *p);
-bool kgsl_pool_avaialable(int size);
-#endif /* __KGSL_POOL_H */
-
diff --git a/drivers/gpu/msm/kgsl_pwrctrl.c b/drivers/gpu/msm/kgsl_pwrctrl.c
index e887f1ed6867..e4689a12d086 100644
--- a/drivers/gpu/msm/kgsl_pwrctrl.c
+++ b/drivers/gpu/msm/kgsl_pwrctrl.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2010-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2010-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -21,28 +21,45 @@
 #include <linux/delay.h>
 #include <linux/msm_adreno_devfreq.h>
 #include <linux/of_device.h>
-#include <linux/thermal.h>
 
 #include "kgsl.h"
 #include "kgsl_pwrscale.h"
 #include "kgsl_device.h"
-#include "kgsl_gmu.h"
 #include "kgsl_trace.h"
+#include <soc/qcom/devfreq_devbw.h>
 
 #define KGSL_PWRFLAGS_POWER_ON 0
 #define KGSL_PWRFLAGS_CLK_ON   1
 #define KGSL_PWRFLAGS_AXI_ON   2
 #define KGSL_PWRFLAGS_IRQ_ON   3
-#define KGSL_PWRFLAGS_NAP_OFF  5
+#define KGSL_PWRFLAGS_RETENTION_ON  4
 
 #define UPDATE_BUSY_VAL		1000000
 
+/*
+ * Expected delay for post-interrupt processing on A3xx.
+ * The delay may be longer, gradually increase the delay
+ * to compensate.  If the GPU isn't done by max delay,
+ * it's working on something other than just the final
+ * command sequence so stop waiting for it to be idle.
+ */
+#define INIT_UDELAY		200
+#define MAX_UDELAY		2000
+
 /* Number of jiffies for a full thermal cycle */
 #define TH_HZ			(HZ/5)
 
 #define KGSL_MAX_BUSLEVELS	20
 
 #define DEFAULT_BUS_P 25
+#define DEFAULT_BUS_DIV (100 / DEFAULT_BUS_P)
+
+/*
+ * The effective duration of qos request in usecs. After
+ * timeout, qos request is cancelled automatically.
+ * Kept 80ms default, inline with default GPU idle time.
+ */
+#define KGSL_L2PC_CPU_TIMEOUT	(80 * 1000)
 
 /* Order deeply matters here because reasons. New entries go on the end */
 static const char * const clocks[] = {
@@ -58,13 +75,10 @@ static const char * const clocks[] = {
 	"gtcu_iface_clk",
 	"alwayson_clk",
 	"isense_clk",
-	"rbcpr_clk",
-	"iref_clk",
-	"gmu_clk",
-	"ahb_clk"
+	"rbcpr_clk"
 };
 
-static unsigned long ib_votes[KGSL_MAX_BUSLEVELS];
+static unsigned int ib_votes[KGSL_MAX_BUSLEVELS];
 static int last_vote_buslevel;
 static int max_vote_buslevel;
 
@@ -76,13 +90,7 @@ static void kgsl_pwrctrl_set_state(struct kgsl_device *device,
 				unsigned int state);
 static void kgsl_pwrctrl_request_state(struct kgsl_device *device,
 				unsigned int state);
-static int _isense_clk_set_rate(struct kgsl_pwrctrl *pwr, int level);
-static int kgsl_pwrctrl_clk_set_rate(struct clk *grp_clk, unsigned int freq,
-				const char *name);
-static void _gpu_clk_prepare_enable(struct kgsl_device *device,
-				struct clk *clk, const char *name);
-static void _bimc_clk_prepare_enable(struct kgsl_device *device,
-				struct clk *clk, const char *name);
+static void kgsl_pwrctrl_retention_clk(struct kgsl_device *device, int state);
 
 /**
  * _record_pwrevent() - Record the history of the new event
@@ -98,7 +106,6 @@ static void _record_pwrevent(struct kgsl_device *device,
 	struct kgsl_pwrscale *psc = &device->pwrscale;
 	struct kgsl_pwr_history *history = &psc->history[event];
 	int i = history->index;
-
 	if (history->events == NULL)
 		return;
 	history->events[i].duration = ktime_us_delta(t,
@@ -121,17 +128,13 @@ static void _record_pwrevent(struct kgsl_device *device,
 	}
 }
 
-#ifdef CONFIG_DEVFREQ_GOV_QCOM_GPUBW_MON
-#include <soc/qcom/devfreq_devbw.h>
-
 /**
  * kgsl_get_bw() - Return latest msm bus IB vote
  */
-static unsigned long kgsl_get_bw(void)
+static unsigned int kgsl_get_bw(void)
 {
 	return ib_votes[last_vote_buslevel];
 }
-#endif
 
 /**
  * _ab_buslevel_update() - Return latest msm bus AB vote
@@ -141,9 +144,8 @@ static unsigned long kgsl_get_bw(void)
 static void _ab_buslevel_update(struct kgsl_pwrctrl *pwr,
 				unsigned long *ab)
 {
-	unsigned long ib = ib_votes[last_vote_buslevel];
-	unsigned long max_bw = ib_votes[max_vote_buslevel];
-
+	unsigned int ib = ib_votes[last_vote_buslevel];
+	unsigned int max_bw = ib_votes[max_vote_buslevel];
 	if (!ab)
 		return;
 	if (ib == 0)
@@ -154,6 +156,9 @@ static void _ab_buslevel_update(struct kgsl_pwrctrl *pwr,
 		*ab = pwr->bus_ab_mbytes;
 	else
 		*ab = (pwr->bus_percent_ab * max_bw) / 100;
+
+	if (*ab > ib)
+		*ab = ib;
 }
 
 /**
@@ -171,18 +176,19 @@ static unsigned int _adjust_pwrlevel(struct kgsl_pwrctrl *pwr, int level,
 					int popp)
 {
 	unsigned int max_pwrlevel = max_t(unsigned int, pwr->thermal_pwrlevel,
-					pwr->max_pwrlevel);
-	unsigned int min_pwrlevel = min_t(unsigned int,
-					pwr->thermal_pwrlevel_floor,
-					pwr->min_pwrlevel);
+		pwr->max_pwrlevel);
+	unsigned int min_pwrlevel = max_t(unsigned int, pwr->thermal_pwrlevel,
+		pwr->min_pwrlevel);
 
 	switch (pwrc->type) {
 	case KGSL_CONSTRAINT_PWRLEVEL: {
 		switch (pwrc->sub_type) {
 		case KGSL_CONSTRAINT_PWR_MAX:
 			return max_pwrlevel;
+			break;
 		case KGSL_CONSTRAINT_PWR_MIN:
 			return min_pwrlevel;
+			break;
 		default:
 			break;
 		}
@@ -201,81 +207,6 @@ static unsigned int _adjust_pwrlevel(struct kgsl_pwrctrl *pwr, int level,
 	return level;
 }
 
-#ifdef CONFIG_DEVFREQ_GOV_QCOM_GPUBW_MON
-static void kgsl_pwrctrl_vbif_update(unsigned long ab)
-{
-	/* ask a governor to vote on behalf of us */
-	devfreq_vbif_update_bw(ib_votes[last_vote_buslevel], ab);
-}
-#else
-static void kgsl_pwrctrl_vbif_update(unsigned long ab)
-{
-}
-#endif
-
-/**
- * kgsl_bus_scale_request() - set GPU BW vote
- * @device: Pointer to the kgsl_device struct
- * @buslevel: index of bw vector[] table
- */
-static int kgsl_bus_scale_request(struct kgsl_device *device,
-		unsigned int buslevel)
-{
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	int ret = 0;
-
-	if (pwr->pcl) {
-		/* Linux bus driver scales BW */
-		ret = msm_bus_scale_client_update_request(pwr->pcl, buslevel);
-	}
-
-	if (ret)
-		KGSL_PWR_ERR(device, "GPU BW scaling failure: %d\n", ret);
-
-	return ret;
-}
-
-/**
- * kgsl_clk_set_rate() - set GPU clock rate
- * @device: Pointer to the kgsl_device struct
- * @pwrlevel: power level in pwrlevels[] table
- */
-int kgsl_clk_set_rate(struct kgsl_device *device,
-		unsigned int pwrlevel)
-{
-	struct gmu_device *gmu = &device->gmu;
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	struct kgsl_pwrlevel *pl = &pwr->pwrlevels[pwrlevel];
-	int ret = 0;
-
-	/* GMU scales GPU freq */
-	if (kgsl_gmu_isenabled(device)) {
-		/* If GMU has not been started, save it */
-		if (!test_bit(GMU_HFI_ON, &gmu->flags)) {
-			/* store clock change request */
-			set_bit(GMU_DCVS_REPLAY, &gmu->flags);
-			return 0;
-		}
-
-		/* If the GMU is on we cannot vote for the lowest level */
-		if (pwrlevel == (gmu->num_gpupwrlevels - 1)) {
-			WARN(1, "Cannot set 0 GPU frequency with GMU\n");
-			return -EINVAL;
-		}
-		ret = gmu_dcvs_set(gmu, pwrlevel, INVALID_DCVS_IDX);
-		/* indicate actual clock  change */
-		clear_bit(GMU_DCVS_REPLAY, &gmu->flags);
-	} else
-		/* Linux clock driver scales GPU freq */
-		ret = kgsl_pwrctrl_clk_set_rate(pwr->grp_clks[0],
-			pl->gpu_freq, clocks[0]);
-
-	if (ret)
-		KGSL_PWR_ERR(device, "GPU clk freq set failure: %d\n", ret);
-
-	return ret;
-}
-
 /**
  * kgsl_pwrctrl_buslevel_update() - Recalculate the bus vote and send it
  * @device: Pointer to the kgsl_device struct
@@ -320,9 +251,13 @@ void kgsl_pwrctrl_buslevel_update(struct kgsl_device *device,
 		msm_bus_scale_client_update_request(pwr->ocmem_pcl,
 			on ? pwr->active_pwrlevel : pwr->num_pwrlevels - 1);
 
-	kgsl_bus_scale_request(device, buslevel);
+	/* vote for bus if gpubw-dev support is not enabled */
+	if (pwr->pcl)
+		msm_bus_scale_client_update_request(pwr->pcl, buslevel);
 
-	kgsl_pwrctrl_vbif_update(ab);
+	/* ask a governor to vote on behalf of us */
+	if (pwr->devbw)
+		devfreq_vbif_update_bw(ib_votes[last_vote_buslevel], ab);
 }
 EXPORT_SYMBOL(kgsl_pwrctrl_buslevel_update);
 
@@ -352,14 +287,12 @@ static void kgsl_pwrctrl_pwrlevel_change_settings(struct kgsl_device *device,
 
 /**
  * kgsl_pwrctrl_set_thermal_cycle() - set the thermal cycle if required
- * @device: Pointer to the kgsl_device struct
+ * @pwr: Pointer to the kgsl_pwrctrl struct
  * @new_level: the level to transition to
  */
-void kgsl_pwrctrl_set_thermal_cycle(struct kgsl_device *device,
+static void kgsl_pwrctrl_set_thermal_cycle(struct kgsl_pwrctrl *pwr,
 						unsigned int new_level)
 {
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-
 	if ((new_level != pwr->thermal_pwrlevel) || !pwr->sysfs_pwr_limit)
 		return;
 	if (pwr->thermal_pwrlevel == pwr->sysfs_pwr_limit->level) {
@@ -380,15 +313,21 @@ void kgsl_pwrctrl_set_thermal_cycle(struct kgsl_device *device,
 }
 
 /**
- * kgsl_pwrctrl_adjust_pwrlevel() - Adjust the power level if
- * required by thermal, max/min, constraints, etc
+ * kgsl_pwrctrl_pwrlevel_change() - Validate and change power levels
  * @device: Pointer to the kgsl_device struct
  * @new_level: Requested powerlevel, an index into the pwrlevel array
+ *
+ * Check that any power level constraints are still valid.  Update the
+ * requested level according to any thermal, max/min, or power constraints.
+ * If a new GPU level is going to be set, update the bus to that level's
+ * default value.  Do not change the bus if a constraint keeps the new
+ * level at the current level.  Set the new GPU frequency.
  */
-unsigned int kgsl_pwrctrl_adjust_pwrlevel(struct kgsl_device *device,
+void kgsl_pwrctrl_pwrlevel_change(struct kgsl_device *device,
 				unsigned int new_level)
 {
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
+	struct kgsl_pwrlevel *pwrlevel;
 	unsigned int old_level = pwr->active_pwrlevel;
 
 	/* If a pwr constraint is expired, remove it */
@@ -406,42 +345,18 @@ unsigned int kgsl_pwrctrl_adjust_pwrlevel(struct kgsl_device *device,
 	 * Adjust the power level if required by thermal, max/min,
 	 * constraints, etc
 	 */
-	return _adjust_pwrlevel(pwr, new_level, &pwr->constraint,
+	new_level = _adjust_pwrlevel(pwr, new_level, &pwr->constraint,
 					device->pwrscale.popp_level);
-}
-
-/**
- * kgsl_pwrctrl_pwrlevel_change() - Validate and change power levels
- * @device: Pointer to the kgsl_device struct
- * @new_level: Requested powerlevel, an index into the pwrlevel array
- *
- * Check that any power level constraints are still valid.  Update the
- * requested level according to any thermal, max/min, or power constraints.
- * If a new GPU level is going to be set, update the bus to that level's
- * default value.  Do not change the bus if a constraint keeps the new
- * level at the current level.  Set the new GPU frequency.
- */
-void kgsl_pwrctrl_pwrlevel_change(struct kgsl_device *device,
-				unsigned int new_level)
-{
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	struct kgsl_pwrlevel *pwrlevel;
-	unsigned int old_level = pwr->active_pwrlevel;
-
-	new_level = kgsl_pwrctrl_adjust_pwrlevel(device, new_level);
 
 	/*
 	 * If thermal cycling is required and the new level hits the
 	 * thermal limit, kick off the cycling.
 	 */
-	kgsl_pwrctrl_set_thermal_cycle(device, new_level);
+	kgsl_pwrctrl_set_thermal_cycle(pwr, new_level);
 
-	if (new_level == old_level &&
-		!test_bit(GMU_DCVS_REPLAY, &device->gmu.flags))
+	if (new_level == old_level)
 		return;
 
-	kgsl_pwrscale_update_stats(device);
-
 	/*
 	 * Set the active and previous powerlevel first in case the clocks are
 	 * off - if we don't do this then the pwrlevel change won't take effect
@@ -467,9 +382,7 @@ void kgsl_pwrctrl_pwrlevel_change(struct kgsl_device *device,
 	pwrlevel = &pwr->pwrlevels[pwr->active_pwrlevel];
 	/* Change register settings if any  BEFORE pwrlevel change*/
 	kgsl_pwrctrl_pwrlevel_change_settings(device, 0);
-	kgsl_clk_set_rate(device, pwr->active_pwrlevel);
-	_isense_clk_set_rate(pwr, pwr->active_pwrlevel);
-
+	clk_set_rate(pwr->grp_clks[0], pwrlevel->gpu_freq);
 	trace_kgsl_pwrlevel(device,
 			pwr->active_pwrlevel, pwrlevel->gpu_freq,
 			pwr->previous_pwrlevel,
@@ -483,19 +396,16 @@ void kgsl_pwrctrl_pwrlevel_change(struct kgsl_device *device,
 	 * in target device tree.
 	 */
 	if (pwr->gpu_bimc_int_clk) {
-		if (pwr->active_pwrlevel == 0 &&
-				!pwr->gpu_bimc_interface_enabled) {
-			kgsl_pwrctrl_clk_set_rate(pwr->gpu_bimc_int_clk,
-					pwr->gpu_bimc_int_clk_freq,
-					"bimc_gpu_clk");
-			_bimc_clk_prepare_enable(device,
-					pwr->gpu_bimc_int_clk,
-					"bimc_gpu_clk");
-			pwr->gpu_bimc_interface_enabled = 1;
-		} else if (pwr->previous_pwrlevel == 0
-				&& pwr->gpu_bimc_interface_enabled) {
-			clk_disable_unprepare(pwr->gpu_bimc_int_clk);
-			pwr->gpu_bimc_interface_enabled = 0;
+			if (pwr->active_pwrlevel == 0 &&
+					!pwr->gpu_bimc_interface_enabled) {
+				clk_set_rate(pwr->gpu_bimc_int_clk,
+						pwr->gpu_bimc_int_clk_freq);
+				clk_prepare_enable(pwr->gpu_bimc_int_clk);
+				pwr->gpu_bimc_interface_enabled = 1;
+			} else if (pwr->previous_pwrlevel == 0
+					&& pwr->gpu_bimc_interface_enabled) {
+				clk_disable_unprepare(pwr->gpu_bimc_int_clk);
+				pwr->gpu_bimc_interface_enabled = 0;
 		}
 	}
 
@@ -547,8 +457,9 @@ void kgsl_pwrctrl_set_constraint(struct kgsl_device *device,
 		trace_kgsl_constraint(device, pwrc_old->type, constraint, 1);
 	} else if ((pwrc_old->type == pwrc->type) &&
 		(pwrc_old->hint.pwrlevel.level == constraint)) {
-		pwrc_old->owner_id = id;
-		pwrc_old->expires = jiffies + device->pwrctrl.interval_timeout;
+			pwrc_old->owner_id = id;
+			pwrc_old->expires = jiffies +
+					device->pwrctrl.interval_timeout;
 	}
 }
 EXPORT_SYMBOL(kgsl_pwrctrl_set_constraint);
@@ -556,14 +467,12 @@ EXPORT_SYMBOL(kgsl_pwrctrl_set_constraint);
 /**
  * kgsl_pwrctrl_update_l2pc() - Update existing qos request
  * @device: Pointer to the kgsl_device struct
- * @timeout_us: the effective duration of qos request in usecs.
  *
  * Updates an existing qos request to avoid L2PC on the
  * CPUs (which are selected through dtsi) on which GPU
  * thread is running. This would help for performance.
  */
-void kgsl_pwrctrl_update_l2pc(struct kgsl_device *device,
-			unsigned long timeout_us)
+void kgsl_pwrctrl_update_l2pc(struct kgsl_device *device)
 {
 	int cpu;
 
@@ -576,8 +485,8 @@ void kgsl_pwrctrl_update_l2pc(struct kgsl_device *device,
 	if ((1 << cpu) & device->pwrctrl.l2pc_cpus_mask) {
 		pm_qos_update_request_timeout(
 				&device->pwrctrl.l2pc_cpus_qos,
-				device->pwrctrl.pm_qos_cpu_mask_latency,
-				timeout_us);
+				device->pwrctrl.pm_qos_active_latency,
+				KGSL_L2PC_CPU_TIMEOUT);
 	}
 }
 EXPORT_SYMBOL(kgsl_pwrctrl_update_l2pc);
@@ -622,7 +531,6 @@ static ssize_t kgsl_pwrctrl_thermal_pwrlevel_show(struct device *dev,
 
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
 	struct kgsl_pwrctrl *pwr;
-
 	if (device == NULL)
 		return 0;
 	pwr = &device->pwrctrl;
@@ -669,17 +577,28 @@ static ssize_t kgsl_pwrctrl_max_pwrlevel_show(struct device *dev,
 
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
 	struct kgsl_pwrctrl *pwr;
-
 	if (device == NULL)
 		return 0;
 	pwr = &device->pwrctrl;
 	return snprintf(buf, PAGE_SIZE, "%u\n", pwr->max_pwrlevel);
 }
 
-static void kgsl_pwrctrl_min_pwrlevel_set(struct kgsl_device *device,
-					int level)
-{
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
+static ssize_t kgsl_pwrctrl_min_pwrlevel_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t count)
+{	struct kgsl_device *device = kgsl_device_from_dev(dev);
+	struct kgsl_pwrctrl *pwr;
+	int ret;
+	unsigned int level = 0;
+
+	if (device == NULL)
+		return 0;
+
+	pwr = &device->pwrctrl;
+
+	ret = kgsl_sysfs_store(buf, &level);
+	if (ret)
+		return ret;
 
 	mutex_lock(&device->mutex);
 	if (level > pwr->num_pwrlevels - 2)
@@ -695,24 +614,6 @@ static void kgsl_pwrctrl_min_pwrlevel_set(struct kgsl_device *device,
 	kgsl_pwrctrl_pwrlevel_change(device, pwr->active_pwrlevel);
 
 	mutex_unlock(&device->mutex);
-}
-
-static ssize_t kgsl_pwrctrl_min_pwrlevel_store(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	int ret;
-	unsigned int level = 0;
-
-	if (device == NULL)
-		return 0;
-
-	ret = kgsl_sysfs_store(buf, &level);
-	if (ret)
-		return ret;
-
-	kgsl_pwrctrl_min_pwrlevel_set(device, level);
 
 	return count;
 }
@@ -723,7 +624,6 @@ static ssize_t kgsl_pwrctrl_min_pwrlevel_show(struct device *dev,
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
 	struct kgsl_pwrctrl *pwr;
-
 	if (device == NULL)
 		return 0;
 	pwr = &device->pwrctrl;
@@ -737,7 +637,6 @@ static ssize_t kgsl_pwrctrl_num_pwrlevels_show(struct device *dev,
 
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
 	struct kgsl_pwrctrl *pwr;
-
 	if (device == NULL)
 		return 0;
 	pwr = &device->pwrctrl;
@@ -750,7 +649,7 @@ static int _get_nearest_pwrlevel(struct kgsl_pwrctrl *pwr, unsigned int clock)
 {
 	int i;
 
-	for (i = pwr->num_pwrlevels - 2; i >= 0; i--) {
+	for (i = pwr->num_pwrlevels - 1; i >= 0; i--) {
 		if (abs(pwr->pwrlevels[i].gpu_freq - clock) < 5000000)
 			return i;
 	}
@@ -758,19 +657,29 @@ static int _get_nearest_pwrlevel(struct kgsl_pwrctrl *pwr, unsigned int clock)
 	return -ERANGE;
 }
 
-static void kgsl_pwrctrl_max_clock_set(struct kgsl_device *device, int val)
+static ssize_t kgsl_pwrctrl_max_gpuclk_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t count)
 {
+	struct kgsl_device *device = kgsl_device_from_dev(dev);
 	struct kgsl_pwrctrl *pwr;
-	int level;
+	unsigned int val = 0;
+	int level, ret;
+
+	if (device == NULL)
+		return 0;
 
 	pwr = &device->pwrctrl;
 
+	ret = kgsl_sysfs_store(buf, &val);
+	if (ret)
+		return ret;
+
 	mutex_lock(&device->mutex);
 	level = _get_nearest_pwrlevel(pwr, val);
 	/* If the requested power level is not supported by hw, try cycling */
 	if (level < 0) {
 		unsigned int hfreq, diff, udiff, i;
-
 		if ((val < pwr->pwrlevels[pwr->num_pwrlevels - 1].gpu_freq) ||
 			(val > pwr->pwrlevels[0].gpu_freq))
 			goto err;
@@ -799,37 +708,21 @@ static void kgsl_pwrctrl_max_clock_set(struct kgsl_device *device, int val)
 	if (pwr->sysfs_pwr_limit)
 		kgsl_pwr_limits_set_freq(pwr->sysfs_pwr_limit,
 					pwr->pwrlevels[level].gpu_freq);
-	return;
+	return count;
 
 err:
 	mutex_unlock(&device->mutex);
-}
-
-static ssize_t kgsl_pwrctrl_max_gpuclk_store(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	unsigned int val = 0;
-	int ret;
-
-	if (device == NULL)
-		return 0;
-
-	ret = kgsl_sysfs_store(buf, &val);
-	if (ret)
-		return ret;
-
-	kgsl_pwrctrl_max_clock_set(device, val);
-
 	return count;
 }
 
-static unsigned int kgsl_pwrctrl_max_clock_get(struct kgsl_device *device)
+static ssize_t kgsl_pwrctrl_max_gpuclk_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
 {
+
+	struct kgsl_device *device = kgsl_device_from_dev(dev);
 	struct kgsl_pwrctrl *pwr;
 	unsigned int freq;
-
 	if (device == NULL)
 		return 0;
 	pwr = &device->pwrctrl;
@@ -843,17 +736,7 @@ static unsigned int kgsl_pwrctrl_max_clock_get(struct kgsl_device *device)
 			(TH_HZ - pwr->thermal_timeout) * (hfreq / TH_HZ);
 	}
 
-	return freq;
-}
-
-static ssize_t kgsl_pwrctrl_max_gpuclk_show(struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
-	return snprintf(buf, PAGE_SIZE, "%d\n",
-		kgsl_pwrctrl_max_clock_get(device));
+	return snprintf(buf, PAGE_SIZE, "%d\n", freq);
 }
 
 static ssize_t kgsl_pwrctrl_gpuclk_store(struct device *dev,
@@ -889,7 +772,6 @@ static ssize_t kgsl_pwrctrl_gpuclk_show(struct device *dev,
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
 	struct kgsl_pwrctrl *pwr;
-
 	if (device == NULL)
 		return 0;
 	pwr = &device->pwrctrl;
@@ -924,6 +806,8 @@ static ssize_t __timer_store(struct device *dev, struct device_attribute *attr,
 	/* Let the timeout be requested in ms, but convert to jiffies. */
 	if (timer == KGSL_PWR_IDLE_TIMER)
 		device->pwrctrl.interval_timeout = msecs_to_jiffies(val);
+	else if (timer == KGSL_PWR_DEEP_NAP_TIMER)
+		device->pwrctrl.deep_nap_timeout = msecs_to_jiffies(val);
 
 	mutex_unlock(&device->mutex);
 
@@ -942,7 +826,6 @@ static ssize_t kgsl_pwrctrl_idle_timer_show(struct device *dev,
 					char *buf)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	if (device == NULL)
 		return 0;
 	/* Show the idle_timeout converted to msec */
@@ -950,6 +833,27 @@ static ssize_t kgsl_pwrctrl_idle_timer_show(struct device *dev,
 		jiffies_to_msecs(device->pwrctrl.interval_timeout));
 }
 
+static ssize_t kgsl_pwrctrl_deep_nap_timer_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t count)
+{
+
+	return __timer_store(dev, attr, buf, count, KGSL_PWR_DEEP_NAP_TIMER);
+}
+
+static ssize_t kgsl_pwrctrl_deep_nap_timer_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct kgsl_device *device = kgsl_device_from_dev(dev);
+
+	if (device == NULL)
+		return 0;
+	/* Show the idle_timeout converted to msec */
+	return snprintf(buf, PAGE_SIZE, "%u\n",
+		jiffies_to_msecs(device->pwrctrl.deep_nap_timeout));
+}
+
 static ssize_t kgsl_pwrctrl_pmqos_active_latency_store(struct device *dev,
 					struct device_attribute *attr,
 					const char *buf, size_t count)
@@ -977,7 +881,6 @@ static ssize_t kgsl_pwrctrl_pmqos_active_latency_show(struct device *dev,
 					   char *buf)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	if (device == NULL)
 		return 0;
 	return snprintf(buf, PAGE_SIZE, "%d\n",
@@ -1016,40 +919,10 @@ static ssize_t kgsl_pwrctrl_gpu_available_frequencies_show(
 	if (device == NULL)
 		return 0;
 	pwr = &device->pwrctrl;
-	for (index = 0; index < pwr->num_pwrlevels - 1; index++) {
-		num_chars += scnprintf(buf + num_chars,
-			PAGE_SIZE - num_chars - 1,
-			"%d ", pwr->pwrlevels[index].gpu_freq);
-		/* One space for trailing null and another for the newline */
-		if (num_chars >= PAGE_SIZE - 2)
-			break;
-	}
-	buf[num_chars++] = '\n';
-	return num_chars;
-}
-
-static ssize_t kgsl_pwrctrl_gpu_clock_stats_show(
-					struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	struct kgsl_pwrctrl *pwr;
-	int index, num_chars = 0;
-
-	if (device == NULL)
-		return 0;
-	pwr = &device->pwrctrl;
-	mutex_lock(&device->mutex);
-	kgsl_pwrscale_update_stats(device);
-	mutex_unlock(&device->mutex);
 	for (index = 0; index < pwr->num_pwrlevels - 1; index++)
-		num_chars += snprintf(buf + num_chars, PAGE_SIZE - num_chars,
-			"%llu ", pwr->clock_times[index]);
-
-	if (num_chars < PAGE_SIZE)
-		buf[num_chars++] = '\n';
-
+		num_chars += snprintf(buf + num_chars, PAGE_SIZE, "%d ",
+		pwr->pwrlevels[index].gpu_freq);
+	buf[num_chars++] = '\n';
 	return num_chars;
 }
 
@@ -1058,7 +931,6 @@ static ssize_t kgsl_pwrctrl_reset_count_show(struct device *dev,
 					char *buf)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	if (device == NULL)
 		return 0;
 	return snprintf(buf, PAGE_SIZE, "%d\n", device->reset_counter);
@@ -1069,8 +941,6 @@ static void __force_on(struct kgsl_device *device, int flag, int on)
 	if (on) {
 		switch (flag) {
 		case KGSL_PWRFLAGS_CLK_ON:
-			/* make sure pwrrail is ON before enabling clocks */
-			kgsl_pwrctrl_pwrrail(device, KGSL_PWRFLAGS_ON);
 			kgsl_pwrctrl_clk(device, KGSL_PWRFLAGS_ON,
 				KGSL_STATE_ACTIVE);
 			break;
@@ -1080,6 +950,9 @@ static void __force_on(struct kgsl_device *device, int flag, int on)
 		case KGSL_PWRFLAGS_POWER_ON:
 			kgsl_pwrctrl_pwrrail(device, KGSL_PWRFLAGS_ON);
 			break;
+		case KGSL_PWRFLAGS_RETENTION_ON:
+			kgsl_pwrctrl_retention_clk(device, KGSL_PWRFLAGS_ON);
+			break;
 		}
 		set_bit(flag, &device->pwrctrl.ctrl_flags);
 	} else {
@@ -1092,7 +965,6 @@ static ssize_t __force_on_show(struct device *dev,
 					char *buf, int flag)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	if (device == NULL)
 		return 0;
 	return snprintf(buf, PAGE_SIZE, "%d\n",
@@ -1164,19 +1036,19 @@ static ssize_t kgsl_pwrctrl_force_rail_on_store(struct device *dev,
 	return __force_on_store(dev, attr, buf, count, KGSL_PWRFLAGS_POWER_ON);
 }
 
-static ssize_t kgsl_pwrctrl_force_no_nap_show(struct device *dev,
+static ssize_t kgsl_pwrctrl_force_non_retention_on_show(struct device *dev,
 					struct device_attribute *attr,
 					char *buf)
 {
-	return __force_on_show(dev, attr, buf, KGSL_PWRFLAGS_NAP_OFF);
+	return __force_on_show(dev, attr, buf, KGSL_PWRFLAGS_RETENTION_ON);
 }
 
-static ssize_t kgsl_pwrctrl_force_no_nap_store(struct device *dev,
+static ssize_t kgsl_pwrctrl_force_non_retention_on_store(struct device *dev,
 					struct device_attribute *attr,
 					const char *buf, size_t count)
 {
 	return __force_on_store(dev, attr, buf, count,
-					KGSL_PWRFLAGS_NAP_OFF);
+					KGSL_PWRFLAGS_RETENTION_ON);
 }
 
 static ssize_t kgsl_pwrctrl_bus_split_show(struct device *dev,
@@ -1184,7 +1056,6 @@ static ssize_t kgsl_pwrctrl_bus_split_show(struct device *dev,
 					char *buf)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	if (device == NULL)
 		return 0;
 	return snprintf(buf, PAGE_SIZE, "%d\n",
@@ -1218,7 +1089,6 @@ static ssize_t kgsl_pwrctrl_default_pwrlevel_show(struct device *dev,
 					char *buf)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	if (device == NULL)
 		return 0;
 	return snprintf(buf, PAGE_SIZE, "%d\n",
@@ -1289,259 +1159,25 @@ static ssize_t kgsl_popp_show(struct device *dev,
 					   char *buf)
 {
 	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
 	if (device == NULL)
 		return 0;
 	return snprintf(buf, PAGE_SIZE, "%d\n",
 		test_bit(POPP_ON, &device->pwrscale.popp_state));
 }
 
-static ssize_t kgsl_pwrctrl_gpu_model_show(struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	char model_str[32] = {0};
-
-	if (device == NULL)
-		return 0;
-
-	device->ftbl->gpu_model(device, model_str, sizeof(model_str));
-
-	return snprintf(buf, PAGE_SIZE, "%s\n", model_str);
-}
-
-static ssize_t kgsl_pwrctrl_gpu_busy_percentage_show(struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	int ret;
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	struct kgsl_clk_stats *stats;
-	unsigned int busy_percent = 0;
-
-	if (device == NULL)
-		return 0;
-	stats = &device->pwrctrl.clk_stats;
-
-	if (stats->total_old != 0)
-		busy_percent = (stats->busy_old * 100) / stats->total_old;
-
-	ret = snprintf(buf, PAGE_SIZE, "%d %%\n", busy_percent);
-
-	/* Reset the stats if GPU is OFF */
-	if (!test_bit(KGSL_PWRFLAGS_AXI_ON, &device->pwrctrl.power_flags)) {
-		stats->busy_old = 0;
-		stats->total_old = 0;
-	}
-	return ret;
-}
-
-static ssize_t kgsl_pwrctrl_min_clock_mhz_show(struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	struct kgsl_pwrctrl *pwr;
-
-	if (device == NULL)
-		return 0;
-	pwr = &device->pwrctrl;
-
-	return snprintf(buf, PAGE_SIZE, "%d\n",
-			pwr->pwrlevels[pwr->min_pwrlevel].gpu_freq / 1000000);
-}
-
-static ssize_t kgsl_pwrctrl_min_clock_mhz_store(struct device *dev,
-					struct device_attribute *attr,
-					const char *buf, size_t count)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	int level, ret;
-	unsigned int freq;
-	struct kgsl_pwrctrl *pwr;
-
-	if (device == NULL)
-		return 0;
-
-	pwr = &device->pwrctrl;
-
-	ret = kgsl_sysfs_store(buf, &freq);
-	if (ret)
-		return ret;
-
-	freq *= 1000000;
-	level = _get_nearest_pwrlevel(pwr, freq);
-
-	if (level >= 0)
-		kgsl_pwrctrl_min_pwrlevel_set(device, level);
-
-	return count;
-}
-
-static ssize_t kgsl_pwrctrl_max_clock_mhz_show(struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	unsigned int freq;
-
-	if (device == NULL)
-		return 0;
-
-	freq = kgsl_pwrctrl_max_clock_get(device);
-
-	return snprintf(buf, PAGE_SIZE, "%d\n", freq / 1000000);
-}
-
-static ssize_t kgsl_pwrctrl_max_clock_mhz_store(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	unsigned int val = 0;
-	int ret;
-
-	if (device == NULL)
-		return 0;
-
-	ret = kgsl_sysfs_store(buf, &val);
-	if (ret)
-		return ret;
-
-	val *= 1000000;
-	kgsl_pwrctrl_max_clock_set(device, val);
-
-	return count;
-}
-
-static ssize_t kgsl_pwrctrl_clock_mhz_show(struct device *dev,
-				    struct device_attribute *attr,
-				    char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-
-	if (device == NULL)
-		return 0;
-
-	return snprintf(buf, PAGE_SIZE, "%ld\n",
-			kgsl_pwrctrl_active_freq(&device->pwrctrl) / 1000000);
-}
-
-static ssize_t kgsl_pwrctrl_freq_table_mhz_show(
-					struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	struct kgsl_pwrctrl *pwr;
-	int index, num_chars = 0;
-
-	if (device == NULL)
-		return 0;
-
-	pwr = &device->pwrctrl;
-	for (index = 0; index < pwr->num_pwrlevels - 1; index++) {
-		num_chars += scnprintf(buf + num_chars,
-			PAGE_SIZE - num_chars - 1,
-			"%d ", pwr->pwrlevels[index].gpu_freq / 1000000);
-		/* One space for trailing null and another for the newline */
-		if (num_chars >= PAGE_SIZE - 2)
-			break;
-	}
-
-	buf[num_chars++] = '\n';
-
-	return num_chars;
-}
-
-static ssize_t kgsl_pwrctrl_temp_show(struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	struct kgsl_pwrctrl *pwr;
-	struct thermal_zone_device *thermal_dev;
-	int ret, temperature = 0;
-
-	if (device == NULL)
-		goto done;
-
-	pwr = &device->pwrctrl;
-
-	if (!pwr->tzone_name)
-		goto done;
-
-	thermal_dev = thermal_zone_get_zone_by_name((char *)pwr->tzone_name);
-	if (thermal_dev == NULL)
-		goto done;
-
-	ret = thermal_zone_get_temp(thermal_dev, &temperature);
-	if (ret)
-		goto done;
-
-	return snprintf(buf, PAGE_SIZE, "%d\n",
-			temperature);
-done:
-	return 0;
-}
-
-static ssize_t kgsl_pwrctrl_pwrscale_store(struct device *dev,
-					   struct device_attribute *attr,
-					   const char *buf, size_t count)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	int ret;
-	unsigned int enable = 0;
-
-	if (device == NULL)
-		return 0;
-
-	ret = kgsl_sysfs_store(buf, &enable);
-	if (ret)
-		return ret;
-
-	mutex_lock(&device->mutex);
-
-	if (enable)
-		kgsl_pwrscale_enable(device);
-	else
-		kgsl_pwrscale_disable(device, false);
-
-	mutex_unlock(&device->mutex);
-
-	return count;
-}
-
-static ssize_t kgsl_pwrctrl_pwrscale_show(struct device *dev,
-					  struct device_attribute *attr,
-					  char *buf)
-{
-	struct kgsl_device *device = kgsl_device_from_dev(dev);
-	struct kgsl_pwrscale *psc;
-
-	if (device == NULL)
-		return 0;
-	psc = &device->pwrscale;
-
-	return snprintf(buf, PAGE_SIZE, "%u\n", psc->enabled);
-}
-
-static DEVICE_ATTR(temp, 0444, kgsl_pwrctrl_temp_show, NULL);
 static DEVICE_ATTR(gpuclk, 0644, kgsl_pwrctrl_gpuclk_show,
 	kgsl_pwrctrl_gpuclk_store);
 static DEVICE_ATTR(max_gpuclk, 0644, kgsl_pwrctrl_max_gpuclk_show,
 	kgsl_pwrctrl_max_gpuclk_store);
 static DEVICE_ATTR(idle_timer, 0644, kgsl_pwrctrl_idle_timer_show,
 	kgsl_pwrctrl_idle_timer_store);
+static DEVICE_ATTR(deep_nap_timer, 0644, kgsl_pwrctrl_deep_nap_timer_show,
+	kgsl_pwrctrl_deep_nap_timer_store);
 static DEVICE_ATTR(gpubusy, 0444, kgsl_pwrctrl_gpubusy_show,
 	NULL);
 static DEVICE_ATTR(gpu_available_frequencies, 0444,
 	kgsl_pwrctrl_gpu_available_frequencies_show,
 	NULL);
-static DEVICE_ATTR(gpu_clock_stats, 0444,
-	kgsl_pwrctrl_gpu_clock_stats_show,
-	NULL);
 static DEVICE_ATTR(max_pwrlevel, 0644,
 	kgsl_pwrctrl_max_pwrlevel_show,
 	kgsl_pwrctrl_max_pwrlevel_store);
@@ -1576,30 +1212,17 @@ static DEVICE_ATTR(default_pwrlevel, 0644,
 	kgsl_pwrctrl_default_pwrlevel_show,
 	kgsl_pwrctrl_default_pwrlevel_store);
 static DEVICE_ATTR(popp, 0644, kgsl_popp_show, kgsl_popp_store);
-static DEVICE_ATTR(force_no_nap, 0644,
-	kgsl_pwrctrl_force_no_nap_show,
-	kgsl_pwrctrl_force_no_nap_store);
-static DEVICE_ATTR(gpu_model, 0444, kgsl_pwrctrl_gpu_model_show, NULL);
-static DEVICE_ATTR(gpu_busy_percentage, 0444,
-	kgsl_pwrctrl_gpu_busy_percentage_show, NULL);
-static DEVICE_ATTR(min_clock_mhz, 0644, kgsl_pwrctrl_min_clock_mhz_show,
-	kgsl_pwrctrl_min_clock_mhz_store);
-static DEVICE_ATTR(max_clock_mhz, 0644, kgsl_pwrctrl_max_clock_mhz_show,
-	kgsl_pwrctrl_max_clock_mhz_store);
-static DEVICE_ATTR(clock_mhz, 0444, kgsl_pwrctrl_clock_mhz_show, NULL);
-static DEVICE_ATTR(freq_table_mhz, 0444,
-	kgsl_pwrctrl_freq_table_mhz_show, NULL);
-static DEVICE_ATTR(pwrscale, 0644,
-	kgsl_pwrctrl_pwrscale_show,
-	kgsl_pwrctrl_pwrscale_store);
+static DEVICE_ATTR(force_non_retention_on, 0644,
+	kgsl_pwrctrl_force_non_retention_on_show,
+	kgsl_pwrctrl_force_non_retention_on_store);
 
 static const struct device_attribute *pwrctrl_attr_list[] = {
 	&dev_attr_gpuclk,
 	&dev_attr_max_gpuclk,
 	&dev_attr_idle_timer,
+	&dev_attr_deep_nap_timer,
 	&dev_attr_gpubusy,
 	&dev_attr_gpu_available_frequencies,
-	&dev_attr_gpu_clock_stats,
 	&dev_attr_max_pwrlevel,
 	&dev_attr_min_pwrlevel,
 	&dev_attr_thermal_pwrlevel,
@@ -1609,55 +1232,16 @@ static const struct device_attribute *pwrctrl_attr_list[] = {
 	&dev_attr_force_clk_on,
 	&dev_attr_force_bus_on,
 	&dev_attr_force_rail_on,
-	&dev_attr_force_no_nap,
+	&dev_attr_force_non_retention_on,
 	&dev_attr_bus_split,
 	&dev_attr_default_pwrlevel,
 	&dev_attr_popp,
-	&dev_attr_gpu_model,
-	&dev_attr_gpu_busy_percentage,
-	&dev_attr_min_clock_mhz,
-	&dev_attr_max_clock_mhz,
-	&dev_attr_clock_mhz,
-	&dev_attr_freq_table_mhz,
-	&dev_attr_temp,
-	&dev_attr_pwrscale,
 	NULL
 };
 
-struct sysfs_link {
-	const char *src;
-	const char *dst;
-};
-
-static struct sysfs_link link_names[] = {
-	{ "gpu_model", "gpu_model",},
-	{ "gpu_busy_percentage", "gpu_busy",},
-	{ "min_clock_mhz", "gpu_min_clock",},
-	{ "max_clock_mhz", "gpu_max_clock",},
-	{ "clock_mhz", "gpu_clock",},
-	{ "freq_table_mhz", "gpu_freq_table",},
-	{ "temp", "gpu_tmu",},
-};
-
 int kgsl_pwrctrl_init_sysfs(struct kgsl_device *device)
 {
-	int i, ret;
-
-	ret = kgsl_create_device_sysfs_files(device->dev, pwrctrl_attr_list);
-	if (ret)
-		return ret;
-
-	device->gpu_sysfs_kobj = kobject_create_and_add("gpu", kernel_kobj);
-	if (IS_ERR_OR_NULL(device->gpu_sysfs_kobj))
-		return (device->gpu_sysfs_kobj == NULL) ?
-		-ENOMEM : PTR_ERR(device->gpu_sysfs_kobj);
-
-	for (i = 0; i < ARRAY_SIZE(link_names); i++)
-		kgsl_gpu_sysfs_add_link(device->gpu_sysfs_kobj,
-			&device->dev->kobj, link_names[i].src,
-			link_names[i].dst);
-
-	return 0;
+	return kgsl_create_device_sysfs_files(device->dev, pwrctrl_attr_list);
 }
 
 void kgsl_pwrctrl_uninit_sysfs(struct kgsl_device *device)
@@ -1665,14 +1249,11 @@ void kgsl_pwrctrl_uninit_sysfs(struct kgsl_device *device)
 	kgsl_remove_device_sysfs_files(device->dev, pwrctrl_attr_list);
 }
 
-/*
- * Track the amount of time the gpu is on vs the total system time.
- * Regularly update the percentage of busy time displayed by sysfs.
- */
+/* Track the amount of time the gpu is on vs the total system time. *
+ * Regularly update the percentage of busy time displayed by sysfs. */
 void kgsl_pwrctrl_busy_time(struct kgsl_device *device, u64 time, u64 busy)
 {
 	struct kgsl_clk_stats *stats = &device->pwrctrl.clk_stats;
-
 	stats->total += time;
 	stats->busy += busy;
 
@@ -1689,14 +1270,60 @@ void kgsl_pwrctrl_busy_time(struct kgsl_device *device, u64 time, u64 busy)
 }
 EXPORT_SYMBOL(kgsl_pwrctrl_busy_time);
 
+static void kgsl_pwrctrl_retention_clk(struct kgsl_device *device, int state)
+{
+	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
+	int i = 0;
+
+	if (!(pwr->gx_retention) || test_bit(KGSL_PWRFLAGS_RETENTION_ON,
+					&device->pwrctrl.ctrl_flags))
+		return;
+
+	if (state == KGSL_PWRFLAGS_OFF) {
+		if (test_and_clear_bit(KGSL_PWRFLAGS_RETENTION_ON,
+			&pwr->power_flags)) {
+			trace_kgsl_retention_clk(device, state);
+			/* prepare the mx clk to avoid RPM transactions*/
+			clk_set_rate(pwr->dummy_mx_clk,
+				pwr->pwrlevels
+				[pwr->active_pwrlevel].
+				gpu_freq);
+			clk_prepare(pwr->dummy_mx_clk);
+			/*
+			 * Unprepare Gfx clocks to put Gfx rail to
+			 * retention voltage.
+			 */
+			for (i = KGSL_MAX_CLKS - 1; i > 0; i--)
+				if (pwr->grp_clks[i])
+					clk_unprepare(pwr->grp_clks[i]);
+		}
+	} else if (state == KGSL_PWRFLAGS_ON) {
+		if (!test_and_set_bit(KGSL_PWRFLAGS_RETENTION_ON,
+					&pwr->power_flags)) {
+			trace_kgsl_retention_clk(device, state);
+			/*
+			 * Prepare Gfx clocks to put Gfx rail out
+			 * of rentention
+			 */
+			for (i = KGSL_MAX_CLKS - 1; i > 0; i--)
+				if (pwr->grp_clks[i])
+					clk_prepare(pwr->grp_clks[i]);
+
+			/* unprepare the dummy mx clk*/
+			clk_unprepare(pwr->dummy_mx_clk);
+			clk_set_rate(pwr->dummy_mx_clk,
+				pwr->pwrlevels[pwr->num_pwrlevels - 1].
+				gpu_freq);
+		}
+	}
+}
+
 static void kgsl_pwrctrl_clk(struct kgsl_device *device, int state,
 					  int requested_state)
 {
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
 	int i = 0;
 
-	if (kgsl_gmu_isenabled(device))
-		return;
 	if (test_bit(KGSL_PWRFLAGS_CLK_ON, &pwr->ctrl_flags))
 		return;
 
@@ -1716,27 +1343,23 @@ static void kgsl_pwrctrl_clk(struct kgsl_device *device, int state,
 				clk_disable(pwr->grp_clks[i]);
 			/* High latency clock maintenance. */
 			if ((pwr->pwrlevels[0].gpu_freq > 0) &&
-				(requested_state != KGSL_STATE_NAP)) {
+				(requested_state != KGSL_STATE_NAP) &&
+				(requested_state !=
+						KGSL_STATE_DEEP_NAP)) {
 				for (i = KGSL_MAX_CLKS - 1; i > 0; i--)
 					clk_unprepare(pwr->grp_clks[i]);
-				kgsl_clk_set_rate(device,
-						pwr->num_pwrlevels - 1);
-				_isense_clk_set_rate(pwr,
-					pwr->num_pwrlevels - 1);
+				clk_set_rate(pwr->grp_clks[0],
+					pwr->pwrlevels[pwr->num_pwrlevels - 1].
+					gpu_freq);
 			}
-
-			/* Turn off the IOMMU clocks */
-			kgsl_mmu_disable_clk(&device->mmu);
-		} else if (requested_state == KGSL_STATE_SLUMBER) {
+		} else if (requested_state == KGSL_STATE_SLEEP) {
 			/* High latency clock maintenance. */
 			for (i = KGSL_MAX_CLKS - 1; i > 0; i--)
 				clk_unprepare(pwr->grp_clks[i]);
-			if ((pwr->pwrlevels[0].gpu_freq > 0)) {
-				kgsl_clk_set_rate(device,
-						pwr->num_pwrlevels - 1);
-				_isense_clk_set_rate(pwr,
-					pwr->num_pwrlevels - 1);
-			}
+			if ((pwr->pwrlevels[0].gpu_freq > 0))
+				clk_set_rate(pwr->grp_clks[0],
+					pwr->pwrlevels[pwr->num_pwrlevels - 1].
+					gpu_freq);
 		}
 	} else if (state == KGSL_PWRFLAGS_ON) {
 		if (!test_and_set_bit(KGSL_PWRFLAGS_CLK_ON,
@@ -1744,63 +1367,35 @@ static void kgsl_pwrctrl_clk(struct kgsl_device *device, int state,
 			trace_kgsl_clk(device, state,
 					kgsl_pwrctrl_active_freq(pwr));
 			/* High latency clock maintenance. */
-			if (device->state != KGSL_STATE_NAP) {
-				if (pwr->pwrlevels[0].gpu_freq > 0) {
-					kgsl_clk_set_rate(device,
-							pwr->active_pwrlevel);
-					_isense_clk_set_rate(pwr,
-						pwr->active_pwrlevel);
-				}
+			if ((device->state != KGSL_STATE_NAP) &&
+			(device->state != KGSL_STATE_DEEP_NAP)) {
+				if (pwr->pwrlevels[0].gpu_freq > 0)
+					clk_set_rate(pwr->grp_clks[0],
+						pwr->pwrlevels
+						[pwr->active_pwrlevel].
+						gpu_freq);
+				for (i = KGSL_MAX_CLKS - 1; i > 0; i--)
+					clk_prepare(pwr->grp_clks[i]);
 			}
-
+			/* as last step, enable grp_clk
+			   this is to let GPU interrupt to come */
 			for (i = KGSL_MAX_CLKS - 1; i > 0; i--)
-				_gpu_clk_prepare_enable(device,
-						pwr->grp_clks[i], clocks[i]);
-
+				clk_enable(pwr->grp_clks[i]);
 			/* Enable the gpu-bimc-interface clocks */
 			if (pwr->gpu_bimc_int_clk) {
 				if (pwr->active_pwrlevel == 0 &&
 					!pwr->gpu_bimc_interface_enabled) {
-					kgsl_pwrctrl_clk_set_rate(
-						pwr->gpu_bimc_int_clk,
-						pwr->gpu_bimc_int_clk_freq,
-						"bimc_gpu_clk");
-					_bimc_clk_prepare_enable(device,
-						pwr->gpu_bimc_int_clk,
-						"bimc_gpu_clk");
+					clk_set_rate(pwr->gpu_bimc_int_clk,
+						pwr->gpu_bimc_int_clk_freq);
+					clk_prepare_enable(
+						pwr->gpu_bimc_int_clk);
 					pwr->gpu_bimc_interface_enabled = 1;
 				}
 			}
-
-			/* Turn on the IOMMU clocks */
-			kgsl_mmu_enable_clk(&device->mmu);
 		}
-
 	}
 }
 
-#ifdef CONFIG_DEVFREQ_GOV_QCOM_GPUBW_MON
-static void kgsl_pwrctrl_suspend_devbw(struct kgsl_pwrctrl *pwr)
-{
-	if (pwr->devbw)
-		devfreq_suspend_devbw(pwr->devbw);
-}
-
-static void kgsl_pwrctrl_resume_devbw(struct kgsl_pwrctrl *pwr)
-{
-	if (pwr->devbw)
-		devfreq_resume_devbw(pwr->devbw);
-}
-#else
-static void kgsl_pwrctrl_suspend_devbw(struct kgsl_pwrctrl *pwr)
-{
-}
-
-static void kgsl_pwrctrl_resume_devbw(struct kgsl_pwrctrl *pwr)
-{
-}
-#endif
-
 static void kgsl_pwrctrl_axi(struct kgsl_device *device, int state)
 {
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
@@ -1814,7 +1409,8 @@ static void kgsl_pwrctrl_axi(struct kgsl_device *device, int state)
 			trace_kgsl_bus(device, state);
 			kgsl_pwrctrl_buslevel_update(device, false);
 
-			kgsl_pwrctrl_suspend_devbw(pwr);
+			if (pwr->devbw)
+				devfreq_suspend_devbw(pwr->devbw);
 		}
 	} else if (state == KGSL_PWRFLAGS_ON) {
 		if (!test_and_set_bit(KGSL_PWRFLAGS_AXI_ON,
@@ -1822,7 +1418,8 @@ static void kgsl_pwrctrl_axi(struct kgsl_device *device, int state)
 			trace_kgsl_bus(device, state);
 			kgsl_pwrctrl_buslevel_update(device, true);
 
-			kgsl_pwrctrl_resume_devbw(pwr);
+			if (pwr->devbw)
+				devfreq_resume_devbw(pwr->devbw);
 		}
 	}
 }
@@ -1871,14 +1468,7 @@ static int kgsl_pwrctrl_pwrrail(struct kgsl_device *device, int state)
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
 	int status = 0;
 
-	if (kgsl_gmu_isenabled(device))
-		return 0;
-	/*
-	 * Disabling the regulator means also disabling dependent clocks.
-	 * Hence don't disable it if force clock ON is set.
-	 */
-	if (test_bit(KGSL_PWRFLAGS_POWER_ON, &pwr->ctrl_flags) ||
-		test_bit(KGSL_PWRFLAGS_CLK_ON, &pwr->ctrl_flags))
+	if (test_bit(KGSL_PWRFLAGS_POWER_ON, &pwr->ctrl_flags))
 		return 0;
 
 	if (state == KGSL_PWRFLAGS_OFF) {
@@ -1890,13 +1480,13 @@ static int kgsl_pwrctrl_pwrrail(struct kgsl_device *device, int state)
 	} else if (state == KGSL_PWRFLAGS_ON) {
 		if (!test_and_set_bit(KGSL_PWRFLAGS_POWER_ON,
 			&pwr->power_flags)) {
-			status = _enable_regulators(device, pwr);
+				status = _enable_regulators(device, pwr);
 
-			if (status)
-				clear_bit(KGSL_PWRFLAGS_POWER_ON,
-					&pwr->power_flags);
-			else
-				trace_kgsl_rail(device, state);
+				if (status)
+					clear_bit(KGSL_PWRFLAGS_POWER_ON,
+						&pwr->power_flags);
+				else
+					trace_kgsl_rail(device, state);
 		}
 	}
 
@@ -1974,18 +1564,15 @@ static void kgsl_thermal_timer(unsigned long data)
 	kgsl_schedule_work(&device->pwrctrl.thermal_cycle_ws);
 }
 
-#ifdef CONFIG_DEVFREQ_GOV_QCOM_GPUBW_MON
-static int kgsl_pwrctrl_vbif_init(void)
-{
-	devfreq_vbif_register_callback(kgsl_get_bw);
-	return 0;
-}
-#else
-static int kgsl_pwrctrl_vbif_init(void)
+void kgsl_deep_nap_timer(unsigned long data)
 {
-	return 0;
+	struct kgsl_device *device = (struct kgsl_device *) data;
+
+	if (device->state == KGSL_STATE_NAP) {
+		kgsl_pwrctrl_request_state(device, KGSL_STATE_DEEP_NAP);
+		kgsl_schedule_work(&device->idle_check_ws);
+	}
 }
-#endif
 
 static int _get_regulator(struct kgsl_device *device,
 		struct kgsl_regulator *regulator, const char *str)
@@ -2052,7 +1639,6 @@ static int _get_clocks(struct kgsl_device *device)
 	const char *name;
 	struct property *prop;
 
-	pwr->isense_clk_indx = 0;
 	of_property_for_each_string(dev->of_node, "clock-names", prop, name) {
 		int i;
 
@@ -2071,158 +1657,16 @@ static int _get_clocks(struct kgsl_device *device)
 				return ret;
 			}
 
-			if (!strcmp(name, "isense_clk"))
-				pwr->isense_clk_indx = i;
 			break;
 		}
 	}
 
-	if (pwr->isense_clk_indx && of_property_read_u32(dev->of_node,
-		"qcom,isense-clk-on-level", &pwr->isense_clk_on_level)) {
-		KGSL_CORE_ERR("Couldn't get isense clock on level\n");
-		return -ENXIO;
-	}
 	return 0;
 }
 
-static int _isense_clk_set_rate(struct kgsl_pwrctrl *pwr, int level)
-{
-	int rate;
-
-	if (!pwr->isense_clk_indx)
-		return -EINVAL;
-
-	rate = clk_round_rate(pwr->grp_clks[pwr->isense_clk_indx],
-		level > pwr->isense_clk_on_level ?
-		KGSL_XO_CLK_FREQ : KGSL_ISENSE_CLK_FREQ);
-	return kgsl_pwrctrl_clk_set_rate(pwr->grp_clks[pwr->isense_clk_indx],
-			rate, clocks[pwr->isense_clk_indx]);
-}
-
-/*
- * _gpu_clk_prepare_enable - Enable the specified GPU clock
- * Try once to enable it and then BUG() for debug
- */
-static void _gpu_clk_prepare_enable(struct kgsl_device *device,
-		struct clk *clk, const char *name)
-{
-	int ret;
-
-	if (device->state == KGSL_STATE_NAP) {
-		ret = clk_enable(clk);
-		if (ret)
-			goto err;
-		return;
-	}
-
-	ret = clk_prepare_enable(clk);
-	if (!ret)
-		return;
-err:
-	/* Failure is fatal so BUG() to facilitate debug */
-	KGSL_DRV_FATAL(device, "KGSL:%s enable error:%d\n", name, ret);
-}
-
-/*
- * _bimc_clk_prepare_enable - Enable the specified GPU clock
- *  Try once to enable it and then BUG() for debug
- */
-static void _bimc_clk_prepare_enable(struct kgsl_device *device,
-		struct clk *clk, const char *name)
-{
-	int ret = clk_prepare_enable(clk);
-	/* Failure is fatal so BUG() to facilitate debug */
-	if (ret)
-		KGSL_DRV_FATAL(device, "KGSL:%s enable error:%d\n", name, ret);
-}
-
-static int kgsl_pwrctrl_clk_set_rate(struct clk *grp_clk, unsigned int freq,
-		const char *name)
-{
-	int ret = clk_set_rate(grp_clk, freq);
-
-	WARN(ret, "KGSL:%s set freq %d failed:%d\n", name, freq, ret);
-	return ret;
-}
-
-static inline void _close_pcl(struct kgsl_pwrctrl *pwr)
-{
-	if (pwr->pcl)
-		msm_bus_scale_unregister_client(pwr->pcl);
-
-	pwr->pcl = 0;
-}
-
-static inline void _close_ocmem_pcl(struct kgsl_pwrctrl *pwr)
-{
-	if (pwr->ocmem_pcl)
-		msm_bus_scale_unregister_client(pwr->ocmem_pcl);
-
-	pwr->ocmem_pcl = 0;
-}
-
-static inline void _close_regulators(struct kgsl_pwrctrl *pwr)
-{
-	int i;
-
-	for (i = 0; i < KGSL_MAX_REGULATORS; i++)
-		pwr->regulators[i].reg = NULL;
-}
-
-static inline void _close_clks(struct kgsl_device *device)
-{
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	int i;
-
-	for (i = 0; i < KGSL_MAX_CLKS; i++)
-		pwr->grp_clks[i] = NULL;
-
-	if (pwr->gpu_bimc_int_clk)
-		devm_clk_put(&device->pdev->dev, pwr->gpu_bimc_int_clk);
-}
-
-static bool _gpu_freq_supported(struct kgsl_pwrctrl *pwr, unsigned int freq)
-{
-	int i;
-
-	for (i = pwr->num_pwrlevels - 2; i >= 0; i--) {
-		if (pwr->pwrlevels[i].gpu_freq == freq)
-			return true;
-	}
-
-	return false;
-}
-
-static void kgsl_pwrctrl_disable_unused_opp(struct kgsl_device *device)
-{
-	struct device *dev = &device->pdev->dev;
-	struct dev_pm_opp *opp;
-	unsigned long freq = 0;
-	int ret;
-
-	ret = dev_pm_opp_get_opp_count(dev);
-	/* Return early, If no OPP table or OPP count is zero */
-	if (ret <= 0)
-		return;
-
-	while (1) {
-		rcu_read_lock();
-		opp = dev_pm_opp_find_freq_ceil(dev, &freq);
-		rcu_read_unlock();
-
-		if (IS_ERR(opp))
-			break;
-
-		if (!_gpu_freq_supported(&device->pwrctrl, freq))
-			dev_pm_opp_disable(dev, freq);
-
-		freq++;
-	}
-}
-
 int kgsl_pwrctrl_init(struct kgsl_device *device)
 {
-	int i, k, m, n = 0, result, freq;
+	int i, k, m, n = 0, result;
 	struct platform_device *pdev = device->pdev;
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
 	struct device_node *ocmem_bus_node;
@@ -2237,12 +1681,27 @@ int kgsl_pwrctrl_init(struct kgsl_device *device)
 
 	result = _get_clocks(device);
 	if (result)
-		goto error_cleanup_clks;
+		return result;
 
 	/* Make sure we have a source clk for freq setting */
 	if (pwr->grp_clks[0] == NULL)
 		pwr->grp_clks[0] = pwr->grp_clks[1];
 
+	if (of_property_read_u32(pdev->dev.of_node, "qcom,deep-nap-timeout",
+		&pwr->deep_nap_timeout))
+		pwr->deep_nap_timeout = 20;
+
+	pwr->gx_retention = of_property_read_bool(pdev->dev.of_node,
+						"qcom,gx-retention");
+	if (pwr->gx_retention) {
+		pwr->dummy_mx_clk = clk_get(&pdev->dev, "mx_clk");
+		if (IS_ERR(pwr->dummy_mx_clk)) {
+			pwr->gx_retention = 0;
+			pwr->dummy_mx_clk = NULL;
+			KGSL_CORE_ERR("Couldn't get clock: mx_clk\n");
+		}
+	}
+
 	/* Getting gfx-bimc-interface-clk frequency */
 	if (!of_property_read_u32(pdev->dev.of_node,
 			"qcom,gpu-bimc-interface-clk-freq",
@@ -2250,13 +1709,11 @@ int kgsl_pwrctrl_init(struct kgsl_device *device)
 		pwr->gpu_bimc_int_clk = devm_clk_get(&pdev->dev,
 					"bimc_gpu_clk");
 
-	if (of_property_read_bool(pdev->dev.of_node, "qcom,no-nap"))
-		device->pwrctrl.ctrl_flags |= BIT(KGSL_PWRFLAGS_NAP_OFF);
+	pwr->power_flags = BIT(KGSL_PWRFLAGS_RETENTION_ON);
 
 	if (pwr->num_pwrlevels == 0) {
 		KGSL_PWR_ERR(device, "No power levels are defined\n");
-		result = -EINVAL;
-		goto error_cleanup_clks;
+		return -EINVAL;
 	}
 
 	/* Initialize the user and thermal clock constraints */
@@ -2264,50 +1721,47 @@ int kgsl_pwrctrl_init(struct kgsl_device *device)
 	pwr->max_pwrlevel = 0;
 	pwr->min_pwrlevel = pwr->num_pwrlevels - 2;
 	pwr->thermal_pwrlevel = 0;
-	pwr->thermal_pwrlevel_floor = pwr->min_pwrlevel;
 
 	pwr->wakeup_maxpwrlevel = 0;
 
 	for (i = 0; i < pwr->num_pwrlevels; i++) {
-		freq = pwr->pwrlevels[i].gpu_freq;
+		unsigned int freq = pwr->pwrlevels[i].gpu_freq;
 
 		if (freq > 0)
 			freq = clk_round_rate(pwr->grp_clks[0], freq);
 
-		if (freq >= pwr->pwrlevels[i].gpu_freq)
-			pwr->pwrlevels[i].gpu_freq = freq;
+		pwr->pwrlevels[i].gpu_freq = freq;
 	}
 
-	kgsl_pwrctrl_disable_unused_opp(device);
+	clk_set_rate(pwr->grp_clks[0],
+		pwr->pwrlevels[pwr->num_pwrlevels - 1].gpu_freq);
 
-	kgsl_clk_set_rate(device, pwr->num_pwrlevels - 1);
-
-	freq = clk_round_rate(pwr->grp_clks[6], KGSL_RBBMTIMER_CLK_FREQ);
-	if (freq > 0)
-		kgsl_pwrctrl_clk_set_rate(pwr->grp_clks[6],
-			freq, clocks[6]);
-
-	_isense_clk_set_rate(pwr, pwr->num_pwrlevels - 1);
+	clk_set_rate(pwr->grp_clks[6],
+		clk_round_rate(pwr->grp_clks[6], KGSL_RBBMTIMER_CLK_FREQ));
 
 	result = get_regulators(device);
 	if (result)
-		goto error_cleanup_regulators;
+		return result;
 
 	pwr->power_flags = 0;
 
+	if (kgsl_property_read_u32(device, "qcom,pm-qos-active-latency",
+		&pwr->pm_qos_active_latency))
+		pwr->pm_qos_active_latency = 501;
+
+	if (kgsl_property_read_u32(device, "qcom,pm-qos-wakeup-latency",
+		&pwr->pm_qos_wakeup_latency))
+		pwr->pm_qos_wakeup_latency = 101;
+
 	kgsl_property_read_u32(device, "qcom,l2pc-cpu-mask",
 			&pwr->l2pc_cpus_mask);
 
-	pwr->l2pc_update_queue = of_property_read_bool(
-				device->pdev->dev.of_node,
-				"qcom,l2pc-update-queue");
-
 	pm_runtime_enable(&pdev->dev);
 
 	ocmem_bus_node = of_find_node_by_name(
 				device->pdev->dev.of_node,
 				"qcom,ocmem-bus-client");
-	/* If platform has split ocmem bus client - use it */
+	/* If platform has splitted ocmem bus client - use it */
 	if (ocmem_bus_node) {
 		ocmem_scale_table = msm_bus_pdata_from_node
 				(device->pdev, ocmem_bus_node);
@@ -2315,10 +1769,8 @@ int kgsl_pwrctrl_init(struct kgsl_device *device)
 			pwr->ocmem_pcl = msm_bus_scale_register_client
 					(ocmem_scale_table);
 
-		if (!pwr->ocmem_pcl) {
-			result = -EINVAL;
-			goto error_disable_pm;
-		}
+		if (!pwr->ocmem_pcl)
+			return -EINVAL;
 	}
 
 	/* Bus width in bytes, set it to zero if not found */
@@ -2348,18 +1800,14 @@ int kgsl_pwrctrl_init(struct kgsl_device *device)
 		 * from the driver.
 		 */
 		pwr->pcl = msm_bus_scale_register_client(bus_scale_table);
-		if (pwr->pcl == 0) {
-			result = -EINVAL;
-			goto error_cleanup_ocmem_pcl;
-		}
+		if (pwr->pcl == 0)
+			return -EINVAL;
 	}
 
 	pwr->bus_ib = kzalloc(bus_scale_table->num_usecases *
 		sizeof(*pwr->bus_ib), GFP_KERNEL);
-	if (pwr->bus_ib == NULL) {
-		result = -ENOMEM;
-		goto error_cleanup_pcl;
-	}
+	if (pwr->bus_ib == NULL)
+		return -ENOMEM;
 
 	/*
 	 * Pull the BW vote out of the bus table.  They will be used to
@@ -2369,7 +1817,6 @@ int kgsl_pwrctrl_init(struct kgsl_device *device)
 		struct msm_bus_paths *usecase =
 				&bus_scale_table->usecase[i];
 		struct msm_bus_vectors *vector = &usecase->vectors[0];
-
 		if (vector->dst == MSM_BUS_SLAVE_EBI_CH0 &&
 				vector->ib != 0) {
 
@@ -2411,33 +1858,41 @@ int kgsl_pwrctrl_init(struct kgsl_device *device)
 	spin_lock_init(&pwr->limits_lock);
 	pwr->sysfs_pwr_limit = kgsl_pwr_limits_add(KGSL_DEVICE_3D0);
 
-	kgsl_pwrctrl_vbif_init();
-
-	/* temperature sensor name */
-	of_property_read_string(pdev->dev.of_node, "qcom,tzone-name",
-		&pwr->tzone_name);
-
-	return result;
+	setup_timer(&pwr->deep_nap_timer, kgsl_deep_nap_timer,
+			(unsigned long) device);
+	devfreq_vbif_register_callback(kgsl_get_bw);
 
-error_cleanup_pcl:
-	_close_pcl(pwr);
-error_cleanup_ocmem_pcl:
-	_close_ocmem_pcl(pwr);
-error_disable_pm:
-	pm_runtime_disable(&pdev->dev);
-error_cleanup_regulators:
-	_close_regulators(pwr);
-error_cleanup_clks:
-	_close_clks(device);
 	return result;
 }
 
 void kgsl_pwrctrl_close(struct kgsl_device *device)
 {
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
+	int i;
 
 	KGSL_PWR_INFO(device, "close device %d\n", device->id);
 
+	pm_runtime_disable(&device->pdev->dev);
+
+	if (pwr->pcl)
+		msm_bus_scale_unregister_client(pwr->pcl);
+
+	pwr->pcl = 0;
+
+	if (pwr->ocmem_pcl)
+		msm_bus_scale_unregister_client(pwr->ocmem_pcl);
+
+	pwr->ocmem_pcl = 0;
+
+	for (i = 0; i < KGSL_MAX_REGULATORS; i++)
+		pwr->regulators[i].reg = NULL;
+
+	for (i = 0; i < KGSL_MAX_REGULATORS; i++)
+		pwr->grp_clks[i] = NULL;
+
+	if (pwr->gpu_bimc_int_clk)
+		devm_clk_put(&device->pdev->dev, pwr->gpu_bimc_int_clk);
+
 	pwr->power_flags = 0;
 
 	if (!IS_ERR_OR_NULL(pwr->sysfs_pwr_limit)) {
@@ -2446,16 +1901,6 @@ void kgsl_pwrctrl_close(struct kgsl_device *device)
 		pwr->sysfs_pwr_limit = NULL;
 	}
 	kfree(pwr->bus_ib);
-
-	_close_pcl(pwr);
-
-	_close_ocmem_pcl(pwr);
-
-	pm_runtime_disable(&device->pdev->dev);
-
-	_close_regulators(pwr);
-
-	_close_clks(device);
 }
 
 /**
@@ -2470,55 +1915,28 @@ void kgsl_idle_check(struct work_struct *work)
 {
 	struct kgsl_device *device = container_of(work, struct kgsl_device,
 							idle_check_ws);
-	int ret = 0;
-	unsigned int requested_state;
+	WARN_ON(device == NULL);
+	if (device == NULL)
+		return;
 
 	mutex_lock(&device->mutex);
 
-	requested_state = device->requested_state;
-
 	if (device->state == KGSL_STATE_ACTIVE
-		   || device->state ==  KGSL_STATE_NAP) {
-
-		if (!atomic_read(&device->active_cnt)) {
-			spin_lock(&device->submit_lock);
-			if (device->submit_now) {
-				spin_unlock(&device->submit_lock);
-				goto done;
-			}
-			/* Don't allow GPU inline submission in SLUMBER */
-			if (requested_state == KGSL_STATE_SLUMBER)
-				device->slumber = true;
-			spin_unlock(&device->submit_lock);
+		   || device->state ==  KGSL_STATE_NAP
+			|| device->state == KGSL_STATE_DEEP_NAP) {
 
-			ret = kgsl_pwrctrl_change_state(device,
+		if (!atomic_read(&device->active_cnt))
+			kgsl_pwrctrl_change_state(device,
 					device->requested_state);
-			if (ret == -EBUSY) {
-				if (requested_state == KGSL_STATE_SLUMBER) {
-					spin_lock(&device->submit_lock);
-					device->slumber = false;
-					spin_unlock(&device->submit_lock);
-				}
-				/*
-				 * If the GPU is currently busy, restore
-				 * the requested state and reschedule
-				 * idle work.
-				 */
-				kgsl_pwrctrl_request_state(device,
-					requested_state);
-				kgsl_schedule_work(&device->idle_check_ws);
-			}
-		}
-done:
-		if (!ret)
-			kgsl_pwrctrl_request_state(device, KGSL_STATE_NONE);
 
+		kgsl_pwrctrl_request_state(device, KGSL_STATE_NONE);
 		if (device->state == KGSL_STATE_ACTIVE)
 			mod_timer(&device->idle_timer,
 					jiffies +
 					device->pwrctrl.interval_timeout);
 	}
-	kgsl_pwrscale_update(device);
+	if (device->state != KGSL_STATE_DEEP_NAP)
+		kgsl_pwrscale_update(device);
 	mutex_unlock(&device->mutex);
 }
 EXPORT_SYMBOL(kgsl_idle_check);
@@ -2529,7 +1947,10 @@ void kgsl_timer(unsigned long data)
 
 	KGSL_PWR_INFO(device, "idle timer expired device %d\n", device->id);
 	if (device->requested_state != KGSL_STATE_SUSPEND) {
-		kgsl_pwrctrl_request_state(device, KGSL_STATE_SLUMBER);
+		if (device->pwrctrl.strtstp_sleepwake)
+			kgsl_pwrctrl_request_state(device, KGSL_STATE_SLUMBER);
+		else
+			kgsl_pwrctrl_request_state(device, KGSL_STATE_SLEEP);
 		/* Have work run in a non-interrupt context. */
 		kgsl_schedule_work(&device->idle_check_ws);
 	}
@@ -2538,7 +1959,6 @@ void kgsl_timer(unsigned long data)
 static bool kgsl_pwrctrl_isenabled(struct kgsl_device *device)
 {
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-
 	return ((test_bit(KGSL_PWRFLAGS_CLK_ON, &pwr->power_flags) != 0) &&
 		(test_bit(KGSL_PWRFLAGS_AXI_ON, &pwr->power_flags) != 0));
 }
@@ -2555,15 +1975,10 @@ static bool kgsl_pwrctrl_isenabled(struct kgsl_device *device)
  */
 void kgsl_pre_hwaccess(struct kgsl_device *device)
 {
-	/* In order to touch a register you must hold the device mutex */
-	WARN_ON(!mutex_is_locked(&device->mutex));
-
-	/*
-	 * A register access without device power will cause a fatal timeout.
-	 * This is not valid for targets with a GMU.
-	 */
-	if (!kgsl_gmu_isenabled(device))
-		WARN_ON(!kgsl_pwrctrl_isenabled(device));
+	/* In order to touch a register you must hold the device mutex...*/
+	BUG_ON(!mutex_is_locked(&device->mutex));
+	/* and have the clock on! */
+	BUG_ON(!kgsl_pwrctrl_isenabled(device));
 }
 EXPORT_SYMBOL(kgsl_pre_hwaccess);
 
@@ -2583,14 +1998,6 @@ static int kgsl_pwrctrl_enable(struct kgsl_device *device)
 
 	kgsl_pwrctrl_pwrlevel_change(device, level);
 
-	if (kgsl_gmu_isenabled(device)) {
-		int ret = gmu_start(device);
-
-		if (!ret)
-			kgsl_pwrctrl_axi(device, KGSL_PWRFLAGS_ON);
-		return ret;
-	}
-
 	/* Order pwrrail/clk sequence based upon platform */
 	status = kgsl_pwrctrl_pwrrail(device, KGSL_PWRFLAGS_ON);
 	if (status)
@@ -2602,37 +2009,13 @@ static int kgsl_pwrctrl_enable(struct kgsl_device *device)
 
 static void kgsl_pwrctrl_disable(struct kgsl_device *device)
 {
-	if (!IS_ERR_OR_NULL(device->l3_clk))
-		clk_set_rate(device->l3_clk, 0);
-
-	if (kgsl_gmu_isenabled(device)) {
-		kgsl_pwrctrl_axi(device, KGSL_PWRFLAGS_OFF);
-		return gmu_stop(device);
-	}
-
 	/* Order pwrrail/clk sequence based upon platform */
 	device->ftbl->regulator_disable(device);
 	kgsl_pwrctrl_axi(device, KGSL_PWRFLAGS_OFF);
-	kgsl_pwrctrl_clk(device, KGSL_PWRFLAGS_OFF, KGSL_STATE_SLUMBER);
+	kgsl_pwrctrl_clk(device, KGSL_PWRFLAGS_OFF, KGSL_STATE_SLEEP);
 	kgsl_pwrctrl_pwrrail(device, KGSL_PWRFLAGS_OFF);
 }
 
-static void
-kgsl_pwrctrl_clk_set_options(struct kgsl_device *device, bool on)
-{
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	int i;
-
-	for (i = 0; i < KGSL_MAX_CLKS; i++) {
-		if (pwr->grp_clks[i] == NULL)
-			continue;
-
-		if (device->ftbl->clk_set_options)
-			device->ftbl->clk_set_options(device, clocks[i],
-				pwr->grp_clks[i], on);
-	}
-}
-
 /**
  * _init() - Get the GPU ready to start, but don't turn anything on
  * @device - Pointer to the kgsl_device struct
@@ -2640,17 +2023,20 @@ kgsl_pwrctrl_clk_set_options(struct kgsl_device *device, bool on)
 static int _init(struct kgsl_device *device)
 {
 	int status = 0;
-
 	switch (device->state) {
+	case KGSL_STATE_DEEP_NAP:
+		pm_qos_update_request(&device->pwrctrl.pm_qos_req_dma,
+			device->pwrctrl.pm_qos_active_latency);
+		/* Get the device out of retention */
+		kgsl_pwrctrl_retention_clk(device, KGSL_PWRFLAGS_ON);
+		/* fall through */
 	case KGSL_STATE_NAP:
+	case KGSL_STATE_SLEEP:
 		/* Force power on to do the stop */
 		status = kgsl_pwrctrl_enable(device);
 	case KGSL_STATE_ACTIVE:
-		/* fall through */
-	case KGSL_STATE_RESET:
 		kgsl_pwrctrl_irq(device, KGSL_PWRFLAGS_OFF);
 		del_timer_sync(&device->idle_timer);
-		kgsl_pwrscale_midframe_timer_cancel(device);
 		device->ftbl->stop(device);
 		/* fall through */
 	case KGSL_STATE_AWARE:
@@ -2665,7 +2051,7 @@ static int _init(struct kgsl_device *device)
 }
 
 /**
- * _wake() - Power up the GPU from a slumber state
+ * _wake() - Power up the GPU from a slumber/sleep state
  * @device - Pointer to the kgsl_device struct
  *
  * Resume the GPU from a lower power state to ACTIVE.
@@ -2682,7 +2068,6 @@ static int _wake(struct kgsl_device *device)
 		device->ftbl->resume(device);
 		/* fall through */
 	case KGSL_STATE_SLUMBER:
-		kgsl_pwrctrl_clk_set_options(device, true);
 		status = device->ftbl->start(device,
 				device->pwrctrl.superfast);
 		device->pwrctrl.superfast = false;
@@ -2692,10 +2077,18 @@ static int _wake(struct kgsl_device *device)
 			KGSL_DRV_ERR(device, "start failed %d\n", status);
 			break;
 		}
+		/* fall through */
+	case KGSL_STATE_SLEEP:
 		kgsl_pwrctrl_axi(device, KGSL_PWRFLAGS_ON);
 		kgsl_pwrscale_wake(device);
 		kgsl_pwrctrl_irq(device, KGSL_PWRFLAGS_ON);
 		/* fall through */
+	case KGSL_STATE_DEEP_NAP:
+		pm_qos_update_request(&device->pwrctrl.pm_qos_req_dma,
+					device->pwrctrl.pm_qos_active_latency);
+		/* Get the device out of retention */
+		kgsl_pwrctrl_retention_clk(device, KGSL_PWRFLAGS_ON);
+		/* fall through */
 	case KGSL_STATE_NAP:
 		/* Turn on the core clocks */
 		kgsl_pwrctrl_clk(device, KGSL_PWRFLAGS_ON, KGSL_STATE_ACTIVE);
@@ -2717,14 +2110,16 @@ static int _wake(struct kgsl_device *device)
 		pwr->previous_pwrlevel = pwr->active_pwrlevel;
 		mod_timer(&device->idle_timer, jiffies +
 				device->pwrctrl.interval_timeout);
+		del_timer_sync(&device->pwrctrl.deep_nap_timer);
+
 		break;
 	case KGSL_STATE_AWARE:
-		kgsl_pwrctrl_clk_set_options(device, true);
 		/* Enable state before turning on irq */
 		kgsl_pwrctrl_set_state(device, KGSL_STATE_ACTIVE);
 		kgsl_pwrctrl_irq(device, KGSL_PWRFLAGS_ON);
 		mod_timer(&device->idle_timer, jiffies +
 				device->pwrctrl.interval_timeout);
+		del_timer_sync(&device->pwrctrl.deep_nap_timer);
 		break;
 	default:
 		KGSL_PWR_WARN(device, "unhandled state %s\n",
@@ -2750,89 +2145,29 @@ static int
 _aware(struct kgsl_device *device)
 {
 	int status = 0;
-	struct gmu_device *gmu = &device->gmu;
-	unsigned int state = device->state;
-
 	switch (device->state) {
-	case KGSL_STATE_RESET:
-		if (!kgsl_gmu_isenabled(device))
-			break;
-		status = gmu_start(device);
-		break;
 	case KGSL_STATE_INIT:
 		status = kgsl_pwrctrl_enable(device);
 		break;
 	/* The following 3 cases shouldn't occur, but don't panic. */
+	case KGSL_STATE_DEEP_NAP:
 	case KGSL_STATE_NAP:
+	case KGSL_STATE_SLEEP:
 		status = _wake(device);
 	case KGSL_STATE_ACTIVE:
 		kgsl_pwrctrl_irq(device, KGSL_PWRFLAGS_OFF);
 		del_timer_sync(&device->idle_timer);
-		kgsl_pwrscale_midframe_timer_cancel(device);
 		break;
 	case KGSL_STATE_SLUMBER:
-		/* if GMU already in FAULT */
-		if (kgsl_gmu_isenabled(device) &&
-			test_bit(GMU_FAULT, &gmu->flags)) {
-			status = -EINVAL;
-			break;
-		}
-
 		status = kgsl_pwrctrl_enable(device);
 		break;
 	default:
 		status = -EINVAL;
 	}
-
-	if (status) {
-		if (kgsl_gmu_isenabled(device)) {
-			/* GMU hang recovery */
-			kgsl_pwrctrl_set_state(device, KGSL_STATE_RESET);
-			set_bit(GMU_FAULT, &gmu->flags);
-			status = kgsl_pwrctrl_enable(device);
-			if (status) {
-				/*
-				 * Cannot recover GMU failure
-				 * GPU will not be powered on
-				 */
-				WARN_ONCE(1, "Failed to recover GMU\n");
-				if (device->snapshot)
-					device->snapshot->recovered = false;
-				/*
-				 * On recovery failure, we are clearing
-				 * GMU_FAULT bit and also not keeping
-				 * the state as RESET to make sure any
-				 * attempt to wake GMU/GPU after this
-				 * is treated as a fresh start. But on
-				 * recovery failure, GMU HS, clocks and
-				 * IRQs are still ON/enabled because of
-				 * which next GMU/GPU wakeup results in
-				 * multiple warnings from GMU start as HS,
-				 * clocks and IRQ were ON while doing a
-				 * fresh start i.e. wake from SLUMBER.
-				 *
-				 * Suspend the GMU on recovery failure
-				 * to make sure next attempt to wake up
-				 * GMU/GPU is indeed a fresh start.
-				 */
-				gmu_suspend(device);
-				gmu->unrecovered = true;
-				kgsl_pwrctrl_set_state(device, state);
-			} else {
-				if (device->snapshot)
-					device->snapshot->recovered = true;
-				kgsl_pwrctrl_set_state(device,
-					KGSL_STATE_AWARE);
-			}
-
-			clear_bit(GMU_FAULT, &gmu->flags);
-			return status;
-		}
-
+	if (status)
 		kgsl_pwrctrl_request_state(device, KGSL_STATE_NONE);
-	} else {
+	else
 		kgsl_pwrctrl_set_state(device, KGSL_STATE_AWARE);
-	}
 	return status;
 }
 
@@ -2846,28 +2181,26 @@ _nap(struct kgsl_device *device)
 			return -EBUSY;
 		}
 
-		device->ftbl->stop_fault_timer(device);
-		kgsl_pwrscale_midframe_timer_cancel(device);
-
 		/*
 		 * Read HW busy counters before going to NAP state.
 		 * The data might be used by power scale governors
 		 * independently of the HW activity. For example
 		 * the simple-on-demand governor will get the latest
 		 * busy_time data even if the gpu isn't active.
-		 */
+		*/
 		kgsl_pwrscale_update_stats(device);
 
+		mod_timer(&device->pwrctrl.deep_nap_timer, jiffies +
+			msecs_to_jiffies(device->pwrctrl.deep_nap_timeout));
+
 		kgsl_pwrctrl_clk(device, KGSL_PWRFLAGS_OFF, KGSL_STATE_NAP);
 		kgsl_pwrctrl_set_state(device, KGSL_STATE_NAP);
-		/* fallthrough */
+	case KGSL_STATE_SLEEP:
 	case KGSL_STATE_SLUMBER:
-	case KGSL_STATE_RESET:
 		break;
 	case KGSL_STATE_AWARE:
 		KGSL_PWR_WARN(device,
 			"transition AWARE -> NAP is not permitted\n");
-		/* fallthrough */
 	default:
 		kgsl_pwrctrl_request_state(device, KGSL_STATE_NONE);
 		break;
@@ -2875,11 +2208,67 @@ _nap(struct kgsl_device *device)
 	return 0;
 }
 
+static int
+_deep_nap(struct kgsl_device *device)
+{
+	switch (device->state) {
+		/*
+		 * Device is expected to be clock gated to move to
+		 * a deeper low power state. No other transition is permitted
+		 */
+	case KGSL_STATE_NAP:
+		kgsl_pwrctrl_retention_clk(device, KGSL_PWRFLAGS_OFF);
+		pm_qos_update_request(&device->pwrctrl.pm_qos_req_dma,
+						PM_QOS_DEFAULT_VALUE);
+		kgsl_pwrctrl_set_state(device, KGSL_STATE_DEEP_NAP);
+		break;
+	default:
+		kgsl_pwrctrl_request_state(device, KGSL_STATE_NONE);
+		break;
+	}
+	return 0;
+}
+
+static int
+_sleep(struct kgsl_device *device)
+{
+	switch (device->state) {
+	case KGSL_STATE_ACTIVE:
+		if (!device->ftbl->is_hw_collapsible(device)) {
+			kgsl_pwrctrl_request_state(device, KGSL_STATE_NONE);
+			return -EBUSY;
+		}
+		/* fall through */
+	case KGSL_STATE_NAP:
+		kgsl_pwrctrl_irq(device, KGSL_PWRFLAGS_OFF);
+		kgsl_pwrctrl_axi(device, KGSL_PWRFLAGS_OFF);
+		kgsl_pwrscale_sleep(device);
+		kgsl_pwrctrl_clk(device, KGSL_PWRFLAGS_OFF, KGSL_STATE_SLEEP);
+		kgsl_pwrctrl_set_state(device, KGSL_STATE_SLEEP);
+		pm_qos_update_request(&device->pwrctrl.pm_qos_req_dma,
+					PM_QOS_DEFAULT_VALUE);
+		if (device->pwrctrl.l2pc_cpus_mask)
+			pm_qos_update_request(
+					&device->pwrctrl.l2pc_cpus_qos,
+					PM_QOS_DEFAULT_VALUE);
+		break;
+	case KGSL_STATE_SLUMBER:
+		break;
+	case KGSL_STATE_AWARE:
+		KGSL_PWR_WARN(device,
+			"transition AWARE -> SLEEP is not permitted\n");
+	default:
+		kgsl_pwrctrl_request_state(device, KGSL_STATE_NONE);
+		break;
+	}
+
+	return 0;
+}
+
 static int
 _slumber(struct kgsl_device *device)
 {
 	int status = 0;
-
 	switch (device->state) {
 	case KGSL_STATE_ACTIVE:
 		if (!device->ftbl->is_hw_collapsible(device)) {
@@ -2888,18 +2277,21 @@ _slumber(struct kgsl_device *device)
 		}
 		/* fall through */
 	case KGSL_STATE_NAP:
+	case KGSL_STATE_SLEEP:
+	case KGSL_STATE_DEEP_NAP:
 		del_timer_sync(&device->idle_timer);
-		kgsl_pwrscale_midframe_timer_cancel(device);
 		if (device->pwrctrl.thermal_cycle == CYCLE_ACTIVE) {
 			device->pwrctrl.thermal_cycle = CYCLE_ENABLE;
 			del_timer_sync(&device->pwrctrl.thermal_timer);
 		}
+		del_timer_sync(&device->pwrctrl.deep_nap_timer);
 		kgsl_pwrctrl_irq(device, KGSL_PWRFLAGS_OFF);
+		/* Get the device out of retention */
+		kgsl_pwrctrl_retention_clk(device, KGSL_PWRFLAGS_ON);
 		/* make sure power is on to stop the device*/
 		status = kgsl_pwrctrl_enable(device);
 		device->ftbl->suspend_context(device);
 		device->ftbl->stop(device);
-		kgsl_pwrctrl_clk_set_options(device, false);
 		kgsl_pwrctrl_disable(device);
 		kgsl_pwrscale_sleep(device);
 		kgsl_pwrctrl_irq(device, KGSL_PWRFLAGS_OFF);
@@ -2938,9 +2330,8 @@ static int _suspend(struct kgsl_device *device)
 {
 	int ret = 0;
 
-	if ((device->state == KGSL_STATE_NONE) ||
-			(device->state == KGSL_STATE_INIT) ||
-			(device->state == KGSL_STATE_SUSPEND))
+	if ((KGSL_STATE_NONE == device->state) ||
+			(KGSL_STATE_INIT == device->state))
 		return ret;
 
 	/* drain to prevent from more commands being submitted */
@@ -2983,7 +2374,6 @@ static int _suspend(struct kgsl_device *device)
 int kgsl_pwrctrl_change_state(struct kgsl_device *device, int state)
 {
 	int status = 0;
-
 	if (device->state == state)
 		return status;
 	kgsl_pwrctrl_request_state(device, state);
@@ -3002,14 +2392,17 @@ int kgsl_pwrctrl_change_state(struct kgsl_device *device, int state)
 	case KGSL_STATE_NAP:
 		status = _nap(device);
 		break;
+	case KGSL_STATE_SLEEP:
+		status = _sleep(device);
+		break;
 	case KGSL_STATE_SLUMBER:
 		status = _slumber(device);
 		break;
 	case KGSL_STATE_SUSPEND:
 		status = _suspend(device);
 		break;
-	case KGSL_STATE_RESET:
-		kgsl_pwrctrl_set_state(device, KGSL_STATE_RESET);
+	case KGSL_STATE_DEEP_NAP:
+		status = _deep_nap(device);
 		break;
 	default:
 		KGSL_PWR_INFO(device, "bad state request 0x%x\n", state);
@@ -3021,7 +2414,6 @@ int kgsl_pwrctrl_change_state(struct kgsl_device *device, int state)
 	/* Record the state timing info */
 	if (!status) {
 		ktime_t t = ktime_get();
-
 		_record_pwrevent(device, t, KGSL_PWREVENT_STATE);
 	}
 	return status;
@@ -3034,13 +2426,6 @@ static void kgsl_pwrctrl_set_state(struct kgsl_device *device,
 	trace_kgsl_pwr_set_state(device, state);
 	device->state = state;
 	device->requested_state = KGSL_STATE_NONE;
-
-	spin_lock(&device->submit_lock);
-	if (state == KGSL_STATE_SLUMBER || state == KGSL_STATE_SUSPEND)
-		device->slumber = true;
-	else
-		device->slumber = false;
-	spin_unlock(&device->submit_lock);
 }
 
 static void kgsl_pwrctrl_request_state(struct kgsl_device *device,
@@ -3064,12 +2449,14 @@ const char *kgsl_pwrstate_to_str(unsigned int state)
 		return "ACTIVE";
 	case KGSL_STATE_NAP:
 		return "NAP";
+	case KGSL_STATE_DEEP_NAP:
+		return "DEEP_NAP";
+	case KGSL_STATE_SLEEP:
+		return "SLEEP";
 	case KGSL_STATE_SUSPEND:
 		return "SUSPEND";
 	case KGSL_STATE_SLUMBER:
 		return "SLUMBER";
-	case KGSL_STATE_RESET:
-		return "RESET";
 	default:
 		break;
 	}
@@ -3093,9 +2480,7 @@ EXPORT_SYMBOL(kgsl_pwrstate_to_str);
 int kgsl_active_count_get(struct kgsl_device *device)
 {
 	int ret = 0;
-
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return -EINVAL;
+	BUG_ON(!mutex_is_locked(&device->mutex));
 
 	if ((atomic_read(&device->active_cnt) == 0) &&
 		(device->state != KGSL_STATE_ACTIVE)) {
@@ -3125,23 +2510,14 @@ EXPORT_SYMBOL(kgsl_active_count_get);
  */
 void kgsl_active_count_put(struct kgsl_device *device)
 {
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return;
-
-	if (WARN(atomic_read(&device->active_cnt) == 0,
-			"Unbalanced get/put calls to KGSL active count\n"))
-		return;
+	BUG_ON(!mutex_is_locked(&device->mutex));
+	BUG_ON(atomic_read(&device->active_cnt) == 0);
 
 	if (atomic_dec_and_test(&device->active_cnt)) {
-		bool nap_on = !(device->pwrctrl.ctrl_flags &
-			BIT(KGSL_PWRFLAGS_NAP_OFF));
-		if (nap_on && device->state == KGSL_STATE_ACTIVE &&
+		if (device->state == KGSL_STATE_ACTIVE &&
 			device->requested_state == KGSL_STATE_NONE) {
 			kgsl_pwrctrl_request_state(device, KGSL_STATE_NAP);
 			kgsl_schedule_work(&device->idle_check_ws);
-		} else if (!nap_on) {
-			kgsl_pwrscale_update_stats(device);
-			kgsl_pwrscale_update(device);
 		}
 
 		mod_timer(&device->idle_timer,
@@ -3173,12 +2549,10 @@ int kgsl_active_count_wait(struct kgsl_device *device, int count)
 	int result = 0;
 	long wait_jiffies = HZ;
 
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return -EINVAL;
+	BUG_ON(!mutex_is_locked(&device->mutex));
 
 	while (atomic_read(&device->active_cnt) > count) {
 		long ret;
-
 		mutex_unlock(&device->mutex);
 		ret = wait_event_timeout(device->active_cnt_wq,
 			_check_active_count(device, count), wait_jiffies);
@@ -3277,8 +2651,7 @@ EXPORT_SYMBOL(kgsl_pwr_limits_add);
 void kgsl_pwr_limits_del(void *limit_ptr)
 {
 	struct kgsl_pwr_limit *limit = limit_ptr;
-
-	if (IS_ERR_OR_NULL(limit))
+	if (IS_ERR(limit))
 		return;
 
 	_update_limits(limit, KGSL_PWR_DEL_LIMIT, 0);
@@ -3299,7 +2672,7 @@ int kgsl_pwr_limits_set_freq(void *limit_ptr, unsigned int freq)
 	struct kgsl_pwr_limit *limit = limit_ptr;
 	int level;
 
-	if (IS_ERR_OR_NULL(limit))
+	if (IS_ERR(limit))
 		return -EINVAL;
 
 	pwr = &limit->device->pwrctrl;
@@ -3321,7 +2694,7 @@ void kgsl_pwr_limits_set_default(void *limit_ptr)
 {
 	struct kgsl_pwr_limit *limit = limit_ptr;
 
-	if (IS_ERR_OR_NULL(limit))
+	if (IS_ERR(limit))
 		return;
 
 	_update_limits(limit, KGSL_PWR_SET_LIMIT, 0);
diff --git a/drivers/gpu/msm/kgsl_pwrctrl.h b/drivers/gpu/msm/kgsl_pwrctrl.h
index 07807376d4c9..840696952a03 100644
--- a/drivers/gpu/msm/kgsl_pwrctrl.h
+++ b/drivers/gpu/msm/kgsl_pwrctrl.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2010-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2010-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -16,16 +16,18 @@
 #include <linux/pm_qos.h>
 
 /*****************************************************************************
- * power flags
- ****************************************************************************/
+** power flags
+*****************************************************************************/
 #define KGSL_PWRFLAGS_ON   1
 #define KGSL_PWRFLAGS_OFF  0
 
 #define KGSL_PWRLEVEL_TURBO 0
+#define KGSL_PWRLEVEL_NOMINAL 1
+#define KGSL_PWRLEVEL_LAST_OFFSET 2
 
 #define KGSL_PWR_ON	0xFFFF
 
-#define KGSL_MAX_CLKS 16
+#define KGSL_MAX_CLKS 13
 #define KGSL_MAX_REGULATORS 2
 
 #define KGSL_MAX_PWRLEVELS 10
@@ -33,9 +35,7 @@
 /* Only two supported levels, min & max */
 #define KGSL_CONSTRAINT_PWR_MAXLEVELS 2
 
-#define KGSL_XO_CLK_FREQ	19200000
-#define KGSL_RBBMTIMER_CLK_FREQ	KGSL_XO_CLK_FREQ
-#define KGSL_ISENSE_CLK_FREQ	200000000
+#define KGSL_RBBMTIMER_CLK_FREQ	19200000
 
 /* Symbolic table for the constraint type */
 #define KGSL_CONSTRAINT_TYPES \
@@ -50,21 +50,9 @@
 #define KGSL_PWR_DEL_LIMIT 1
 #define KGSL_PWR_SET_LIMIT 2
 
-/*
- * The effective duration of qos request in usecs at queue time.
- * After timeout, qos request is cancelled automatically.
- * Kept 80ms default, inline with default GPU idle time.
- */
-#define KGSL_L2PC_QUEUE_TIMEOUT	(80 * 1000)
-
-/*
- * The effective duration of qos request in usecs at wakeup time.
- * After timeout, qos request is cancelled automatically.
- */
-#define KGSL_L2PC_WAKEUP_TIMEOUT (10 * 1000)
-
 enum kgsl_pwrctrl_timer_type {
 	KGSL_PWR_IDLE_TIMER,
+	KGSL_PWR_DEEP_NAP_TIMER,
 };
 
 /*
@@ -123,30 +111,27 @@ struct kgsl_regulator {
  * struct kgsl_pwrctrl - Power control settings for a KGSL device
  * @interrupt_num - The interrupt number for the device
  * @grp_clks - Array of clocks structures that we control
+ * @dummy_mx_clk - mx clock that is contolled during retention
  * @power_flags - Control flags for power
  * @pwrlevels - List of supported power levels
- * @nb - Notifier block to receive GPU OPP change event
  * @active_pwrlevel - The currently active power level
  * @previous_pwrlevel - The power level before transition
  * @thermal_pwrlevel - maximum powerlevel constraint from thermal
- * @thermal_pwrlevel_floor - minimum powerlevel constraint from thermal
  * @default_pwrlevel - device wake up power level
  * @max_pwrlevel - maximum allowable powerlevel per the user
  * @min_pwrlevel - minimum allowable powerlevel per the user
  * @num_pwrlevels - number of available power levels
  * @interval_timeout - timeout in jiffies to be idle before a power event
- * @clock_times - Each GPU frequency's accumulated active time in us
+ * @strtstp_sleepwake - true if the device supports low latency GPU start/stop
  * @regulators - array of pointers to kgsl_regulator structs
  * @pcl - bus scale identifier
  * @ocmem - ocmem bus scale identifier
  * @irq_name - resource name for the IRQ
  * @clk_stats - structure of clock statistics
  * @l2pc_cpus_mask - mask to avoid L2PC on masked CPUs
- * @l2pc_update_queue - Boolean flag to avoid L2PC on masked CPUs at queue time
  * @l2pc_cpus_qos - qos structure to avoid L2PC on CPUs
  * @pm_qos_req_dma - the power management quality of service structure
  * @pm_qos_active_latency - allowed CPU latency in microseconds when active
- * @pm_qos_cpu_mask_latency - allowed CPU mask latency in microseconds
  * @input_disable - To disable GPU wakeup on touch input event
  * @pm_qos_wakeup_latency - allowed CPU latency in microseconds during wakeup
  * @bus_control - true if the bus calculation is independent
@@ -167,43 +152,38 @@ struct kgsl_regulator {
  * @limits - list head for limits
  * @limits_lock - spin lock to protect limits list
  * @sysfs_pwr_limit - pointer to the sysfs limits node
- * isense_clk_indx - index of isense clock, 0 if no isense
- * isense_clk_on_level - isense clock rate is XO rate below this level.
- * tzone_name - pointer to thermal zone name of GPU temperature sensor
+ * @deep_nap_timer - Timer struct for entering deep nap
+ * @deep_nap_timeout - Timeout for entering deep nap
+ * @gx_retention - true if retention voltage is allowed
  */
 
 struct kgsl_pwrctrl {
 	int interrupt_num;
 	struct clk *grp_clks[KGSL_MAX_CLKS];
+	struct clk *dummy_mx_clk;
 	struct clk *gpu_bimc_int_clk;
-	int isense_clk_indx;
-	int isense_clk_on_level;
 	unsigned long power_flags;
 	unsigned long ctrl_flags;
 	struct kgsl_pwrlevel pwrlevels[KGSL_MAX_PWRLEVELS];
-	struct notifier_block nb;
 	unsigned int active_pwrlevel;
 	unsigned int previous_pwrlevel;
 	unsigned int thermal_pwrlevel;
-	unsigned int thermal_pwrlevel_floor;
 	unsigned int default_pwrlevel;
 	unsigned int wakeup_maxpwrlevel;
 	unsigned int max_pwrlevel;
 	unsigned int min_pwrlevel;
 	unsigned int num_pwrlevels;
 	unsigned long interval_timeout;
-	u64 clock_times[KGSL_MAX_PWRLEVELS];
+	bool strtstp_sleepwake;
 	struct kgsl_regulator regulators[KGSL_MAX_REGULATORS];
 	uint32_t pcl;
 	uint32_t ocmem_pcl;
 	const char *irq_name;
 	struct kgsl_clk_stats clk_stats;
 	unsigned int l2pc_cpus_mask;
-	bool l2pc_update_queue;
 	struct pm_qos_request l2pc_cpus_qos;
 	struct pm_qos_request pm_qos_req_dma;
 	unsigned int pm_qos_active_latency;
-	unsigned int pm_qos_cpu_mask_latency;
 	unsigned int pm_qos_wakeup_latency;
 	bool input_disable;
 	bool bus_control;
@@ -224,9 +204,11 @@ struct kgsl_pwrctrl {
 	struct list_head limits;
 	spinlock_t limits_lock;
 	struct kgsl_pwr_limit *sysfs_pwr_limit;
+	struct timer_list deep_nap_timer;
+	uint32_t deep_nap_timeout;
+	bool gx_retention;
 	unsigned int gpu_bimc_int_clk_freq;
 	bool gpu_bimc_interface_enabled;
-	const char *tzone_name;
 };
 
 int kgsl_pwrctrl_init(struct kgsl_device *device);
@@ -241,12 +223,6 @@ void kgsl_pwrctrl_buslevel_update(struct kgsl_device *device,
 int kgsl_pwrctrl_init_sysfs(struct kgsl_device *device);
 void kgsl_pwrctrl_uninit_sysfs(struct kgsl_device *device);
 int kgsl_pwrctrl_change_state(struct kgsl_device *device, int state);
-int kgsl_clk_set_rate(struct kgsl_device *device,
-	unsigned int pwrlevel);
-unsigned int kgsl_pwrctrl_adjust_pwrlevel(struct kgsl_device *device,
-	unsigned int new_level);
-void kgsl_pwrctrl_set_thermal_cycle(struct kgsl_device *device,
-	unsigned int new_level);
 
 static inline unsigned long kgsl_get_clkrate(struct clk *clk)
 {
@@ -271,6 +247,5 @@ int kgsl_active_count_wait(struct kgsl_device *device, int count);
 void kgsl_pwrctrl_busy_time(struct kgsl_device *device, u64 time, u64 busy);
 void kgsl_pwrctrl_set_constraint(struct kgsl_device *device,
 			struct kgsl_pwr_constraint *pwrc, uint32_t id);
-void kgsl_pwrctrl_update_l2pc(struct kgsl_device *device,
-			unsigned long timeout_us);
+void kgsl_pwrctrl_update_l2pc(struct kgsl_device *device);
 #endif /* __KGSL_PWRCTRL_H */
diff --git a/drivers/gpu/msm/kgsl_pwrscale.c b/drivers/gpu/msm/kgsl_pwrscale.c
index 2b8b6dfcde47..35df58bd703e 100644
--- a/drivers/gpu/msm/kgsl_pwrscale.c
+++ b/drivers/gpu/msm/kgsl_pwrscale.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2010-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2010-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -13,15 +13,15 @@
 
 #include <linux/export.h>
 #include <linux/kernel.h>
-#include <linux/hrtimer.h>
-#include <linux/devfreq_cooling.h>
-#include <linux/pm_opp.h>
 
 #include "kgsl.h"
 #include "kgsl_pwrscale.h"
 #include "kgsl_device.h"
 #include "kgsl_trace.h"
 
+#define FAST_BUS 1
+#define SLOW_BUS -1
+
 /*
  * "SLEEP" is generic counting both NAP & SLUMBER
  * PERIODS generally won't exceed 9 for the relavent 150msec
@@ -40,18 +40,6 @@ static struct kgsl_popp popp_param[POPP_MAX] = {
 	{0, 0},
 };
 
-/**
- * struct kgsl_midframe_info - midframe power stats sampling info
- * @timer - midframe sampling timer
- * @timer_check_ws - Updates powerstats on midframe expiry
- * @device - pointer to kgsl_device
- */
-static struct kgsl_midframe_info {
-	struct hrtimer timer;
-	struct work_struct timer_check_ws;
-	struct kgsl_device *device;
-} *kgsl_midframe = NULL;
-
 static void do_devfreq_suspend(struct work_struct *work);
 static void do_devfreq_resume(struct work_struct *work);
 static void do_devfreq_notify(struct work_struct *work);
@@ -72,7 +60,7 @@ static struct devfreq_dev_status last_status = { .private_data = &last_xstats };
 void kgsl_pwrscale_sleep(struct kgsl_device *device)
 {
 	struct kgsl_pwrscale *psc = &device->pwrscale;
-
+	BUG_ON(!mutex_is_locked(&device->mutex));
 	if (!device->pwrscale.enabled)
 		return;
 	device->pwrscale.on_time = 0;
@@ -96,6 +84,7 @@ void kgsl_pwrscale_wake(struct kgsl_device *device)
 {
 	struct kgsl_power_stats stats;
 	struct kgsl_pwrscale *psc = &device->pwrscale;
+	BUG_ON(!mutex_is_locked(&device->mutex));
 
 	if (!device->pwrscale.enabled)
 		return;
@@ -125,6 +114,7 @@ EXPORT_SYMBOL(kgsl_pwrscale_wake);
  */
 void kgsl_pwrscale_busy(struct kgsl_device *device)
 {
+	BUG_ON(!mutex_is_locked(&device->mutex));
 	if (!device->pwrscale.enabled)
 		return;
 	if (device->pwrscale.on_time == 0)
@@ -140,23 +130,18 @@ EXPORT_SYMBOL(kgsl_pwrscale_busy);
  */
 void kgsl_pwrscale_update_stats(struct kgsl_device *device)
 {
-	struct kgsl_pwrctrl *pwrctrl = &device->pwrctrl;
 	struct kgsl_pwrscale *psc = &device->pwrscale;
-
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return;
+	BUG_ON(!mutex_is_locked(&device->mutex));
 
 	if (!psc->enabled)
 		return;
 
 	if (device->state == KGSL_STATE_ACTIVE) {
 		struct kgsl_power_stats stats;
-
 		device->ftbl->power_stats(device, &stats);
 		if (psc->popp_level) {
 			u64 x = stats.busy_time;
 			u64 y = stats.ram_time;
-
 			do_div(x, 100);
 			do_div(y, 100);
 			x *= popp_param[psc->popp_level].gpu_x;
@@ -168,8 +153,6 @@ void kgsl_pwrscale_update_stats(struct kgsl_device *device)
 		device->pwrscale.accum_stats.busy_time += stats.busy_time;
 		device->pwrscale.accum_stats.ram_time += stats.ram_time;
 		device->pwrscale.accum_stats.ram_wait += stats.ram_wait;
-		pwrctrl->clock_times[pwrctrl->active_pwrlevel] +=
-				stats.busy_time;
 	}
 }
 EXPORT_SYMBOL(kgsl_pwrscale_update_stats);
@@ -184,9 +167,7 @@ EXPORT_SYMBOL(kgsl_pwrscale_update_stats);
 void kgsl_pwrscale_update(struct kgsl_device *device)
 {
 	ktime_t t;
-
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return;
+	BUG_ON(!mutex_is_locked(&device->mutex));
 
 	if (!device->pwrscale.enabled)
 		return;
@@ -202,77 +183,25 @@ void kgsl_pwrscale_update(struct kgsl_device *device)
 	if (device->state != KGSL_STATE_SLUMBER)
 		queue_work(device->pwrscale.devfreq_wq,
 			&device->pwrscale.devfreq_notify_ws);
-
-	kgsl_pwrscale_midframe_timer_restart(device);
 }
 EXPORT_SYMBOL(kgsl_pwrscale_update);
 
-void kgsl_pwrscale_midframe_timer_restart(struct kgsl_device *device)
-{
-	if (kgsl_midframe) {
-		WARN_ON(!mutex_is_locked(&device->mutex));
-
-		/* If the timer is already running, stop it */
-		if (hrtimer_active(&kgsl_midframe->timer))
-			hrtimer_cancel(
-				&kgsl_midframe->timer);
-
-		hrtimer_start(&kgsl_midframe->timer,
-				ns_to_ktime(KGSL_GOVERNOR_CALL_INTERVAL
-					* NSEC_PER_USEC), HRTIMER_MODE_REL);
-	}
-}
-EXPORT_SYMBOL(kgsl_pwrscale_midframe_timer_restart);
-
-void kgsl_pwrscale_midframe_timer_cancel(struct kgsl_device *device)
-{
-	if (kgsl_midframe) {
-		WARN_ON(!mutex_is_locked(&device->mutex));
-		hrtimer_cancel(&kgsl_midframe->timer);
-	}
-}
-EXPORT_SYMBOL(kgsl_pwrscale_midframe_timer_cancel);
-
-static void kgsl_pwrscale_midframe_timer_check(struct work_struct *work)
-{
-	struct kgsl_device *device = kgsl_midframe->device;
-
-	mutex_lock(&device->mutex);
-	if (device->state == KGSL_STATE_ACTIVE)
-		kgsl_pwrscale_update(device);
-	mutex_unlock(&device->mutex);
-}
-
-static enum hrtimer_restart kgsl_pwrscale_midframe_timer(struct hrtimer *timer)
-{
-	struct kgsl_device *device = kgsl_midframe->device;
-
-	queue_work(device->pwrscale.devfreq_wq,
-			&kgsl_midframe->timer_check_ws);
-
-	return HRTIMER_NORESTART;
-}
-
 /*
  * kgsl_pwrscale_disable - temporarily disable the governor
  * @device: The device
- * @turbo: Indicates if pwrlevel should be forced to turbo
  *
  * Temporarily disable the governor, to prevent interference
  * with profiling tools that expect a fixed clock frequency.
  * This function must be called with the device mutex locked.
  */
-void kgsl_pwrscale_disable(struct kgsl_device *device, bool turbo)
+void kgsl_pwrscale_disable(struct kgsl_device *device)
 {
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return;
-
+	BUG_ON(!mutex_is_locked(&device->mutex));
 	if (device->pwrscale.devfreqptr)
 		queue_work(device->pwrscale.devfreq_wq,
 			&device->pwrscale.devfreq_suspend_ws);
 	device->pwrscale.enabled = false;
-	if (turbo)
-		kgsl_pwrctrl_pwrlevel_change(device, KGSL_PWRLEVEL_TURBO);
+	kgsl_pwrctrl_pwrlevel_change(device, KGSL_PWRLEVEL_TURBO);
 }
 EXPORT_SYMBOL(kgsl_pwrscale_disable);
 
@@ -285,8 +214,7 @@ EXPORT_SYMBOL(kgsl_pwrscale_disable);
  */
 void kgsl_pwrscale_enable(struct kgsl_device *device)
 {
-	if (WARN_ON(!mutex_is_locked(&device->mutex)))
-		return;
+	BUG_ON(!mutex_is_locked(&device->mutex));
 
 	if (device->pwrscale.devfreqptr) {
 		queue_work(device->pwrscale.devfreq_wq,
@@ -373,7 +301,7 @@ static bool popp_stable(struct kgsl_device *device)
 		}
 		if (nap_time && go_time) {
 			percent_nap = 100 * nap_time;
-			div64_s64(percent_nap, nap_time + go_time);
+			do_div(percent_nap, nap_time + go_time);
 		}
 		trace_kgsl_popp_nap(device, (int)nap_time / 1000, nap,
 				percent_nap);
@@ -506,18 +434,6 @@ static int popp_trans2(struct kgsl_device *device, int level)
 	return level;
 }
 
-#ifdef DEVFREQ_FLAG_WAKEUP_MAXFREQ
-static inline bool _check_maxfreq(u32 flags)
-{
-	return (flags & DEVFREQ_FLAG_WAKEUP_MAXFREQ);
-}
-#else
-static inline bool _check_maxfreq(u32 flags)
-{
-	return false;
-}
-#endif
-
 /*
  * kgsl_devfreq_target - devfreq_dev_profile.target callback
  * @dev: see devfreq.h
@@ -531,9 +447,8 @@ int kgsl_devfreq_target(struct device *dev, unsigned long *freq, u32 flags)
 	struct kgsl_device *device = dev_get_drvdata(dev);
 	struct kgsl_pwrctrl *pwr;
 	struct kgsl_pwrlevel *pwr_level;
-	int level;
-	unsigned int i;
-	unsigned long cur_freq, rec_freq;
+	int level, i;
+	unsigned long cur_freq;
 
 	if (device == NULL)
 		return -ENODEV;
@@ -543,32 +458,25 @@ int kgsl_devfreq_target(struct device *dev, unsigned long *freq, u32 flags)
 		return 0;
 
 	pwr = &device->pwrctrl;
-	if (_check_maxfreq(flags)) {
+	if (flags & DEVFREQ_FLAG_WAKEUP_MAXFREQ) {
 		/*
 		 * The GPU is about to get suspended,
 		 * but it needs to be at the max power level when waking up
-		 */
+		*/
 		pwr->wakeup_maxpwrlevel = 1;
 		return 0;
 	}
 
-	rec_freq = *freq;
-
 	mutex_lock(&device->mutex);
 	cur_freq = kgsl_pwrctrl_active_freq(pwr);
 	level = pwr->active_pwrlevel;
 	pwr_level = &pwr->pwrlevels[level];
 
 	/* If the governor recommends a new frequency, update it here */
-	if (rec_freq != cur_freq) {
+	if (*freq != cur_freq) {
 		level = pwr->max_pwrlevel;
-		/*
-		 * Array index of pwrlevels[] should be within the permitted
-		 * power levels, i.e., from max_pwrlevel to min_pwrlevel.
-		 */
-		for (i = pwr->min_pwrlevel; (i >= pwr->max_pwrlevel
-					  && i <= pwr->min_pwrlevel); i--)
-			if (rec_freq <= pwr->pwrlevels[i].gpu_freq) {
+		for (i = pwr->min_pwrlevel; i >= pwr->max_pwrlevel; i--)
+			if (*freq <= pwr->pwrlevels[i].gpu_freq) {
 				if (pwr->thermal_cycle == CYCLE_ACTIVE)
 					level = _thermal_adjust(pwr, i);
 				else
@@ -602,7 +510,7 @@ int kgsl_devfreq_get_dev_status(struct device *dev,
 	struct kgsl_device *device = dev_get_drvdata(dev);
 	struct kgsl_pwrctrl *pwrctrl;
 	struct kgsl_pwrscale *pwrscale;
-	ktime_t tmp1, tmp2;
+	ktime_t tmp;
 
 	if (device == NULL)
 		return -ENODEV;
@@ -613,8 +521,6 @@ int kgsl_devfreq_get_dev_status(struct device *dev,
 	pwrctrl = &device->pwrctrl;
 
 	mutex_lock(&device->mutex);
-
-	tmp1 = ktime_get();
 	/*
 	 * If the GPU clock is on grab the latest power counter
 	 * values.  Otherwise the most recent ACTIVE values will
@@ -622,16 +528,14 @@ int kgsl_devfreq_get_dev_status(struct device *dev,
 	 */
 	kgsl_pwrscale_update_stats(device);
 
-	tmp2 = ktime_get();
-	stat->total_time = ktime_us_delta(tmp2, pwrscale->time);
-	pwrscale->time = tmp1;
+	tmp = ktime_get();
+	stat->total_time = ktime_us_delta(tmp, pwrscale->time);
+	pwrscale->time = tmp;
 
 	stat->busy_time = pwrscale->accum_stats.busy_time;
 
 	stat->current_frequency = kgsl_pwrctrl_active_freq(&device->pwrctrl);
 
-	stat->private_data = &device->active_context_count;
-
 	/*
 	 * keep the latest devfreq_dev_status values
 	 * and vbif counters data
@@ -651,8 +555,7 @@ int kgsl_devfreq_get_dev_status(struct device *dev,
 	}
 
 	kgsl_pwrctrl_busy_time(device, stat->total_time, stat->busy_time);
-	trace_kgsl_pwrstats(device, stat->total_time,
-		&pwrscale->accum_stats, device->active_context_count);
+	trace_kgsl_pwrstats(device, stat->total_time, &pwrscale->accum_stats);
 	memset(&pwrscale->accum_stats, 0, sizeof(pwrscale->accum_stats));
 
 	mutex_unlock(&device->mutex);
@@ -689,9 +592,9 @@ EXPORT_SYMBOL(kgsl_devfreq_get_cur_freq);
 /*
  * kgsl_devfreq_add_notifier - add a fine grained notifier.
  * @dev: The device
- * @nb: Notifier block that will receive updates.
+ * @nb: Notifier block that will recieve updates.
  *
- * Add a notifier to receive ADRENO_DEVFREQ_NOTIFY_* events
+ * Add a notifier to recieve ADRENO_DEVFREQ_NOTIFY_* events
  * from the device.
  */
 int kgsl_devfreq_add_notifier(struct device *dev,
@@ -743,7 +646,6 @@ int kgsl_busmon_get_dev_status(struct device *dev,
 			struct devfreq_dev_status *stat)
 {
 	struct xstats *b;
-
 	stat->total_time = last_status.total_time;
 	stat->busy_time = last_status.busy_time;
 	stat->current_frequency = last_status.current_frequency;
@@ -758,30 +660,6 @@ int kgsl_busmon_get_dev_status(struct device *dev,
 	return 0;
 }
 
-#ifdef DEVFREQ_FLAG_FAST_HINT
-static inline bool _check_fast_hint(u32 flags)
-{
-	return (flags & DEVFREQ_FLAG_FAST_HINT);
-}
-#else
-static inline bool _check_fast_hint(u32 flags)
-{
-	return false;
-}
-#endif
-
-#ifdef DEVFREQ_FLAG_SLOW_HINT
-static inline bool _check_slow_hint(u32 flags)
-{
-	return (flags & DEVFREQ_FLAG_SLOW_HINT);
-}
-#else
-static inline bool _check_slow_hint(u32 flags)
-{
-	return false;
-}
-#endif
-
 /*
  * kgsl_busmon_target - devfreq_dev_profile.target callback
  * @dev: see devfreq.h
@@ -830,16 +708,12 @@ int kgsl_busmon_target(struct device *dev, unsigned long *freq, u32 flags)
 	}
 
 	b = pwr->bus_mod;
-	if (_check_fast_hint(bus_flag))
-		pwr->bus_mod++;
-	else if (_check_slow_hint(bus_flag))
-		pwr->bus_mod--;
-
-	/* trim calculated change to fit range */
-	if (pwr_level->bus_freq + pwr->bus_mod < pwr_level->bus_min)
-		pwr->bus_mod = -(pwr_level->bus_freq - pwr_level->bus_min);
-	else if (pwr_level->bus_freq + pwr->bus_mod > pwr_level->bus_max)
-		pwr->bus_mod = pwr_level->bus_max - pwr_level->bus_freq;
+	if ((bus_flag & DEVFREQ_FLAG_FAST_HINT) &&
+		((pwr_level->bus_freq + pwr->bus_mod) < pwr_level->bus_max))
+			pwr->bus_mod++;
+	 else if ((bus_flag & DEVFREQ_FLAG_SLOW_HINT) &&
+		((pwr_level->bus_freq + pwr->bus_mod) > pwr_level->bus_min))
+			pwr->bus_mod--;
 
 	/* Update bus vote if AB or IB is modified */
 	if ((pwr->bus_mod != b) || (pwr->bus_ab_mbytes != ab_mbytes)) {
@@ -857,119 +731,6 @@ int kgsl_busmon_get_cur_freq(struct device *dev, unsigned long *freq)
 	return 0;
 }
 
-/*
- * opp_notify - Callback function registered to receive OPP events.
- * @nb: The notifier block
- * @type: The event type. Two OPP events are expected in this function:
- *      - OPP_EVENT_ENABLE: an GPU OPP is enabled. The in_opp parameter
- *	contains the OPP that is enabled
- *	- OPP_EVENT_DISALBE: an GPU OPP is disabled. The in_opp parameter
- *	contains the OPP that is disabled.
- * @in_opp: the GPU OPP whose status is changed and triggered the event
- *
- * GPU OPP event callback function. The function subscribe GPU OPP status
- * change and update thermal power level accordingly.
- */
-
-static int opp_notify(struct notifier_block *nb,
-	unsigned long type, void *in_opp)
-{
-	int result = -EINVAL, level, min_level, max_level;
-	struct kgsl_pwrctrl *pwr = container_of(nb, struct kgsl_pwrctrl, nb);
-	struct kgsl_device *device = container_of(pwr,
-			struct kgsl_device, pwrctrl);
-	struct device *dev = &device->pdev->dev;
-	struct dev_pm_opp *opp;
-	unsigned long min_freq = 0, max_freq = pwr->pwrlevels[0].gpu_freq;
-
-	if (type != OPP_EVENT_ENABLE && type != OPP_EVENT_DISABLE)
-		return result;
-
-	rcu_read_lock();
-	opp = dev_pm_opp_find_freq_floor(dev, &max_freq);
-	if (IS_ERR(opp)) {
-		rcu_read_unlock();
-		return PTR_ERR(opp);
-	}
-
-	opp = dev_pm_opp_find_freq_ceil(dev, &min_freq);
-	if (IS_ERR(opp))
-		min_freq = pwr->pwrlevels[pwr->min_pwrlevel].gpu_freq;
-
-	rcu_read_unlock();
-
-	mutex_lock(&device->mutex);
-
-	max_level = pwr->thermal_pwrlevel;
-	min_level = pwr->thermal_pwrlevel_floor;
-
-	/* Thermal limit cannot be lower than lowest non-zero operating freq */
-	for (level = 0; level < (pwr->num_pwrlevels - 1); level++) {
-		if (pwr->pwrlevels[level].gpu_freq == max_freq)
-			max_level = level;
-		if (pwr->pwrlevels[level].gpu_freq == min_freq)
-			min_level = level;
-	}
-
-	pwr->thermal_pwrlevel = max_level;
-	pwr->thermal_pwrlevel_floor = min_level;
-
-	/* Update the current level using the new limit */
-	kgsl_pwrctrl_pwrlevel_change(device, pwr->active_pwrlevel);
-	mutex_unlock(&device->mutex);
-
-	return 0;
-}
-
-/*
- * kgsl_opp_add_notifier - add a fine grained notifier.
- * @dev: The device
- * @nb: Notifier block that will receive updates.
- *
- * Add a notifier to receive GPU OPP_EVENT_* events
- * from the OPP framework.
- */
-static int kgsl_opp_add_notifier(struct device *dev,
-		struct notifier_block *nb)
-{
-	struct srcu_notifier_head *nh;
-	int ret = 0;
-
-	rcu_read_lock();
-	nh = dev_pm_opp_get_notifier(dev);
-	if (IS_ERR(nh))
-		ret = PTR_ERR(nh);
-	rcu_read_unlock();
-	if (!ret)
-		ret = srcu_notifier_chain_register(nh, nb);
-
-	return ret;
-}
-
-/*
- * kgsl_opp_remove_notifier - remove registered opp event notifier.
- * @dev: The device
- * @nb: Notifier block that will receive updates.
- *
- * Remove gpu notifier that receives GPU OPP_EVENT_* events
- * from the OPP framework.
- */
-static int kgsl_opp_remove_notifier(struct device *dev,
-		struct notifier_block *nb)
-{
-	struct srcu_notifier_head *nh;
-	int ret = 0;
-
-	rcu_read_lock();
-	nh = dev_pm_opp_get_notifier(dev);
-	if (IS_ERR(nh))
-		ret = PTR_ERR(nh);
-	rcu_read_unlock();
-	if (!ret)
-		ret = srcu_notifier_chain_unregister(nh, nb);
-
-	return ret;
-}
 
 /*
  * kgsl_pwrscale_init - Initialize pwrscale.
@@ -1000,10 +761,6 @@ int kgsl_pwrscale_init(struct device *dev, const char *governor)
 	gpu_profile = &pwrscale->gpu_profile;
 	profile = &pwrscale->gpu_profile.profile;
 
-	pwr->nb.notifier_call = opp_notify;
-
-	kgsl_opp_add_notifier(dev, &pwr->nb);
-
 	srcu_init_notifier_head(&pwrscale->nh);
 
 	profile->initial_freq =
@@ -1033,29 +790,6 @@ int kgsl_pwrscale_init(struct device *dev, const char *governor)
 	data->disable_busy_time_burst = of_property_read_bool(
 		device->pdev->dev.of_node, "qcom,disable-busy-time-burst");
 
-	if (pwrscale->ctxt_aware_enable) {
-		data->ctxt_aware_enable = pwrscale->ctxt_aware_enable;
-		data->bin.ctxt_aware_target_pwrlevel =
-			pwrscale->ctxt_aware_target_pwrlevel;
-		data->bin.ctxt_aware_busy_penalty =
-			pwrscale->ctxt_aware_busy_penalty;
-	}
-
-	if (of_property_read_bool(device->pdev->dev.of_node,
-			"qcom,enable-midframe-timer")) {
-		kgsl_midframe = kzalloc(
-				sizeof(struct kgsl_midframe_info), GFP_KERNEL);
-		if (kgsl_midframe) {
-			hrtimer_init(&kgsl_midframe->timer,
-					CLOCK_MONOTONIC, HRTIMER_MODE_REL);
-			kgsl_midframe->timer.function =
-					kgsl_pwrscale_midframe_timer;
-			kgsl_midframe->device = device;
-		} else
-			KGSL_PWR_ERR(device,
-				"Failed to enable-midframe-timer feature\n");
-	}
-
 	/*
 	 * If there is a separate GX power rail, allow
 	 * independent modification to its voltage through
@@ -1083,10 +817,6 @@ int kgsl_pwrscale_init(struct device *dev, const char *governor)
 	}
 
 	pwrscale->devfreqptr = devfreq;
-	pwrscale->cooling_dev = of_devfreq_cooling_register(
-					device->pdev->dev.of_node, devfreq);
-	if (IS_ERR(pwrscale->cooling_dev))
-		pwrscale->cooling_dev = NULL;
 
 	pwrscale->gpu_profile.bus_devfreq = NULL;
 	if (data->bus.num) {
@@ -1108,9 +838,6 @@ int kgsl_pwrscale_init(struct device *dev, const char *governor)
 	INIT_WORK(&pwrscale->devfreq_suspend_ws, do_devfreq_suspend);
 	INIT_WORK(&pwrscale->devfreq_resume_ws, do_devfreq_resume);
 	INIT_WORK(&pwrscale->devfreq_notify_ws, do_devfreq_notify);
-	if (kgsl_midframe)
-		INIT_WORK(&kgsl_midframe->timer_check_ws,
-				kgsl_pwrscale_midframe_timer_check);
 
 	pwrscale->next_governor_call = ktime_add_us(ktime_get(),
 			KGSL_GOVERNOR_CALL_INTERVAL);
@@ -1123,14 +850,6 @@ int kgsl_pwrscale_init(struct device *dev, const char *governor)
 		pwrscale->history[i].type = i;
 	}
 
-	/* Add links to the devfreq sysfs nodes */
-	kgsl_gpu_sysfs_add_link(device->gpu_sysfs_kobj,
-			 &pwrscale->devfreqptr->dev.kobj, "governor",
-			"gpu_governor");
-	kgsl_gpu_sysfs_add_link(device->gpu_sysfs_kobj,
-			 &pwrscale->devfreqptr->dev.kobj,
-			"available_governors", "gpu_available_governor");
-
 	return 0;
 }
 EXPORT_SYMBOL(kgsl_pwrscale_init);
@@ -1145,24 +864,17 @@ void kgsl_pwrscale_close(struct kgsl_device *device)
 {
 	int i;
 	struct kgsl_pwrscale *pwrscale;
-	struct kgsl_pwrctrl *pwr;
 
-	pwr = &device->pwrctrl;
+	BUG_ON(!mutex_is_locked(&device->mutex));
+
 	pwrscale = &device->pwrscale;
 	if (!pwrscale->devfreqptr)
 		return;
-	if (pwrscale->cooling_dev)
-		devfreq_cooling_unregister(pwrscale->cooling_dev);
-
-	kgsl_pwrscale_midframe_timer_cancel(device);
 	flush_workqueue(pwrscale->devfreq_wq);
 	destroy_workqueue(pwrscale->devfreq_wq);
 	devfreq_remove_device(device->pwrscale.devfreqptr);
-	kfree(kgsl_midframe);
-	kgsl_midframe = NULL;
 	device->pwrscale.devfreqptr = NULL;
 	srcu_cleanup_notifier_head(&device->pwrscale.nh);
-	kgsl_opp_remove_notifier(&device->pdev->dev, &pwr->nb);
 	for (i = 0; i < KGSL_PWREVENT_MAX; i++)
 		kfree(pwrscale->history[i].events);
 }
@@ -1191,7 +903,6 @@ static void do_devfreq_notify(struct work_struct *work)
 	struct kgsl_pwrscale *pwrscale = container_of(work,
 			struct kgsl_pwrscale, devfreq_notify_ws);
 	struct devfreq *devfreq = pwrscale->devfreqptr;
-
 	srcu_notifier_call_chain(&pwrscale->nh,
 				 ADRENO_DEVFREQ_NOTIFY_RETIRE,
 				 devfreq);
diff --git a/drivers/gpu/msm/kgsl_pwrscale.h b/drivers/gpu/msm/kgsl_pwrscale.h
index 9e28b6594b8d..3d2e7e857470 100644
--- a/drivers/gpu/msm/kgsl_pwrscale.h
+++ b/drivers/gpu/msm/kgsl_pwrscale.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2010-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2010-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -90,12 +90,6 @@ struct kgsl_pwr_history {
  * @history - History of power events with timestamps and durations
  * @popp_level - Current level of POPP mitigation
  * @popp_state - Control state for POPP, on/off, recently pushed, etc
- * @cooling_dev - Thermal cooling device handle
- * @ctxt_aware_enable - Whether or not ctxt aware DCVS feature is enabled
- * @ctxt_aware_busy_penalty - The time in microseconds required to trigger
- * ctxt aware power level jump
- * @ctxt_aware_target_pwrlevel - pwrlevel to jump on in case of ctxt aware
- * power level jump
  */
 struct kgsl_pwrscale {
 	struct devfreq *devfreqptr;
@@ -117,10 +111,6 @@ struct kgsl_pwrscale {
 	struct kgsl_pwr_history history[KGSL_PWREVENT_MAX];
 	int popp_level;
 	unsigned long popp_state;
-	struct thermal_cooling_device *cooling_dev;
-	bool ctxt_aware_enable;
-	unsigned int ctxt_aware_target_pwrlevel;
-	unsigned int ctxt_aware_busy_penalty;
 };
 
 int kgsl_pwrscale_init(struct device *dev, const char *governor);
@@ -132,20 +122,15 @@ void kgsl_pwrscale_busy(struct kgsl_device *device);
 void kgsl_pwrscale_sleep(struct kgsl_device *device);
 void kgsl_pwrscale_wake(struct kgsl_device *device);
 
-void kgsl_pwrscale_midframe_timer_restart(struct kgsl_device *device);
-void kgsl_pwrscale_midframe_timer_cancel(struct kgsl_device *device);
-
 void kgsl_pwrscale_enable(struct kgsl_device *device);
-void kgsl_pwrscale_disable(struct kgsl_device *device, bool turbo);
+void kgsl_pwrscale_disable(struct kgsl_device *device);
 
 int kgsl_devfreq_target(struct device *dev, unsigned long *freq, u32 flags);
-int kgsl_devfreq_get_dev_status(struct device *dev,
-			struct devfreq_dev_status *stat);
+int kgsl_devfreq_get_dev_status(struct device *, struct devfreq_dev_status *);
 int kgsl_devfreq_get_cur_freq(struct device *dev, unsigned long *freq);
 
 int kgsl_busmon_target(struct device *dev, unsigned long *freq, u32 flags);
-int kgsl_busmon_get_dev_status(struct device *dev,
-			struct devfreq_dev_status *stat);
+int kgsl_busmon_get_dev_status(struct device *, struct devfreq_dev_status *);
 int kgsl_busmon_get_cur_freq(struct device *dev, unsigned long *freq);
 
 bool kgsl_popp_check(struct kgsl_device *device);
diff --git a/drivers/gpu/msm/kgsl_sharedmem.c b/drivers/gpu/msm/kgsl_sharedmem.c
index df88b9ae20ac..e8396ab0321a 100644
--- a/drivers/gpu/msm/kgsl_sharedmem.c
+++ b/drivers/gpu/msm/kgsl_sharedmem.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2002,2007-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2017 The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -20,10 +20,10 @@
 #include <linux/scatterlist.h>
 #include <soc/qcom/scm.h>
 #include <soc/qcom/secure_buffer.h>
-#include <linux/ratelimit.h>
 
 #include "kgsl.h"
 #include "kgsl_sharedmem.h"
+#include "kgsl_cffdump.h"
 #include "kgsl_device.h"
 #include "kgsl_log.h"
 #include "kgsl_mmu.h"
@@ -97,73 +97,6 @@ struct mem_entry_stats {
 
 static void kgsl_cma_unlock_secure(struct kgsl_memdesc *memdesc);
 
-static ssize_t
-imported_mem_show(struct kgsl_process_private *priv,
-				int type, char *buf)
-{
-	struct kgsl_mem_entry *entry;
-	uint64_t imported_mem = 0;
-	int id = 0;
-
-	spin_lock(&priv->mem_lock);
-	for (entry = idr_get_next(&priv->mem_idr, &id); entry;
-		id++, entry = idr_get_next(&priv->mem_idr, &id)) {
-
-		int egl_surface_count = 0, egl_image_count = 0;
-		struct kgsl_memdesc *m = &entry->memdesc;
-
-		if ((kgsl_memdesc_usermem_type(m) != KGSL_MEM_ENTRY_ION) ||
-			entry->pending_free || (kgsl_mem_entry_get(entry) == 0))
-			continue;
-		spin_unlock(&priv->mem_lock);
-
-		kgsl_get_egl_counts(entry, &egl_surface_count,
-				&egl_image_count);
-
-		if (kgsl_memdesc_get_memtype(m) ==
-				KGSL_MEMTYPE_EGL_SURFACE)
-			imported_mem += m->size;
-		else if (egl_surface_count == 0) {
-			uint64_t size = m->size;
-
-			do_div(size, (egl_image_count ?
-					egl_image_count : 1));
-			imported_mem += size;
-		}
-
-		kgsl_mem_entry_put(entry);
-		spin_lock(&priv->mem_lock);
-	}
-	spin_unlock(&priv->mem_lock);
-
-	return scnprintf(buf, PAGE_SIZE, "%llu\n", imported_mem);
-}
-
-static ssize_t
-gpumem_mapped_show(struct kgsl_process_private *priv,
-				int type, char *buf)
-{
-	return scnprintf(buf, PAGE_SIZE, "%llu\n",
-			priv->gpumem_mapped);
-}
-
-static ssize_t
-gpumem_unmapped_show(struct kgsl_process_private *priv, int type, char *buf)
-{
-	if (priv->gpumem_mapped > priv->stats[type].cur)
-		return -EIO;
-
-	return scnprintf(buf, PAGE_SIZE, "%llu\n",
-			priv->stats[type].cur - priv->gpumem_mapped);
-}
-
-static struct kgsl_mem_entry_attribute debug_memstats[] = {
-	__MEM_ENTRY_ATTR(0, imported_mem, imported_mem_show),
-	__MEM_ENTRY_ATTR(0, gpumem_mapped, gpumem_mapped_show),
-	__MEM_ENTRY_ATTR(KGSL_MEM_ENTRY_KERNEL, gpumem_unmapped,
-				gpumem_unmapped_show),
-};
-
 /**
  * Show the current amount of memory allocated for the given memtype
  */
@@ -193,10 +126,12 @@ static ssize_t mem_entry_sysfs_show(struct kobject *kobj,
 	ssize_t ret;
 
 	/*
-	 * kgsl_process_init_sysfs takes a refcount to the process_private,
-	 * which is put when the kobj is released. This implies that priv will
-	 * not be freed until this function completes, and no further locking
-	 * is needed.
+	 * 1. sysfs_remove_file waits for reads to complete before the node
+	 *    is deleted.
+	 * 2. kgsl_process_init_sysfs takes a refcount to the process_private,
+	 *    which is put at the end of kgsl_process_uninit_sysfs.
+	 * These two conditions imply that priv will not be freed until this
+	 * function completes, and no further locking is needed.
 	 */
 	priv = kobj ? container_of(kobj, struct kgsl_process_private, kobj) :
 			NULL;
@@ -209,22 +144,12 @@ static ssize_t mem_entry_sysfs_show(struct kobject *kobj,
 	return ret;
 }
 
-static void mem_entry_release(struct kobject *kobj)
-{
-	struct kgsl_process_private *priv;
-
-	priv = container_of(kobj, struct kgsl_process_private, kobj);
-	/* Put the refcount we got in kgsl_process_init_sysfs */
-	kgsl_process_private_put(priv);
-}
-
 static const struct sysfs_ops mem_entry_sysfs_ops = {
 	.show = mem_entry_sysfs_show,
 };
 
 static struct kobj_type ktype_mem_entry = {
 	.sysfs_ops = &mem_entry_sysfs_ops,
-	.release = &mem_entry_release,
 };
 
 static struct mem_entry_stats mem_stats[] = {
@@ -247,6 +172,8 @@ kgsl_process_uninit_sysfs(struct kgsl_process_private *private)
 	}
 
 	kobject_put(&private->kobj);
+	/* Put the refcount we got in kgsl_process_init_sysfs */
+	kgsl_process_private_put(private);
 }
 
 /**
@@ -286,13 +213,7 @@ void kgsl_process_init_sysfs(struct kgsl_device *device,
 			&mem_stats[i].max_attr.attr))
 			WARN(1, "Couldn't create sysfs file '%s'\n",
 				mem_stats[i].max_attr.attr.name);
-	}
 
-	for (i = 0; i < ARRAY_SIZE(debug_memstats); i++) {
-		if (sysfs_create_file(&private->kobj,
-			&debug_memstats[i].attr))
-			WARN(1, "Couldn't create sysfs file '%s'\n",
-				debug_memstats[i].attr.name);
 	}
 }
 
@@ -391,6 +312,9 @@ kgsl_sharedmem_init_sysfs(void)
 		drv_attr_list);
 }
 
+static int kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
+				uint64_t size);
+
 static int kgsl_cma_alloc_secure(struct kgsl_device *device,
 			struct kgsl_memdesc *memdesc, uint64_t size);
 
@@ -413,7 +337,7 @@ int kgsl_allocate_user(struct kgsl_device *device,
 {
 	int ret;
 
-	kgsl_memdesc_init(device, memdesc, flags);
+	memdesc->flags = flags;
 
 	if (kgsl_mmu_get_mmutype(device) == KGSL_MMU_TYPE_NONE)
 		ret = kgsl_sharedmem_alloc_contig(device, memdesc, size);
@@ -445,8 +369,6 @@ static int kgsl_page_alloc_vmfault(struct kgsl_memdesc *memdesc,
 		get_page(page);
 		vmf->page = page;
 
-		memdesc->mapsize += PAGE_SIZE;
-
 		return 0;
 	}
 
@@ -464,8 +386,7 @@ static void kgsl_page_alloc_unmap_kernel(struct kgsl_memdesc *memdesc)
 {
 	mutex_lock(&kernel_map_global_lock);
 	if (!memdesc->hostptr) {
-		/* If already unmapped the refcount should be 0 */
-		WARN_ON(memdesc->hostptr_count);
+		BUG_ON(memdesc->hostptr_count);
 		goto done;
 	}
 	memdesc->hostptr_count--;
@@ -481,6 +402,9 @@ static void kgsl_page_alloc_unmap_kernel(struct kgsl_memdesc *memdesc)
 
 static void kgsl_page_alloc_free(struct kgsl_memdesc *memdesc)
 {
+	unsigned int i = 0;
+	struct scatterlist *sg;
+
 	kgsl_page_alloc_unmap_kernel(memdesc);
 	/* we certainly do not expect the hostptr to still be mapped */
 	BUG_ON(memdesc->hostptr);
@@ -505,20 +429,40 @@ static void kgsl_page_alloc_free(struct kgsl_memdesc *memdesc)
 		atomic_long_sub(memdesc->size, &kgsl_driver.stats.page_alloc);
 	}
 
-	if (memdesc->priv & KGSL_MEMDESC_TZ_LOCKED) {
-		struct sg_page_iter sg_iter;
+	/* Free pages using the pages array for non secure paged memory */
+	if (memdesc->pages != NULL) {
+		for (i = 0; i < memdesc->page_count;) {
+			struct page *p = memdesc->pages[i];
 
-		for_each_sg_page(memdesc->sgt->sgl, &sg_iter,
-					memdesc->sgt->nents, 0)
-			ClearPagePrivate(sg_page_iter_page(&sg_iter));
+			i += 1 << compound_order(p);
+			__free_pages(p, compound_order(p));
+		}
+	} else {
+		for_each_sg(memdesc->sgt->sgl, sg, memdesc->sgt->nents, i) {
+			/*
+			 * sg_alloc_table_from_pages() will collapse any
+			 * physically adjacent pages into a single scatterlist
+			 * entry. We cannot just call __free_pages() on the
+			 * entire set since we cannot ensure that the size is a
+			 * whole order. Instead, free each page or compound page
+			 * group individually.
+			 */
+			struct page *p = sg_page(sg), *next;
+			unsigned int j = 0, count;
 
-	}
+			while (j < (sg->length/PAGE_SIZE)) {
+				if (memdesc->priv & KGSL_MEMDESC_TZ_LOCKED)
+					ClearPagePrivate(p);
 
-	/* Free pages using the pages array for non secure paged memory */
-	if (memdesc->pages != NULL)
-		kgsl_pool_free_pages(memdesc->pages, memdesc->page_count);
-	else
-		kgsl_pool_free_sgt(memdesc->sgt);
+				count = 1 << compound_order(p);
+				next = nth_page(p, count);
+				__free_pages(p, compound_order(p));
+				p = next;
+				j += count;
+
+			}
+		}
+	}
 
 }
 
@@ -584,7 +528,7 @@ static int kgsl_contiguous_vmfault(struct kgsl_memdesc *memdesc,
 
 static void kgsl_cma_coherent_free(struct kgsl_memdesc *memdesc)
 {
-	unsigned long attrs = 0;
+	struct dma_attrs *attrs = NULL;
 
 	if (memdesc->hostptr) {
 		if (memdesc->priv & KGSL_MEMDESC_SECURE) {
@@ -639,29 +583,29 @@ static inline unsigned int _fixup_cache_range_op(unsigned int op)
 }
 #endif
 
-static inline void _cache_op(unsigned int op,
-			const void *start, const void *end)
+static int kgsl_do_cache_op(struct page *page, void *addr,
+		uint64_t offset, uint64_t size, unsigned int op)
 {
+	void (*cache_op)(const void *, const void *);
+
 	/*
 	 * The dmac_xxx_range functions handle addresses and sizes that
 	 * are not aligned to the cacheline size correctly.
 	 */
 	switch (_fixup_cache_range_op(op)) {
 	case KGSL_CACHE_OP_FLUSH:
-		dmac_flush_range(start, end);
+		cache_op = dmac_flush_range;
 		break;
 	case KGSL_CACHE_OP_CLEAN:
-		dmac_clean_range(start, end);
+		cache_op = dmac_clean_range;
 		break;
 	case KGSL_CACHE_OP_INV:
-		dmac_inv_range(start, end);
+		cache_op = dmac_inv_range;
 		break;
+	default:
+		return -EINVAL;
 	}
-}
 
-static int kgsl_do_cache_op(struct page *page, void *addr,
-		uint64_t offset, uint64_t size, unsigned int op)
-{
 	if (page != NULL) {
 		unsigned long pfn = page_to_pfn(page) + offset / PAGE_SIZE;
 		/*
@@ -681,8 +625,7 @@ static int kgsl_do_cache_op(struct page *page, void *addr,
 
 				page = pfn_to_page(pfn++);
 				addr = kmap_atomic(page);
-				_cache_op(op, addr + offset,
-						addr + offset + len);
+				cache_op(addr + offset, addr + offset + len);
 				kunmap_atomic(addr);
 
 				size -= len;
@@ -695,7 +638,7 @@ static int kgsl_do_cache_op(struct page *page, void *addr,
 		addr = page_address(page);
 	}
 
-	_cache_op(op, addr + offset, addr + offset + (size_t) size);
+	cache_op(addr + offset, addr + offset + (size_t) size);
 	return 0;
 }
 
@@ -704,8 +647,8 @@ int kgsl_cache_range_op(struct kgsl_memdesc *memdesc, uint64_t offset,
 {
 	void *addr = NULL;
 	struct sg_table *sgt = NULL;
-	struct scatterlist *sg;
-	unsigned int i, pos = 0;
+	struct scatterlist *sg = NULL;
+	unsigned int i = 0, pos = 0;
 	int ret = 0;
 
 	if (size == 0 || size > UINT_MAX)
@@ -769,53 +712,30 @@ int kgsl_cache_range_op(struct kgsl_memdesc *memdesc, uint64_t offset,
 }
 EXPORT_SYMBOL(kgsl_cache_range_op);
 
-void kgsl_memdesc_init(struct kgsl_device *device,
-			struct kgsl_memdesc *memdesc, uint64_t flags)
+#ifndef CONFIG_ALLOC_BUFFERS_IN_4K_CHUNKS
+static inline int get_page_size(size_t size, unsigned int align)
 {
-	struct kgsl_mmu *mmu = &device->mmu;
-	unsigned int align;
-
-	memset(memdesc, 0, sizeof(*memdesc));
-	/* Turn off SVM if the system doesn't support it */
-	if (!kgsl_mmu_use_cpu_map(mmu))
-		flags &= ~((uint64_t) KGSL_MEMFLAGS_USE_CPU_MAP);
-
-	/* Secure memory disables advanced addressing modes */
-	if (flags & KGSL_MEMFLAGS_SECURE)
-		flags &= ~((uint64_t) KGSL_MEMFLAGS_USE_CPU_MAP);
-
-	/* Disable IO coherence if it is not supported on the chip */
-	if (!MMU_FEATURE(mmu, KGSL_MMU_IO_COHERENT))
-		flags &= ~((uint64_t) KGSL_MEMFLAGS_IOCOHERENT);
-
-	if (MMU_FEATURE(mmu, KGSL_MMU_NEED_GUARD_PAGE))
-		memdesc->priv |= KGSL_MEMDESC_GUARD_PAGE;
-
-	if (flags & KGSL_MEMFLAGS_SECURE)
-		memdesc->priv |= KGSL_MEMDESC_SECURE;
-
-	memdesc->flags = flags;
-	memdesc->dev = device->dev->parent;
-
-	align = max_t(unsigned int,
-		(memdesc->flags & KGSL_MEMALIGN_MASK) >> KGSL_MEMALIGN_SHIFT,
-		ilog2(PAGE_SIZE));
-	kgsl_memdesc_set_align(memdesc, align);
+	return (align >= ilog2(SZ_64K) && size >= SZ_64K)
+					? SZ_64K : PAGE_SIZE;
+}
+#else
+static inline int get_page_size(size_t size, unsigned int align)
+{
+	return PAGE_SIZE;
 }
+#endif
 
-int
+static int
 kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
 			uint64_t size)
 {
 	int ret = 0;
-	unsigned int j, page_size, len_alloc;
-	unsigned int pcount = 0;
+	unsigned int j, pcount = 0, page_size, len_alloc;
 	size_t len;
+	pgprot_t page_prot = pgprot_writecombine(PAGE_KERNEL);
+	void *ptr;
 	unsigned int align;
-
-	static DEFINE_RATELIMIT_STATE(_rs,
-					DEFAULT_RATELIMIT_INTERVAL,
-					DEFAULT_RATELIMIT_BURST);
+	unsigned int step = ((VMALLOC_END - VMALLOC_START)/8) >> PAGE_SHIFT;
 
 	size = PAGE_ALIGN(size);
 	if (size == 0 || size > UINT_MAX)
@@ -823,29 +743,15 @@ kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
 
 	align = (memdesc->flags & KGSL_MEMALIGN_MASK) >> KGSL_MEMALIGN_SHIFT;
 
-	/*
-	 * As 1MB is the max supported page size, use the alignment
-	 * corresponding to 1MB page to make sure higher order pages
-	 * are used if possible for a given memory size. Also, we
-	 * don't need to update alignment in memdesc flags in case
-	 * higher order page is used, as memdesc flags represent the
-	 * virtual alignment specified by the user which is anyways
-	 * getting satisfied.
-	 */
-	if (align < ilog2(SZ_1M))
-		align = ilog2(SZ_1M);
-
-	page_size = kgsl_get_page_size(size, align);
+	page_size = get_page_size(size, align);
 
 	/*
 	 * The alignment cannot be less than the intended page size - it can be
-	 * larger however to accommodate hardware quirks
+	 * larger however to accomodate hardware quirks
 	 */
 
-	if (align < ilog2(page_size)) {
+	if (align < ilog2(page_size))
 		kgsl_memdesc_set_align(memdesc, ilog2(page_size));
-		align = ilog2(page_size);
-	}
 
 	/*
 	 * There needs to be enough room in the page array to be able to
@@ -865,8 +771,6 @@ kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
 	 */
 
 	memdesc->pages = kgsl_malloc(len_alloc * sizeof(struct page *));
-	memdesc->page_count = 0;
-	memdesc->size = 0;
 
 	if (memdesc->pages == NULL) {
 		ret = -ENOMEM;
@@ -876,15 +780,33 @@ kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
 	len = size;
 
 	while (len > 0) {
-		int page_count;
-
-		page_count = kgsl_pool_alloc_page(&page_size,
-					memdesc->pages + pcount,
-					len_alloc - pcount,
-					&align);
-		if (page_count <= 0) {
-			if (page_count == -EAGAIN)
+		struct page *page;
+		gfp_t gfp_mask = __GFP_HIGHMEM;
+		int j;
+
+		/* don't waste space at the end of the allocation*/
+		if (len < page_size)
+			page_size = PAGE_SIZE;
+
+		/*
+		 * Don't do some of the more aggressive memory recovery
+		 * techniques for large order allocations
+		 */
+		if (page_size != PAGE_SIZE)
+			gfp_mask |= __GFP_COMP | __GFP_NORETRY | __GFP_NOWARN;
+		else
+			gfp_mask |= GFP_KERNEL;
+
+		if (sharedmem_noretry_flag == true)
+			gfp_mask |= __GFP_NORETRY | __GFP_NOWARN;
+
+		page = alloc_pages(gfp_mask, get_order(page_size));
+
+		if (page == NULL) {
+			if (page_size != PAGE_SIZE) {
+				page_size = PAGE_SIZE;
 				continue;
+			}
 
 			/*
 			 * Update sglen and memdesc size,as requested allocation
@@ -893,8 +815,7 @@ kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
 			 */
 			memdesc->size = (size - len);
 
-			if (sharedmem_noretry_flag != true &&
-					__ratelimit(&_rs))
+			if (sharedmem_noretry_flag != true)
 				KGSL_CORE_ERR(
 					"Out of memory: only allocated %lldKB of %lldKB requested\n",
 					(size - len) >> 10, size >> 10);
@@ -903,13 +824,13 @@ kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
 			goto done;
 		}
 
-		pcount += page_count;
+		for (j = 0; j < page_size >> PAGE_SHIFT; j++)
+			memdesc->pages[pcount++] = nth_page(page, j);
+
 		len -= page_size;
 		memdesc->size += page_size;
-		memdesc->page_count += page_count;
+		memdesc->page_count = pcount;
 
-		/* Get the needed page size for the next iteration */
-		page_size = kgsl_get_page_size(len, align);
 	}
 
 	/* Call to the hypervisor to lock any secure buffer allocations */
@@ -964,17 +885,59 @@ kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
 		goto done;
 	}
 
+	/*
+	 * All memory that goes to the user has to be zeroed out before it gets
+	 * exposed to userspace. This means that the memory has to be mapped in
+	 * the kernel, zeroed (memset) and then unmapped.  This also means that
+	 * the dcache has to be flushed to ensure coherency between the kernel
+	 * and user pages. We used to pass __GFP_ZERO to alloc_page which mapped
+	 * zeroed and unmaped each individual page, and then we had to turn
+	 * around and call flush_dcache_page() on that page to clear the caches.
+	 * This was killing us for performance. Instead, we found it is much
+	 * faster to allocate the pages without GFP_ZERO, map a chunk of the
+	 * range ('step' pages), memset it, flush it and then unmap
+	 * - this results in a factor of 4 improvement for speed for large
+	 * buffers. There is a small decrease in speed for small buffers,
+	 * but only on the order of a few microseconds at best. The 'step'
+	 * size is based on a guess at the amount of free vmalloc space, but
+	 * will scale down if there's not enough free space.
+	 */
+	for (j = 0; j < pcount; j += step) {
+		step = min(step, pcount - j);
+
+		ptr = vmap(&memdesc->pages[j], step, VM_IOREMAP, page_prot);
+
+		if (ptr != NULL) {
+			memset(ptr, 0, step * PAGE_SIZE);
+			dmac_flush_range(ptr, ptr + step * PAGE_SIZE);
+			vunmap(ptr);
+		} else {
+			int k;
+			/* Very, very, very slow path */
+
+			for (k = j; k < j + step; k++) {
+				ptr = kmap_atomic(memdesc->pages[k]);
+				memset(ptr, 0, PAGE_SIZE);
+				dmac_flush_range(ptr, ptr + PAGE_SIZE);
+				kunmap_atomic(ptr);
+			}
+			/* scale down the step size to avoid this path */
+			if (step > 1)
+				step >>= 1;
+		}
+	}
+
 	KGSL_STATS_ADD(memdesc->size, &kgsl_driver.stats.page_alloc,
 		&kgsl_driver.stats.page_alloc_max);
 
 done:
 	if (ret) {
 		if (memdesc->pages) {
-			unsigned int count = 1;
+			for (j = 0; j < memdesc->page_count;) {
+				struct page *p = memdesc->pages[j];
 
-			for (j = 0; j < pcount; j += count) {
-				count = 1 << compound_order(memdesc->pages[j]);
-				kgsl_pool_free_page(memdesc->pages[j]);
+				j += 1 << compound_order(p);
+				__free_pages(p, compound_order(p));
 			}
 		}
 
@@ -1003,6 +966,8 @@ void kgsl_sharedmem_free(struct kgsl_memdesc *memdesc)
 
 	if (memdesc->pages)
 		kgsl_free(memdesc->pages);
+
+	memset(memdesc, 0, sizeof(*memdesc));
 }
 EXPORT_SYMBOL(kgsl_sharedmem_free);
 
@@ -1012,23 +977,15 @@ kgsl_sharedmem_readl(const struct kgsl_memdesc *memdesc,
 			uint64_t offsetbytes)
 {
 	uint32_t *src;
-
-	if (WARN_ON(memdesc == NULL || memdesc->hostptr == NULL ||
-		dst == NULL))
-		return -EINVAL;
-
+	BUG_ON(memdesc == NULL || memdesc->hostptr == NULL || dst == NULL);
 	WARN_ON(offsetbytes % sizeof(uint32_t) != 0);
 	if (offsetbytes % sizeof(uint32_t) != 0)
 		return -EINVAL;
 
-	WARN_ON(offsetbytes > (memdesc->size - sizeof(uint32_t)));
-	if (offsetbytes > (memdesc->size - sizeof(uint32_t)))
+	WARN_ON(offsetbytes + sizeof(uint32_t) > memdesc->size);
+	if (offsetbytes + sizeof(uint32_t) > memdesc->size)
 		return -ERANGE;
 
-	/*
-	 * We are reading shared memory between CPU and GPU.
-	 * Make sure reads before this are complete
-	 */
 	rmb();
 	src = (uint32_t *)(memdesc->hostptr + offsetbytes);
 	*dst = *src;
@@ -1043,24 +1000,20 @@ kgsl_sharedmem_writel(struct kgsl_device *device,
 			uint32_t src)
 {
 	uint32_t *dst;
-
-	if (WARN_ON(memdesc == NULL || memdesc->hostptr == NULL))
-		return -EINVAL;
-
+	BUG_ON(memdesc == NULL || memdesc->hostptr == NULL);
 	WARN_ON(offsetbytes % sizeof(uint32_t) != 0);
 	if (offsetbytes % sizeof(uint32_t) != 0)
 		return -EINVAL;
 
-	WARN_ON(offsetbytes > (memdesc->size - sizeof(uint32_t)));
-	if (offsetbytes > (memdesc->size - sizeof(uint32_t)))
+	WARN_ON(offsetbytes + sizeof(uint32_t) > memdesc->size);
+	if (offsetbytes + sizeof(uint32_t) > memdesc->size)
 		return -ERANGE;
+	kgsl_cffdump_write(device,
+		memdesc->gpuaddr + offsetbytes,
+		src);
 	dst = (uint32_t *)(memdesc->hostptr + offsetbytes);
 	*dst = src;
 
-	/*
-	 * We are writing to shared memory between CPU and GPU.
-	 * Make sure write above is posted immediately
-	 */
 	wmb();
 
 	return 0;
@@ -1073,17 +1026,13 @@ kgsl_sharedmem_readq(const struct kgsl_memdesc *memdesc,
 			uint64_t offsetbytes)
 {
 	uint64_t *src;
-
-	if (WARN_ON(memdesc == NULL || memdesc->hostptr == NULL ||
-		dst == NULL))
-		return -EINVAL;
-
+	BUG_ON(memdesc == NULL || memdesc->hostptr == NULL || dst == NULL);
 	WARN_ON(offsetbytes % sizeof(uint32_t) != 0);
 	if (offsetbytes % sizeof(uint32_t) != 0)
 		return -EINVAL;
 
-	WARN_ON(offsetbytes > (memdesc->size - sizeof(uint32_t)));
-	if (offsetbytes > (memdesc->size - sizeof(uint32_t)))
+	WARN_ON(offsetbytes + sizeof(uint32_t) > memdesc->size);
+	if (offsetbytes + sizeof(uint32_t) > memdesc->size)
 		return -ERANGE;
 
 	/*
@@ -1104,17 +1053,18 @@ kgsl_sharedmem_writeq(struct kgsl_device *device,
 			uint64_t src)
 {
 	uint64_t *dst;
-
-	if (WARN_ON(memdesc == NULL || memdesc->hostptr == NULL))
-		return -EINVAL;
-
+	BUG_ON(memdesc == NULL || memdesc->hostptr == NULL);
 	WARN_ON(offsetbytes % sizeof(uint32_t) != 0);
 	if (offsetbytes % sizeof(uint32_t) != 0)
 		return -EINVAL;
 
-	WARN_ON(offsetbytes > (memdesc->size - sizeof(uint32_t)));
-	if (offsetbytes > (memdesc->size - sizeof(uint32_t)))
+	WARN_ON(offsetbytes + sizeof(uint32_t) > memdesc->size);
+	if (offsetbytes + sizeof(uint32_t) > memdesc->size)
 		return -ERANGE;
+	kgsl_cffdump_write(device,
+		lower_32_bits(memdesc->gpuaddr + offsetbytes), src);
+	kgsl_cffdump_write(device,
+		upper_32_bits(memdesc->gpuaddr + offsetbytes), src);
 	dst = (uint64_t *)(memdesc->hostptr + offsetbytes);
 	*dst = src;
 
@@ -1133,12 +1083,12 @@ kgsl_sharedmem_set(struct kgsl_device *device,
 		const struct kgsl_memdesc *memdesc, uint64_t offsetbytes,
 		unsigned int value, uint64_t sizebytes)
 {
-	if (WARN_ON(memdesc == NULL || memdesc->hostptr == NULL))
-		return -EINVAL;
-
-	if (WARN_ON(offsetbytes + sizebytes > memdesc->size))
-		return -EINVAL;
+	BUG_ON(memdesc == NULL || memdesc->hostptr == NULL);
+	BUG_ON(offsetbytes + sizebytes > memdesc->size);
 
+	kgsl_cffdump_memset(device,
+		memdesc->gpuaddr + offsetbytes, value,
+		sizebytes);
 	memset(memdesc->hostptr + offsetbytes, value, sizebytes);
 	return 0;
 }
@@ -1179,7 +1129,7 @@ void kgsl_get_memory_usage(char *name, size_t name_size, uint64_t memflags)
 	else if (type < ARRAY_SIZE(memtype_str) && memtype_str[type] != NULL)
 		strlcpy(name, memtype_str[type], name_size);
 	else
-		snprintf(name, name_size, "VK/others(%3d)", type);
+		snprintf(name, name_size, "unknown(%3d)", type);
 }
 EXPORT_SYMBOL(kgsl_get_memory_usage);
 
@@ -1197,7 +1147,7 @@ int kgsl_sharedmem_alloc_contig(struct kgsl_device *device,
 	memdesc->dev = device->dev->parent;
 
 	memdesc->hostptr = dma_alloc_attrs(memdesc->dev, (size_t) size,
-		&memdesc->physaddr, GFP_KERNEL, 0);
+		&memdesc->physaddr, GFP_KERNEL, NULL);
 
 	if (memdesc->hostptr == NULL) {
 		result = -ENOMEM;
diff --git a/drivers/gpu/msm/kgsl_sharedmem.h b/drivers/gpu/msm/kgsl_sharedmem.h
index 976752d67b22..24c58c51dd5b 100644
--- a/drivers/gpu/msm/kgsl_sharedmem.h
+++ b/drivers/gpu/msm/kgsl_sharedmem.h
@@ -1,4 +1,5 @@
-/* Copyright (c) 2002,2007-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2002,2007-2015,2017, The Linux Foundation. All rights
+ * reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -57,9 +58,6 @@ int kgsl_cache_range_op(struct kgsl_memdesc *memdesc,
 			uint64_t offset, uint64_t size,
 			unsigned int op);
 
-void kgsl_memdesc_init(struct kgsl_device *device,
-			struct kgsl_memdesc *memdesc, uint64_t flags);
-
 void kgsl_process_init_sysfs(struct kgsl_device *device,
 		struct kgsl_process_private *private);
 void kgsl_process_uninit_sysfs(struct kgsl_process_private *private);
@@ -73,9 +71,6 @@ int kgsl_allocate_user(struct kgsl_device *device,
 
 void kgsl_get_memory_usage(char *str, size_t len, uint64_t memflags);
 
-int kgsl_sharedmem_page_alloc_user(struct kgsl_memdesc *memdesc,
-				uint64_t size);
-
 #define MEMFLAGS(_flags, _mask, _shift) \
 	((unsigned int) (((_flags) & (_mask)) >> (_shift)))
 
@@ -92,18 +87,6 @@ kgsl_memdesc_get_align(const struct kgsl_memdesc *memdesc)
 		KGSL_MEMALIGN_SHIFT);
 }
 
-/*
- * kgsl_memdesc_get_pagesize - Get pagesize based on alignment
- * @memdesc - the memdesc
- *
- * Returns the pagesize based on memdesc alignment
- */
-static inline int
-kgsl_memdesc_get_pagesize(const struct kgsl_memdesc *memdesc)
-{
-	return (1 << kgsl_memdesc_get_align(memdesc));
-}
-
 /*
  * kgsl_memdesc_get_cachemode - Get cache mode of a memdesc
  * @memdesc: the memdesc
@@ -134,9 +117,8 @@ kgsl_memdesc_set_align(struct kgsl_memdesc *memdesc, unsigned int align)
 	if (align > 32)
 		align = 32;
 
-	memdesc->flags &= ~(uint64_t)KGSL_MEMALIGN_MASK;
-	memdesc->flags |= (uint64_t)((align << KGSL_MEMALIGN_SHIFT) &
-					KGSL_MEMALIGN_MASK);
+	memdesc->flags &= ~KGSL_MEMALIGN_MASK;
+	memdesc->flags |= (align << KGSL_MEMALIGN_SHIFT) & KGSL_MEMALIGN_MASK;
 	return 0;
 }
 
@@ -225,19 +207,12 @@ kgsl_memdesc_has_guard_page(const struct kgsl_memdesc *memdesc)
  *
  * Returns guard page size
  */
-static inline uint64_t
-kgsl_memdesc_guard_page_size(const struct kgsl_memdesc *memdesc)
+static inline int
+kgsl_memdesc_guard_page_size(const struct kgsl_mmu *mmu,
+				const struct kgsl_memdesc *memdesc)
 {
-	if (!kgsl_memdesc_has_guard_page(memdesc))
-		return 0;
-
-	if (kgsl_memdesc_is_secured(memdesc)) {
-		if (memdesc->pagetable != NULL &&
-				memdesc->pagetable->mmu != NULL)
-			return memdesc->pagetable->mmu->secure_align_mask + 1;
-	}
-
-	return PAGE_SIZE;
+	return kgsl_memdesc_is_secured(memdesc) ? mmu->secure_align_mask + 1 :
+								PAGE_SIZE;
 }
 
 /*
@@ -262,7 +237,10 @@ kgsl_memdesc_use_cpu_map(const struct kgsl_memdesc *memdesc)
 static inline uint64_t
 kgsl_memdesc_footprint(const struct kgsl_memdesc *memdesc)
 {
-	return  memdesc->size + kgsl_memdesc_guard_page_size(memdesc);
+	uint64_t size = memdesc->size;
+	if (kgsl_memdesc_has_guard_page(memdesc))
+		size += SZ_4K;
+	return size;
 }
 
 /*
@@ -281,29 +259,16 @@ kgsl_memdesc_footprint(const struct kgsl_memdesc *memdesc)
  */
 static inline int kgsl_allocate_global(struct kgsl_device *device,
 	struct kgsl_memdesc *memdesc, uint64_t size, uint64_t flags,
-	unsigned int priv, const char *name)
+	unsigned int priv)
 {
 	int ret;
 
-	kgsl_memdesc_init(device, memdesc, flags);
-	memdesc->priv |= priv;
-
-	if (((memdesc->priv & KGSL_MEMDESC_CONTIG) != 0) ||
-		(kgsl_mmu_get_mmutype(device) == KGSL_MMU_TYPE_NONE))
-		ret = kgsl_sharedmem_alloc_contig(device, memdesc,
-						(size_t) size);
-	else {
-		ret = kgsl_sharedmem_page_alloc_user(memdesc, (size_t) size);
-		if (ret == 0) {
-			if (kgsl_memdesc_map(memdesc) == NULL) {
-				kgsl_sharedmem_free(memdesc);
-				ret = -ENOMEM;
-			}
-		}
-	}
+	memdesc->flags = flags;
+	memdesc->priv = priv;
 
+	ret = kgsl_sharedmem_alloc_contig(device, memdesc, (size_t) size);
 	if (ret == 0)
-		kgsl_mmu_add_global(device, memdesc, name);
+		kgsl_mmu_add_global(device, memdesc);
 
 	return ret;
 }
@@ -371,35 +336,4 @@ static inline void kgsl_free_sgt(struct sg_table *sgt)
 	}
 }
 
-#include "kgsl_pool.h"
-
-/**
- * kgsl_get_page_size() - Get supported pagesize
- * @size: Size of the page
- * @align: Desired alignment of the size
- *
- * Return supported pagesize
- */
-#ifndef CONFIG_ALLOC_BUFFERS_IN_4K_CHUNKS
-static inline int kgsl_get_page_size(size_t size, unsigned int align)
-{
-	if (align >= ilog2(SZ_1M) && size >= SZ_1M &&
-		kgsl_pool_avaialable(SZ_1M))
-		return SZ_1M;
-	else if (align >= ilog2(SZ_64K) && size >= SZ_64K &&
-		kgsl_pool_avaialable(SZ_64K))
-		return SZ_64K;
-	else if (align >= ilog2(SZ_8K) && size >= SZ_8K &&
-		kgsl_pool_avaialable(SZ_8K))
-		return SZ_8K;
-	else
-		return PAGE_SIZE;
-}
-#else
-static inline int kgsl_get_page_size(size_t size, unsigned int align)
-{
-	return PAGE_SIZE;
-}
-#endif
-
 #endif /* __KGSL_SHAREDMEM_H */
diff --git a/drivers/gpu/msm/kgsl_snapshot.c b/drivers/gpu/msm/kgsl_snapshot.c
index 0ed17d859080..797e38fc740f 100644
--- a/drivers/gpu/msm/kgsl_snapshot.c
+++ b/drivers/gpu/msm/kgsl_snapshot.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -24,8 +24,6 @@
 #include "kgsl_snapshot.h"
 #include "adreno_cp_parser.h"
 
-static void kgsl_snapshot_save_frozen_objs(struct work_struct *work);
-
 /* Placeholder for list of ib objects that contain all objects in that IB */
 
 struct kgsl_snapshot_cp_obj {
@@ -102,8 +100,8 @@ static u8 *_ctxtptr;
 
 static int snapshot_context_info(int id, void *ptr, void *data)
 {
-	struct kgsl_snapshot_linux_context_v2 *header =
-		(struct kgsl_snapshot_linux_context_v2 *)_ctxtptr;
+	struct kgsl_snapshot_linux_context *header =
+		(struct kgsl_snapshot_linux_context *)_ctxtptr;
 	struct kgsl_context *context = ptr;
 	struct kgsl_device *device;
 
@@ -117,12 +115,10 @@ static int snapshot_context_info(int id, void *ptr, void *data)
 
 	kgsl_readtimestamp(device, context, KGSL_TIMESTAMP_QUEUED,
 		&header->timestamp_queued);
-	kgsl_readtimestamp(device, context, KGSL_TIMESTAMP_CONSUMED,
-		&header->timestamp_consumed);
 	kgsl_readtimestamp(device, context, KGSL_TIMESTAMP_RETIRED,
 		&header->timestamp_retired);
 
-	_ctxtptr += sizeof(struct kgsl_snapshot_linux_context_v2);
+	_ctxtptr += sizeof(struct kgsl_snapshot_linux_context);
 
 	return 0;
 }
@@ -131,23 +127,21 @@ static int snapshot_context_info(int id, void *ptr, void *data)
 static size_t snapshot_os(struct kgsl_device *device,
 	u8 *buf, size_t remain, void *priv)
 {
-	struct kgsl_snapshot_linux_v2 *header =
-		(struct kgsl_snapshot_linux_v2 *)buf;
+	struct kgsl_snapshot_linux *header = (struct kgsl_snapshot_linux *)buf;
 	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
 	int ctxtcount = 0;
 	size_t size = sizeof(*header);
+	u64 temp_ptbase;
 	struct kgsl_context *context;
 
-	/*
-	 * Figure out how many active contexts there are - these will
-	 * be appended on the end of the structure
-	 */
+	/* Figure out how many active contexts there are - these will
+	 * be appended on the end of the structure */
 
 	read_lock(&device->context_lock);
 	idr_for_each(&device->context_idr, snapshot_context_count, &ctxtcount);
 	read_unlock(&device->context_lock);
 
-	size += ctxtcount * sizeof(struct kgsl_snapshot_linux_context_v2);
+	size += ctxtcount * sizeof(struct kgsl_snapshot_linux_context);
 
 	/* Make sure there is enough room for the data */
 	if (remain < size) {
@@ -157,13 +151,13 @@ static size_t snapshot_os(struct kgsl_device *device,
 
 	memset(header, 0, sizeof(*header));
 
-	header->osid = KGSL_SNAPSHOT_OS_LINUX_V3;
+	header->osid = KGSL_SNAPSHOT_OS_LINUX;
+
+	header->state = SNAPSHOT_STATE_HUNG;
 
 	/* Get the kernel build information */
-	strlcpy(header->release, init_utsname()->release,
-			sizeof(header->release));
-	strlcpy(header->version, init_utsname()->version,
-			sizeof(header->version));
+	strlcpy(header->release, utsname()->release, sizeof(header->release));
+	strlcpy(header->version, utsname()->version, sizeof(header->version));
 
 	/* Get the Unix time for the timestamp */
 	header->seconds = get_seconds();
@@ -184,8 +178,9 @@ static size_t snapshot_os(struct kgsl_device *device,
 	context = kgsl_context_get(device, header->current_context);
 
 	/* Get the current PT base */
-	header->ptbase = kgsl_mmu_get_current_ttbr0(&device->mmu);
-
+	temp_ptbase = kgsl_mmu_get_current_ttbr0(&device->mmu);
+	/* Truncate to 32 bits in case LPAE is used */
+	header->ptbase = (__u32)temp_ptbase;
 	/* And the PID for the task leader */
 	if (context) {
 		header->pid = context->tid;
@@ -208,44 +203,6 @@ static size_t snapshot_os(struct kgsl_device *device,
 	return size;
 }
 
-/* Snapshot the Linux specific information */
-static size_t snapshot_os_no_ctxt(struct kgsl_device *device,
-	u8 *buf, size_t remain, void *priv)
-{
-	struct kgsl_snapshot_linux_v2 *header =
-		(struct kgsl_snapshot_linux_v2 *)buf;
-	struct kgsl_pwrctrl *pwr = &device->pwrctrl;
-	size_t size = sizeof(*header);
-
-	/* Make sure there is enough room for the data */
-	if (remain < size) {
-		SNAPSHOT_ERR_NOMEM(device, "OS");
-		return 0;
-	}
-
-	memset(header, 0, sizeof(*header));
-
-	header->osid = KGSL_SNAPSHOT_OS_LINUX_V3;
-
-	/* Get the kernel build information */
-	strlcpy(header->release, init_utsname()->release,
-			sizeof(header->release));
-	strlcpy(header->version, init_utsname()->version,
-			sizeof(header->version));
-
-	/* Get the Unix time for the timestamp */
-	header->seconds = get_seconds();
-
-	/* Remember the power information */
-	header->power_flags = pwr->power_flags;
-	header->power_level = pwr->active_pwrlevel;
-	header->power_interval_timeout = pwr->interval_timeout;
-	header->grpclk = kgsl_get_clkrate(pwr->grp_clks[0]);
-
-	/* Return the size of the data segment */
-	return size;
-}
-
 static void kgsl_snapshot_put_object(struct kgsl_snapshot_object *obj)
 {
 	list_del(&obj->node);
@@ -266,7 +223,7 @@ static void kgsl_snapshot_put_object(struct kgsl_snapshot_object *obj)
  * Return 1 if the object is already in the list - this can save us from
  * having to parse the same thing over again. There are 2 lists that are
  * tracking objects so check for the object in both lists
- */
+*/
 int kgsl_snapshot_have_object(struct kgsl_snapshot *snapshot,
 	struct kgsl_process_private *process,
 	uint64_t gpuaddr, uint64_t size)
@@ -349,16 +306,9 @@ int kgsl_snapshot_get_object(struct kgsl_snapshot *snapshot,
 	 */
 
 	mem_type = kgsl_memdesc_get_memtype(&entry->memdesc);
-	if (mem_type == KGSL_MEMTYPE_TEXTURE ||
-		mem_type == KGSL_MEMTYPE_EGL_SURFACE ||
-		mem_type == KGSL_MEMTYPE_EGL_IMAGE) {
-		ret = 0;
-		goto err_put;
-	}
-
-	/* Do not save sparse memory */
-	if (entry->memdesc.flags & KGSL_MEMFLAGS_SPARSE_VIRT ||
-			entry->memdesc.flags & KGSL_MEMFLAGS_SPARSE_PHYS) {
+	if (KGSL_MEMTYPE_TEXTURE == mem_type ||
+		KGSL_MEMTYPE_EGL_SURFACE == mem_type ||
+		KGSL_MEMTYPE_EGL_IMAGE == mem_type) {
 		ret = 0;
 		goto err_put;
 	}
@@ -394,7 +344,6 @@ int kgsl_snapshot_get_object(struct kgsl_snapshot *snapshot,
 				gpuaddr, size)) {
 			uint64_t end1 = obj->gpuaddr + obj->size;
 			uint64_t end2 = gpuaddr + size;
-
 			if (obj->gpuaddr > gpuaddr)
 				obj->gpuaddr = gpuaddr;
 			if (end1 > end2)
@@ -510,6 +459,8 @@ static size_t kgsl_snapshot_dump_indexed_regs(struct kgsl_device *device,
 	unsigned int *data = (unsigned int *)(buf + sizeof(*header));
 	int i;
 
+	BUG_ON(!mutex_is_locked(&device->mutex));
+
 	if (remain < (iregs->count * 4) + sizeof(*header)) {
 		SNAPSHOT_ERR_NOMEM(device, "INDEXED REGS");
 		return 0;
@@ -547,7 +498,6 @@ void kgsl_snapshot_indexed_registers(struct kgsl_device *device,
 		unsigned int count)
 {
 	struct kgsl_snapshot_indexed_registers iregs;
-
 	iregs.index = index;
 	iregs.data = data;
 	iregs.start = start;
@@ -612,34 +562,16 @@ void kgsl_snapshot_add_section(struct kgsl_device *device, u16 id,
 	snapshot->size += header->size;
 }
 
-static void kgsl_free_snapshot(struct kgsl_snapshot *snapshot)
-{
-	struct kgsl_snapshot_object *obj, *tmp;
-
-	wait_for_completion(&snapshot->dump_gate);
-
-	list_for_each_entry_safe(obj, tmp,
-				&snapshot->obj_list, node)
-		kgsl_snapshot_put_object(obj);
-
-	if (snapshot->mempool)
-		vfree(snapshot->mempool);
-
-	kfree(snapshot);
-	KGSL_CORE_ERR("snapshot: objects released\n");
-}
-
 /**
  * kgsl_snapshot() - construct a device snapshot
  * @device: device to snapshot
  * @context: the context that is hung, might be NULL if unknown.
- * @gmu_fault: whether this snapshot is triggered by a GMU fault.
  *
  * Given a device, construct a binary snapshot dump of the current device state
  * and store it in the device snapshot memory.
  */
 void kgsl_device_snapshot(struct kgsl_device *device,
-		struct kgsl_context *context, bool gmu_fault)
+		struct kgsl_context *context)
 {
 	struct kgsl_snapshot_header *header = device->snapshot_memory.ptr;
 	struct kgsl_snapshot *snapshot;
@@ -652,29 +584,16 @@ void kgsl_device_snapshot(struct kgsl_device *device,
 		return;
 	}
 
-	if (WARN(!kgsl_state_is_awake(device),
-		"snapshot: device is powered off\n"))
-		return;
-
+	BUG_ON(!kgsl_state_is_awake(device));
 	/* increment the hang count for good book keeping */
 	device->snapshot_faultcount++;
 
 	/*
-	 * Overwrite a non-GMU fault snapshot if a GMU fault occurs.
+	 * The first hang is always the one we are interested in. Don't capture
+	 * a new snapshot instance if the old one hasn't been grabbed yet
 	 */
-	if (device->snapshot != NULL) {
-		if (!device->prioritize_unrecoverable ||
-				!device->snapshot->recovered)
-			return;
-
-		/*
-		 * If another thread is currently reading it, that thread
-		 * will free it, otherwise free it now.
-		 */
-		if (!device->snapshot->sysfs_read)
-			kgsl_free_snapshot(device->snapshot);
-		device->snapshot = NULL;
-	}
+	if (device->snapshot != NULL)
+		return;
 
 	/* Allocate memory for the snapshot instance */
 	snapshot = kzalloc(sizeof(*snapshot), GFP_KERNEL);
@@ -689,10 +608,7 @@ void kgsl_device_snapshot(struct kgsl_device *device,
 	snapshot->start = device->snapshot_memory.ptr;
 	snapshot->ptr = device->snapshot_memory.ptr;
 	snapshot->remain = device->snapshot_memory.size;
-	snapshot->gmu_fault = gmu_fault;
-	snapshot->recovered = false;
-	snapshot->first_read = true;
-	snapshot->sysfs_read = 0;
+	atomic_set(&snapshot->sysfs_read, 0);
 
 	header = (struct kgsl_snapshot_header *) snapshot->ptr;
 
@@ -704,22 +620,12 @@ void kgsl_device_snapshot(struct kgsl_device *device,
 	snapshot->size += sizeof(*header);
 
 	/* Build the Linux specific header */
-	/* We either want to only dump GMU, or we want to dump GPU and GMU */
-	if (gmu_fault) {
-		/* Dump only the GMU */
-		kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_OS,
-				snapshot, snapshot_os_no_ctxt, NULL);
-
-		if (device->ftbl->snapshot_gmu)
-			device->ftbl->snapshot_gmu(device, snapshot);
-	} else {
-		/* Dump GPU and GMU */
-		kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_OS,
-				snapshot, snapshot_os, NULL);
+	kgsl_snapshot_add_section(device, KGSL_SNAPSHOT_SECTION_OS,
+			snapshot, snapshot_os, NULL);
 
-		if (device->ftbl->snapshot)
-			device->ftbl->snapshot(device, snapshot, context);
-	}
+	/* Get the device specific sections */
+	if (device->ftbl->snapshot)
+		device->ftbl->snapshot(device, snapshot, context);
 
 	/*
 	 * The timestamp is the seconds since boot so it is easier to match to
@@ -789,30 +695,6 @@ container_of(a, struct kgsl_snapshot_attribute, attr)
 #define kobj_to_device(a) \
 container_of(a, struct kgsl_device, snapshot_kobj)
 
-static int snapshot_release(struct kgsl_device *device,
-	struct kgsl_snapshot *snapshot)
-{
-	bool snapshot_free = false;
-	int ret = 0;
-
-	mutex_lock(&device->mutex);
-	snapshot->sysfs_read--;
-
-	/*
-	 * If someone's replaced the snapshot, return an error and free
-	 * the snapshot if this is the last thread to read it.
-	 */
-	if (device->snapshot != snapshot) {
-		ret = -EIO;
-		if (!snapshot->sysfs_read)
-			snapshot_free = true;
-	}
-	mutex_unlock(&device->mutex);
-	if (snapshot_free)
-		kgsl_free_snapshot(snapshot);
-	return ret;
-}
-
 /* Dump the sysfs binary data to the user */
 static ssize_t snapshot_show(struct file *filep, struct kobject *kobj,
 	struct bin_attribute *attr, char *buf, loff_t off,
@@ -820,35 +702,20 @@ static ssize_t snapshot_show(struct file *filep, struct kobject *kobj,
 {
 	struct kgsl_device *device = kobj_to_device(kobj);
 	struct kgsl_snapshot *snapshot;
+	struct kgsl_snapshot_object *obj, *tmp;
 	struct kgsl_snapshot_section_header head;
 	struct snapshot_obj_itr itr;
-	int ret = 0;
+	int ret;
 
 	if (device == NULL)
 		return 0;
 
 	mutex_lock(&device->mutex);
 	snapshot = device->snapshot;
-	if (snapshot != NULL) {
-		/*
-		 * If we're reading at a non-zero offset from a new snapshot,
-		 * that means we want to read from the previous snapshot (which
-		 * was overwritten), so return an error
-		 */
-		if (snapshot->first_read) {
-			if (off)
-				ret = -EIO;
-			else
-				snapshot->first_read = false;
-		}
-		if (!ret)
-			snapshot->sysfs_read++;
-	}
+	if (snapshot != NULL)
+		atomic_inc(&snapshot->sysfs_read);
 	mutex_unlock(&device->mutex);
 
-	if (ret)
-		return ret;
-
 	/* Return nothing if we haven't taken a snapshot yet */
 	if (snapshot == NULL)
 		return 0;
@@ -859,7 +726,7 @@ static ssize_t snapshot_show(struct file *filep, struct kobject *kobj,
 	 */
 	ret = wait_for_completion_interruptible(&snapshot->dump_gate);
 	if (ret) {
-		snapshot_release(device, snapshot);
+		atomic_dec(&snapshot->sysfs_read);
 		return ret;
 	}
 
@@ -896,21 +763,29 @@ static ssize_t snapshot_show(struct file *filep, struct kobject *kobj,
 		bool snapshot_free = false;
 
 		mutex_lock(&device->mutex);
-		if (--snapshot->sysfs_read == 0) {
-			if (device->snapshot == snapshot)
-				device->snapshot = NULL;
+		if (atomic_dec_and_test(&snapshot->sysfs_read)) {
+			device->snapshot = NULL;
 			snapshot_free = true;
 		}
 		mutex_unlock(&device->mutex);
 
-		if (snapshot_free)
-			kgsl_free_snapshot(snapshot);
+		if (snapshot_free) {
+			list_for_each_entry_safe(obj, tmp,
+						&snapshot->obj_list, node)
+				kgsl_snapshot_put_object(obj);
+
+			if (snapshot->mempool)
+				vfree(snapshot->mempool);
+
+			kfree(snapshot);
+			KGSL_CORE_ERR("snapshot: objects released\n");
+		}
 		return 0;
 	}
 
 done:
-	ret = snapshot_release(device, snapshot);
-	return (ret < 0) ? ret : itr.write;
+	atomic_dec(&snapshot->sysfs_read);
+	return itr.write;
 }
 
 /* Show the total number of hangs since device boot */
@@ -929,107 +804,15 @@ static ssize_t faultcount_store(struct kgsl_device *device, const char *buf,
 	return count;
 }
 
-/* Show the force_panic request status */
-static ssize_t force_panic_show(struct kgsl_device *device, char *buf)
-{
-	return snprintf(buf, PAGE_SIZE, "%d\n", device->force_panic);
-}
-
-/* Store the panic request value to force_panic */
-static ssize_t force_panic_store(struct kgsl_device *device, const char *buf,
-	size_t count)
-{
-	unsigned int val = 0;
-	int ret;
-
-	if (device && count > 0)
-		device->force_panic = 0;
-
-	ret = kgsl_sysfs_store(buf, &val);
-
-	if (!ret && device)
-		device->force_panic = (bool)val;
-
-	return (ssize_t) ret < 0 ? ret : count;
-}
-
-/* Show the prioritize_unrecoverable status */
-static ssize_t prioritize_unrecoverable_show(
-		struct kgsl_device *device, char *buf)
-{
-	return snprintf(buf, PAGE_SIZE, "%d\n",
-			device->prioritize_unrecoverable);
-}
-
-/* Store the priority value to prioritize unrecoverable */
-static ssize_t prioritize_unrecoverable_store(
-		struct kgsl_device *device, const char *buf, size_t count)
-{
-	unsigned int val = 0;
-	int ret = 0;
-
-	ret = kgsl_sysfs_store(buf, &val);
-	if (!ret && device)
-		device->prioritize_unrecoverable = (bool) val;
-
-	return (ssize_t) ret < 0 ? ret : count;
-}
-
-/* Show the snapshot_crashdumper request status */
-static ssize_t snapshot_crashdumper_show(struct kgsl_device *device, char *buf)
-{
-	return snprintf(buf, PAGE_SIZE, "%d\n", device->snapshot_crashdumper);
-}
-
-
-/* Store the value to snapshot_crashdumper*/
-static ssize_t snapshot_crashdumper_store(struct kgsl_device *device,
-	const char *buf, size_t count)
-{
-	unsigned int val = 0;
-	int ret;
-
-	if (device && count > 0)
-		device->snapshot_crashdumper = 1;
-
-	ret = kgsl_sysfs_store(buf, &val);
-
-	if (!ret && device)
-		device->snapshot_crashdumper = (bool)val;
-
-	return (ssize_t) ret < 0 ? ret : count;
-}
-
 /* Show the timestamp of the last collected snapshot */
 static ssize_t timestamp_show(struct kgsl_device *device, char *buf)
 {
-	unsigned long timestamp;
+	unsigned long timestamp =
+		device->snapshot ? device->snapshot->timestamp : 0;
 
-	mutex_lock(&device->mutex);
-	timestamp = device->snapshot ? device->snapshot->timestamp : 0;
-	mutex_unlock(&device->mutex);
 	return snprintf(buf, PAGE_SIZE, "%lu\n", timestamp);
 }
 
-static ssize_t snapshot_legacy_show(struct kgsl_device *device, char *buf)
-{
-	return snprintf(buf, PAGE_SIZE, "%d\n", device->snapshot_legacy);
-}
-
-static ssize_t snapshot_legacy_store(struct kgsl_device *device,
-	const char *buf, size_t count)
-{
-	unsigned int val = 0;
-	int ret;
-
-	ret = kgsl_sysfs_store(buf, &val);
-
-	if (!ret && device)
-		device->snapshot_legacy = (bool)val;
-
-	return (ssize_t) ret < 0 ? ret : count;
-}
-
 static struct bin_attribute snapshot_attr = {
 	.attr.name = "dump",
 	.attr.mode = 0444,
@@ -1046,13 +829,6 @@ struct kgsl_snapshot_attribute attr_##_name = { \
 
 static SNAPSHOT_ATTR(timestamp, 0444, timestamp_show, NULL);
 static SNAPSHOT_ATTR(faultcount, 0644, faultcount_show, faultcount_store);
-static SNAPSHOT_ATTR(force_panic, 0644, force_panic_show, force_panic_store);
-static SNAPSHOT_ATTR(prioritize_unrecoverable, 0644,
-		prioritize_unrecoverable_show, prioritize_unrecoverable_store);
-static SNAPSHOT_ATTR(snapshot_crashdumper, 0644, snapshot_crashdumper_show,
-	snapshot_crashdumper_store);
-static SNAPSHOT_ATTR(snapshot_legacy, 0644, snapshot_legacy_show,
-	snapshot_legacy_store);
 
 static ssize_t snapshot_sysfs_show(struct kobject *kobj,
 	struct attribute *attr, char *buf)
@@ -1095,7 +871,7 @@ static struct kobj_type ktype_snapshot = {
 
 /**
  * kgsl_device_snapshot_init() - add resources for the device GPU snapshot
- * @device: The device to initialize
+ * @device: The device to initalize
  *
  * Allocate memory for a GPU snapshot for the specified device,
  * and create the sysfs files to manage it
@@ -1132,10 +908,6 @@ int kgsl_device_snapshot_init(struct kgsl_device *device)
 
 	device->snapshot = NULL;
 	device->snapshot_faultcount = 0;
-	device->force_panic = 0;
-	device->prioritize_unrecoverable = true;
-	device->snapshot_crashdumper = 1;
-	device->snapshot_legacy = 0;
 
 	ret = kobject_init_and_add(&device->snapshot_kobj, &ktype_snapshot,
 		&device->dev->kobj, "snapshot");
@@ -1151,26 +923,6 @@ int kgsl_device_snapshot_init(struct kgsl_device *device)
 		goto done;
 
 	ret  = sysfs_create_file(&device->snapshot_kobj, &attr_faultcount.attr);
-	if (ret)
-		goto done;
-
-	ret  = sysfs_create_file(&device->snapshot_kobj,
-			&attr_force_panic.attr);
-	if (ret)
-		goto done;
-
-	ret = sysfs_create_file(&device->snapshot_kobj,
-			&attr_prioritize_unrecoverable.attr);
-	if (ret)
-		goto done;
-
-	ret  = sysfs_create_file(&device->snapshot_kobj,
-			&attr_snapshot_crashdumper.attr);
-	if (ret)
-		goto done;
-
-	ret  = sysfs_create_file(&device->snapshot_kobj,
-			&attr_snapshot_legacy.attr);
 
 done:
 	return ret;
@@ -1196,8 +948,6 @@ void kgsl_device_snapshot_close(struct kgsl_device *device)
 	device->snapshot_memory.ptr = NULL;
 	device->snapshot_memory.size = 0;
 	device->snapshot_faultcount = 0;
-	device->force_panic = 0;
-	device->snapshot_crashdumper = 1;
 }
 EXPORT_SYMBOL(kgsl_device_snapshot_close);
 
@@ -1225,8 +975,7 @@ int kgsl_snapshot_add_ib_obj_list(struct kgsl_snapshot *snapshot,
 	return 0;
 }
 
-static size_t _mempool_add_object(struct kgsl_snapshot *snapshot, u8 *data,
-		struct kgsl_snapshot_object *obj)
+static size_t _mempool_add_object(u8 *data, struct kgsl_snapshot_object *obj)
 {
 	struct kgsl_snapshot_section_header *section =
 		(struct kgsl_snapshot_section_header *)data;
@@ -1252,14 +1001,6 @@ static size_t _mempool_add_object(struct kgsl_snapshot *snapshot, u8 *data,
 		kgsl_mmu_pagetable_get_ttbr0(obj->entry->priv->pagetable);
 	header->type = obj->type;
 
-	if (kgsl_addr_range_overlap(obj->gpuaddr, obj->size,
-				snapshot->ib1base, snapshot->ib1size))
-		snapshot->ib1dumped = true;
-
-	if (kgsl_addr_range_overlap(obj->gpuaddr, obj->size,
-				snapshot->ib2base, snapshot->ib2size))
-		snapshot->ib2dumped = true;
-
 	memcpy(dest, obj->entry->memdesc.hostptr + obj->offset, size);
 	kgsl_memdesc_unmap(&obj->entry->memdesc);
 
@@ -1272,21 +1013,14 @@ static size_t _mempool_add_object(struct kgsl_snapshot *snapshot, u8 *data,
  * is taken
  * @work: The work item that scheduled this work
  */
-static void kgsl_snapshot_save_frozen_objs(struct work_struct *work)
+void kgsl_snapshot_save_frozen_objs(struct work_struct *work)
 {
 	struct kgsl_snapshot *snapshot = container_of(work,
 				struct kgsl_snapshot, work);
-	struct kgsl_device *device = kgsl_get_device(KGSL_DEVICE_3D0);
 	struct kgsl_snapshot_object *obj, *tmp;
 	size_t size = 0;
 	void *ptr;
 
-	if (IS_ERR_OR_NULL(device))
-		return;
-
-	if (snapshot->gmu_fault)
-		goto gmu_only;
-
 	kgsl_snapshot_process_ib_obj_list(snapshot);
 
 	list_for_each_entry(obj, &snapshot->obj_list, node) {
@@ -1308,8 +1042,7 @@ static void kgsl_snapshot_save_frozen_objs(struct work_struct *work)
 	/* even if vmalloc fails, make sure we clean up the obj_list */
 	list_for_each_entry_safe(obj, tmp, &snapshot->obj_list, node) {
 		if (snapshot->mempool) {
-			size_t ret = _mempool_add_object(snapshot, ptr, obj);
-
+			size_t ret = _mempool_add_object(ptr, obj);
 			ptr += ret;
 			snapshot->mempool_size += ret;
 		}
@@ -1324,16 +1057,6 @@ static void kgsl_snapshot_save_frozen_objs(struct work_struct *work)
 	kgsl_process_private_put(snapshot->process);
 	snapshot->process = NULL;
 
-	if (snapshot->ib1base && !snapshot->ib1dumped)
-		KGSL_DRV_ERR(device,
-				"snapshot: Active IB1:%016llx not dumped\n",
-				snapshot->ib1base);
-	else if (snapshot->ib2base && !snapshot->ib2dumped)
-		KGSL_DRV_ERR(device,
-			       "snapshot: Active IB2:%016llx not dumped\n",
-				snapshot->ib2base);
-
-gmu_only:
 	complete_all(&snapshot->dump_gate);
-	BUG_ON(device->force_panic);
+	return;
 }
diff --git a/drivers/gpu/msm/kgsl_snapshot.h b/drivers/gpu/msm/kgsl_snapshot.h
index 340a7db2f67a..8167ff83a18b 100644
--- a/drivers/gpu/msm/kgsl_snapshot.h
+++ b/drivers/gpu/msm/kgsl_snapshot.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -58,15 +58,17 @@ struct kgsl_snapshot_section_header {
 #define KGSL_SNAPSHOT_SECTION_MEMLIST      0x0E01
 #define KGSL_SNAPSHOT_SECTION_MEMLIST_V2   0x0E02
 #define KGSL_SNAPSHOT_SECTION_SHADER       0x1201
-#define KGSL_SNAPSHOT_SECTION_MVC          0x1501
 
 #define KGSL_SNAPSHOT_SECTION_END          0xFFFF
 
 /* OS sub-section header */
 #define KGSL_SNAPSHOT_OS_LINUX             0x0001
-#define KGSL_SNAPSHOT_OS_LINUX_V3          0x00000202
 
 /* Linux OS specific information */
+
+#define SNAPSHOT_STATE_HUNG 0
+#define SNAPSHOT_STATE_RUNNING 1
+
 struct kgsl_snapshot_linux {
 	int osid;                   /* subsection OS identifier */
 	int state;		    /* 1 if the thread is running, 0 for hung */
@@ -85,23 +87,6 @@ struct kgsl_snapshot_linux {
 	unsigned char comm[16];	    /* Name of the process that owns the PT */
 } __packed;
 
-struct kgsl_snapshot_linux_v2 {
-	int osid;                   /* subsection OS identifier */
-	__u32 seconds;		    /* Unix timestamp for the snapshot */
-	__u32 power_flags;            /* Current power flags */
-	__u32 power_level;            /* Current power level */
-	__u32 power_interval_timeout; /* Power interval timeout */
-	__u32 grpclk;                 /* Current GP clock value */
-	__u32 busclk;		    /* Current busclk value */
-	__u64 ptbase;		    /* Current ptbase */
-	__u32 pid;		    /* PID of the process that owns the PT */
-	__u32 current_context;	    /* ID of the current context */
-	__u32 ctxtcount;	    /* Number of contexts appended to section */
-	unsigned char release[32];  /* kernel release */
-	unsigned char version[32];  /* kernel version */
-	unsigned char comm[16];	    /* Name of the process that owns the PT */
-} __packed;
-
 /*
  * This structure contains a record of an active context.
  * These are appended one after another in the OS section below
@@ -114,12 +99,6 @@ struct kgsl_snapshot_linux_context {
 	__u32 timestamp_retired;	/* The last timestamp retired by HW */
 };
 
-struct kgsl_snapshot_linux_context_v2 {
-	__u32 id;			/* The context ID */
-	__u32 timestamp_queued;		/* The last queued timestamp */
-	__u32 timestamp_consumed;	/* The last timestamp consumed by HW */
-	__u32 timestamp_retired;	/* The last timestamp retired by HW */
-};
 /* Ringbuffer sub-section header */
 struct kgsl_snapshot_rb {
 	int start;  /* dword at the start of the dump */
@@ -197,12 +176,6 @@ struct kgsl_snapshot_indexed_regs {
 	int count;     /* Number of dwords in the data */
 } __packed;
 
-/* MVC register sub-section header */
-struct kgsl_snapshot_mvc_regs {
-	int ctxt_id;
-	int cluster_id;
-} __packed;
-
 /* Istore sub-section header */
 struct kgsl_snapshot_istore {
 	int count;   /* Number of instructions in the istore */
@@ -225,7 +198,6 @@ struct kgsl_snapshot_istore {
 #define SNAPSHOT_DEBUG_CP_ROQ     10
 #define SNAPSHOT_DEBUG_SHADER_MEMORY 11
 #define SNAPSHOT_DEBUG_CP_MERCIU 12
-#define SNAPSHOT_DEBUG_SQE_VERSION 14
 
 struct kgsl_snapshot_debug {
 	int type;    /* Type identifier for the attached tata */
@@ -263,6 +235,4 @@ struct kgsl_snapshot_gpu_object_v2 {
 	__u64 size;    /* Size of the object (in dwords) */
 } __packed;
 
-void kgsl_snapshot_push_object(struct kgsl_process_private *process,
-	uint64_t gpuaddr, uint64_t dwords);
 #endif
diff --git a/drivers/gpu/msm/kgsl_sync.c b/drivers/gpu/msm/kgsl_sync.c
index d484894e06f2..3b7b0cd03a7c 100644
--- a/drivers/gpu/msm/kgsl_sync.c
+++ b/drivers/gpu/msm/kgsl_sync.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2018, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -13,6 +13,9 @@
 
 #include <linux/err.h>
 #include <linux/file.h>
+#ifdef CONFIG_ONESHOT_SYNC
+#include <linux/oneshot_sync.h>
+#endif
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/uaccess.h>
@@ -21,87 +24,68 @@
 
 #include "kgsl_sync.h"
 
-static void kgsl_sync_timeline_signal(struct kgsl_sync_timeline *timeline,
+static void kgsl_sync_timeline_signal(struct sync_timeline *timeline,
 	unsigned int timestamp);
 
-static const struct fence_ops kgsl_sync_fence_ops;
-
-static struct kgsl_sync_fence *kgsl_sync_fence_create(
-					struct kgsl_context *context,
-					unsigned int timestamp)
+static struct sync_pt *kgsl_sync_pt_create(struct sync_timeline *timeline,
+	struct kgsl_context *context, unsigned int timestamp)
 {
-	struct kgsl_sync_fence *kfence;
-	struct kgsl_sync_timeline *ktimeline = context->ktimeline;
-	unsigned long flags;
-
-	/* Get a refcount to the timeline. Put when released */
-	if (!kref_get_unless_zero(&ktimeline->kref))
-		return NULL;
-
-	kfence = kzalloc(sizeof(*kfence), GFP_KERNEL);
-	if (kfence == NULL) {
-		kgsl_sync_timeline_put(ktimeline);
-		KGSL_DRV_ERR(context->device, "Couldn't allocate fence\n");
-		return NULL;
-	}
-
-	kfence->parent = ktimeline;
-	kfence->context_id = context->id;
-	kfence->timestamp = timestamp;
-
-	fence_init(&kfence->fence, &kgsl_sync_fence_ops, &ktimeline->lock,
-		ktimeline->fence_context, timestamp);
-
-	/*
-	 * sync_file_create() takes a refcount to the fence. This refcount is
-	 * put when the fence is signaled.
-	 */
-	kfence->sync_file = sync_file_create(&kfence->fence);
-
-	if (kfence->sync_file == NULL) {
-		kgsl_sync_timeline_put(ktimeline);
-		KGSL_DRV_ERR(context->device, "Create sync_file failed\n");
-		kfree(kfence);
-		return NULL;
+	struct sync_pt *pt;
+	pt = sync_pt_create(timeline, (int) sizeof(struct kgsl_sync_pt));
+	if (pt) {
+		struct kgsl_sync_pt *kpt = (struct kgsl_sync_pt *) pt;
+		kpt->context = context;
+		kpt->timestamp = timestamp;
 	}
-
-	spin_lock_irqsave(&ktimeline->lock, flags);
-	list_add_tail(&kfence->child_list, &ktimeline->child_list_head);
-	spin_unlock_irqrestore(&ktimeline->lock, flags);
-
-	return kfence;
+	return pt;
 }
 
-static void kgsl_sync_fence_release(struct fence *fence)
+/*
+ * This should only be called on sync_pts which have been created but
+ * not added to a fence.
+ */
+static void kgsl_sync_pt_destroy(struct sync_pt *pt)
 {
-	struct kgsl_sync_fence *kfence = (struct kgsl_sync_fence *)fence;
+	sync_pt_free(pt);
+}
 
-	kgsl_sync_timeline_put(kfence->parent);
-	kfree(kfence);
+static struct sync_pt *kgsl_sync_pt_dup(struct sync_pt *pt)
+{
+	struct kgsl_sync_pt *kpt = (struct kgsl_sync_pt *) pt;
+	return kgsl_sync_pt_create(pt->parent, kpt->context, kpt->timestamp);
 }
 
-/* Called with ktimeline->lock held */
-bool kgsl_sync_fence_has_signaled(struct fence *fence)
+static int kgsl_sync_pt_has_signaled(struct sync_pt *pt)
 {
-	struct kgsl_sync_fence *kfence = (struct kgsl_sync_fence *)fence;
-	struct kgsl_sync_timeline *ktimeline = kfence->parent;
-	unsigned int ts = kfence->timestamp;
+	struct kgsl_sync_pt *kpt = (struct kgsl_sync_pt *) pt;
+	struct kgsl_sync_timeline *ktimeline =
+		 (struct kgsl_sync_timeline *) pt->parent;
+	unsigned int ts = kpt->timestamp;
+	int ret = 0;
+
+	spin_lock(&ktimeline->lock);
+	ret = (timestamp_cmp(ktimeline->last_timestamp, ts) >= 0);
+	spin_unlock(&ktimeline->lock);
 
-	return (timestamp_cmp(ktimeline->last_timestamp, ts) >= 0);
+	return ret;
 }
 
-bool kgsl_enable_signaling(struct fence *fence)
+static int kgsl_sync_pt_compare(struct sync_pt *a, struct sync_pt *b)
 {
-	return !kgsl_sync_fence_has_signaled(fence);
+	struct kgsl_sync_pt *kpt_a = (struct kgsl_sync_pt *) a;
+	struct kgsl_sync_pt *kpt_b = (struct kgsl_sync_pt *) b;
+	unsigned int ts_a = kpt_a->timestamp;
+	unsigned int ts_b = kpt_b->timestamp;
+	return timestamp_cmp(ts_a, ts_b);
 }
 
-struct kgsl_sync_fence_event_priv {
+struct kgsl_fence_event_priv {
 	struct kgsl_context *context;
 	unsigned int timestamp;
 };
 
 /**
- * kgsl_sync_fence_event_cb - Event callback for a fence timestamp event
+ * kgsl_fence_event_cb - Event callback for a fence timestamp event
  * @device - The KGSL device that expired the timestamp
  * @context- Pointer to the context that owns the event
  * @priv: Private data for the callback
@@ -110,12 +94,11 @@ struct kgsl_sync_fence_event_priv {
  * Signal a fence following the expiration of a timestamp
  */
 
-static void kgsl_sync_fence_event_cb(struct kgsl_device *device,
+static void kgsl_fence_event_cb(struct kgsl_device *device,
 		struct kgsl_event_group *group, void *priv, int result)
 {
-	struct kgsl_sync_fence_event_priv *ev = priv;
-
-	kgsl_sync_timeline_signal(ev->context->ktimeline, ev->timestamp);
+	struct kgsl_fence_event_priv *ev = priv;
+	kgsl_sync_timeline_signal(ev->context->timeline, ev->timestamp);
 	kgsl_context_put(ev->context);
 	kfree(ev);
 }
@@ -123,7 +106,7 @@ static void kgsl_sync_fence_event_cb(struct kgsl_device *device,
 static int _add_fence_event(struct kgsl_device *device,
 	struct kgsl_context *context, unsigned int timestamp)
 {
-	struct kgsl_sync_fence_event_priv *event;
+	struct kgsl_fence_event_priv *event;
 	int ret;
 
 	event = kmalloc(sizeof(*event), GFP_KERNEL);
@@ -141,9 +124,10 @@ static int _add_fence_event(struct kgsl_device *device,
 
 	event->context = context;
 	event->timestamp = timestamp;
+	event->context = context;
 
 	ret = kgsl_add_event(device, &context->events, timestamp,
-		kgsl_sync_fence_event_cb, event);
+		kgsl_fence_event_cb, event);
 
 	if (ret) {
 		kgsl_context_put(context);
@@ -153,17 +137,6 @@ static int _add_fence_event(struct kgsl_device *device,
 	return ret;
 }
 
-/* Only to be used if creating a related event failed */
-static void kgsl_sync_cancel(struct kgsl_sync_fence *kfence)
-{
-	spin_lock(&kfence->parent->lock);
-	if (!list_empty(&kfence->child_list)) {
-		list_del_init(&kfence->child_list);
-		fence_put(&kfence->fence);
-	}
-	spin_unlock(&kfence->parent->lock);
-}
-
 /**
  * kgsl_add_fence_event - Create a new fence event
  * @device - KGSL device to create the event on
@@ -183,8 +156,10 @@ int kgsl_add_fence_event(struct kgsl_device *device,
 {
 	struct kgsl_timestamp_event_fence priv;
 	struct kgsl_context *context;
-	struct kgsl_sync_fence *kfence = NULL;
+	struct sync_pt *pt;
+	struct sync_fence *fence = NULL;
 	int ret = -EINVAL;
+	char fence_name[sizeof(fence->name)] = {};
 	unsigned int cur;
 
 	priv.fence_fd = -1;
@@ -200,10 +175,23 @@ int kgsl_add_fence_event(struct kgsl_device *device,
 	if (test_bit(KGSL_CONTEXT_PRIV_INVALID, &context->priv))
 		goto out;
 
-	kfence = kgsl_sync_fence_create(context, timestamp);
-	if (kfence == NULL) {
-		KGSL_DRV_CRIT_RATELIMIT(device,
-					"kgsl_sync_fence_create failed\n");
+	pt = kgsl_sync_pt_create(context->timeline, context, timestamp);
+	if (pt == NULL) {
+		KGSL_DRV_CRIT_RATELIMIT(device, "kgsl_sync_pt_create failed\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+	snprintf(fence_name, sizeof(fence_name),
+		"%s-pid-%d-ctx-%d-ts-%d",
+		device->name, current->group_leader->pid,
+		context_id, timestamp);
+
+
+	fence = sync_fence_create(fence_name, pt);
+	if (fence == NULL) {
+		/* only destroy pt when not added to fence */
+		kgsl_sync_pt_destroy(pt);
+		KGSL_DRV_CRIT_RATELIMIT(device, "sync_fence_create failed\n");
 		ret = -ENOMEM;
 		goto out;
 	}
@@ -227,7 +215,7 @@ int kgsl_add_fence_event(struct kgsl_device *device,
 
 	if (timestamp_cmp(cur, timestamp) >= 0) {
 		ret = 0;
-		kgsl_sync_timeline_signal(context->ktimeline, cur);
+		kgsl_sync_timeline_signal(context->timeline, cur);
 	} else {
 		ret = _add_fence_event(device, context, timestamp);
 		if (ret)
@@ -238,51 +226,43 @@ int kgsl_add_fence_event(struct kgsl_device *device,
 		ret = -EFAULT;
 		goto out;
 	}
-	fd_install(priv.fence_fd, kfence->sync_file->file);
-
+	sync_fence_install(fence, priv.fence_fd);
 out:
 	kgsl_context_put(context);
 	if (ret) {
 		if (priv.fence_fd >= 0)
 			put_unused_fd(priv.fence_fd);
 
-		if (kfence) {
-			kgsl_sync_cancel(kfence);
-			/*
-			 * Put the refcount of sync file. This will release
-			 * kfence->fence as well.
-			 */
-			fput(kfence->sync_file->file);
-		}
+		if (fence)
+			sync_fence_put(fence);
 	}
 	return ret;
 }
 
-static unsigned int kgsl_sync_fence_get_timestamp(
-					struct kgsl_sync_timeline *ktimeline,
-					enum kgsl_timestamp_type type)
+static unsigned int kgsl_sync_get_timestamp(
+	struct kgsl_sync_timeline *ktimeline, enum kgsl_timestamp_type type)
 {
 	unsigned int ret = 0;
+	struct kgsl_context *context;
 
 	if (ktimeline->device == NULL)
 		return 0;
 
-	kgsl_readtimestamp(ktimeline->device, ktimeline->context, type, &ret);
+	context = kgsl_context_get(ktimeline->device,
+			ktimeline->context_id);
 
+	if (context)
+		kgsl_readtimestamp(ktimeline->device, context, type, &ret);
+
+	kgsl_context_put(context);
 	return ret;
 }
 
-static void kgsl_sync_timeline_value_str(struct fence *fence,
-					char *str, int size)
+static void kgsl_sync_timeline_value_str(struct sync_timeline *sync_timeline,
+					 char *str, int size)
 {
-	struct kgsl_sync_fence *kfence = (struct kgsl_sync_fence *)fence;
-	struct kgsl_sync_timeline *ktimeline = kfence->parent;
-
-	unsigned int timestamp_retired;
-	unsigned int timestamp_queued;
-
-	if (!kref_get_unless_zero(&ktimeline->kref))
-		return;
+	struct kgsl_sync_timeline *ktimeline =
+		(struct kgsl_sync_timeline *) sync_timeline;
 
 	/*
 	 * This callback can be called before the device and spinlock are
@@ -291,250 +271,186 @@ static void kgsl_sync_timeline_value_str(struct fence *fence,
 	 * timestamp of the context will be reported as 0, which is correct
 	 * because the context and timeline are just getting initialized.
 	 */
-	timestamp_retired = kgsl_sync_fence_get_timestamp(ktimeline,
-					KGSL_TIMESTAMP_RETIRED);
-	timestamp_queued = kgsl_sync_fence_get_timestamp(ktimeline,
-					KGSL_TIMESTAMP_QUEUED);
+	unsigned int timestamp_retired = kgsl_sync_get_timestamp(ktimeline,
+		KGSL_TIMESTAMP_RETIRED);
+	unsigned int timestamp_queued = kgsl_sync_get_timestamp(ktimeline,
+		KGSL_TIMESTAMP_QUEUED);
 
 	snprintf(str, size, "%u queued:%u retired:%u",
 		ktimeline->last_timestamp,
 		timestamp_queued, timestamp_retired);
+}
 
-	kgsl_sync_timeline_put(ktimeline);
+static void kgsl_sync_pt_value_str(struct sync_pt *sync_pt,
+				   char *str, int size)
+{
+	struct kgsl_sync_pt *kpt = (struct kgsl_sync_pt *) sync_pt;
+	snprintf(str, size, "%u", kpt->timestamp);
 }
 
-static void kgsl_sync_fence_value_str(struct fence *fence, char *str, int size)
+static int kgsl_sync_fill_driver_data(struct sync_pt *sync_pt, void *data,
+					int size)
 {
-	struct kgsl_sync_fence *kfence = (struct kgsl_sync_fence *)fence;
+	struct kgsl_sync_pt *kpt = (struct kgsl_sync_pt *) sync_pt;
+
+	if (size < sizeof(kpt->timestamp))
+		return -ENOMEM;
 
-	snprintf(str, size, "%u", kfence->timestamp);
+	memcpy(data, &kpt->timestamp, sizeof(kpt->timestamp));
+	return sizeof(kpt->timestamp);
 }
 
-static const char *kgsl_sync_fence_driver_name(struct fence *fence)
+static void kgsl_sync_pt_log(struct sync_pt *sync_pt)
 {
-	return "kgsl-timeline";
+	struct kgsl_sync_pt *kpt = (struct kgsl_sync_pt *) sync_pt;
+	pr_info("-----\n");
+	kgsl_context_dump(kpt->context);
+	pr_info("-----\n");
 }
 
-static const char *kgsl_sync_timeline_name(struct fence *fence)
+static void kgsl_sync_timeline_release_obj(struct sync_timeline *sync_timeline)
 {
-	struct kgsl_sync_fence *kfence = (struct kgsl_sync_fence *)fence;
-	struct kgsl_sync_timeline *ktimeline = kfence->parent;
-
-	return ktimeline->name;
+	/*
+	 * Make sure to free the timeline only after destroy flag is set.
+	 * This is to avoid further accessing to the timeline from KGSL and
+	 * also to catch any unbalanced kref of timeline.
+	 */
+	BUG_ON(sync_timeline && (sync_timeline->destroyed != true));
 }
+static const struct sync_timeline_ops kgsl_sync_timeline_ops = {
+	.driver_name = "kgsl-timeline",
+	.dup = kgsl_sync_pt_dup,
+	.has_signaled = kgsl_sync_pt_has_signaled,
+	.compare = kgsl_sync_pt_compare,
+	.timeline_value_str = kgsl_sync_timeline_value_str,
+	.pt_value_str = kgsl_sync_pt_value_str,
+	.fill_driver_data = kgsl_sync_fill_driver_data,
+	.release_obj = kgsl_sync_timeline_release_obj,
+	.pt_log = kgsl_sync_pt_log,
+};
 
 int kgsl_sync_timeline_create(struct kgsl_context *context)
 {
 	struct kgsl_sync_timeline *ktimeline;
 
-	/*
-	 * Generate a name which includes the thread name, thread id, process
+	/* Generate a name which includes the thread name, thread id, process
 	 * name, process id, and context id. This makes it possible to
-	 * identify the context of a timeline in the sync dump.
-	 */
-	char ktimeline_name[sizeof(ktimeline->name)] = {};
-
-	/* Put context when timeline is released */
-	if (!_kgsl_context_get(context))
-		return -ENOENT;
-
+	 * identify the context of a timeline in the sync dump. */
+	char ktimeline_name[sizeof(context->timeline->name)] = {};
 	snprintf(ktimeline_name, sizeof(ktimeline_name),
-		"%s_%d-%.15s(%d)-%.15s(%d)",
-		context->device->name, context->id,
+		"%s_%.15s(%d)-%.15s(%d)-%d",
+		context->device->name,
 		current->group_leader->comm, current->group_leader->pid,
-		current->comm, current->pid);
+		current->comm, current->pid, context->id);
 
-	ktimeline = kzalloc(sizeof(*ktimeline), GFP_KERNEL);
-	if (ktimeline == NULL) {
-		kgsl_context_put(context);
-		return -ENOMEM;
-	}
+	context->timeline = sync_timeline_create(&kgsl_sync_timeline_ops,
+		(int) sizeof(struct kgsl_sync_timeline), ktimeline_name);
+	if (context->timeline == NULL)
+		return -EINVAL;
 
-	kref_init(&ktimeline->kref);
-	strlcpy(ktimeline->name, ktimeline_name, KGSL_TIMELINE_NAME_LEN);
-	ktimeline->fence_context = fence_context_alloc(1);
+	ktimeline = (struct kgsl_sync_timeline *) context->timeline;
 	ktimeline->last_timestamp = 0;
-	INIT_LIST_HEAD(&ktimeline->child_list_head);
-	spin_lock_init(&ktimeline->lock);
 	ktimeline->device = context->device;
-	ktimeline->context = context;
-
-	context->ktimeline = ktimeline;
+	ktimeline->context_id = context->id;
 
+	spin_lock_init(&ktimeline->lock);
 	return 0;
 }
 
-static void kgsl_sync_timeline_signal(struct kgsl_sync_timeline *ktimeline,
-					unsigned int timestamp)
+static void kgsl_sync_timeline_signal(struct sync_timeline *timeline,
+	unsigned int timestamp)
 {
-	unsigned long flags;
-	struct kgsl_sync_fence *kfence, *next;
-
-	if (!kref_get_unless_zero(&ktimeline->kref))
-		return;
+	struct kgsl_sync_timeline *ktimeline =
+		(struct kgsl_sync_timeline *) timeline;
 
-	spin_lock_irqsave(&ktimeline->lock, flags);
+	spin_lock(&ktimeline->lock);
 	if (timestamp_cmp(timestamp, ktimeline->last_timestamp) > 0)
 		ktimeline->last_timestamp = timestamp;
+	spin_unlock(&ktimeline->lock);
 
-	list_for_each_entry_safe(kfence, next, &ktimeline->child_list_head,
-				child_list) {
-		if (fence_is_signaled_locked(&kfence->fence)) {
-			list_del_init(&kfence->child_list);
-			fence_put(&kfence->fence);
-		}
-	}
-
-	spin_unlock_irqrestore(&ktimeline->lock, flags);
-	kgsl_sync_timeline_put(ktimeline);
+	sync_timeline_signal(timeline);
 }
 
 void kgsl_sync_timeline_destroy(struct kgsl_context *context)
 {
-	kfree(context->ktimeline);
-}
-
-static void kgsl_sync_timeline_release(struct kref *kref)
-{
-	struct kgsl_sync_timeline *ktimeline =
-		container_of(kref, struct kgsl_sync_timeline, kref);
-
-	/*
-	 * Only put the context refcount here. The context destroy function
-	 * will call kgsl_sync_timeline_destroy() to kfree it
-	 */
-	kgsl_context_put(ktimeline->context);
-}
-
-void kgsl_sync_timeline_put(struct kgsl_sync_timeline *ktimeline)
-{
-	if (ktimeline)
-		kref_put(&ktimeline->kref, kgsl_sync_timeline_release);
-}
-
-static const struct fence_ops kgsl_sync_fence_ops = {
-	.get_driver_name = kgsl_sync_fence_driver_name,
-	.get_timeline_name = kgsl_sync_timeline_name,
-	.enable_signaling = kgsl_enable_signaling,
-	.signaled = kgsl_sync_fence_has_signaled,
-	.wait = fence_default_wait,
-	.release = kgsl_sync_fence_release,
-
-	.fence_value_str = kgsl_sync_fence_value_str,
-	.timeline_value_str = kgsl_sync_timeline_value_str,
-};
-
-static void kgsl_sync_fence_callback(struct fence *fence, struct fence_cb *cb)
-{
-	struct kgsl_sync_fence_cb *kcb = (struct kgsl_sync_fence_cb *)cb;
-
-	/*
-	 * If the callback is marked for cancellation in a separate thread,
-	 * let the other thread do the cleanup.
-	 */
-	if (kcb->func(kcb->priv)) {
-		fence_put(kcb->fence);
-		kfree(kcb);
-	}
+	sync_timeline_destroy(context->timeline);
 }
 
-static void kgsl_get_fence_name(struct fence *fence,
-	char *fence_name, int name_len)
+static void kgsl_sync_callback(struct sync_fence *fence,
+	struct sync_fence_waiter *waiter)
 {
-	char *ptr = fence_name;
-	char *last = fence_name + name_len;
-
-	ptr +=  snprintf(ptr, last - ptr, "%s %s",
-			fence->ops->get_driver_name(fence),
-			fence->ops->get_timeline_name(fence));
-
-	if ((ptr + 2) >= last)
-		return;
-
-	if (fence->ops->fence_value_str) {
-		ptr += snprintf(ptr, last - ptr, ": ");
-		fence->ops->fence_value_str(fence, ptr, last - ptr);
-	}
+	struct kgsl_sync_fence_waiter *kwaiter =
+		(struct kgsl_sync_fence_waiter *) waiter;
+	kwaiter->func(kwaiter->priv);
+	sync_fence_put(kwaiter->fence);
+	kfree(kwaiter);
 }
 
-struct kgsl_sync_fence_cb *kgsl_sync_fence_async_wait(int fd,
-	bool (*func)(void *priv), void *priv, char *fence_name, int name_len)
+struct kgsl_sync_fence_waiter *kgsl_sync_fence_async_wait(int fd,
+	void (*func)(void *priv), void *priv)
 {
-	struct kgsl_sync_fence_cb *kcb;
-	struct fence *fence;
+	struct kgsl_sync_fence_waiter *kwaiter;
+	struct sync_fence *fence;
 	int status;
 
-	fence = sync_file_get_fence(fd);
+	fence = sync_fence_fdget(fd);
 	if (fence == NULL)
 		return ERR_PTR(-EINVAL);
 
-	/* create the callback */
-	kcb = kzalloc(sizeof(*kcb), GFP_ATOMIC);
-	if (kcb == NULL) {
-		fence_put(fence);
+	/* create the waiter */
+	kwaiter = kzalloc(sizeof(*kwaiter), GFP_ATOMIC);
+	if (kwaiter == NULL) {
+		sync_fence_put(fence);
 		return ERR_PTR(-ENOMEM);
 	}
 
-	kcb->fence = fence;
-	kcb->priv = priv;
-	kcb->func = func;
+	kwaiter->fence = fence;
+	kwaiter->priv = priv;
+	kwaiter->func = func;
 
-	if (fence_name)
-		kgsl_get_fence_name(fence, fence_name, name_len);
+	strlcpy(kwaiter->name, fence->name, sizeof(kwaiter->name));
 
-	/* if status then error or signaled */
-	status = fence_add_callback(fence, &kcb->fence_cb,
-				kgsl_sync_fence_callback);
+	sync_fence_waiter_init((struct sync_fence_waiter *) kwaiter,
+		kgsl_sync_callback);
 
+	/* if status then error or signaled */
+	status = sync_fence_wait_async(fence,
+		(struct sync_fence_waiter *) kwaiter);
 	if (status) {
-		kfree(kcb);
-		if (!fence_is_signaled(fence))
-			kcb = ERR_PTR(status);
+		kfree(kwaiter);
+		sync_fence_put(fence);
+		if (status < 0)
+			kwaiter = ERR_PTR(status);
 		else
-			kcb = NULL;
-		fence_put(fence);
+			kwaiter = NULL;
 	}
 
-	return kcb;
+	return kwaiter;
 }
 
-/*
- * Cancel the fence async callback and do the cleanup. The caller must make
- * sure that the callback (if run before cancelling) returns false, so that
- * no other thread frees the pointer.
- */
-void kgsl_sync_fence_async_cancel(struct kgsl_sync_fence_cb *kcb)
+int kgsl_sync_fence_async_cancel(struct kgsl_sync_fence_waiter *kwaiter)
 {
-	if (kcb == NULL)
-		return;
+	if (kwaiter == NULL)
+		return 0;
 
-	/*
-	 * After fence_remove_callback() returns, the fence callback is
-	 * either not called at all, or completed without freeing kcb.
-	 * This thread can then put the fence refcount and free kcb.
-	 */
-	fence_remove_callback(kcb->fence, &kcb->fence_cb);
-	fence_put(kcb->fence);
-	kfree(kcb);
+	if (sync_fence_cancel_async(kwaiter->fence,
+		(struct sync_fence_waiter *) kwaiter) == 0) {
+		sync_fence_put(kwaiter->fence);
+		kfree(kwaiter);
+		return 1;
+	}
+	return 0;
 }
 
+#ifdef CONFIG_ONESHOT_SYNC
+
 struct kgsl_syncsource {
 	struct kref refcount;
-	char name[KGSL_TIMELINE_NAME_LEN];
 	int id;
 	struct kgsl_process_private *private;
-	struct list_head child_list_head;
-	spinlock_t lock;
-};
-
-struct kgsl_syncsource_fence {
-	struct fence fence;
-	struct kgsl_syncsource *parent;
-	struct list_head child_list;
+	struct oneshot_sync_timeline *oneshot;
 };
 
-static const struct fence_ops kgsl_syncsource_fence_ops;
-
 long kgsl_ioctl_syncsource_create(struct kgsl_device_private *dev_priv,
 					unsigned int cmd, void *data)
 {
@@ -543,10 +459,7 @@ long kgsl_ioctl_syncsource_create(struct kgsl_device_private *dev_priv,
 	int ret = -EINVAL;
 	int id = 0;
 	struct kgsl_process_private *private = dev_priv->process_priv;
-	char name[KGSL_TIMELINE_NAME_LEN];
-
-	if (!kgsl_process_private_get(private))
-		return ret;
+	char name[32];
 
 	syncsource = kzalloc(sizeof(*syncsource), GFP_KERNEL);
 	if (syncsource == NULL) {
@@ -557,11 +470,14 @@ long kgsl_ioctl_syncsource_create(struct kgsl_device_private *dev_priv,
 	snprintf(name, sizeof(name), "kgsl-syncsource-pid-%d",
 			current->group_leader->pid);
 
+	syncsource->oneshot = oneshot_timeline_create(name);
+	if (syncsource->oneshot == NULL) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
 	kref_init(&syncsource->refcount);
-	strlcpy(syncsource->name, name, KGSL_TIMELINE_NAME_LEN);
 	syncsource->private = private;
-	INIT_LIST_HEAD(&syncsource->child_list_head);
-	spin_lock_init(&syncsource->lock);
 
 	idr_preload(GFP_KERNEL);
 	spin_lock(&private->syncsource_lock);
@@ -579,7 +495,8 @@ long kgsl_ioctl_syncsource_create(struct kgsl_device_private *dev_priv,
 
 out:
 	if (ret) {
-		kgsl_process_private_put(private);
+		if (syncsource && syncsource->oneshot)
+			oneshot_timeline_destroy(syncsource->oneshot);
 		kfree(syncsource);
 	}
 
@@ -611,8 +528,13 @@ static void kgsl_syncsource_destroy(struct kref *kref)
 
 	struct kgsl_process_private *private = syncsource->private;
 
-	/* Done with process private. Release the refcount */
-	kgsl_process_private_put(private);
+	spin_lock(&private->syncsource_lock);
+	if (syncsource->id != 0) {
+		idr_remove(&private->syncsource_idr, syncsource->id);
+		syncsource->id = 0;
+	}
+	oneshot_timeline_destroy(syncsource->oneshot);
+	spin_unlock(&private->syncsource_lock);
 
 	kfree(syncsource);
 }
@@ -623,26 +545,6 @@ void kgsl_syncsource_put(struct kgsl_syncsource *syncsource)
 		kref_put(&syncsource->refcount, kgsl_syncsource_destroy);
 }
 
-static void kgsl_syncsource_cleanup(struct kgsl_process_private *private,
-				struct kgsl_syncsource *syncsource)
-{
-	struct kgsl_syncsource_fence *sfence, *next;
-
-	/* Signal all fences to release any callbacks */
-	spin_lock(&syncsource->lock);
-
-	list_for_each_entry_safe(sfence, next, &syncsource->child_list_head,
-				child_list) {
-		fence_signal_locked(&sfence->fence);
-		list_del_init(&sfence->child_list);
-	}
-
-	spin_unlock(&syncsource->lock);
-
-	/* put reference from syncsource creation */
-	kgsl_syncsource_put(syncsource);
-}
-
 long kgsl_ioctl_syncsource_destroy(struct kgsl_device_private *dev_priv,
 					unsigned int cmd, void *data)
 {
@@ -653,18 +555,18 @@ long kgsl_ioctl_syncsource_destroy(struct kgsl_device_private *dev_priv,
 	spin_lock(&private->syncsource_lock);
 	syncsource = idr_find(&private->syncsource_idr, param->id);
 
-	if (syncsource == NULL) {
-		spin_unlock(&private->syncsource_lock);
-		return -EINVAL;
-	}
-
-	if (syncsource->id != 0) {
-		idr_remove(&private->syncsource_idr, syncsource->id);
+	if (syncsource) {
+		idr_remove(&private->syncsource_idr, param->id);
 		syncsource->id = 0;
 	}
+
 	spin_unlock(&private->syncsource_lock);
 
-	kgsl_syncsource_cleanup(private, syncsource);
+	if (syncsource == NULL)
+		return -EINVAL;
+
+	/* put reference from syncsource creation */
+	kgsl_syncsource_put(syncsource);
 	return 0;
 }
 
@@ -674,34 +576,21 @@ long kgsl_ioctl_syncsource_create_fence(struct kgsl_device_private *dev_priv,
 	struct kgsl_syncsource_create_fence *param = data;
 	struct kgsl_syncsource *syncsource = NULL;
 	int ret = -EINVAL;
-	struct kgsl_syncsource_fence *sfence = NULL;
-	struct sync_file *sync_file = NULL;
+	struct sync_fence *fence = NULL;
 	int fd = -1;
+	char name[32];
+
 
-	/*
-	 * Take a refcount that is released when the fence is released
-	 * (or if fence can't be added to the syncsource).
-	 */
 	syncsource = kgsl_syncsource_get(dev_priv->process_priv,
 					param->id);
 	if (syncsource == NULL)
 		goto out;
 
-	sfence = kzalloc(sizeof(*sfence), GFP_KERNEL);
-	if (sfence == NULL) {
-		ret = -ENOMEM;
-		goto out;
-	}
-	sfence->parent = syncsource;
-
-	/* Use a new fence context for each fence */
-	fence_init(&sfence->fence, &kgsl_syncsource_fence_ops,
-		&syncsource->lock, fence_context_alloc(1), 1);
+	snprintf(name, sizeof(name), "kgsl-syncsource-pid-%d-%d",
+			current->group_leader->pid, syncsource->id);
 
-	sync_file = sync_file_create(&sfence->fence);
-
-	if (sync_file == NULL) {
-		KGSL_DRV_ERR(dev_priv->device, "Create sync_file failed\n");
+	fence = oneshot_fence_create(syncsource->oneshot, name);
+	if (fence == NULL) {
 		ret = -ENOMEM;
 		goto out;
 	}
@@ -713,55 +602,18 @@ long kgsl_ioctl_syncsource_create_fence(struct kgsl_device_private *dev_priv,
 	}
 	ret = 0;
 
-	fd_install(fd, sync_file->file);
+	sync_fence_install(fence, fd);
 
 	param->fence_fd = fd;
-
-	spin_lock(&syncsource->lock);
-	list_add_tail(&sfence->child_list, &syncsource->child_list_head);
-	spin_unlock(&syncsource->lock);
 out:
-	/*
-	 * We're transferring ownership of the fence to the sync file.
-	 * The sync file takes an extra refcount when it is created, so put
-	 * our refcount.
-	 */
-	if (sync_file)
-		fence_put(&sfence->fence);
-
 	if (ret) {
-		if (sync_file)
-			fput(sync_file->file);
-		else if (sfence)
-			fence_put(&sfence->fence);
-		else
-			kgsl_syncsource_put(syncsource);
-	}
-
-	return ret;
-}
-
-static int kgsl_syncsource_signal(struct kgsl_syncsource *syncsource,
-					struct fence *fence)
-{
-	struct kgsl_syncsource_fence *sfence, *next;
-	int ret = -EINVAL;
-
-	spin_lock(&syncsource->lock);
+		if (fence)
+			sync_fence_put(fence);
+		if (fd >= 0)
+			put_unused_fd(fd);
 
-	list_for_each_entry_safe(sfence, next, &syncsource->child_list_head,
-				child_list) {
-		if (fence == &sfence->fence) {
-			fence_signal_locked(fence);
-			list_del_init(&sfence->child_list);
-
-			ret = 0;
-			break;
-		}
 	}
-
-	spin_unlock(&syncsource->lock);
-
+	kgsl_syncsource_put(syncsource);
 	return ret;
 }
 
@@ -771,104 +623,24 @@ long kgsl_ioctl_syncsource_signal_fence(struct kgsl_device_private *dev_priv,
 	int ret = -EINVAL;
 	struct kgsl_syncsource_signal_fence *param = data;
 	struct kgsl_syncsource *syncsource = NULL;
-	struct fence *fence = NULL;
+	struct sync_fence *fence = NULL;
 
 	syncsource = kgsl_syncsource_get(dev_priv->process_priv,
 					param->id);
 	if (syncsource == NULL)
 		goto out;
 
-	fence = sync_file_get_fence(param->fence_fd);
+	fence = sync_fence_fdget(param->fence_fd);
 	if (fence == NULL) {
 		ret = -EBADF;
 		goto out;
 	}
 
-	ret = kgsl_syncsource_signal(syncsource, fence);
+	ret = oneshot_fence_signal(syncsource->oneshot, fence);
 out:
 	if (fence)
-		fence_put(fence);
-	if (syncsource)
-		kgsl_syncsource_put(syncsource);
+		sync_fence_put(fence);
+	kgsl_syncsource_put(syncsource);
 	return ret;
 }
-
-static void kgsl_syncsource_fence_release(struct fence *fence)
-{
-	struct kgsl_syncsource_fence *sfence =
-			(struct kgsl_syncsource_fence *)fence;
-
-	/* Signal if it's not signaled yet */
-	kgsl_syncsource_signal(sfence->parent, fence);
-
-	/* Release the refcount on the syncsource */
-	kgsl_syncsource_put(sfence->parent);
-
-	kfree(sfence);
-}
-
-void kgsl_syncsource_process_release_syncsources(
-		struct kgsl_process_private *private)
-{
-	struct kgsl_syncsource *syncsource;
-	int next = 0;
-
-	while (1) {
-		spin_lock(&private->syncsource_lock);
-		syncsource = idr_get_next(&private->syncsource_idr, &next);
-
-		if (syncsource == NULL) {
-			spin_unlock(&private->syncsource_lock);
-			break;
-		}
-
-		if (syncsource->id != 0) {
-			idr_remove(&private->syncsource_idr, syncsource->id);
-			syncsource->id = 0;
-		}
-		spin_unlock(&private->syncsource_lock);
-
-		kgsl_syncsource_cleanup(private, syncsource);
-		next = next + 1;
-	}
-}
-
-static const char *kgsl_syncsource_get_timeline_name(struct fence *fence)
-{
-	struct kgsl_syncsource_fence *sfence =
-			(struct kgsl_syncsource_fence *)fence;
-	struct kgsl_syncsource *syncsource = sfence->parent;
-
-	return syncsource->name;
-}
-
-static bool kgsl_syncsource_enable_signaling(struct fence *fence)
-{
-	return true;
-}
-
-static const char *kgsl_syncsource_driver_name(struct fence *fence)
-{
-	return "kgsl-syncsource-timeline";
-}
-
-static void kgsl_syncsource_fence_value_str(struct fence *fence,
-						char *str, int size)
-{
-	/*
-	 * Each fence is independent of the others on the same timeline.
-	 * We use a different context for each of them.
-	 */
-	snprintf(str, size, "%llu", fence->context);
-}
-
-static const struct fence_ops kgsl_syncsource_fence_ops = {
-	.get_driver_name = kgsl_syncsource_driver_name,
-	.get_timeline_name = kgsl_syncsource_get_timeline_name,
-	.enable_signaling = kgsl_syncsource_enable_signaling,
-	.wait = fence_default_wait,
-	.release = kgsl_syncsource_fence_release,
-
-	.fence_value_str = kgsl_syncsource_fence_value_str,
-};
-
+#endif
diff --git a/drivers/gpu/msm/kgsl_sync.h b/drivers/gpu/msm/kgsl_sync.h
index 6998b40f77eb..0b6a8d24d930 100644
--- a/drivers/gpu/msm/kgsl_sync.h
+++ b/drivers/gpu/msm/kgsl_sync.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2012-2014,2017-2018 The Linux Foundation. All rights reserved.
+/* Copyright (c) 2012-2014, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -13,69 +13,29 @@
 #ifndef __KGSL_SYNC_H
 #define __KGSL_SYNC_H
 
-#include <linux/sync_file.h>
+#include <linux/sync.h>
 #include "kgsl_device.h"
 
-#define KGSL_TIMELINE_NAME_LEN 32
-
-/**
- * struct kgsl_sync_timeline - A sync timeline associated with a kgsl context
- * @kref: Refcount to keep the struct alive until all its fences are released
- * @name: String to describe this timeline
- * @fence_context: Used by the fence driver to identify fences belonging to
- *		   this context
- * @child_list_head: List head for all fences on this timeline
- * @lock: Spinlock to protect this timeline
- * @last_timestamp: Last timestamp when signaling fences
- * @device: kgsl device
- * @context: kgsl context
- */
 struct kgsl_sync_timeline {
-	struct kref kref;
-	char name[KGSL_TIMELINE_NAME_LEN];
-
-	u64 fence_context;
-
-	struct list_head child_list_head;
-
-	spinlock_t lock;
+	struct sync_timeline timeline;
 	unsigned int last_timestamp;
 	struct kgsl_device *device;
-	struct kgsl_context *context;
+	u32 context_id;
+	spinlock_t lock;
 };
 
-/**
- * struct kgsl_sync_fence - A struct containing a fence and other data
- *				associated with it
- * @fence: The fence struct
- * @sync_file: Pointer to the sync file
- * @parent: Pointer to the kgsl sync timeline this fence is on
- * @child_list: List of fences on the same timeline
- * @context_id: kgsl context id
- * @timestamp: Context timestamp that this fence is associated with
- */
-struct kgsl_sync_fence {
-	struct fence fence;
-	struct sync_file *sync_file;
-	struct kgsl_sync_timeline *parent;
-	struct list_head child_list;
-	u32 context_id;
+struct kgsl_sync_pt {
+	struct sync_pt pt;
+	struct kgsl_context *context;
 	unsigned int timestamp;
 };
 
-/**
- * struct kgsl_sync_fence_cb - Used for fence callbacks
- * fence_cb: Fence callback struct
- * fence: Pointer to the fence for which the callback is done
- * priv: Private data for the callback
- * func: Pointer to the kgsl function to call. This function should return
- * false if the sync callback is marked for cancellation in a separate thread.
- */
-struct kgsl_sync_fence_cb {
-	struct fence_cb fence_cb;
-	struct fence *fence;
+struct kgsl_sync_fence_waiter {
+	struct sync_fence_waiter waiter;
+	struct sync_fence *fence;
+	char name[32];
+	void (*func)(void *priv);
 	void *priv;
-	bool (*func)(void *priv);
 };
 
 struct kgsl_syncsource;
@@ -84,36 +44,14 @@ struct kgsl_syncsource;
 int kgsl_add_fence_event(struct kgsl_device *device,
 	u32 context_id, u32 timestamp, void __user *data, int len,
 	struct kgsl_device_private *owner);
-
 int kgsl_sync_timeline_create(struct kgsl_context *context);
-
 void kgsl_sync_timeline_destroy(struct kgsl_context *context);
-
-void kgsl_sync_timeline_put(struct kgsl_sync_timeline *ktimeline);
-
-struct kgsl_sync_fence_cb *kgsl_sync_fence_async_wait(int fd,
-					bool (*func)(void *priv), void *priv,
-					char *fence_name, int name_len);
-
-void kgsl_sync_fence_async_cancel(struct kgsl_sync_fence_cb *kcb);
-
-long kgsl_ioctl_syncsource_create(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_syncsource_destroy(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_syncsource_create_fence(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-long kgsl_ioctl_syncsource_signal_fence(struct kgsl_device_private *dev_priv,
-					unsigned int cmd, void *data);
-
-void kgsl_syncsource_put(struct kgsl_syncsource *syncsource);
-
-void kgsl_syncsource_process_release_syncsources(
-		struct kgsl_process_private *private);
-
-void kgsl_dump_fence(struct kgsl_drawobj_sync_event *event,
-					char *fence_str, int len);
-
+struct kgsl_sync_fence_waiter *kgsl_sync_fence_async_wait(int fd,
+	void (*func)(void *priv), void *priv);
+int kgsl_sync_fence_async_cancel(struct kgsl_sync_fence_waiter *waiter);
+static inline void kgsl_sync_fence_log(struct sync_fence *fence)
+{
+}
 #else
 static inline int kgsl_add_fence_event(struct kgsl_device *device,
 	u32 context_id, u32 timestamp, void __user *data, int len,
@@ -124,7 +62,7 @@ static inline int kgsl_add_fence_event(struct kgsl_device *device,
 
 static inline int kgsl_sync_timeline_create(struct kgsl_context *context)
 {
-	context->ktimeline = NULL;
+	context->timeline = NULL;
 	return 0;
 }
 
@@ -132,23 +70,38 @@ static inline void kgsl_sync_timeline_destroy(struct kgsl_context *context)
 {
 }
 
-static inline void kgsl_sync_timeline_put(struct kgsl_sync_timeline *ktimeline)
+static inline struct
+kgsl_sync_fence_waiter *kgsl_sync_fence_async_wait(int fd,
+	void (*func)(void *priv), void *priv)
 {
+	return NULL;
 }
 
-
-static inline struct kgsl_sync_fence_cb *kgsl_sync_fence_async_wait(int fd,
-					bool (*func)(void *priv), void *priv,
-					char *fence_name, int name_len)
+static inline int
+kgsl_sync_fence_async_cancel(struct kgsl_sync_fence_waiter *waiter)
 {
-	return NULL;
+	return 1;
 }
 
-static inline void
-kgsl_sync_fence_async_cancel(struct kgsl_sync_fence_cb *kcb)
+static inline void kgsl_sync_fence_log(struct sync_fence *fence)
 {
 }
 
+#endif
+
+#ifdef CONFIG_ONESHOT_SYNC
+long kgsl_ioctl_syncsource_create(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data);
+long kgsl_ioctl_syncsource_destroy(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data);
+long kgsl_ioctl_syncsource_create_fence(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data);
+long kgsl_ioctl_syncsource_signal_fence(struct kgsl_device_private *dev_priv,
+					unsigned int cmd, void *data);
+
+void kgsl_syncsource_put(struct kgsl_syncsource *syncsource);
+
+#else
 static inline long
 kgsl_ioctl_syncsource_create(struct kgsl_device_private *dev_priv,
 					unsigned int cmd, void *data)
@@ -181,18 +134,6 @@ static inline void kgsl_syncsource_put(struct kgsl_syncsource *syncsource)
 {
 
 }
-
-static inline void kgsl_syncsource_process_release_syncsources(
-		struct kgsl_process_private *private)
-{
-
-}
-
-static inline void kgsl_dump_fence(struct kgsl_drawobj_sync_event *event,
-					char *fence_str, int len)
-{
-}
-
-#endif /* CONFIG_SYNC_FILE */
+#endif
 
 #endif /* __KGSL_SYNC_H */
diff --git a/drivers/gpu/msm/kgsl_trace.h b/drivers/gpu/msm/kgsl_trace.h
index c7690a14abcd..da7a282acc62 100644
--- a/drivers/gpu/msm/kgsl_trace.h
+++ b/drivers/gpu/msm/kgsl_trace.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2011-2017, The Linux Foundation. All rights reserved.
+/* Copyright (c) 2011-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -36,13 +36,14 @@ TRACE_EVENT(kgsl_issueibcmds,
 
 	TP_PROTO(struct kgsl_device *device,
 			int drawctxt_id,
+			struct kgsl_cmdbatch *cmdbatch,
 			unsigned int numibs,
 			int timestamp,
 			int flags,
 			int result,
 			unsigned int type),
 
-	TP_ARGS(device, drawctxt_id, numibs, timestamp,
+	TP_ARGS(device, drawctxt_id, cmdbatch, numibs, timestamp,
 		flags, result, type),
 
 	TP_STRUCT__entry(
@@ -66,13 +67,14 @@ TRACE_EVENT(kgsl_issueibcmds,
 	),
 
 	TP_printk(
-		"d_name=%s ctx=%u ib=0x0 numibs=%u ts=%u flags=%s result=%d type=%s",
+		"d_name=%s ctx=%u ib=0x0 numibs=%u ts=%u "
+		"flags=%s result=%d type=%s",
 		__get_str(device_name),
 		__entry->drawctxt_id,
 		__entry->numibs,
 		__entry->timestamp,
 		__entry->flags ? __print_flags(__entry->flags, "|",
-						KGSL_DRAWOBJ_FLAGS) : "None",
+						KGSL_CMDBATCH_FLAGS) : "None",
 		__entry->result,
 		__print_symbolic(__entry->drawctxt_type, KGSL_CONTEXT_TYPES)
 	)
@@ -219,6 +221,11 @@ DEFINE_EVENT(kgsl_pwr_template, kgsl_rail,
 	TP_ARGS(device, on)
 );
 
+DEFINE_EVENT(kgsl_pwr_template, kgsl_retention_clk,
+	TP_PROTO(struct kgsl_device *device, int on),
+	TP_ARGS(device, on)
+);
+
 TRACE_EVENT(kgsl_clk,
 
 	TP_PROTO(struct kgsl_device *device, unsigned int on,
@@ -337,9 +344,9 @@ TRACE_EVENT(kgsl_gpubusy,
 
 TRACE_EVENT(kgsl_pwrstats,
 	TP_PROTO(struct kgsl_device *device, s64 time,
-		struct kgsl_power_stats *pstats, u32 ctxt_count),
+		struct kgsl_power_stats *pstats),
 
-	TP_ARGS(device, time, pstats, ctxt_count),
+	TP_ARGS(device, time, pstats),
 
 	TP_STRUCT__entry(
 		__string(device_name, device->name)
@@ -347,7 +354,6 @@ TRACE_EVENT(kgsl_pwrstats,
 		__field(u64, busy_time)
 		__field(u64, ram_time)
 		__field(u64, ram_wait)
-		__field(u32, context_count)
 	),
 
 	TP_fast_assign(
@@ -356,13 +362,12 @@ TRACE_EVENT(kgsl_pwrstats,
 		__entry->busy_time = pstats->busy_time;
 		__entry->ram_time = pstats->ram_time;
 		__entry->ram_wait = pstats->ram_wait;
-		__entry->context_count = ctxt_count;
 	),
 
 	TP_printk(
-		"d_name=%s total=%lld busy=%lld ram_time=%lld ram_wait=%lld context_count=%u",
+		"d_name=%s total=%lld busy=%lld ram_time=%lld ram_wait=%lld",
 		__get_str(device_name), __entry->total_time, __entry->busy_time,
-		__entry->ram_time, __entry->ram_wait, __entry->context_count
+		__entry->ram_time, __entry->ram_wait
 	)
 );
 
@@ -651,7 +656,8 @@ DECLARE_EVENT_CLASS(kgsl_mem_timestamp_template,
 	),
 
 	TP_printk(
-		"d_name=%s gpuaddr=0x%llx size=%llu type=%s usage=%s id=%u ctx=%u curr_ts=%u free_ts=%u",
+		"d_name=%s gpuaddr=0x%llx size=%llu type=%s usage=%s id=%u ctx=%u"
+		" curr_ts=%u free_ts=%u",
 		__get_str(device_name),
 		__entry->gpuaddr,
 		__entry->size,
@@ -820,14 +826,14 @@ TRACE_EVENT(kgsl_constraint,
 
 TRACE_EVENT(kgsl_mmu_pagefault,
 
-	TP_PROTO(struct kgsl_device *device, unsigned long page,
+	TP_PROTO(struct kgsl_device *device, unsigned int page,
 		 unsigned int pt, const char *op),
 
 	TP_ARGS(device, page, pt, op),
 
 	TP_STRUCT__entry(
 		__string(device_name, device->name)
-		__field(unsigned long, page)
+		__field(unsigned int, page)
 		__field(unsigned int, pt)
 		__string(op, op)
 	),
@@ -840,7 +846,7 @@ TRACE_EVENT(kgsl_mmu_pagefault,
 	),
 
 	TP_printk(
-		"d_name=%s page=0x%lx pt=%u op=%s",
+		"d_name=%s page=0x%08x pt=%u op=%s",
 		__get_str(device_name), __entry->page, __entry->pt,
 		__get_str(op)
 	)
@@ -1025,62 +1031,59 @@ TRACE_EVENT(kgsl_pagetable_destroy,
 );
 
 DECLARE_EVENT_CLASS(syncpoint_timestamp_template,
-	TP_PROTO(struct kgsl_drawobj_sync *syncobj,
-		struct kgsl_context *context,
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, struct kgsl_context *context,
 		unsigned int timestamp),
-	TP_ARGS(syncobj, context, timestamp),
+	TP_ARGS(cmdbatch, context, timestamp),
 	TP_STRUCT__entry(
-		__field(unsigned int, syncobj_context_id)
+		__field(unsigned int, cmdbatch_context_id)
 		__field(unsigned int, context_id)
 		__field(unsigned int, timestamp)
 	),
 	TP_fast_assign(
-		__entry->syncobj_context_id = syncobj->base.context->id;
+		__entry->cmdbatch_context_id = cmdbatch->context->id;
 		__entry->context_id = context->id;
 		__entry->timestamp = timestamp;
 	),
 	TP_printk("ctx=%d sync ctx=%d ts=%d",
-		__entry->syncobj_context_id, __entry->context_id,
+		__entry->cmdbatch_context_id, __entry->context_id,
 		__entry->timestamp)
 );
 
 DEFINE_EVENT(syncpoint_timestamp_template, syncpoint_timestamp,
-	TP_PROTO(struct kgsl_drawobj_sync *syncobj,
-		struct kgsl_context *context,
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, struct kgsl_context *context,
 		unsigned int timestamp),
-	TP_ARGS(syncobj, context, timestamp)
+	TP_ARGS(cmdbatch, context, timestamp)
 );
 
 DEFINE_EVENT(syncpoint_timestamp_template, syncpoint_timestamp_expire,
-	TP_PROTO(struct kgsl_drawobj_sync *syncobj,
-		struct kgsl_context *context,
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, struct kgsl_context *context,
 		unsigned int timestamp),
-	TP_ARGS(syncobj, context, timestamp)
+	TP_ARGS(cmdbatch, context, timestamp)
 );
 
 DECLARE_EVENT_CLASS(syncpoint_fence_template,
-	TP_PROTO(struct kgsl_drawobj_sync *syncobj, char *name),
-	TP_ARGS(syncobj, name),
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, char *name),
+	TP_ARGS(cmdbatch, name),
 	TP_STRUCT__entry(
 		__string(fence_name, name)
-		__field(unsigned int, syncobj_context_id)
+		__field(unsigned int, cmdbatch_context_id)
 	),
 	TP_fast_assign(
-		__entry->syncobj_context_id = syncobj->base.context->id;
+		__entry->cmdbatch_context_id = cmdbatch->context->id;
 		__assign_str(fence_name, name);
 	),
 	TP_printk("ctx=%d fence=%s",
-		__entry->syncobj_context_id, __get_str(fence_name))
+		__entry->cmdbatch_context_id, __get_str(fence_name))
 );
 
 DEFINE_EVENT(syncpoint_fence_template, syncpoint_fence,
-	TP_PROTO(struct kgsl_drawobj_sync *syncobj, char *name),
-	TP_ARGS(syncobj, name)
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, char *name),
+	TP_ARGS(cmdbatch, name)
 );
 
 DEFINE_EVENT(syncpoint_fence_template, syncpoint_fence_expire,
-	TP_PROTO(struct kgsl_drawobj_sync *syncobj, char *name),
-	TP_ARGS(syncobj, name)
+	TP_PROTO(struct kgsl_cmdbatch *cmdbatch, char *name),
+	TP_ARGS(cmdbatch, name)
 );
 
 TRACE_EVENT(kgsl_msg,
@@ -1097,184 +1100,6 @@ TRACE_EVENT(kgsl_msg,
 	)
 );
 
-DECLARE_EVENT_CLASS(sparse_alloc_template,
-	TP_PROTO(unsigned int id, uint64_t size, unsigned int pagesize),
-	TP_ARGS(id, size, pagesize),
-	TP_STRUCT__entry(
-		__field(unsigned int, id)
-		__field(uint64_t, size)
-		__field(unsigned int, pagesize)
-	),
-	TP_fast_assign(
-		__entry->id = id;
-		__entry->size = size;
-		__entry->pagesize = pagesize;
-	),
-	TP_printk("id=%d size=0x%llX pagesize=0x%X",
-		__entry->id, __entry->size, __entry->pagesize)
-);
-
-DEFINE_EVENT(sparse_alloc_template, sparse_phys_alloc,
-	TP_PROTO(unsigned int id, uint64_t size, unsigned int pagesize),
-	TP_ARGS(id, size, pagesize)
-);
-
-DEFINE_EVENT(sparse_alloc_template, sparse_virt_alloc,
-	TP_PROTO(unsigned int id, uint64_t size, unsigned int pagesize),
-	TP_ARGS(id, size, pagesize)
-);
-
-DECLARE_EVENT_CLASS(sparse_free_template,
-	TP_PROTO(unsigned int id),
-	TP_ARGS(id),
-	TP_STRUCT__entry(
-		__field(unsigned int, id)
-	),
-	TP_fast_assign(
-		__entry->id = id;
-	),
-	TP_printk("id=%d", __entry->id)
-);
-
-DEFINE_EVENT(sparse_free_template, sparse_phys_free,
-	TP_PROTO(unsigned int id),
-	TP_ARGS(id)
-);
-
-DEFINE_EVENT(sparse_free_template, sparse_virt_free,
-	TP_PROTO(unsigned int id),
-	TP_ARGS(id)
-);
-
-TRACE_EVENT(sparse_bind,
-	TP_PROTO(unsigned int v_id, uint64_t v_off,
-		unsigned int p_id, uint64_t p_off,
-		uint64_t size, uint64_t flags),
-	TP_ARGS(v_id, v_off, p_id, p_off, size, flags),
-	TP_STRUCT__entry(
-		__field(unsigned int, v_id)
-		__field(uint64_t, v_off)
-		__field(unsigned int, p_id)
-		__field(uint64_t, p_off)
-		__field(uint64_t, size)
-		__field(uint64_t, flags)
-	),
-	TP_fast_assign(
-		__entry->v_id = v_id;
-		__entry->v_off = v_off;
-		__entry->p_id = p_id;
-		__entry->p_off = p_off;
-		__entry->size = size;
-		__entry->flags = flags;
-	),
-	TP_printk(
-	"v_id=%d v_off=0x%llX p_id=%d p_off=0x%llX size=0x%llX flags=0x%llX",
-		__entry->v_id, __entry->v_off,
-		__entry->p_id, __entry->p_off,
-		__entry->size, __entry->flags)
-);
-
-TRACE_EVENT(sparse_unbind,
-	TP_PROTO(unsigned int v_id, uint64_t v_off, uint64_t size),
-	TP_ARGS(v_id, v_off, size),
-	TP_STRUCT__entry(
-		__field(unsigned int, v_id)
-		__field(uint64_t, v_off)
-		__field(uint64_t, size)
-	),
-	TP_fast_assign(
-		__entry->v_id = v_id;
-		__entry->v_off = v_off;
-		__entry->size = size;
-	),
-	TP_printk("v_id=%d v_off=0x%llX size=0x%llX",
-		__entry->v_id, __entry->v_off, __entry->size)
-);
-
-
-TRACE_EVENT(kgsl_clock_throttling,
-	TP_PROTO(
-		int idle_10pct,
-		int crc_50pct,
-		int crc_more50pct,
-		int crc_less50pct,
-		int adj
-	),
-	TP_ARGS(
-		idle_10pct,
-		crc_50pct,
-		crc_more50pct,
-		crc_less50pct,
-		adj
-	),
-	TP_STRUCT__entry(
-		__field(int, idle_10pct)
-		__field(int, crc_50pct)
-		__field(int, crc_more50pct)
-		__field(int, crc_less50pct)
-		__field(int, adj)
-	),
-	TP_fast_assign(
-		__entry->idle_10pct = idle_10pct;
-		__entry->crc_50pct = crc_50pct;
-		__entry->crc_more50pct = crc_more50pct;
-		__entry->crc_less50pct = crc_less50pct;
-		__entry->adj = adj;
-	),
-	TP_printk("idle_10=%d crc_50=%d crc_more50=%d crc_less50=%d adj=%d",
-		__entry->idle_10pct, __entry->crc_50pct, __entry->crc_more50pct,
-		__entry->crc_less50pct, __entry->adj
-	)
-);
-
-DECLARE_EVENT_CLASS(gmu_oob_template,
-	TP_PROTO(unsigned int mask),
-	TP_ARGS(mask),
-	TP_STRUCT__entry(
-		__field(unsigned int, mask)
-	),
-	TP_fast_assign(
-		__entry->mask = mask;
-	),
-	TP_printk("mask=0x%08x", __entry->mask)
-);
-
-DEFINE_EVENT(gmu_oob_template, kgsl_gmu_oob_set,
-	TP_PROTO(unsigned int mask),
-	TP_ARGS(mask)
-);
-
-DEFINE_EVENT(gmu_oob_template, kgsl_gmu_oob_clear,
-	TP_PROTO(unsigned int mask),
-	TP_ARGS(mask)
-);
-
-DECLARE_EVENT_CLASS(hfi_msg_template,
-	TP_PROTO(unsigned int id, unsigned int size, unsigned int seqnum),
-	TP_ARGS(id, size, seqnum),
-	TP_STRUCT__entry(
-		__field(unsigned int, id)
-		__field(unsigned int, size)
-		__field(unsigned int, seq)
-	),
-	TP_fast_assign(
-		__entry->id = id;
-		__entry->size = size;
-		__entry->seq = seqnum;
-	),
-	TP_printk("id=0x%x size=0x%x seqnum=0x%x",
-		__entry->id, __entry->size, __entry->seq)
-);
-
-DEFINE_EVENT(hfi_msg_template, kgsl_hfi_send,
-	TP_PROTO(unsigned int id, unsigned int size, unsigned int seqnum),
-	TP_ARGS(id, size, seqnum)
-);
-
-DEFINE_EVENT(hfi_msg_template, kgsl_hfi_receive,
-	TP_PROTO(unsigned int id, unsigned int size, unsigned int seqnum),
-	TP_ARGS(id, size, seqnum)
-);
 
 #endif /* _KGSL_TRACE_H */
 
diff --git a/drivers/staging/android/Kconfig b/drivers/staging/android/Kconfig
index a17c483f906e..898c213a7b69 100644
--- a/drivers/staging/android/Kconfig
+++ b/drivers/staging/android/Kconfig
@@ -33,6 +33,33 @@ config ANDROID_LOW_MEMORY_KILLER_AUTODETECT_OOM_ADJ_VALUES
 	  /sys/module/lowmemorykiller/parameters/adj and convert them
 	  to oom_score_adj values.
 
+config SYNC
+	bool "Synchronization framework"
+	default n
+	select ANON_INODES
+	---help---
+	  This option enables the framework for synchronization between multiple
+	  drivers.  Sync implementations can take advantage of hardware
+	  synchronization built into devices like GPUs.
+
+config SW_SYNC
+	bool "Software synchronization objects"
+	default n
+	depends on SYNC
+	---help---
+	  A sync object driver that uses a 32bit counter to coordinate
+	  syncrhronization.  Useful when there is no hardware primitive backing
+	  the synchronization.
+
+config ONESHOT_SYNC
+	bool "One shot sync objects"
+	depends on SYNC
+	help
+	  This sync driver provides a way to create sync objects that may
+	  be signaled by userspace. Unlike other sync objects, the
+	  sync objects created by this driver may be signaled in any order
+	  without changing the state of other sync objects on the timeline.
+
 config ANDROID_VSOC
 	tristate "Android Virtual SoC support"
 	default n
diff --git a/drivers/staging/android/Makefile b/drivers/staging/android/Makefile
index 93c5f5a7390a..ff87a7fa69e0 100644
--- a/drivers/staging/android/Makefile
+++ b/drivers/staging/android/Makefile
@@ -5,4 +5,7 @@ obj-$(CONFIG_FIQ_DEBUGGER)		+= fiq_debugger/
 
 obj-$(CONFIG_ASHMEM)			+= ashmem.o
 obj-$(CONFIG_ANDROID_LOW_MEMORY_KILLER)	+= lowmemorykiller.o
+obj-$(CONFIG_SYNC)			+= sync.o
+obj-$(CONFIG_SW_SYNC)			+= sw_sync.o
+obj-$(CONFIG_ONESHOT_SYNC)		+= oneshot_sync.o
 obj-$(CONFIG_ANDROID_VSOC)		+= vsoc.o
diff --git a/drivers/staging/android/oneshot_sync.c b/drivers/staging/android/oneshot_sync.c
new file mode 100644
index 000000000000..d8c8db61d00f
--- /dev/null
+++ b/drivers/staging/android/oneshot_sync.c
@@ -0,0 +1,434 @@
+/* Copyright (c) 2014-2016, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/syscalls.h>
+#include <linux/uaccess.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/sync.h>
+#include <linux/oneshot_sync.h>
+
+/**
+ * struct oneshot_sync_timeline - a userspace signaled, out of order, timeline
+ * @obj: base sync timeline
+ * @lock: spinlock to guard other members
+ * @state_list: list of oneshot_sync_states.
+ * @id: next id for points creating oneshot_sync_pts
+ */
+struct oneshot_sync_timeline {
+	struct sync_timeline obj;
+	spinlock_t lock;
+	struct list_head state_list;
+	unsigned int id;
+};
+
+#define to_oneshot_timeline(_p) \
+	container_of((_p), struct oneshot_sync_timeline, obj)
+
+/**
+ * struct oneshot_sync_state - signal state for a group of oneshot points
+ * @refcount: reference count for this structure.
+ * @signaled: is this signaled or not?
+ * @id: identifier for this state
+ * @orig_fence: fence used to create this state, no is reference count held.
+ * @timeline: back pointer to the timeline.
+ */
+struct oneshot_sync_state {
+	struct kref refcount;
+	struct list_head node;
+	bool signaled;
+	unsigned int id;
+	struct sync_fence *orig_fence;
+	struct oneshot_sync_timeline *timeline;
+};
+
+/**
+ * struct oneshot_sync_pt
+ * @sync_pt: base sync point structure
+ * @state: reference counted pointer to the state of this pt
+ */
+struct oneshot_sync_pt {
+	struct sync_pt sync_pt;
+	struct oneshot_sync_state *state;
+	bool dup;
+};
+#define to_oneshot_pt(_p) container_of((_p), struct oneshot_sync_pt, sync_pt)
+
+static void oneshot_state_destroy(struct kref *ref)
+{
+	struct oneshot_sync_state *state =
+		container_of(ref, struct oneshot_sync_state, refcount);
+
+	spin_lock(&state->timeline->lock);
+	list_del(&state->node);
+	spin_unlock(&state->timeline->lock);
+
+	kfree(state);
+}
+
+static void oneshot_state_put(struct oneshot_sync_state *state)
+{
+	kref_put(&state->refcount, oneshot_state_destroy);
+}
+
+static struct oneshot_sync_pt *
+oneshot_pt_create(struct oneshot_sync_timeline *timeline)
+{
+	struct oneshot_sync_pt *pt = NULL;
+
+	pt = (struct oneshot_sync_pt *)sync_pt_create(&timeline->obj,
+						     sizeof(*pt));
+	if (pt == NULL)
+		return NULL;
+
+	pt->state = kzalloc(sizeof(struct oneshot_sync_state), GFP_KERNEL);
+	if (pt->state == NULL)
+		goto error;
+
+	kref_init(&pt->state->refcount);
+	pt->state->signaled = false;
+	pt->state->timeline = timeline;
+
+	spin_lock(&timeline->lock);
+	/* assign an id to the state, which could be shared by several pts. */
+	pt->state->id = ++(timeline->id);
+	/* add this pt to the list of pts that can be signaled by userspace */
+	list_add_tail(&pt->state->node, &timeline->state_list);
+	spin_unlock(&timeline->lock);
+
+	return pt;
+error:
+	if (pt)
+		sync_pt_free(&pt->sync_pt);
+	return NULL;
+}
+
+static struct sync_pt *oneshot_pt_dup(struct sync_pt *sync_pt)
+{
+	struct oneshot_sync_pt *out_pt;
+	struct oneshot_sync_pt *pt = to_oneshot_pt(sync_pt);
+
+	if (!kref_get_unless_zero(&pt->state->refcount))
+		return NULL;
+
+	out_pt = (struct oneshot_sync_pt *)
+		sync_pt_create(sync_pt->parent, sizeof(*out_pt));
+
+	if (out_pt == NULL) {
+		oneshot_state_put(pt->state);
+		return NULL;
+	}
+	out_pt->state = pt->state;
+	out_pt->dup = true;
+
+	return &out_pt->sync_pt;
+}
+
+static int oneshot_pt_has_signaled(struct sync_pt *sync_pt)
+{
+	struct oneshot_sync_pt *pt = to_oneshot_pt(sync_pt);
+
+	return pt->state->signaled;
+}
+
+static int oneshot_pt_compare(struct sync_pt *a, struct sync_pt *b)
+{
+	struct oneshot_sync_pt *pt_a = to_oneshot_pt(a);
+	struct oneshot_sync_pt *pt_b = to_oneshot_pt(b);
+	/*
+	 * since oneshot sync points are order-independent,
+	 * return an arbitrary order which just happens to
+	 * prevent sync.c from collapsing the points.
+	 */
+	return (pt_a->state == pt_b->state) ? 0 : 1;
+}
+
+static void oneshot_pt_free(struct sync_pt *sync_pt)
+{
+	struct oneshot_sync_pt *pt = to_oneshot_pt(sync_pt);
+
+	struct oneshot_sync_timeline *timeline = sync_pt->parent ?
+		to_oneshot_timeline(sync_pt->parent) : NULL;
+
+	if (timeline != NULL) {
+		spin_lock(&timeline->lock);
+		/*
+		 * If this is the original pt (and fence), signal to avoid
+		 * deadlock. Unfornately, we can't signal the timeline here
+		 * safely, so there could be a delay until the pt's
+		 * state change is noticed.
+		 */
+
+		if (pt->dup == false) {
+			/*
+			 * If the original pt goes away, force it signaled to
+			 * avoid deadlock.
+			 */
+			if (!pt->state->signaled) {
+				pr_debug("id %d: fence closed before signal.\n",
+						pt->state->id);
+				pt->state->signaled = true;
+			}
+		}
+		spin_unlock(&timeline->lock);
+	}
+	oneshot_state_put(pt->state);
+}
+
+static void oneshot_pt_value_str(struct sync_pt *sync_pt, char *str, int size)
+{
+	struct oneshot_sync_pt *pt = to_oneshot_pt(sync_pt);
+
+	snprintf(str, size, "%u", pt->state->id);
+}
+
+static struct sync_timeline_ops oneshot_timeline_ops = {
+	.driver_name = "oneshot",
+	.dup = oneshot_pt_dup,
+	.has_signaled = oneshot_pt_has_signaled,
+	.compare = oneshot_pt_compare,
+	.free_pt = oneshot_pt_free,
+	.pt_value_str = oneshot_pt_value_str,
+};
+
+struct oneshot_sync_timeline *oneshot_timeline_create(const char *name)
+{
+	struct oneshot_sync_timeline *timeline = NULL;
+	static const char *default_name = "oneshot-timeline";
+
+	if (name == NULL)
+		name = default_name;
+
+	timeline = (struct oneshot_sync_timeline *)
+			sync_timeline_create(&oneshot_timeline_ops,
+					     sizeof(*timeline),
+					     name);
+
+	if (timeline == NULL)
+		return NULL;
+
+	INIT_LIST_HEAD(&timeline->state_list);
+	spin_lock_init(&timeline->lock);
+
+	return timeline;
+}
+EXPORT_SYMBOL(oneshot_timeline_create);
+
+void oneshot_timeline_destroy(struct oneshot_sync_timeline *timeline)
+{
+	if (timeline)
+		sync_timeline_destroy(&timeline->obj);
+}
+EXPORT_SYMBOL(oneshot_timeline_destroy);
+
+struct sync_fence *oneshot_fence_create(struct oneshot_sync_timeline *timeline,
+					const char *name)
+{
+	struct sync_fence *fence = NULL;
+	struct oneshot_sync_pt *pt = NULL;
+
+	pt = oneshot_pt_create(timeline);
+	if (pt == NULL)
+		return NULL;
+
+	fence = sync_fence_create(name, &pt->sync_pt);
+	if (fence == NULL) {
+		sync_pt_free(&pt->sync_pt);
+		return NULL;
+	}
+
+	pt->state->orig_fence = fence;
+
+	return fence;
+}
+EXPORT_SYMBOL(oneshot_fence_create);
+
+int oneshot_fence_signal(struct oneshot_sync_timeline *timeline,
+			struct sync_fence *fence)
+{
+	int ret = -EINVAL;
+	struct oneshot_sync_state *state = NULL;
+	bool signaled = false;
+
+	if (timeline == NULL || fence == NULL)
+		return -EINVAL;
+
+	spin_lock(&timeline->lock);
+	list_for_each_entry(state, &timeline->state_list, node) {
+		/*
+		 * If we have the point from this fence on our list,
+		 * this is is the original fence we created, so signal it.
+		 */
+		if (state->orig_fence == fence) {
+			/* ignore attempts to signal multiple times */
+			if (!state->signaled) {
+				state->signaled = true;
+				signaled = true;
+			}
+			ret = 0;
+			break;
+		}
+	}
+	spin_unlock(&timeline->lock);
+	if (ret == -EINVAL)
+		pr_debug("fence: %pK not from this timeline\n", fence);
+
+	if (signaled)
+		sync_timeline_signal(&timeline->obj);
+	return ret;
+}
+EXPORT_SYMBOL(oneshot_fence_signal);
+
+#ifdef CONFIG_ONESHOT_SYNC_USER
+
+static int oneshot_open(struct inode *inode, struct file *file)
+{
+	struct oneshot_sync_timeline *timeline = NULL;
+	char name[32];
+	char task_comm[TASK_COMM_LEN];
+
+	get_task_comm(task_comm, current);
+	snprintf(name, sizeof(name), "%s-oneshot", task_comm);
+
+	timeline = oneshot_timeline_create(name);
+	if (timeline == NULL)
+		return -ENOMEM;
+
+	file->private_data = timeline;
+	return 0;
+}
+
+static int oneshot_release(struct inode *inode, struct file *file)
+{
+	struct oneshot_sync_timeline *timeline = file->private_data;
+
+	oneshot_timeline_destroy(timeline);
+
+	return 0;
+}
+
+static long oneshot_ioctl_fence_create(struct oneshot_sync_timeline *timeline,
+				 unsigned long arg)
+{
+	struct oneshot_sync_create_fence param;
+	int ret = -ENOMEM;
+	struct sync_fence *fence = NULL;
+	int fd = get_unused_fd();
+
+	if (fd < 0)
+		return fd;
+
+	if (copy_from_user(&param, (void __user *)arg, sizeof(param))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	fence = oneshot_fence_create(timeline, param.name);
+	if (fence == NULL) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	param.fence_fd = fd;
+
+	if (copy_to_user((void __user *)arg, &param, sizeof(param))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	sync_fence_install(fence, fd);
+	ret = 0;
+out:
+	if (ret) {
+		if (fence)
+			sync_fence_put(fence);
+		put_unused_fd(fd);
+	}
+	return ret;
+}
+
+
+
+static long oneshot_ioctl_fence_signal(struct oneshot_sync_timeline *timeline,
+				 unsigned long arg)
+{
+	int ret = -EINVAL;
+	int fd = -1;
+	struct sync_fence *fence = NULL;
+
+	if (get_user(fd, (int __user *)arg))
+		return -EFAULT;
+
+	fence = sync_fence_fdget(fd);
+	if (fence == NULL)
+		return -EBADF;
+
+	ret = oneshot_fence_signal(timeline, fence);
+	sync_fence_put(fence);
+
+	return ret;
+}
+
+static long oneshot_ioctl(struct file *file, unsigned int cmd,
+			  unsigned long arg)
+{
+	struct oneshot_sync_timeline *timeline = file->private_data;
+
+	switch (cmd) {
+	case ONESHOT_SYNC_IOC_CREATE_FENCE:
+		return oneshot_ioctl_fence_create(timeline, arg);
+
+	case ONESHOT_SYNC_IOC_SIGNAL_FENCE:
+		return oneshot_ioctl_fence_signal(timeline, arg);
+
+	default:
+		return -ENOTTY;
+	}
+}
+
+static const struct file_operations oneshot_fops = {
+	.owner = THIS_MODULE,
+	.open = oneshot_open,
+	.release = oneshot_release,
+	.unlocked_ioctl = oneshot_ioctl,
+	.compat_ioctl = oneshot_ioctl,
+};
+static struct miscdevice oneshot_dev = {
+	.minor	= MISC_DYNAMIC_MINOR,
+	.name	= "oneshot_sync",
+	.fops	= &oneshot_fops,
+};
+
+static int __init oneshot_init(void)
+{
+	return misc_register(&oneshot_dev);
+}
+
+static void __exit oneshot_remove(void)
+{
+	misc_deregister(&oneshot_dev);
+}
+
+module_init(oneshot_init);
+module_exit(oneshot_remove);
+
+#endif /* CONFIG_ONESHOT_SYNC_USER */
+MODULE_LICENSE("GPL v2");
+
diff --git a/drivers/staging/android/sw_sync.c b/drivers/staging/android/sw_sync.c
new file mode 100644
index 000000000000..1dc48002a698
--- /dev/null
+++ b/drivers/staging/android/sw_sync.c
@@ -0,0 +1,266 @@
+/*
+ * drivers/base/sw_sync.c
+ *
+ * Copyright (C) 2012 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/export.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/syscalls.h>
+#include <linux/uaccess.h>
+#include <linux/sw_sync.h>
+
+static int sw_sync_cmp(u32 a, u32 b)
+{
+	if (a == b)
+		return 0;
+
+	return ((s32)a - (s32)b) < 0 ? -1 : 1;
+}
+
+struct sync_pt *sw_sync_pt_create(struct sw_sync_timeline *obj, u32 value)
+{
+	struct sw_sync_pt *pt;
+
+	pt = (struct sw_sync_pt *)
+		sync_pt_create(&obj->obj, sizeof(struct sw_sync_pt));
+
+	pt->value = value;
+
+	return (struct sync_pt *)pt;
+}
+EXPORT_SYMBOL(sw_sync_pt_create);
+
+static struct sync_pt *sw_sync_pt_dup(struct sync_pt *sync_pt)
+{
+	struct sw_sync_pt *pt = (struct sw_sync_pt *)sync_pt;
+	struct sw_sync_timeline *obj =
+		(struct sw_sync_timeline *)sync_pt->parent;
+
+	return (struct sync_pt *)sw_sync_pt_create(obj, pt->value);
+}
+
+static int sw_sync_pt_has_signaled(struct sync_pt *sync_pt)
+{
+	struct sw_sync_pt *pt = (struct sw_sync_pt *)sync_pt;
+	struct sw_sync_timeline *obj =
+		(struct sw_sync_timeline *)sync_pt->parent;
+
+	return sw_sync_cmp(obj->value, pt->value) >= 0;
+}
+
+static int sw_sync_pt_compare(struct sync_pt *a, struct sync_pt *b)
+{
+	struct sw_sync_pt *pt_a = (struct sw_sync_pt *)a;
+	struct sw_sync_pt *pt_b = (struct sw_sync_pt *)b;
+
+	return sw_sync_cmp(pt_a->value, pt_b->value);
+}
+
+static int sw_sync_fill_driver_data(struct sync_pt *sync_pt,
+				    void *data, int size)
+{
+	struct sw_sync_pt *pt = (struct sw_sync_pt *)sync_pt;
+
+	if (size < sizeof(pt->value))
+		return -ENOMEM;
+
+	memcpy(data, &pt->value, sizeof(pt->value));
+
+	return sizeof(pt->value);
+}
+
+static void sw_sync_timeline_value_str(struct sync_timeline *sync_timeline,
+				       char *str, int size)
+{
+	struct sw_sync_timeline *timeline =
+		(struct sw_sync_timeline *)sync_timeline;
+	snprintf(str, size, "%d", timeline->value);
+}
+
+static void sw_sync_pt_value_str(struct sync_pt *sync_pt,
+				 char *str, int size)
+{
+	struct sw_sync_pt *pt = (struct sw_sync_pt *)sync_pt;
+
+	snprintf(str, size, "%d", pt->value);
+}
+
+static struct sync_timeline_ops sw_sync_timeline_ops = {
+	.driver_name = "sw_sync",
+	.dup = sw_sync_pt_dup,
+	.has_signaled = sw_sync_pt_has_signaled,
+	.compare = sw_sync_pt_compare,
+	.fill_driver_data = sw_sync_fill_driver_data,
+	.timeline_value_str = sw_sync_timeline_value_str,
+	.pt_value_str = sw_sync_pt_value_str,
+};
+
+struct sw_sync_timeline *sw_sync_timeline_create(const char *name)
+{
+	struct sw_sync_timeline *obj = (struct sw_sync_timeline *)
+		sync_timeline_create(&sw_sync_timeline_ops,
+				     sizeof(struct sw_sync_timeline),
+				     name);
+
+	return obj;
+}
+EXPORT_SYMBOL(sw_sync_timeline_create);
+
+void sw_sync_timeline_inc(struct sw_sync_timeline *obj, u32 inc)
+{
+	obj->value += inc;
+
+	sync_timeline_signal(&obj->obj);
+}
+EXPORT_SYMBOL(sw_sync_timeline_inc);
+
+#ifdef CONFIG_SW_SYNC_USER
+/* *WARNING*
+ *
+ * improper use of this can result in deadlocking kernel drivers from userspace.
+ */
+
+/* opening sw_sync create a new sync obj */
+static int sw_sync_open(struct inode *inode, struct file *file)
+{
+	struct sw_sync_timeline *obj;
+	char task_comm[TASK_COMM_LEN];
+
+	get_task_comm(task_comm, current);
+
+	obj = sw_sync_timeline_create(task_comm);
+	if (obj == NULL)
+		return -ENOMEM;
+
+	file->private_data = obj;
+
+	return 0;
+}
+
+static int sw_sync_release(struct inode *inode, struct file *file)
+{
+	struct sw_sync_timeline *obj = file->private_data;
+
+	sync_timeline_destroy(&obj->obj);
+	return 0;
+}
+
+static long sw_sync_ioctl_create_fence(struct sw_sync_timeline *obj,
+				       unsigned long arg)
+{
+	int fd = get_unused_fd_flags(O_CLOEXEC);
+	int err;
+	struct sync_pt *pt;
+	struct sync_fence *fence;
+	struct sw_sync_create_fence_data data;
+
+	if (fd < 0)
+		return fd;
+
+	if (copy_from_user(&data, (void __user *)arg, sizeof(data))) {
+		err = -EFAULT;
+		goto err;
+	}
+
+	pt = sw_sync_pt_create(obj, data.value);
+	if (pt == NULL) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	data.name[sizeof(data.name) - 1] = '\0';
+	fence = sync_fence_create(data.name, pt);
+	if (fence == NULL) {
+		sync_pt_free(pt);
+		err = -ENOMEM;
+		goto err;
+	}
+
+	data.fence = fd;
+	if (copy_to_user((void __user *)arg, &data, sizeof(data))) {
+		sync_fence_put(fence);
+		err = -EFAULT;
+		goto err;
+	}
+
+	sync_fence_install(fence, fd);
+
+	return 0;
+
+err:
+	put_unused_fd(fd);
+	return err;
+}
+
+static long sw_sync_ioctl_inc(struct sw_sync_timeline *obj, unsigned long arg)
+{
+	u32 value;
+
+	if (copy_from_user(&value, (void __user *)arg, sizeof(value)))
+		return -EFAULT;
+
+	sw_sync_timeline_inc(obj, value);
+
+	return 0;
+}
+
+static long sw_sync_ioctl(struct file *file, unsigned int cmd,
+			  unsigned long arg)
+{
+	struct sw_sync_timeline *obj = file->private_data;
+
+	switch (cmd) {
+	case SW_SYNC_IOC_CREATE_FENCE:
+		return sw_sync_ioctl_create_fence(obj, arg);
+
+	case SW_SYNC_IOC_INC:
+		return sw_sync_ioctl_inc(obj, arg);
+
+	default:
+		return -ENOTTY;
+	}
+}
+
+static const struct file_operations sw_sync_fops = {
+	.owner = THIS_MODULE,
+	.open = sw_sync_open,
+	.release = sw_sync_release,
+	.unlocked_ioctl = sw_sync_ioctl,
+	.compat_ioctl = sw_sync_ioctl,
+};
+
+static struct miscdevice sw_sync_dev = {
+	.minor	= MISC_DYNAMIC_MINOR,
+	.name	= "sw_sync",
+	.fops	= &sw_sync_fops,
+};
+
+static int __init sw_sync_device_init(void)
+{
+	return misc_register(&sw_sync_dev);
+}
+
+static void __exit sw_sync_device_remove(void)
+{
+	misc_deregister(&sw_sync_dev);
+}
+
+module_init(sw_sync_device_init);
+module_exit(sw_sync_device_remove);
+
+#endif /* CONFIG_SW_SYNC_USER */
diff --git a/drivers/staging/android/sync.c b/drivers/staging/android/sync.c
new file mode 100644
index 000000000000..2df8a1337c4a
--- /dev/null
+++ b/drivers/staging/android/sync.c
@@ -0,0 +1,1028 @@
+/*
+ * drivers/base/sync.c
+ *
+ * Copyright (C) 2012 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/debugfs.h>
+#include <linux/export.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/poll.h>
+#include <linux/sched.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/anon_inodes.h>
+#include <linux/sync.h>
+
+#define CREATE_TRACE_POINTS
+#include "trace/sync.h"
+
+static void sync_fence_signal_pt(struct sync_pt *pt);
+static int _sync_pt_has_signaled(struct sync_pt *pt);
+static void sync_fence_free(struct kref *kref);
+static void sync_dump(void);
+
+static LIST_HEAD(sync_timeline_list_head);
+static DEFINE_SPINLOCK(sync_timeline_list_lock);
+
+static LIST_HEAD(sync_fence_list_head);
+static DEFINE_SPINLOCK(sync_fence_list_lock);
+
+struct sync_timeline *sync_timeline_create(const struct sync_timeline_ops *ops,
+					   int size, const char *name)
+{
+	struct sync_timeline *obj;
+	unsigned long flags;
+
+	if (size < sizeof(struct sync_timeline))
+		return NULL;
+
+	obj = kzalloc(size, GFP_KERNEL);
+	if (obj == NULL)
+		return NULL;
+
+	kref_init(&obj->kref);
+	obj->ops = ops;
+	strlcpy(obj->name, name, sizeof(obj->name));
+
+	INIT_LIST_HEAD(&obj->child_list_head);
+	spin_lock_init(&obj->child_list_lock);
+
+	INIT_LIST_HEAD(&obj->active_list_head);
+	spin_lock_init(&obj->active_list_lock);
+
+	spin_lock_irqsave(&sync_timeline_list_lock, flags);
+	list_add_tail(&obj->sync_timeline_list, &sync_timeline_list_head);
+	spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
+
+	return obj;
+}
+EXPORT_SYMBOL(sync_timeline_create);
+
+static void sync_timeline_free(struct kref *kref)
+{
+	struct sync_timeline *obj =
+		container_of(kref, struct sync_timeline, kref);
+	unsigned long flags;
+
+	spin_lock_irqsave(&sync_timeline_list_lock, flags);
+	list_del(&obj->sync_timeline_list);
+	spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
+
+	if (obj->ops->release_obj)
+		obj->ops->release_obj(obj);
+
+	kfree(obj);
+}
+
+void sync_timeline_destroy(struct sync_timeline *obj)
+{
+	obj->destroyed = true;
+	/*
+	 * Ensure timeline is marked as destroyed before
+	 * changing timeline's fences status.
+	 */
+	smp_wmb();
+
+	/*
+	 * signal any children that their parent is going away.
+	 */
+	sync_timeline_signal(obj);
+
+	kref_put(&obj->kref, sync_timeline_free);
+}
+EXPORT_SYMBOL(sync_timeline_destroy);
+
+static void sync_timeline_add_pt(struct sync_timeline *obj, struct sync_pt *pt)
+{
+	unsigned long flags;
+
+	pt->parent = obj;
+
+	spin_lock_irqsave(&obj->child_list_lock, flags);
+	list_add_tail(&pt->child_list, &obj->child_list_head);
+	spin_unlock_irqrestore(&obj->child_list_lock, flags);
+}
+
+static void sync_timeline_remove_pt(struct sync_pt *pt)
+{
+	struct sync_timeline *obj = pt->parent;
+	unsigned long flags;
+
+	spin_lock_irqsave(&obj->active_list_lock, flags);
+	if (!list_empty(&pt->active_list))
+		list_del_init(&pt->active_list);
+	spin_unlock_irqrestore(&obj->active_list_lock, flags);
+
+	spin_lock_irqsave(&obj->child_list_lock, flags);
+	if (!list_empty(&pt->child_list))
+		list_del_init(&pt->child_list);
+
+	spin_unlock_irqrestore(&obj->child_list_lock, flags);
+}
+
+void sync_timeline_signal(struct sync_timeline *obj)
+{
+	unsigned long flags;
+	LIST_HEAD(signaled_pts);
+	struct list_head *pos, *n;
+
+	trace_sync_timeline(obj);
+
+	spin_lock_irqsave(&obj->active_list_lock, flags);
+
+	list_for_each_safe(pos, n, &obj->active_list_head) {
+		struct sync_pt *pt =
+			container_of(pos, struct sync_pt, active_list);
+
+		if (_sync_pt_has_signaled(pt)) {
+			list_del_init(pos);
+			list_add(&pt->signaled_list, &signaled_pts);
+			kref_get(&pt->fence->kref);
+		}
+	}
+
+	spin_unlock_irqrestore(&obj->active_list_lock, flags);
+
+	list_for_each_safe(pos, n, &signaled_pts) {
+		struct sync_pt *pt =
+			container_of(pos, struct sync_pt, signaled_list);
+
+		list_del_init(pos);
+		sync_fence_signal_pt(pt);
+		kref_put(&pt->fence->kref, sync_fence_free);
+	}
+}
+EXPORT_SYMBOL(sync_timeline_signal);
+
+struct sync_pt *sync_pt_create(struct sync_timeline *parent, int size)
+{
+	struct sync_pt *pt;
+
+	if (size < sizeof(struct sync_pt))
+		return NULL;
+
+	pt = kzalloc(size, GFP_KERNEL);
+	if (pt == NULL)
+		return NULL;
+
+	INIT_LIST_HEAD(&pt->active_list);
+	kref_get(&parent->kref);
+	sync_timeline_add_pt(parent, pt);
+
+	return pt;
+}
+EXPORT_SYMBOL(sync_pt_create);
+
+void sync_pt_free(struct sync_pt *pt)
+{
+	if (pt->parent->ops->free_pt)
+		pt->parent->ops->free_pt(pt);
+
+	sync_timeline_remove_pt(pt);
+
+	kref_put(&pt->parent->kref, sync_timeline_free);
+
+	kfree(pt);
+}
+EXPORT_SYMBOL(sync_pt_free);
+
+/* call with pt->parent->active_list_lock held */
+static int _sync_pt_has_signaled(struct sync_pt *pt)
+{
+	int old_status = pt->status;
+
+	if (!pt->status)
+		pt->status = pt->parent->ops->has_signaled(pt);
+
+	if (!pt->status && pt->parent->destroyed)
+		pt->status = -ENOENT;
+
+	if (pt->status != old_status)
+		pt->timestamp = ktime_get();
+
+	return pt->status;
+}
+
+static struct sync_pt *sync_pt_dup(struct sync_pt *pt)
+{
+	return pt->parent->ops->dup(pt);
+}
+
+/* Adds a sync pt to the active queue.  Called when added to a fence */
+static void sync_pt_activate(struct sync_pt *pt)
+{
+	struct sync_timeline *obj = pt->parent;
+	unsigned long flags;
+	int err;
+
+	spin_lock_irqsave(&obj->active_list_lock, flags);
+
+	err = _sync_pt_has_signaled(pt);
+	if (err != 0)
+		goto out;
+
+	list_add_tail(&pt->active_list, &obj->active_list_head);
+
+out:
+	spin_unlock_irqrestore(&obj->active_list_lock, flags);
+}
+
+static int sync_fence_release(struct inode *inode, struct file *file);
+static unsigned int sync_fence_poll(struct file *file, poll_table *wait);
+static long sync_fence_ioctl(struct file *file, unsigned int cmd,
+			     unsigned long arg);
+
+
+static const struct file_operations sync_fence_fops = {
+	.release = sync_fence_release,
+	.poll = sync_fence_poll,
+	.unlocked_ioctl = sync_fence_ioctl,
+	.compat_ioctl = sync_fence_ioctl,
+};
+
+static struct sync_fence *sync_fence_alloc(const char *name)
+{
+	struct sync_fence *fence;
+	unsigned long flags;
+
+	fence = kzalloc(sizeof(struct sync_fence), GFP_KERNEL);
+	if (fence == NULL)
+		return NULL;
+
+	fence->file = anon_inode_getfile("sync_fence", &sync_fence_fops,
+					 fence, 0);
+	if (IS_ERR(fence->file))
+		goto err;
+
+	kref_init(&fence->kref);
+	strlcpy(fence->name, name, sizeof(fence->name));
+
+	INIT_LIST_HEAD(&fence->pt_list_head);
+	INIT_LIST_HEAD(&fence->waiter_list_head);
+	spin_lock_init(&fence->waiter_list_lock);
+
+	init_waitqueue_head(&fence->wq);
+
+	spin_lock_irqsave(&sync_fence_list_lock, flags);
+	list_add_tail(&fence->sync_fence_list, &sync_fence_list_head);
+	spin_unlock_irqrestore(&sync_fence_list_lock, flags);
+
+	return fence;
+
+err:
+	kfree(fence);
+	return NULL;
+}
+
+/* TODO: implement a create which takes more that one sync_pt */
+struct sync_fence *sync_fence_create(const char *name, struct sync_pt *pt)
+{
+	struct sync_fence *fence;
+
+	if (pt->fence)
+		return NULL;
+
+	fence = sync_fence_alloc(name);
+	if (fence == NULL)
+		return NULL;
+
+	pt->fence = fence;
+	list_add(&pt->pt_list, &fence->pt_list_head);
+	sync_pt_activate(pt);
+
+	/*
+	 * signal the fence in case pt was activated before
+	 * sync_pt_activate(pt) was called
+	 */
+	sync_fence_signal_pt(pt);
+
+	return fence;
+}
+EXPORT_SYMBOL(sync_fence_create);
+
+static int sync_fence_copy_pts(struct sync_fence *dst, struct sync_fence *src)
+{
+	struct list_head *pos;
+
+	list_for_each(pos, &src->pt_list_head) {
+		struct sync_pt *orig_pt =
+			container_of(pos, struct sync_pt, pt_list);
+		struct sync_pt *new_pt = sync_pt_dup(orig_pt);
+
+		if (new_pt == NULL)
+			return -ENOMEM;
+
+		new_pt->fence = dst;
+		list_add(&new_pt->pt_list, &dst->pt_list_head);
+	}
+
+	return 0;
+}
+
+static int sync_fence_merge_pts(struct sync_fence *dst, struct sync_fence *src)
+{
+	struct list_head *src_pos, *dst_pos, *n;
+
+	list_for_each(src_pos, &src->pt_list_head) {
+		struct sync_pt *src_pt =
+			container_of(src_pos, struct sync_pt, pt_list);
+		bool collapsed = false;
+
+		list_for_each_safe(dst_pos, n, &dst->pt_list_head) {
+			struct sync_pt *dst_pt =
+				container_of(dst_pos, struct sync_pt, pt_list);
+			/* collapse two sync_pts on the same timeline
+			 * to a single sync_pt that will signal at
+			 * the later of the two
+			 */
+			if (dst_pt->parent == src_pt->parent) {
+				if (dst_pt->parent->ops->compare(dst_pt, src_pt)
+						 == -1) {
+					struct sync_pt *new_pt =
+						sync_pt_dup(src_pt);
+					if (new_pt == NULL)
+						return -ENOMEM;
+
+					new_pt->fence = dst;
+					list_replace(&dst_pt->pt_list,
+						     &new_pt->pt_list);
+					sync_pt_free(dst_pt);
+				}
+				collapsed = true;
+				break;
+			}
+		}
+
+		if (!collapsed) {
+			struct sync_pt *new_pt = sync_pt_dup(src_pt);
+
+			if (new_pt == NULL)
+				return -ENOMEM;
+
+			new_pt->fence = dst;
+			list_add(&new_pt->pt_list, &dst->pt_list_head);
+		}
+	}
+
+	return 0;
+}
+
+static void sync_fence_detach_pts(struct sync_fence *fence)
+{
+	struct list_head *pos, *n;
+
+	list_for_each_safe(pos, n, &fence->pt_list_head) {
+		struct sync_pt *pt = container_of(pos, struct sync_pt, pt_list);
+
+		sync_timeline_remove_pt(pt);
+	}
+}
+
+static void sync_fence_free_pts(struct sync_fence *fence)
+{
+	struct list_head *pos, *n;
+
+	list_for_each_safe(pos, n, &fence->pt_list_head) {
+		struct sync_pt *pt = container_of(pos, struct sync_pt, pt_list);
+
+		sync_pt_free(pt);
+	}
+}
+
+struct sync_fence *sync_fence_fdget(int fd)
+{
+	struct file *file = fget(fd);
+
+	if (file == NULL)
+		return NULL;
+
+	if (file->f_op != &sync_fence_fops)
+		goto err;
+
+	return file->private_data;
+
+err:
+	fput(file);
+	return NULL;
+}
+EXPORT_SYMBOL(sync_fence_fdget);
+
+void sync_fence_put(struct sync_fence *fence)
+{
+	fput(fence->file);
+}
+EXPORT_SYMBOL(sync_fence_put);
+
+void sync_fence_install(struct sync_fence *fence, int fd)
+{
+	fd_install(fd, fence->file);
+}
+EXPORT_SYMBOL(sync_fence_install);
+
+static int sync_fence_get_status(struct sync_fence *fence)
+{
+	struct list_head *pos;
+	int status = 1;
+
+	list_for_each(pos, &fence->pt_list_head) {
+		struct sync_pt *pt = container_of(pos, struct sync_pt, pt_list);
+		int pt_status = pt->status;
+
+		if (pt_status < 0) {
+			status = pt_status;
+			break;
+		} else if (status == 1) {
+			status = pt_status;
+		}
+	}
+
+	return status;
+}
+
+struct sync_fence *sync_fence_merge(const char *name,
+				    struct sync_fence *a, struct sync_fence *b)
+{
+	struct sync_fence *fence;
+	struct list_head *pos;
+	int err;
+
+	fence = sync_fence_alloc(name);
+	if (fence == NULL)
+		return NULL;
+
+	err = sync_fence_copy_pts(fence, a);
+	if (err < 0)
+		goto err;
+
+	err = sync_fence_merge_pts(fence, b);
+	if (err < 0)
+		goto err;
+
+	list_for_each(pos, &fence->pt_list_head) {
+		struct sync_pt *pt =
+			container_of(pos, struct sync_pt, pt_list);
+		sync_pt_activate(pt);
+	}
+
+	/*
+	 * signal the fence in case one of it's pts were activated before
+	 * they were activated
+	 */
+	sync_fence_signal_pt(list_first_entry(&fence->pt_list_head,
+					      struct sync_pt,
+					      pt_list));
+
+	return fence;
+err:
+	sync_fence_free_pts(fence);
+	kfree(fence);
+	return NULL;
+}
+EXPORT_SYMBOL(sync_fence_merge);
+
+static void sync_fence_signal_pt(struct sync_pt *pt)
+{
+	LIST_HEAD(signaled_waiters);
+	struct sync_fence *fence = pt->fence;
+	struct list_head *pos;
+	struct list_head *n;
+	unsigned long flags;
+	int status;
+
+	status = sync_fence_get_status(fence);
+
+	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	/*
+	 * this should protect against two threads racing on the signaled
+	 * false -> true transition
+	 */
+	if (status && !fence->status) {
+		list_for_each_safe(pos, n, &fence->waiter_list_head)
+			list_move(pos, &signaled_waiters);
+
+		fence->status = status;
+	} else {
+		status = 0;
+	}
+	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+
+	if (status) {
+		list_for_each_safe(pos, n, &signaled_waiters) {
+			struct sync_fence_waiter *waiter =
+				container_of(pos, struct sync_fence_waiter,
+					     waiter_list);
+
+			list_del(pos);
+			waiter->callback(fence, waiter);
+		}
+		wake_up(&fence->wq);
+	}
+}
+
+int sync_fence_wait_async(struct sync_fence *fence,
+			  struct sync_fence_waiter *waiter)
+{
+	unsigned long flags;
+	int err = 0;
+
+	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+
+	if (fence->status) {
+		err = fence->status;
+		goto out;
+	}
+
+	list_add_tail(&waiter->waiter_list, &fence->waiter_list_head);
+out:
+	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+
+	return err;
+}
+EXPORT_SYMBOL(sync_fence_wait_async);
+
+int sync_fence_cancel_async(struct sync_fence *fence,
+			     struct sync_fence_waiter *waiter)
+{
+	struct list_head *pos;
+	struct list_head *n;
+	unsigned long flags;
+	int ret = -ENOENT;
+
+	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	/*
+	 * Make sure waiter is still in waiter_list because it is possible for
+	 * the waiter to be removed from the list while the callback is still
+	 * pending.
+	 */
+	list_for_each_safe(pos, n, &fence->waiter_list_head) {
+		struct sync_fence_waiter *list_waiter =
+			container_of(pos, struct sync_fence_waiter,
+				     waiter_list);
+		if (list_waiter == waiter) {
+			list_del(pos);
+			ret = 0;
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+	return ret;
+}
+EXPORT_SYMBOL(sync_fence_cancel_async);
+
+static bool sync_fence_check(struct sync_fence *fence)
+{
+	/*
+	 * Make sure that reads to fence->status are ordered with the
+	 * wait queue event triggering
+	 */
+	smp_rmb();
+	return fence->status != 0;
+}
+
+int sync_fence_wait(struct sync_fence *fence, long timeout)
+{
+	int err = 0;
+	struct sync_pt *pt;
+
+	trace_sync_wait(fence, 1);
+	list_for_each_entry(pt, &fence->pt_list_head, pt_list)
+		trace_sync_pt(pt);
+
+	if (timeout > 0) {
+		timeout = msecs_to_jiffies(timeout);
+		err = wait_event_interruptible_timeout(fence->wq,
+						       sync_fence_check(fence),
+						       timeout);
+	} else if (timeout < 0) {
+		err = wait_event_interruptible(fence->wq,
+					       sync_fence_check(fence));
+	}
+	trace_sync_wait(fence, 0);
+
+	if (err < 0)
+		return err;
+
+	if (fence->status < 0) {
+		pr_info("fence error %d on [%pK]\n", fence->status, fence);
+		sync_dump();
+		return fence->status;
+	}
+
+	if (fence->status == 0) {
+		if (timeout > 0) {
+			pr_info("fence timeout on [%pK] after %dms\n", fence,
+				jiffies_to_msecs(timeout));
+			sync_dump();
+		}
+		return -ETIME;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(sync_fence_wait);
+
+static void sync_fence_free(struct kref *kref)
+{
+	struct sync_fence *fence = container_of(kref, struct sync_fence, kref);
+
+	sync_fence_free_pts(fence);
+
+	kfree(fence);
+}
+
+static int sync_fence_release(struct inode *inode, struct file *file)
+{
+	struct sync_fence *fence = file->private_data;
+	unsigned long flags;
+
+	/*
+	 * We need to remove all ways to access this fence before droping
+	 * our ref.
+	 *
+	 * start with its membership in the global fence list
+	 */
+	spin_lock_irqsave(&sync_fence_list_lock, flags);
+	list_del(&fence->sync_fence_list);
+	spin_unlock_irqrestore(&sync_fence_list_lock, flags);
+
+	/*
+	 * remove its pts from their parents so that sync_timeline_signal()
+	 * can't reference the fence.
+	 */
+	sync_fence_detach_pts(fence);
+
+	kref_put(&fence->kref, sync_fence_free);
+
+	return 0;
+}
+
+static unsigned int sync_fence_poll(struct file *file, poll_table *wait)
+{
+	struct sync_fence *fence = file->private_data;
+
+	poll_wait(file, &fence->wq, wait);
+
+	/*
+	 * Make sure that reads to fence->status are ordered with the
+	 * wait queue event triggering
+	 */
+	smp_rmb();
+
+	if (fence->status == 1)
+		return POLLIN;
+	else if (fence->status < 0)
+		return POLLERR;
+	else
+		return 0;
+}
+
+static long sync_fence_ioctl_wait(struct sync_fence *fence, unsigned long arg)
+{
+	__s32 value;
+
+	if (copy_from_user(&value, (void __user *)arg, sizeof(value)))
+		return -EFAULT;
+
+	return sync_fence_wait(fence, value);
+}
+
+static long sync_fence_ioctl_merge(struct sync_fence *fence, unsigned long arg)
+{
+	int fd = get_unused_fd_flags(O_CLOEXEC);
+	int err;
+	struct sync_fence *fence2, *fence3;
+	struct sync_merge_data data;
+
+	if (fd < 0)
+		return fd;
+
+	if (copy_from_user(&data, (void __user *)arg, sizeof(data))) {
+		err = -EFAULT;
+		goto err_put_fd;
+	}
+
+	fence2 = sync_fence_fdget(data.fd2);
+	if (fence2 == NULL) {
+		err = -ENOENT;
+		goto err_put_fd;
+	}
+
+	data.name[sizeof(data.name) - 1] = '\0';
+	fence3 = sync_fence_merge(data.name, fence, fence2);
+	if (fence3 == NULL) {
+		err = -ENOMEM;
+		goto err_put_fence2;
+	}
+
+	data.fence = fd;
+	if (copy_to_user((void __user *)arg, &data, sizeof(data))) {
+		err = -EFAULT;
+		goto err_put_fence3;
+	}
+
+	sync_fence_install(fence3, fd);
+	sync_fence_put(fence2);
+	return 0;
+
+err_put_fence3:
+	sync_fence_put(fence3);
+
+err_put_fence2:
+	sync_fence_put(fence2);
+
+err_put_fd:
+	put_unused_fd(fd);
+	return err;
+}
+
+static int sync_fill_pt_info(struct sync_pt *pt, void *data, int size)
+{
+	struct sync_pt_info *info = data;
+	int ret;
+
+	if (size < sizeof(struct sync_pt_info))
+		return -ENOMEM;
+
+	info->len = sizeof(struct sync_pt_info);
+
+	if (pt->parent->ops->fill_driver_data) {
+		ret = pt->parent->ops->fill_driver_data(pt, info->driver_data,
+							size - sizeof(*info));
+		if (ret < 0)
+			return ret;
+
+		info->len += ret;
+	}
+
+	strlcpy(info->obj_name, pt->parent->name, sizeof(info->obj_name));
+	strlcpy(info->driver_name, pt->parent->ops->driver_name,
+		sizeof(info->driver_name));
+	info->status = pt->status;
+	info->timestamp_ns = ktime_to_ns(pt->timestamp);
+
+	return info->len;
+}
+
+static long sync_fence_ioctl_fence_info(struct sync_fence *fence,
+					unsigned long arg)
+{
+	struct sync_fence_info_data *data;
+	struct list_head *pos;
+	__u32 size;
+	__u32 len = 0;
+	int ret;
+
+	if (copy_from_user(&size, (void __user *)arg, sizeof(size)))
+		return -EFAULT;
+
+	if (size < sizeof(struct sync_fence_info_data))
+		return -EINVAL;
+
+	if (size > 4096)
+		size = 4096;
+
+	data = kzalloc(size, GFP_KERNEL);
+	if (data == NULL)
+		return -ENOMEM;
+
+	strlcpy(data->name, fence->name, sizeof(data->name));
+	data->status = fence->status;
+	len = sizeof(struct sync_fence_info_data);
+
+	list_for_each(pos, &fence->pt_list_head) {
+		struct sync_pt *pt =
+			container_of(pos, struct sync_pt, pt_list);
+
+		ret = sync_fill_pt_info(pt, (u8 *)data + len, size - len);
+
+		if (ret < 0)
+			goto out;
+
+		len += ret;
+	}
+
+	data->len = len;
+
+	if (copy_to_user((void __user *)arg, data, len))
+		ret = -EFAULT;
+	else
+		ret = 0;
+
+out:
+	kfree(data);
+
+	return ret;
+}
+
+static long sync_fence_ioctl(struct file *file, unsigned int cmd,
+			     unsigned long arg)
+{
+	struct sync_fence *fence = file->private_data;
+
+	switch (cmd) {
+	case SYNC_IOC_WAIT:
+		return sync_fence_ioctl_wait(fence, arg);
+
+	case SYNC_IOC_MERGE:
+		return sync_fence_ioctl_merge(fence, arg);
+
+	case SYNC_IOC_FENCE_INFO:
+		return sync_fence_ioctl_fence_info(fence, arg);
+
+	default:
+		return -ENOTTY;
+	}
+}
+
+#ifdef CONFIG_DEBUG_FS
+static const char *sync_status_str(int status)
+{
+	if (status > 0)
+		return "signaled";
+	else if (status == 0)
+		return "active";
+	else
+		return "error";
+}
+
+static void sync_print_pt(struct seq_file *s, struct sync_pt *pt, bool fence)
+{
+	int status = pt->status;
+
+	seq_printf(s, "  %s%spt %s",
+		   fence ? pt->parent->name : "",
+		   fence ? "_" : "",
+		   sync_status_str(status));
+	if (pt->status) {
+		struct timeval tv = ktime_to_timeval(pt->timestamp);
+
+		seq_printf(s, "@%ld.%06ld", tv.tv_sec, tv.tv_usec);
+	}
+
+	if (pt->parent->ops->timeline_value_str &&
+	    pt->parent->ops->pt_value_str) {
+		char value[64];
+
+		pt->parent->ops->pt_value_str(pt, value, sizeof(value));
+		seq_printf(s, ": %s", value);
+		if (fence) {
+			pt->parent->ops->timeline_value_str(pt->parent, value,
+						    sizeof(value));
+			seq_printf(s, " / %s", value);
+		}
+	} else if (pt->parent->ops->print_pt) {
+		seq_puts(s, ": ");
+		pt->parent->ops->print_pt(s, pt);
+	}
+
+	seq_puts(s, "\n");
+}
+
+static void sync_print_obj(struct seq_file *s, struct sync_timeline *obj)
+{
+	struct list_head *pos;
+	unsigned long flags;
+
+	seq_printf(s, "%s %s", obj->name, obj->ops->driver_name);
+
+	if (obj->ops->timeline_value_str) {
+		char value[64];
+
+		obj->ops->timeline_value_str(obj, value, sizeof(value));
+		seq_printf(s, ": %s", value);
+	} else if (obj->ops->print_obj) {
+		seq_puts(s, ": ");
+		obj->ops->print_obj(s, obj);
+	}
+
+	seq_puts(s, "\n");
+
+	spin_lock_irqsave(&obj->child_list_lock, flags);
+	list_for_each(pos, &obj->child_list_head) {
+		struct sync_pt *pt =
+			container_of(pos, struct sync_pt, child_list);
+		sync_print_pt(s, pt, false);
+	}
+	spin_unlock_irqrestore(&obj->child_list_lock, flags);
+}
+
+static void sync_print_fence(struct seq_file *s, struct sync_fence *fence)
+{
+	struct list_head *pos;
+	unsigned long flags;
+
+	seq_printf(s, "[%pK] %s: %s\n", fence, fence->name,
+		   sync_status_str(fence->status));
+
+	list_for_each(pos, &fence->pt_list_head) {
+		struct sync_pt *pt =
+			container_of(pos, struct sync_pt, pt_list);
+		sync_print_pt(s, pt, true);
+	}
+
+	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	list_for_each(pos, &fence->waiter_list_head) {
+		struct sync_fence_waiter *waiter =
+			container_of(pos, struct sync_fence_waiter,
+				     waiter_list);
+
+		seq_printf(s, "waiter %pF\n", waiter->callback);
+	}
+	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+}
+
+static int sync_debugfs_show(struct seq_file *s, void *unused)
+{
+	unsigned long flags;
+	struct list_head *pos;
+
+	seq_puts(s, "objs:\n--------------\n");
+
+	spin_lock_irqsave(&sync_timeline_list_lock, flags);
+	list_for_each(pos, &sync_timeline_list_head) {
+		struct sync_timeline *obj =
+			container_of(pos, struct sync_timeline,
+				     sync_timeline_list);
+
+		sync_print_obj(s, obj);
+		seq_puts(s, "\n");
+	}
+	spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
+
+	seq_puts(s, "fences:\n--------------\n");
+
+	spin_lock_irqsave(&sync_fence_list_lock, flags);
+	list_for_each(pos, &sync_fence_list_head) {
+		struct sync_fence *fence =
+			container_of(pos, struct sync_fence, sync_fence_list);
+
+		sync_print_fence(s, fence);
+		seq_puts(s, "\n");
+	}
+	spin_unlock_irqrestore(&sync_fence_list_lock, flags);
+	return 0;
+}
+
+static int sync_debugfs_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sync_debugfs_show, inode->i_private);
+}
+
+static const struct file_operations sync_debugfs_fops = {
+	.open           = sync_debugfs_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = single_release,
+};
+
+static __init int sync_debugfs_init(void)
+{
+	debugfs_create_file("sync", S_IRUGO, NULL, NULL, &sync_debugfs_fops);
+	return 0;
+}
+late_initcall(sync_debugfs_init);
+
+#define DUMP_CHUNK 256
+static char sync_dump_buf[64 * 1024];
+static void sync_dump(void)
+{
+	struct seq_file s = {
+		.buf = sync_dump_buf,
+		.size = sizeof(sync_dump_buf) - 1,
+	};
+	int i;
+
+	sync_debugfs_show(&s, NULL);
+
+	for (i = 0; i < s.count; i += DUMP_CHUNK) {
+		if ((s.count - i) > DUMP_CHUNK) {
+			char c = s.buf[i + DUMP_CHUNK];
+
+			s.buf[i + DUMP_CHUNK] = 0;
+			pr_cont("%s", s.buf + i);
+			s.buf[i + DUMP_CHUNK] = c;
+		} else {
+			s.buf[s.count] = 0;
+			pr_cont("%s", s.buf + i);
+		}
+	}
+}
+#else
+static void sync_dump(void)
+{
+}
+#endif
diff --git a/drivers/staging/android/trace/sync.h b/drivers/staging/android/trace/sync.h
new file mode 100644
index 000000000000..63309f207dc1
--- /dev/null
+++ b/drivers/staging/android/trace/sync.h
@@ -0,0 +1,82 @@
+#undef TRACE_SYSTEM
+#define TRACE_INCLUDE_PATH ../../drivers/staging/android/trace
+#define TRACE_SYSTEM sync
+
+#if !defined(_TRACE_SYNC_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_SYNC_H
+
+#include <linux/sync.h>
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(sync_timeline,
+	TP_PROTO(struct sync_timeline *timeline),
+
+	TP_ARGS(timeline),
+
+	TP_STRUCT__entry(
+			__string(name, timeline->name)
+			__array(char, value, 32)
+	),
+
+	TP_fast_assign(
+			__assign_str(name, timeline->name);
+			if (timeline->ops->timeline_value_str) {
+				timeline->ops->timeline_value_str(timeline,
+							__entry->value,
+							sizeof(__entry->value));
+			} else {
+				__entry->value[0] = '\0';
+			}
+	),
+
+	TP_printk("name=%s value=%s", __get_str(name), __entry->value)
+);
+
+TRACE_EVENT(sync_wait,
+	TP_PROTO(struct sync_fence *fence, int begin),
+
+	TP_ARGS(fence, begin),
+
+	TP_STRUCT__entry(
+			__string(name, fence->name)
+			__field(s32, status)
+			__field(u32, begin)
+	),
+
+	TP_fast_assign(
+			__assign_str(name, fence->name);
+			__entry->status = fence->status;
+			__entry->begin = begin;
+	),
+
+	TP_printk("%s name=%s state=%d", __entry->begin ? "begin" : "end",
+			__get_str(name), __entry->status)
+);
+
+TRACE_EVENT(sync_pt,
+	TP_PROTO(struct sync_pt *pt),
+
+	TP_ARGS(pt),
+
+	TP_STRUCT__entry(
+		__string(timeline, pt->parent->name)
+		__array(char, value, 32)
+	),
+
+	TP_fast_assign(
+		__assign_str(timeline, pt->parent->name);
+		if (pt->parent->ops->pt_value_str) {
+			pt->parent->ops->pt_value_str(pt, __entry->value,
+							sizeof(__entry->value));
+		} else {
+			__entry->value[0] = '\0';
+		}
+	),
+
+	TP_printk("name=%s value=%s", __get_str(timeline), __entry->value)
+);
+
+#endif /* if !defined(_TRACE_SYNC_H) || defined(TRACE_HEADER_MULTI_READ) */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/linux/oneshot_sync.h b/include/linux/oneshot_sync.h
new file mode 100644
index 000000000000..1fc7e6f6e57c
--- /dev/null
+++ b/include/linux/oneshot_sync.h
@@ -0,0 +1,58 @@
+/* Copyright (c) 2014, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+#ifndef LINUX_ONESHOT_SYNC_H
+#define LINUX_ONESHOT_SYNC_H
+
+#include <uapi/linux/oneshot_sync.h>
+
+struct oneshot_sync_timeline;
+struct sync_fence;
+
+#ifdef CONFIG_ONESHOT_SYNC
+
+struct oneshot_sync_timeline *oneshot_timeline_create(const char *name);
+
+void oneshot_timeline_destroy(struct oneshot_sync_timeline *);
+
+struct sync_fence *oneshot_fence_create(struct oneshot_sync_timeline *,
+					const char *name);
+
+int oneshot_fence_signal(struct oneshot_sync_timeline *, struct sync_fence *);
+
+#else
+
+static inline struct oneshot_sync_timeline *
+oneshot_timeline_create(const char *name)
+{
+	return NULL;
+}
+
+void oneshot_timeline_destroy(struct oneshot_sync_timeline *timeline)
+{
+}
+
+struct sync_fence *oneshot_fence_create(struct oneshot_sync_timeline *timeline,
+					const char *name)
+{
+	return NULL;
+}
+
+int oneshot_fence_signal(struct oneshot_sync_timeline *timeline,
+			struct sync_fence *fence)
+{
+	return -EINVAL;
+}
+
+#endif
+
+#endif /* LINUX_ONESHOT_SYNC_H */
diff --git a/include/uapi/linux/oneshot_sync.h b/include/uapi/linux/oneshot_sync.h
new file mode 100644
index 000000000000..c55ef2f5a349
--- /dev/null
+++ b/include/uapi/linux/oneshot_sync.h
@@ -0,0 +1,49 @@
+#ifndef ONESHOT_SYNC_H
+#define ONESHOT_SYNC_H
+
+/**
+ * DOC: Oneshot sync Userspace API
+ *
+ * Opening a file descriptor from /dev/oneshot_sync creates a * sync timeline
+ * for userspace signaled fences. Userspace may create new fences from a
+ * /dev/oneshot_sync file descriptor and then signal them by passing the fence
+ * file descriptor in an ioctl() call on the fd used to create the fence.
+ * Unlike most sync timelines, there is no ordering on a oneshot timeline.
+ * Each fence may be signaled in any order without affecting the state of other
+ * fences on the timeline.
+ */
+
+#define ONESHOT_SYNC_IOC_MAGIC '1'
+
+/**
+ * struct oneshot_sync_create_fence - argument to create fence ioctl
+ * @name: name of the new fence, to aid debugging.
+ * @fence_fd: returned sync_fence file descriptor
+ */
+struct oneshot_sync_create_fence {
+	char name[32];
+	int fence_fd;
+};
+
+/**
+ * DOC: ONESHOT_SYNC_IOC_CREATE_FENCE - create a userspace signaled fence
+ *
+ * Create a fence that may be signaled by userspace by calling
+ * ONESHOT_SYNC_IOC_SIGNAL_FENCE. There are no order dependencies between
+ * these fences, but otherwise they behave like normal sync fences.
+ * Argument is struct oneshot_sync_create_fence.
+ */
+#define ONESHOT_SYNC_IOC_CREATE_FENCE _IOWR(ONESHOT_SYNC_IOC_MAGIC, 1,\
+		struct oneshot_sync_create_fence)
+
+/**
+ * DOC: ONESHOT_SYNC_IOC_SIGNAL_FENCE - signal a fence
+ *
+ * Signal a fence that was created by a ONESHOT_SYNC_IOC_CREATE_FENCE
+ * call on the same file descriptor. This allows a fence to be shared
+ * to other processes but only signaled by the process owning the fd
+ * used to create the fence.  Argument is the fence file descriptor.
+ */
+#define ONESHOT_SYNC_IOC_SIGNAL_FENCE _IOWR(ONESHOT_SYNC_IOC_MAGIC, 2,\
+		int)
+#endif
-- 
2.35.1

